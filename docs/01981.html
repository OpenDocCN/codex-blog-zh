<html>
<head>
<title>What are three approaches for variable selection and when to use which</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变量选择的三种方法是什么，何时使用哪一种</h1>
<blockquote>原文：<a href="https://medium.com/codex/what-are-three-approaches-for-variable-selection-and-when-to-use-which-54de12f32464?source=collection_archive---------0-----------------------#2021-06-20">https://medium.com/codex/what-are-three-approaches-for-variable-selection-and-when-to-use-which-54de12f32464?source=collection_archive---------0-----------------------#2021-06-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/634e8353b36ab2d067f9890d8267a59d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snmumCXbmzUI9aIOQCZs-Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片:<a class="ae iu" href="https://unsplash.com/photos/GLg0z5z8EQU" rel="noopener ugc nofollow" target="_blank">阿尔特姆·贝利艾金</a></figcaption></figure><p id="0722" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当您处理真实世界的数据时，不可避免地会遇到包含多个变量的数据集。在这些变量中，您将如何选择将哪些变量包含在您的模型中？幸运的是，你有几个选择可以考虑。</p><h1 id="3785" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">子集选择</strong></h1><p id="afe2" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">第一个选项是子集选择，它使用预测因子的子集来进行预测。我们将研究三种类型的子集选择:最佳子集选择、向前逐步选择和向后逐步选择。</p><h2 id="46b5" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">最佳子集选择</h2><p id="34a1" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">顾名思义，最佳子集选择为每个子集大小找到最佳模型。换句话说，当有p个预测值时，它为1个变量模型、2个变量模型和多达p个变量模型产生最佳模型。例如，当有a、b和c变量时，最佳子集选择将对1变量模型评估a、b、c，对2变量模型评估ab、ac、bc，对3变量模型评估abc，以确定最佳模型。其他方法是向前和向后选择。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/1a54c7535abe0e91540f41ee419c02af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GzkIHyn6h0vkjo96a7E9SA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">最佳子集选择如何评估模型</figcaption></figure><h2 id="55b4" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">向前和向后选择</h2><p id="d982" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">正向逐步选择从一个空模型开始，并添加一个最能改进模型的变量。因此，对于一个1变量模型，它尝试将a、b或c添加到一个空模型中，并添加一个给出最佳结果的模型。比如说我们选择了a。然后对于一个2变量模型，它将尝试添加b或c，并选择一个改进最多的。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/291223c9bc3ca8fa173a04bcc4322dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YjD-RRim3R2zzJNkwAFVUw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">当在一元模型中选择a时，前向逐步选择如何评估模型</figcaption></figure><p id="389f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与向前逐步选择不同，向后逐步选择从所有变量开始，并删除提供最少改进的变量。</p><h2 id="e68d" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">选择最佳模型</h2><p id="8e9a" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">所以这些方法给了我们每个子集大小的模型。但是在这些当中，我们如何选择一个呢？这里可以用Mallows的Cp，调整后的R，贝叶斯信息准则<em class="lq"> (BIC) </em>。</p><p id="8869" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Cp比较不同模型之间的精度和偏差。因此，较小的Cp意味着模型在估计系数和预测响应方面相对精确，这对应于较低的测试误差。BIC告诉我们，相对于其他模型，该模型对新数据的预测有多好，方法是给具有低测试误差的模型较低的BIC值。因此，对于这两个指标，我们希望获得相对较低的值。</p><p id="bed4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一方面，调整后的R告诉我们一个模型对反应变量的解释有多好。与Cp和BIC不同，高调整R对应于低测试误差。</p><p id="4219" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，这些只是衡量标准。选择变量最重要的一点是用领域知识研究实际模型，以评估模型是否合理。</p><h1 id="2685" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">收缩</strong></h1><p id="ab52" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">你可以考虑的第二个选择是收缩。与以前的方法不同，收缩方法包含所有预测值，并将系数估计值向零收缩，因此它们对响应的影响较小。使用收缩的方法是岭和套索回归。</p><h2 id="260b" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">里脊回归</h2><p id="0dbb" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">类似于最小二乘回归通过最小化RSS来估计系数，岭回归通过最小化</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/743f42e7159f96055c25858c1b2291a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*qWpDNZd-zroBigwdBD-zAQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">岭回归的系数估计方程</figcaption></figure><p id="23c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据等式，λ称为调谐参数，λ∑βⱼ称为惩罚项。当λ等于零时，惩罚项将不起作用。因此，该方程将成为最小二乘估计。当λ趋近于无穷大时，岭回归系数估计趋近于零，模型成为零模型。如您所见，系数估计值因λ值而异。所以，选一个好的很关键。</p><p id="d68e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">需要注意的一点是岭回归不是尺度等变的。例如，1000厘米对模型的影响比1美元大得多，因为1000比1大得多。因此，在执行回归之前标准化预测值至关重要。</p><h2 id="bc02" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">套索回归</h2><p id="adf2" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">与岭回归相比，它最小化了</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/81d98ec0e26eb111d5903e1b4d8bc98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*SI44wHWWJUZwl8GkCEQD9A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">lasso回归的系数估计方程</figcaption></figure><p id="29a3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用于系数估计。除了lasso与岭回归有不同的惩罚项，lasso回归与岭回归非常相似。与岭回归一样，标准化预测值和选择好的λ至关重要。当λ = 0时，它也给出最小二乘拟合，但当λ接近无穷大时，它给出零模型。那么拉索和里奇在判罚条款上的区别是什么呢？</p><p id="6758" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为脊回归的罚项是平方的，而套索回归的罚项是绝对值的，所以由罚项定义的约束区域的边对于脊是圆的，而对于套索是直的。为了更容易理解这个想法，请看下面的图表。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/ce79a7090bba283404a34e5bed9b1159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jLCid725DDBAffb8Rxg8Vw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">摘自<a class="ae iu" href="https://www.statlearning.com/" rel="noopener ugc nofollow" target="_blank">统计学习简介第222页的图:R中的应用</a> /蓝色区域是约束区域，红色椭圆是RSS值的轮廓。(左)套索回归/(右)岭回归</figcaption></figure><p id="ad91" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的图中，红色椭圆是RSS值的轮廓，蓝色区域是套索和山脊的约束区域。正如我上面提到的，岭回归的边缘是圆的，因为惩罚项就像一个循环函数:<em class="lq"> x + y = r </em>。因此，约束区域不能满足系数估计值等于零的轴上的RSS。</p><p id="0d22" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">相比之下，由于惩罚项中的绝对值形成直线——想象一下<em class="lq"> |x|+|y|=1 </em>，lasso的约束区域具有直边。因此，约束区域可以满足轴上的RSS，并将系数估计值变为0。为了简单起见，上面的图是针对p=2的，但是当p &gt; 2时，同样的概念也适用。</p><h2 id="6c71" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">选择最佳模型</h2><p id="1e51" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">正如我上面提到的，选择一个好的λ值与选择最佳模型直接相关。然而，我们不知道λ的合适值是多少。因此，我们计算λ值网格的交叉验证误差，并选择最低的一个。</p><h1 id="6a94" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">降维方法</strong></h1><p id="85c6" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">最后一种方法涉及转换独立变量，然后使用这些转换后的变量拟合模型。主成分和偏最小二乘法属于这一类。</p><h2 id="8a25" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">主成分分析</h2><p id="1be9" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">这种方法的基本思想是使用一大组变量中的一组低维特征来解释数据。为此，它使用具有以下功能的主分量方向</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/38e8fa1921232d7b3fa52abc20d0d230.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*B46mkRDsvjdZBQSUrb6WoA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">主成分方程</figcaption></figure><p id="ac26" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在函数内部，<em class="lq"> ϕ </em>被称为主分量加载，它定义了函数的方向。为了捕捉最大方向，其值被选择为产生最大方差。在这种情况下，方差表示从投影点(下图中的红点)到原点(0，0)的距离平方的平均值。因此，为了增加方差，投影点应该远离原点，这可以通过沿着数据点的方向设置主分量方向来实现。要直观地看到这个想法，请查看下图。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/ea81306d3cd9a904a92c387ab7e7e0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UpFltkN-kT9aGqfLhOR9xg.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">gif创建者:<a class="ae iu" href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis" rel="noopener ugc nofollow" target="_blank"> Zakaria Jaadi </a></figcaption></figure><p id="caf2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">主分量载荷的一个限制是它们的平方和应该等于1，因为如果没有这个限制，它们的值将变得无限大。</p><p id="d304" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一件需要注意的重要事情是标准化变量。因为主成分方向对具有最高方差的预测值赋予最高权重，所以具有更大范围值的预测值成为显著方向的可能性更大。例如，不管真实的关系如何，该等式将对范围从0到1000的预测值赋予比范围从0到1的另一个预测值更大的权重，因为前一个预测值更分散。由于标准化变量，标准化数据点的平均值为0，如上图所示。</p><h2 id="d917" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">主成分回归PCR</h2><p id="ca9f" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">利用数据集中的p个预测值，我们可以构建多达p个主成分方向。其中，第一主方向是沿着观察变化最大的方向。第二主成分与第一主成分不相关，因此垂直于第一主成分，并且沿着数据点的下一个最大方向。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/6f691dcd431fb397909e06d39bdc0f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CbG3tloZ4x_qtSiEqfQfgg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图来自<a class="ae iu" href="https://www.statlearning.com/" rel="noopener ugc nofollow" target="_blank">统计学习介绍第230页:应用于R </a> /绿色实线表示第一主成分，蓝色虚线表示第二主成分</figcaption></figure><p id="16f6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">具有这些方向的PCR的关键思想是少量的M个主成分就足以解释大部分可变性。因此，选择一个合适的M至关重要。</p><h2 id="5841" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">偏最小二乘法</h2><p id="0961" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们可以使用的另一个降维方法是PLS。这个方法和PCR一样，只不过是监督版的。所以主成分载荷的计算，有点不同。</p><p id="9dba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lq"> PLS没有选择给出最大方差的主成分载荷，而是通过设置每个载荷等于Y到Xj的简单线性回归系数来计算方向</em> [5]。因此，与将最高权重放在最高方差上的PCR不同，PLS将最高权重放在与响应最相关的变量上。</p><h2 id="b308" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">选择最佳模型</h2><p id="c05d" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">就像我们在收缩中所做的一样，交叉验证误差用于确定在我们的模型中使用的M的数量。</p></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><h1 id="db85" class="jt ju hi bd jv jw me jy jz ka mf kc kd ke mg kg kh ki mh kk kl km mi ko kp kq bi translated">泰勒:博士</h1><p id="3b83" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><strong class="ix hj">子集选择</strong>:为每个子集大小选择最佳模型→使用Cp、BIC、调整R和领域知识选择模型</p><p id="ef8f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">收缩</strong>:最小化系数估计函数(<em class="lq"> RSS +惩罚项</em> ) →通过评估交叉验证误差找到合适的λ值</p><p id="d1cb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">降维</strong>:使用M个主成分方向对预测因子进行降维→通过评估交叉验证误差找到合适的M值</p></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><h1 id="1e92" class="jt ju hi bd jv jw me jy jz ka mf kc kd ke mg kg kh ki mh kk kl km mi ko kp kq bi translated"><strong class="ak">何时使用哪个</strong></h1><p id="17d2" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">对于所有这些方法，您可能想知道在什么情况下哪种方法是合适的。因此，这里有一个快速查看每种方法的利弊。首先，让我们看看子集选择。</p><h2 id="060e" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">子集选择</h2><p id="d0e4" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">与其他子集选择相比，最佳子集选择的主要优势在于它为我们提供了最佳模型。但是，因为它探索变量的2ᵖ组合，所以当p增加时，它的计算开销很快变大-当p=20时，它变成1，048，576。向前和向后选择改善了这种限制。</p><p id="51ce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为它们不探索每个组合，所以它们在计算上比最佳子集选择更好。但和往常一样，优势是有代价的。它们不会产生像最佳子集选择那样的最佳模型。</p><p id="988d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，有些人不赞成选择方法，因为他们认为这种方法是p-hacking。也就是说，它多次尝试相同的方法，偶然得到低p值，并认为这是有意义的。这就像把10枚硬币翻转1000次，连续得到10个头像一样。然后声称50:50的头尾比例是错误的，因为我连续得到10个头，而这仅仅是偶然发生的。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/23d6948d3fe6012cb4cee06af6835d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*iyggnliaQ4dZ-RkL-MwKUw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">表明通过多次运行相同的方法可以获得低p值</figcaption></figure><p id="7f37" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，与其使用选择方法进行解释性分析，不如在探索性步骤中使用，以找到少量显著的预测因子。但即便如此，也有更好的方法。</p><h2 id="093b" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">收缩</h2><p id="8bcf" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">由于ridge和lasso在惩罚条件上的差异，当有许多预测因子与反应类似时，ridge往往比lasso做得更好。相比之下，lasso往往在强预测因子很少、其他系数接近于零时表现更好。然而，在进行任何分析之前，我们不知道预测因素和反应之间的关系。因此，最好两者都尝试，选择交叉验证的模型。</p><p id="e49a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一个区别是可解释性。因为岭包括模型中的所有变量，所以通常很难解释结果。然而，由于lasso执行可变选择，因此什么与响应相关，什么不相关就变得很清楚了。因此，当你想了解预测和反应之间的关系时，lasso可能是一个更好的选择。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/2470a24945ee7dc1201fefc82430fae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ejRfcfRNTVC2R9_442Sa_A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">岭和套索如何产生不同的系数估计。(上)岭回归/(下)套索回归</figcaption></figure><h2 id="1c57" class="kw ju hi bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">降维</h2><p id="20a6" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">PCR和PLS之间的一个很大的区别是PCR是一种无监督的方法，而PLS是一种有监督的方法。因此，PCR在其计算中不包括响应，而PLS包括响应。因此，当响应与具有低方差的方向相关时，PCR将无法准确预测，因为它仅基于产生最高方差来确定主要方向。但是，由于PLS考虑到了响应，所以它的性能会好得多。</p><p id="28bf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不管每种方法的优缺点如何，这两种方法都可以用于高度相关的变量，因为它们用线性独立的方向重新定义变量。此外，当最初的几个方向足以迅速减少偏差时，它们往往做得很好，而当需要大量主成分来预测时，它们就做得不好。</p><h1 id="f3b0" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">最后一个音符</h1><p id="44f2" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在进行任何分析之前，很难知道预测因素和反应之间的真实关系。因此，一种方法不能普遍地支配其他方法。因此，我认为尝试不同的方法，用领域知识分析不同的模型，以了解真实的关系，可能比盲目地应用方法和只使用数值更好。</p></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><h1 id="e738" class="jt ju hi bd jv jw me jy jz ka mf kc kd ke mg kg kh ki mh kk kl km mi ko kp kq bi translated">参考</h1><p id="8b58" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">[1] AB，克里夫。"有没有应该使用逐步回归的情况？"<em class="lq">交叉验证</em>，2017年1月25日，stats . stack exchange . com/questions/258026/are-there-any-environments-where-stepwise-regression-should-use。</p><p id="c30d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[2]陈，号文。<em class="lq">线性回归模型的模型选择</em>，JB bender . github . io/stats 506/F17/Projects/group 21 _ Model _ Selection . html</p><p id="802b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[3]彼得·弗洛姆。"逐步停止:为什么逐步选择不好，你应该用什么来代替."<em class="lq">中</em>，走向数据科学，2018 . 12 . 11，走向数据科学. com/stopping-stepwise-why-stepwise-selection-is bad-and-why-you-should-use-insteading-90818 B3 f 52 df。</p><p id="354e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[4]贾迪，扎卡里亚。"主成分分析(PCA)的逐步解释."<em class="lq">内置于【2021年4月1日builtin . com/data-science/step-step-explain-principal-component-analysis。</em></p><p id="30be" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[5] James，Gareth等<em class="lq">统计学习导论:在R中的应用</em>。斯普林格，2021。</p><p id="f28b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[6]咩。"当AIC和调整后的$R $导致不同的结论."<em class="lq">交叉验证</em>，2018年4月25日，stats . stack exchange . com/questions/140965/when-AIC-and-adjusted-R2-lead-to-different-结论。</p><p id="6800" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[7]罗宾·穆尼尔。" PCA vs Lasso回归:数据科学和机器学习."<em class="lq">卡格尔</em>，<a class="ae iu" href="http://www.kaggle.com/questions-and-answers/180977." rel="noopener ugc nofollow" target="_blank">www.kaggle.com/questions-and-answers/180977.</a></p><p id="77e1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[8]米哈日·奥列扎克。"正规化:山脊，套索和弹性网."<em class="lq"> DataCamp社区</em>，<a class="ae iu" href="http://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net." rel="noopener ugc nofollow" target="_blank">www . data camp . com/Community/tutorials/tutorial-ridge-lasso-elastic-net。</a></p><p id="699d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[9]“主成分回归vs偏最小二乘回归。”<em class="lq"> Scikit </em>，Scikit-learn . org/stable/auto _ examples/cross _ decomposition/plot _ PCR _ vs _ pls . html</p><p id="a839" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[10]里克·威克林。"你应该使用主成分回归吗？"<em class="lq">DO循环</em>，2017年10月25日，blogs . SAS . com/content/IML/2017/10/25/principal-component-regression-disregresses . html。</p></div></div>    
</body>
</html>