<html>
<head>
<title>From Liberal Arts to NLP with Disaster Tweets Prediction: Confession of A Kaggle BIPOC Grantee</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从文科到自然语言处理与灾难推特预测:一个Kaggle BIPOC受让人的自白</h1>
<blockquote>原文：<a href="https://medium.com/codex/from-liberal-arts-to-nlp-a-confession-of-a-kaggle-bipoc-grantee-5bb876ee641a?source=collection_archive---------3-----------------------#2021-06-14">https://medium.com/codex/from-liberal-arts-to-nlp-a-confession-of-a-kaggle-bipoc-grantee-5bb876ee641a?source=collection_archive---------3-----------------------#2021-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/9999c8a06fbfc3260b3d834945158177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tuqZ_NyUkmvi4anf"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Yosh Ginsu 在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="ebd1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“艺术还是科学？”这可能是我在高中最后一年听到最多的问题。对许多人来说，艺术和科学不能混为一谈——你必须选择其中之一，并在大学四年中坚持你的选择。不过，我想知道事实是否如此。<em class="jt">嗯，为什么不两者兼得呢？</em></p><p id="48cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我最终选择学习文科，主修商业和经济，因为这似乎是最灵活的。我认为自己非常好奇，把自己限制在一门学科上很违背我的本性。在疫情之前的几年里，我在大学里参加了各种各样的活动，从政治学讲座到创业孵化器。我只是喜欢保持开放的心态，不断学习新事物。</p><p id="4e83" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尽管我有各种各样的兴趣，但我对数据科学几乎一无所知，直到三年前，出于好奇，我决定参加一个ML活动。受到在ML的帮助下可以完成的大量事情的启发，我决定开始学习一些数据科学基础知识，并从那时起继续学习更多关于机器学习的知识。</p><p id="2491" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我的机器学习之旅中，两年前我偶然发现了Kaggle。在那之前，我一直在学习各种关于数据科学的MOOC课程，希望获得一些实践经验。我在Kaggle上查了一些基本的ML项目，从经典的虹膜预测开始。从现有代码中学习有助于我理解整体结构，并对模型训练的方式有所了解。作为一个ML初学者，亲自运行代码并第一次看到ML模型是如何被训练的，在很多方面都很棒。看到很酷的代码实时运行确实令人兴奋，但对我来说更重要的是，它有助于揭开ML许多方面的神秘面纱，并帮助我熟悉基本ML项目的概念和整体工作流程。</p><p id="6f1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我离开曼梯·里将近一年，因为毕业后我更忙于准备我的职业生涯。我必须承认，平衡学习、未来职业和我对ML的热情并不是一件容易的事情。然而，尽管有这样的中断，我对ML的热情仍然很强烈，我一直在想回到它身边。就在那时，我发现了Kaggle BIPOC资助项目。在我看到Kaggle的公告后，我毫不犹豫地开始准备我的申请表，我想说这可能是我一生中最好的决定之一。几个月后，令我惊讶的是，我从Kaggle那里听说我被选为受资助者之一。通过这个为期三个月的项目，我有机会发展我的数据科学技能，并增强我在相关领域追求职业生涯的信心。在这篇博文中，我想谈谈我在这个项目中的经历，以及为什么你也应该试一试！</p><h1 id="a8e3" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">为什么是Kaggle BIPOC Grant？</h1><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/98fa0d6e51ab0e6cc12bc64aa540f2f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-lM6ekjtGtubKNsE"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae iu" href="https://unsplash.com/@chne_?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Tachina Lee </a>拍摄的照片</figcaption></figure><h2 id="219e" class="kx jv hi bd jw ky kz la ka lb lc ld ke jg le lf ki jk lg lh km jo li lj kq lk bi translated"><strong class="ak">表示事项</strong></h2><p id="3dca" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">就性别和种族代表性而言，数据科学作为一个领域正受到缺乏多样性的困扰。雪上加霜的是，在没有多样性的情况下开发的人工智能可以进一步<a class="ae iu" href="https://time.com/5520558/artificial-intelligence-racial-gender-bias/" rel="noopener ugc nofollow" target="_blank">歧视代表性不足的群体</a>，这反过来又加剧了歧视和代表性不足的恶性循环。在数据科学领域扩大BIPOC社区不仅意义重大，而且早就应该这样做了。</p><h2 id="5837" class="kx jv hi bd jw ky kz la ka lb lc ld ke jg le lf ki jk lg lh km jo li lj kq lk bi translated">这是初学者友好的</h2><p id="9fec" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">虽然我已经接触了一些数据科学，但我在这个领域仍然相对较新，并认为自己是一个初学者。Kaggle BIPOC Grant欢迎所有经验层次的人。如果你梦想成为一名数据科学家，这就是你的机会！</p><h2 id="c377" class="kx jv hi bd jw ky kz la ka lb lc ld ke jg le lf ki jk lg lh km jo li lj kq lk bi translated">导师会让你走上正轨</h2><p id="390a" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">掌握数据科学技能需要投入。就我个人而言，我与数据科学有些分分合合的关系；生活中经常有其他事情阻碍我，让我看不到自己最初的目标。从一位经验丰富的数据科学家那里获得私人指导极大地帮助了我专注于对我来说真正重要的事情(例如，在数据科学方面变得更好！)</p><h1 id="d544" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">我从BIPOC赠款经历中学到的:成为卡格勒的秘诀</h1><ul class=""><li id="1df4" class="lq lr hi ix b iy ll jc lm jg ls jk lt jo lu js lv lw lx ly bi translated"><strong class="ix hj">从“入门”项目开始</strong></li></ul><p id="823c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你对Kaggle完全陌生，直接进入常规比赛可能太具挑战性。查看一些<a class="ae iu" href="https://www.kaggle.com/competitions?hostSegmentIdFilter=5" rel="noopener ugc nofollow" target="_blank">入门</a>比赛，开始你的第一个Kaggle项目！</p><ul class=""><li id="fc61" class="lq lr hi ix b iy iz jc jd jg lz jk ma jo mb js lv lw lx ly bi translated"><strong class="ix hj">了解项目背后的故事</strong></li></ul><p id="c65a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">迈出走你的第一步会很有挑战性，所以确保你在做的时候很开心。尽管直接投入到很酷的技术事物中可能很有诱惑力，但了解比赛背后的故事是至关重要的，因为这将帮助你选择对你有意义的比赛，这将使你在第一次Kaggle旅程中保持动力。在开始技术工作之前，阅读项目描述，查看数据集，了解总体范围和评分标准——稍后你会感谢自己的。</p><ul class=""><li id="a228" class="lq lr hi ix b iy iz jc jd jg lz jk ma jo mb js lv lw lx ly bi translated"><strong class="ix hj">看看公开的笔记本</strong></li></ul><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/3c8bba8ac57a2d0662d373c0b744f2de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LXNTjLIAbAlAYVWoVu2ahQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">探索代码部分并从公共代码中学习</figcaption></figure><p id="4f7d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从现有代码中学习是我最喜欢的学习策略之一。它让我保持专注，并让我看到Kaggle比赛中的实际工作流程。公共笔记本可以通过竞赛页面上的代码选项卡轻松访问。</p><ul class=""><li id="8509" class="lq lr hi ix b iy iz jc jd jg lz jk ma jo mb js lv lw lx ly bi translated"><strong class="ix hj">阅读并参与讨论</strong></li></ul><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/fca55d1df52c1347a27165f3b9cdf691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BuldtKXlpnFLbSi0Ewer4Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">讨论线程对于从其他Kagglers那里获得见解和提示非常有用</figcaption></figure><ul class=""><li id="ab76" class="lq lr hi ix b iy iz jc jd jg lz jk ma jo mb js lv lw lx ly bi translated"><strong class="ix hj">卡住时参考知识页面</strong></li></ul><p id="4253" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当你陷入困境时，有大量的资源可以参考。当我需要一些额外的帮助时，我经常浏览<a class="ae iu" href="https://stackoverflow.com/" rel="noopener ugc nofollow" target="_blank">栈溢出</a>或<a class="ae iu" href="https://towardsdatascience.com/" rel="noopener" target="_blank">到数据科学</a>。</p><ul class=""><li id="115f" class="lq lr hi ix b iy iz jc jd jg lz jk ma jo mb js lv lw lx ly bi translated"><strong class="ix hj">通过寻找ML伙伴保持忠诚</strong></li></ul><p id="41e3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">学习数据科学可能相当孤独。与其他ML学习者合作有助于我保持动力并坚持我的追求，即使事情变得具有挑战性。你可以通过在线讨论板、学习小组找到你的ML伙伴，或者试着参加像Kaggle的BIPOC基金这样的奖学金和资助项目！</p><h1 id="7593" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">我选择的项目</h1><p id="0b57" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">在这一点上，你可能想知道我实际上为这个资助项目承担了什么样的项目。我决定利用这个机会从小做起，迈出我进入Kaggle竞赛和NLP的第一步。我参加了《入门》系列的一个比赛，名字叫<a class="ae iu" href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener ugc nofollow" target="_blank"> <em class="jt">自然语言处理带灾推文</em> </a> <em class="jt">。</em>该比赛的目的是预测推文是否是关于真实的灾难。数据集很小，允许用有限的计算资源进行训练。我在使用数据集的不同模型中玩得很开心，并通过观察和适应公共笔记本学习了不同的比赛方法。向下滚动查看我的项目的详细介绍。</p><h1 id="5276" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">利用程序提供的资源</strong></h1><p id="a41d" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">除了主要项目，我还利用了许多受资助者可以利用的资源。在提供的许多课程中，我特别喜欢<a class="ae iu" href="https://www.coursera.org/learn/mlops-fundamentals" rel="noopener ugc nofollow" target="_blank"> MLOps Fundamentals </a>和Kaggle Masters、数据科学家和ML工程师的许多ML讲座。到目前为止，我从未在我的数据科学项目中使用过GCP，因此了解GCP的MLOps真的让我大开眼界。</p><h1 id="7c6b" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">确认</h1><p id="6528" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">虽然资助项目已经结束，但我相信我会用我所学到的东西来支持我在数据科学领域建立自己的事业的梦想。</p><p id="a6f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我要感谢让这个项目成为可能的每一个人:Kaggle团队(对Julia大声喊出来)启动了这个项目！)、客座讲师、导师和我的受助伙伴。特别是，我想对我的良师益友辛说一声，每当我遇到困难时，他总是鼓励我，帮助我。没有你们我不可能成功！</p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="aa6b" class="ju jv hi bd jw jx ml jz ka kb mm kd ke kf mn kh ki kj mo kl km kn mp kp kq kr bi translated"><strong class="ak"> Kaggle项目工作流程</strong></h1><p id="9fe9" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">在这个Kaggle项目中，我将把我的工作分解成不同的步骤。这也适用于其他Kaggle项目。我采取的步骤通常包括:阅读比赛信息；探索性数据分析；数据预处理；模特培训；模型评估；提交预测进行评分。</p><p id="169b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">要获得可运行代码，查看我的</em> <a class="ae iu" href="https://www.kaggle.com/nicolewarinn/nlp-disaster-tweets-electra" rel="noopener ugc nofollow" target="_blank"> <em class="jt"> Kaggle笔记本</em> </a> <em class="jt">和</em> <a class="ae iu" href="https://github.com/nattananwarin/Kaggle_NLP_Disaster_Tweets" rel="noopener ugc nofollow" target="_blank"> <em class="jt"> GitHub资源库</em> </a> <em class="jt">！</em></p><p id="735f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">探索性数据分析</strong></p><p id="bfe2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，导入必要的库。</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="a89b" class="kx jv hi mr b fi mv mw l mx my">import os<br/>import re<br/>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from collections import defaultdict, Counter<br/>from wordcloud import WordCloud <br/>from nltk.tokenize import word_tokenize<br/>import torch<br/>from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler<br/>from transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW<br/>from transformers import get_linear_schedule_with_warmup<br/>import random<br/>import time<br/>import datetime<br/>from sklearn.model_selection import train_test_split</span></pre><p id="8a93" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，进行基本配置，例如设置显示选项、可视化图形大小和字体大小。然后打印出将要使用的处理单元。</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="1c2a" class="kx jv hi mr b fi mv mw l mx my">pd.set_option.display_max_columns = None<br/>pd.set_option.display_max_rows = None<br/>sns.set(style="white", font_scale=1.2)<br/>plt.rcParams["figure.figsize"] = [10,15]</span><span id="dd30" class="kx jv hi mr b fi mz mw l mx my">if torch.cuda.is_available():  <br/>    device = torch.device("cuda")<br/>    print('We will use the GPU:', torch.cuda.get_device_name(0))    <br/>else:<br/>    print('No GPU available, using the CPU instead.')<br/>    device = torch.device("cpu")</span></pre><p id="89f6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">导入训练和测试数据集。</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="2741" class="kx jv hi mr b fi mv mw l mx my">train = pd.read_csv("../input/nlp-getting-started/train.csv")<br/>test = pd.read_csv("../input/nlp-getting-started/test.csv")<br/>sample_sub = pd.read_csv("../input/nlp-getting-started/sample_submission.csv")</span><span id="adfb" class="kx jv hi mr b fi mz mw l mx my">df_train= train<br/>df_test= test</span></pre><p id="2566" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Info()函数提供数据集的整体信息。</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="89c2" class="kx jv hi mr b fi mv mw l mx my">print(train.info())<br/>print('________________________________________')<br/>print(test.info())</span></pre><p id="d6a8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Head()函数有助于您了解数据集。</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="5ee7" class="kx jv hi mr b fi mv mw l mx my">train.head()</span><span id="4bd6" class="kx jv hi mr b fi mz mw l mx my">test.head()</span></pre><p id="c678" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">打印出行数、列数和训练/测试分割比。</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="fe5a" class="kx jv hi mr b fi mv mw l mx my">print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))<br/>print('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))<br/>print('Train/Test Split Ratio equals {}:1'.format(train.shape[0]/test.shape[0]))</span></pre><p id="97ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">找出是否存在任何空变量</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="7f17" class="kx jv hi mr b fi mv mw l mx my">null_counts = pd.DataFrame({"Num_Null": train.isnull().sum()})<br/>null_counts["Pct_Null"] = null_counts["Num_Null"] / train.count() * 100<br/>null_counts</span></pre><ul class=""><li id="0692" class="lq lr hi ix b iy iz jc jd jg lz jk ma jo mb js lv lw lx ly bi translated"><strong class="ix hj">关键词可视化</strong></li></ul><p id="1fe2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用seaborn来可视化最常见关键字的条形图。如果需要，可以导出。下图将显示在输出中。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/5ed1aae3c9953a469e6aaa7cd1bb2d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OXN7hrjHk9pHszTiJp31Zg.png"/></div></div></figure><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="5fcf" class="kx jv hi mr b fi mv mw l mx my">keywords_vc = pd.DataFrame({"Count": train["keyword"].value_counts()})<br/>top50= sns.barplot(y=keywords_vc[0:50].index, x=keywords_vc[0:50]["Count"], orient='h')<br/>plt.title("Top 50 Keywords")<br/>fig = top50.get_figure()<br/>fig.savefig("top50.png") <br/>fig.show()</span></pre><p id="d2b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用len()函数查看训练数据集中唯一关键字的总数</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="27dd" class="kx jv hi mr b fi mv mw l mx my">len(train["keyword"].value_counts())</span></pre><p id="014b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">可视化并比较来自训练数据集的灾难和非灾难推文之间的热门关键词。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/2b7ffa5406df0bc5612b4007927e79fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vuwAVR7_gmJo9S31qMBDqw.png"/></div></div></figure><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="98aa" class="kx jv hi mr b fi mv mw l mx my">disaster_keywords = train.loc[train["target"] == 1]["keyword"].value_counts()<br/>nondisaster_keywords = train.loc[train["target"] == 0]["keyword"].value_counts()</span><span id="4e85" class="kx jv hi mr b fi mz mw l mx my">fig2, ax = plt.subplots(1,2, figsize=(20,15))<br/>sns.barplot(y=disaster_keywords[0:50].index, x=disaster_keywords[0:50], orient='h', ax=ax[0], palette="Reds_d")<br/>sns.barplot(y=nondisaster_keywords[0:50].index, x=nondisaster_keywords[0:50], orient='h', ax=ax[1], palette="Blues_d")<br/>ax[0].set_title("Top 50 Keywords - Disaster Tweets")<br/>ax[0].set_xlabel("Keyword Frequency")<br/>ax[1].set_title("Top 50 Keywords - Non-Disaster Tweets")<br/>ax[1].set_xlabel("Keyword Frequency")<br/>fig2.tight_layout()<br/>fig2.savefig("top50compare.png")<br/>fig2.show()</span></pre><p id="f8c0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">数据预处理</strong></p><p id="bd23" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在模型训练之前，需要适当地清理数据。数据质量越高，模型越好，反之亦然。</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="9f52" class="kx jv hi mr b fi mv mw l mx my">def preprocess(text):<br/>    text=text.lower()<br/>    text = re.sub(r'https?:\/\/.*[\r\n]*', '', text)<br/>    text = re.sub(r'http?:\/\/.*[\r\n]*', '', text)<br/>    text=text.replace(r'&amp;amp;?',r'and')<br/>    text=text.replace(r'&amp;lt;',r'&lt;')<br/>    text=text.replace(r'&amp;gt;',r'&gt;')<br/>    text = re.sub(r"(?:\@)\w+", '', text)<br/>    text=text.encode("ascii",errors="ignore").decode()<br/>    text=re.sub(r'[:"#$%&amp;\*+,-/:;&lt;=&gt;@\\^_`{|}~]+','',text)<br/>    text=re.sub(r'[!]+','!',text)<br/>    text=re.sub(r'[?]+','?',text)<br/>    text=re.sub(r'[.]+','.',text)<br/>    text=re.sub(r"'","",text)<br/>    text=re.sub(r"\(","",text)<br/>    text=re.sub(r"\)","",text)<br/>    text=" ".join(text.split())<br/>    return text<br/>df_train['text'] = df_train['text'].apply(preprocess)<br/>df_test['text'] = df_test['text'].apply(preprocess)<br/>df_train=df_train[df_train["text"]!='']</span></pre><p id="40ed" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">创建一个干净的df_train来删除不必要的列</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="864e" class="kx jv hi mr b fi mv mw l mx my">df_train=df_train[["text","target"]]</span></pre><p id="69fb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从文本和目标列创建数组。</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="8297" class="kx jv hi mr b fi mv mw l mx my">texts = df_train.text.values<br/>labels = df_train.target.values</span></pre><p id="1b69" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">模特培训</strong></p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="63db" class="kx jv hi mr b fi mv mw l mx my">torch.cuda.empty_cache()<br/>tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')<br/>model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator',num_labels=2)<br/>model.cuda()</span></pre><p id="f957" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">____________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="aff4" class="kx jv hi mr b fi mv mw l mx my">indices=tokenizer.batch_encode_plus(texts,max_length=64,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)<br/>input_ids=indices["input_ids"]<br/>attention_masks=indices["attention_mask"]</span></pre><p id="8932" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">____________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="7c48" class="kx jv hi mr b fi mv mw l mx my">train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, <br/>                                                            random_state=42, test_size=0.2)</span><span id="392d" class="kx jv hi mr b fi mz mw l mx my">train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,<br/>                                             random_state=42, test_size=0.2)</span></pre><p id="65ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">__________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="1283" class="kx jv hi mr b fi mv mw l mx my">train_inputs = torch.tensor(train_inputs)<br/>validation_inputs = torch.tensor(validation_inputs)<br/>train_labels = torch.tensor(train_labels, dtype=torch.long)<br/>validation_labels = torch.tensor(validation_labels, dtype=torch.long)<br/>train_masks = torch.tensor(train_masks, dtype=torch.long)<br/>validation_masks = torch.tensor(validation_masks, dtype=torch.long)</span></pre><p id="491b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">__________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="c2b3" class="kx jv hi mr b fi mv mw l mx my">batch_size = 32</span><span id="f4f9" class="kx jv hi mr b fi mz mw l mx my">train_data = TensorDataset(train_inputs, train_masks, train_labels)<br/>train_sampler = RandomSampler(train_data)<br/>train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)</span><span id="4924" class="kx jv hi mr b fi mz mw l mx my">validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)<br/>validation_sampler = SequentialSampler(validation_data)<br/>validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)</span></pre><p id="d8de" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="8622" class="kx jv hi mr b fi mv mw l mx my">optimizer = AdamW(model.parameters(),<br/>                  lr = 6e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5<br/>                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.<br/>                )</span><span id="b9fe" class="kx jv hi mr b fi mz mw l mx my"># Number of training epochs (authors recommend between 2 and 4)<br/>epochs = 5</span><span id="ca30" class="kx jv hi mr b fi mz mw l mx my"># Total number of training steps is number of batches * number of epochs.<br/>total_steps = len(train_dataloader) * epochs</span><span id="ab8e" class="kx jv hi mr b fi mz mw l mx my"># Create the learning rate scheduler.<br/>scheduler = get_linear_schedule_with_warmup(optimizer, <br/>                                            num_warmup_steps = 0, <br/>                                            num_training_steps = total_steps)</span></pre><p id="6785" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="9786" class="kx jv hi mr b fi mv mw l mx my">def flat_accuracy(preds, labels):<br/>    pred_flat = np.argmax(preds, axis=1).flatten()<br/>    labels_flat = labels.flatten()<br/>    return np.sum(pred_flat == labels_flat) / len(labels_flat)</span></pre><p id="304a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">_________________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="7338" class="kx jv hi mr b fi mv mw l mx my">def format_time(elapsed):<br/>    '''<br/>    Takes a time in seconds and returns a string hh:mm:ss<br/>    '''<br/>    # Round to the nearest second.<br/>    elapsed_rounded = int(round((elapsed)))<br/>    <br/>    # Format as hh:mm:ss<br/>    return str(datetime.timedelta(seconds=elapsed_rounded))</span></pre><p id="b5af" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">______________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="ddf5" class="kx jv hi mr b fi mv mw l mx my"># Set the seed value all over the place to make this reproducible.<br/>seed_val = 42</span><span id="367c" class="kx jv hi mr b fi mz mw l mx my">random.seed(seed_val)<br/>np.random.seed(seed_val)<br/>torch.manual_seed(seed_val)<br/>torch.cuda.manual_seed_all(seed_val)</span><span id="5663" class="kx jv hi mr b fi mz mw l mx my"># Store the average loss after each epoch so we can plot them.<br/>loss_values = []</span><span id="3446" class="kx jv hi mr b fi mz mw l mx my"># For each epoch...<br/>for epoch_i in range(0, epochs):<br/>    <br/>    # Perform one full pass over the training set.<br/>    print("")<br/>    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))<br/>    print('Training...')</span><span id="0b74" class="kx jv hi mr b fi mz mw l mx my"># Measure how long the training epoch takes.<br/>    t0 = time.time()</span><span id="0f46" class="kx jv hi mr b fi mz mw l mx my"># Reset the total loss for this epoch.<br/>    total_loss = 0</span><span id="c9f0" class="kx jv hi mr b fi mz mw l mx my"># Put the model into training mode. Don't be mislead--the call to <br/>    model.train()</span><span id="dde9" class="kx jv hi mr b fi mz mw l mx my"># For each batch of training data...<br/>    for step, batch in enumerate(train_dataloader):</span><span id="10e1" class="kx jv hi mr b fi mz mw l mx my"># Progress update every 100 batches.<br/>        if step % 50 == 0 and not step == 0:<br/>            # Calculate elapsed time in minutes.<br/>            elapsed = format_time(time.time() - t0)<br/>            <br/>            # Report progress.<br/>            print('  Batch {:&gt;5,}  of  {:&gt;5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))</span><span id="117d" class="kx jv hi mr b fi mz mw l mx my"># As we unpack the batch, we'll also copy each tensor to the GPU  <br/>        b_input_ids = batch[0].to(device)<br/>        b_input_mask = batch[1].to(device)<br/>        b_labels = batch[2].to(device)</span><span id="001c" class="kx jv hi mr b fi mz mw l mx my">model.zero_grad()</span><span id="e798" class="kx jv hi mr b fi mz mw l mx my">outputs = model(b_input_ids, <br/>                    token_type_ids=None, <br/>                    attention_mask=b_input_mask, <br/>                    labels=b_labels)</span><span id="b204" class="kx jv hi mr b fi mz mw l mx my">loss = outputs[0]</span><span id="d24a" class="kx jv hi mr b fi mz mw l mx my">total_loss += loss.item()</span><span id="7ace" class="kx jv hi mr b fi mz mw l mx my">loss.backward()</span><span id="2cdd" class="kx jv hi mr b fi mz mw l mx my">torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)</span><span id="0472" class="kx jv hi mr b fi mz mw l mx my">optimizer.step()</span><span id="7a2a" class="kx jv hi mr b fi mz mw l mx my">scheduler.step()</span><span id="21d7" class="kx jv hi mr b fi mz mw l mx my"># Calculate the average loss over the training data.<br/>    avg_train_loss = total_loss / len(train_dataloader)            <br/>    <br/>    # Store the loss value for plotting the learning curve.<br/>    loss_values.append(avg_train_loss)</span><span id="b1ef" class="kx jv hi mr b fi mz mw l mx my">print("  Average training loss: {0:.2f}".format(avg_train_loss))<br/>    print("  Training epoch took: {:}".format(format_time(time.time() - t0)))<br/>      <br/>print("")<br/>print("Training complete!")</span></pre><p id="9e71" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">模型评估</strong></p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="0575" class="kx jv hi mr b fi mv mw l mx my"># Validation</span><span id="d7dc" class="kx jv hi mr b fi mz mw l mx my">print("")<br/>print("Running Validation...")</span><span id="9969" class="kx jv hi mr b fi mz mw l mx my">t0 = time.time()</span><span id="78bf" class="kx jv hi mr b fi mz mw l mx my">model.eval()</span><span id="d765" class="kx jv hi mr b fi mz mw l mx my">preds=[]<br/>true=[]</span><span id="bb3c" class="kx jv hi mr b fi mz mw l mx my">eval_loss, eval_accuracy = 0, 0<br/>nb_eval_steps, nb_eval_examples = 0, 0</span><span id="e759" class="kx jv hi mr b fi mz mw l mx my"># Evaluate data for one epoch<br/>for batch in validation_dataloader:<br/>    <br/>    # Add batch to GPU<br/>    batch = tuple(t.to(device) for t in batch)<br/>    <br/>    # Unpack the inputs from our dataloader<br/>    b_input_ids, b_input_mask, b_labels = batch<br/>    <br/>    # Telling the model not to compute or store gradients, saving memory and<br/>    # speeding up validation<br/>    with torch.no_grad():</span><span id="9a09" class="kx jv hi mr b fi mz mw l mx my"># Forward pass, calculate logit predictions.<br/>        # This will return the logits rather than the loss because we have<br/>        # not provided labels.<br/>        # token_type_ids is the same as the "segment ids", which <br/>        # differentiates sentence 1 and 2 in 2-sentence tasks.</span><span id="bddc" class="kx jv hi mr b fi mz mw l mx my">outputs = model(b_input_ids, <br/>                        token_type_ids=None, <br/>                        attention_mask=b_input_mask)<br/>    <br/>    # Get the "logits" output by the model. The "logits" are the output<br/>    # values prior to applying an activation function like the softmax.<br/>    logits = outputs[0]</span><span id="4fcb" class="kx jv hi mr b fi mz mw l mx my"># Move logits and labels to CPU<br/>    logits = logits.detach().cpu().numpy()<br/>    label_ids = b_labels.to('cpu').numpy()<br/>    <br/>    preds.append(logits)<br/>    true.append(label_ids)<br/>    # Calculate the accuracy for this batch of test sentences.<br/>    tmp_eval_accuracy = flat_accuracy(logits, label_ids)<br/>    <br/>    # Accumulate the total accuracy.<br/>    eval_accuracy += tmp_eval_accuracy</span><span id="3edb" class="kx jv hi mr b fi mz mw l mx my"># Track the number of batches<br/>    nb_eval_steps += 1</span><span id="1586" class="kx jv hi mr b fi mz mw l mx my"># Report the final accuracy for this validation run.<br/>print("Accuracy: {0:.2f}".format(eval_accuracy/nb_eval_steps))<br/>print("Validation took: {:}".format(format_time(time.time() - t0)))</span></pre><p id="49a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">____________________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="0f2b" class="kx jv hi mr b fi mv mw l mx my"># Combine the predictions for each batch into a single list of 0s and 1s.<br/>flat_predictions = [item for sublist in preds for item in sublist]<br/>flat_predictions = np.argmax(flat_predictions, axis=1).flatten()<br/># Combine the correct labels for each batch into a single list.<br/>flat_true_labels = [item for sublist in true for item in sublist]</span></pre><p id="5e31" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">_________________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="02be" class="kx jv hi mr b fi mv mw l mx my">comments1 = df_test.text.values</span><span id="4ea5" class="kx jv hi mr b fi mz mw l mx my">indices1=tokenizer.batch_encode_plus(comments1,max_length=128,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)<br/>input_ids1=indices1["input_ids"]<br/>attention_masks1=indices1["attention_mask"]</span><span id="10d7" class="kx jv hi mr b fi mz mw l mx my">prediction_inputs1= torch.tensor(input_ids1)<br/>prediction_masks1 = torch.tensor(attention_masks1)</span><span id="d924" class="kx jv hi mr b fi mz mw l mx my"># Set the batch size.  <br/>batch_size = 32</span><span id="11fc" class="kx jv hi mr b fi mz mw l mx my"># Create the DataLoader.<br/>prediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1)<br/>prediction_sampler1 = SequentialSampler(prediction_data1)<br/>prediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)</span></pre><p id="f21f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">_________________________</p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="c0b5" class="kx jv hi mr b fi mv mw l mx my">print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))</span><span id="1d11" class="kx jv hi mr b fi mz mw l mx my"># Put model in evaluation mode<br/>model.eval()</span><span id="12cf" class="kx jv hi mr b fi mz mw l mx my"># Tracking variables <br/>predictions = []</span><span id="4663" class="kx jv hi mr b fi mz mw l mx my"># Predict <br/>for batch in prediction_dataloader1:<br/>  # Add batch to GPU<br/>  batch = tuple(t.to(device) for t in batch)<br/>  <br/>  # Unpack the inputs from our dataloader<br/>  b_input_ids1, b_input_mask1 = batch<br/>  <br/>  # Telling the model not to compute or store gradients, saving memory and <br/>  # speeding up prediction<br/>  with torch.no_grad():<br/>      # Forward pass, calculate logit predictions<br/>      outputs1 = model(b_input_ids1, token_type_ids=None, <br/>                      attention_mask=b_input_mask1)</span><span id="958c" class="kx jv hi mr b fi mz mw l mx my">logits1 = outputs1[0]</span><span id="f586" class="kx jv hi mr b fi mz mw l mx my"># Move logits and labels to CPU<br/>  logits1 = logits1.detach().cpu().numpy()<br/>  <br/>  <br/>  # Store predictions and true labels<br/>  predictions.append(logits1)</span><span id="24f8" class="kx jv hi mr b fi mz mw l mx my">flat_predictions = [item for sublist in predictions for item in sublist]<br/>flat_predictions = np.argmax(flat_predictions, axis=1).flatten()</span></pre><p id="bd36" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">提交预测评分</strong></p><pre class="kt ku kv kw fd mq mr ms mt aw mu bi"><span id="1ef0" class="kx jv hi mr b fi mv mw l mx my">#Create Submission<br/>submit=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':flat_predictions})<br/>print(submit)<br/>submit.to_csv('submit.csv', index=False)</span></pre></div></div>    
</body>
</html>