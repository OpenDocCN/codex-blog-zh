<html>
<head>
<title>Digit Classifier using Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用神经网络的数字分类器</h1>
<blockquote>原文：<a href="https://medium.com/codex/digit-classifier-using-neural-networks-ad17749a8f00?source=collection_archive---------7-----------------------#2021-10-01">https://medium.com/codex/digit-classifier-using-neural-networks-ad17749a8f00?source=collection_archive---------7-----------------------#2021-10-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a7937e08cb149fe30ac3ca5157c42feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2lSh1qQCTtV5Rt3G"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:https://www.behance.net/gallery/81929059/Neural-Network</figcaption></figure><p id="54b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">嘿，在这篇文章中，我将向你展示如何用Python构建一个初学者友好的神经网络框架。这段代码的主要目的是帮助新手学习神经网络的基础知识。我们将使用神经网络来识别手写数字。神经网络将能够表示形成非线性假设的复杂模型。如果这对你没有意义，不要担心，这篇文章会帮助你理解。如果你对神经网络不熟悉，请阅读我之前的帖子，了解神经网络的基本思想。(<a class="ae iu" rel="noopener" href="/codex/what-are-neural-networks-3a0965e2ebfb"> <strong class="ix hj"> <em class="jt">点击这里</em> </strong> </a>导航到我之前的帖子。)</p><h2 id="0a91" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">模型表示</h2><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/820027d66b5df175a37d8cc184c5492e.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*Gz_D3jXNgFg0pyyUW09fvg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:作者</figcaption></figure><p id="8d5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的神经网络如上图所示。它有3层→输入层、隐藏层和输出层。由于我们正在处理图片，我们的神经网络无法接受图像作为输入；相反，我们必须提供来自图像的像素作为输入(<strong class="ix hj">注意:</strong>图像是由像素组成的)。为了确保所有图片的大小相同，我们将把它们缩放到20x20像素。通过将它们展开成1D阵列，它给了我们400D向量，该向量将作为我们的神经网络的输入层单元(不包括总是输出+1的额外偏置单元)。</p><p id="f4b4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们导入所需的模块并加载数据集，</p></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><pre class="kq kr ks kt fd lb lc ld le aw lf bi"><span id="ef53" class="ju jv hi lc b fi lg lh l li lj">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from scipy.io import loadmat<br/>import matplotlib.image as img</span></pre></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><pre class="lb lc ld le aw lf bi"><span id="f68e" class="ju jv hi lc b fi lk ll lm ln lo lh l li lj">mat = loadmat('ex4data1.mat')<br/>X = mat['X']<br/>y = mat['y']<br/>X.shape, y.shape</span></pre><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/16d4943a8c3e650fe9c4a59a8e019802.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*I1QssenEc7M2oiZwbq1QyA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">x形，y形</figcaption></figure></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><p id="121d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们通过下面的命令来可视化我们的数据集，</p></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><pre class="kq kr ks kt fd lb lc ld le aw lf bi"><span id="1d5c" class="ju jv hi lc b fi lg lh l li lj">fig, axis = plt.subplots(10, 10, figsize=(8, 8))<br/>for i in range(10):<br/>    for j in range(10):<br/>        axis[i, j].imshow(<br/>            X[np.random.randint(0, 5001), :].reshape(20, 20, order='F'), cmap='gray')<br/>        axis[i, j].axis('off')</span></pre><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/0f8692aadf064f32bc8b2b2e905986ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*aCNg5QaVcNXDohgNRIqkLg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来自数据集的示例</figcaption></figure></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><h2 id="7355" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">乙状结肠的</h2><p id="9dd2" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">我们在之前的帖子中已经对此进行了更多的讨论。所以，我就不解释了。基本上，sigmoid是一个激活函数，它接受一个实值输入，并将其压缩到0到1之间。</p></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><pre class="kq kr ks kt fd lb lc ld le aw lf bi"><span id="09a2" class="ju jv hi lc b fi lg lh l li lj">def sigmoid(z):<br/>    return 1/(1+np.exp(-z))</span></pre></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><h2 id="cd24" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">正向传播和成本函数</h2><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/c6e6c6eb7a1f3e5deaf5284699897596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/0*l7ibS0GhwJz8M61w"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://www.researchgate.net/profile/Antonio-Parmezan" rel="noopener ugc nofollow" target="_blank">来自Researchgate的安东尼奥·拉斐尔·萨比努·帕梅赞</a></figcaption></figure><p id="a247" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上图是神经网络中一层的前向传播。正向传播的公式如下:</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/5bb6ada815a3d199cd0f71434fa9677c.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*HuMJXeEh3McEeKUoJKR-TQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">三层神经网络的前向传播</figcaption></figure><p id="f624" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将x(输入)设为a，然后将a乘以θ(即上图中所示的权重w)并添加偏差(即b或θ₀),最后我们将a和θ的点积发送到激活函数(在我们的情况下为sigmoid函数)。对输入层中的所有400个值和隐藏层中的所有值重复这一过程。为了找到好的参数，使用下面的成本函数:</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/7e1d866bdffe7885341bf68ab7580d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tf6rZ3eox9g-59hEf1qI6Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">价值函数</figcaption></figure><p id="e510" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，成本函数看起来类似于逻辑回归的成本函数，但具有额外的正则化项，这有助于提高我们算法的准确性。这个成本函数帮助我们学习好的参数。</p><h2 id="eace" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">反向传播</h2><p id="55cd" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">反向传播是用于改变<em class="jt">权重</em>和<em class="jt">偏差</em>的技术，以便神经网络的输出变得更加准确。在前向传播中，我们从左向右移动，但在反向传播中，我们从右向左移动。让我们考虑简单的神经网络:</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/faa2f2db901b59e47b1e85f8120a2fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yGZN7OCRKnE77vqi-dTbwg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:作者</figcaption></figure><p id="d601" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">反向传播就是对正向函数从右求导。如果下面的推导对你来说没有意义，不要担心，它肯定是好的，下面的推导是为那些熟悉微积分的人准备的。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/539927ecd48368b11280ae32fc69b9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*Al1N2zJ5qjgEnUGBb-zEkw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:作者</figcaption></figure><p id="fe2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> Sigmoid gradient </strong>对于计算a(1-a)的Sigmoid的梯度是一个有用的函数。我们的神经网络的反向传播公式是:</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/80b5aa61a758d79ec5c445add1636f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*xdUW4yvCuJKo4379iXnz5g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:作者</figcaption></figure></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><pre class="kq kr ks kt fd lb lc ld le aw lf bi"><span id="a2c8" class="ju jv hi lc b fi lg lh l li lj">def costFunction(nn_params, X, y, input_layer_size, hidden_layer_size, num_labels, Lambda):<br/>    <br/>    Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size+1)<br/>    Theta2 = nn_params[((input_layer_size+1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size+1)<br/>    <br/>    #Feedforward and Cost Function<br/>    m = X.shape[0]<br/>    X = np.column_stack((np.ones((m ,1)), X)) #5000 x 401<br/>    a2 = sigmoid(X@Theta1.T) #5000 x 25<br/>    a2 = np.hstack((np.ones((m, 1)), a2)) #5000 x 26<br/>    a3 = sigmoid(a2@Theta2.T) #5000 x 10<br/>    <br/>    y_matrix = np.zeros((m, num_labels)) #5000 x 10<br/>    for i in range(1, num_labels+1):<br/>        y_matrix[:, i-1][:, np.newaxis] = np.where(y==i, 1, 0)<br/>        <br/>    J = np.sum(np.sum( -y_matrix * np.log(a3) - (1 - y_matrix) * np.log(1 - a3) ))  <br/>    reg = Lambda/(2*m) * (np.sum(Theta1[:, 1:]**2) + np.sum(Theta2[:, 1:]**2))<br/>    <br/>    J = (1/m) * J<br/>    reg_J = J + reg<br/>    <br/>    grad1 = np.zeros((Theta1.shape))<br/>    grad2 = np.zeros((Theta2.shape))<br/>    <br/>    for i in range(m):<br/>        xi = X[i, :] #1 x 401<br/>        a2i = a2[i, :] #1 x 26<br/>        a3i = a3[i, :] #1 x 10<br/>        <br/>        d3 = a3i - y_matrix[i, :]<br/>        d2 = (Theta2.T @ d3.T) * sigmoidGradient(np.hstack((1, xi @ Theta1.T)))<br/>        <br/>        grad1 = grad1 + d2[1:][:, np.newaxis] @ xi[:, np.newaxis].T<br/>        grad2 = grad2 + d3.T[:, np.newaxis] @ a2i[:, np.newaxis].T<br/>    <br/>    grad1 = 1/m * grad1<br/>    grad2 = 1/m * grad2     <br/>    grad1_reg = grad1 + Lambda/m * np.hstack((np.zeros((Theta1.shape[0], 1)), Theta1[:, 1:]))<br/>    grad2_reg = grad2 + Lambda/m * np.hstack((np.zeros((Theta2.shape[0], 1)), Theta2[:, 1:]))<br/>        <br/>    return J, grad1, grad2, reg_J, grad1_reg, grad2_reg</span><span id="7848" class="ju jv hi lc b fi mc lh l li lj">input_layer_size = 400<br/>hidden_layer_size = 25<br/>num_labels = 10<br/><br/>nn_params = np.append(Theta1.flatten(), Theta2.flatten())<br/>J, reg_J = costFunction(nn_params, X, y, input_layer_size, hidden_layer_size, num_labels, 1)[0:4:3]<br/><br/>print(f"Cost at parameters(non-regularized): {J}\nCost at parameters(Regularized): {reg_J}")</span></pre><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es md"><img src="../Images/5b94da8006b6e02f25029afedd973dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HiPURxGWbaWUzC0Iz2AAuw.png"/></div></figure></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><h2 id="e943" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">随机初始化</h2><p id="cfc3" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">在神经网络中，我们不应该将θ初始化为零，这使得我们的神经网络对称(即，每个单元检测相同的特征)，当我们将输入乘以θ(为零)时，我们将总是得到零作为输出。因此，为了打破对称性(即，每个单元应该检测不同的特征，如边缘、水平线等。，)我们随机初始化θ。随机初始化的一个有效策略是在range[-ϵᵢₙᵢₜ,ϵᵢₙᵢₜ](where ϵᵢₙᵢₜ=0.12).中均匀地随机选择θ值</p></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><pre class="kq kr ks kt fd lb lc ld le aw lf bi"><span id="2ea1" class="ju jv hi lc b fi lg lh l li lj">def randomInitailization(L_in, L_out):<br/>    epi = np.sqrt(6)/np.sqrt(L_in+L_out)<br/>    W = np.random.rand(L_out, L_in+1) * 2*epi - epi<br/>    return W</span><span id="dede" class="ju jv hi lc b fi mc lh l li lj"><br/>initial_Theta1 = randomInitailization(input_layer_size, hidden_layer_size)<br/>initial_Theta2 = randomInitailization(hidden_layer_size, num_labels)<br/>initial_nn_params = np.append(initial_Theta1.flatten(), initial_Theta2.flatten())</span></pre></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><h2 id="38b4" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">梯度下降</h2><p id="72f1" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">由于我们要学习θ₁和θ₂，梯度下降算法将与之前的略有不同。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es me"><img src="../Images/812311169ba938ac5a2e15a138bf93a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*g2n5c6qbHz_onGIckvDBYg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:作者</figcaption></figure></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><pre class="kq kr ks kt fd lb lc ld le aw lf bi"><span id="3424" class="ju jv hi lc b fi lg lh l li lj">def gradientDescent(initial_nn_params, X, y, input_layer_size, hidden_layer_size, num_labels, alpha, num_iters, Lambda):<br/>    <br/>    Theta1 = initial_nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size+1)<br/>    Theta2 = initial_nn_params[((input_layer_size+1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size+1)<br/>    <br/>    m = len(y)<br/>    J_history = []<br/>    <br/>    for i in range(num_iters):<br/>        nn_params = np.append(Theta1.flatten(), Theta2.flatten())<br/>        cost, grad1, grad2 = costFunction(nn_params, X, y, input_layer_size, hidden_layer_size, num_labels, Lambda)[3:]<br/>        Theta1 = Theta1 - (alpha * grad1)<br/>        Theta2 = Theta2 - (alpha * grad2)<br/>        J_history.append(cost)<br/>    nn_params_final = np.append(Theta1.flatten(), Theta2.flatten())<br/>    <br/>    return nn_params_final, J_history</span><span id="c24a" class="ju jv hi lc b fi mc lh l li lj">nn_params, J_history = gradientDescent(initial_nn_params, X, y, input_layer_size, hidden_layer_size, num_labels, 0.8, 800, 1)<br/>Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size+1)<br/>Theta2 = nn_params[((input_layer_size+1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size+1)</span></pre></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><h2 id="7b62" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">预言</h2><p id="5945" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">我们可以通过一次正向传播得到预测。</p></div><div class="ab cl ku kv gp kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="hb hc hd he hf"><pre class="kq kr ks kt fd lb lc ld le aw lf bi"><span id="9119" class="ju jv hi lc b fi lg lh l li lj">def predict(Theta1, Theta2, X):<br/>    m = X.shape[0]<br/>    X = np.hstack((np.ones((m, 1)), X))<br/>    a2 = sigmoid(X @ Theta1.T)<br/>    a2 = np.hstack((np.ones((m, 1)), a2))<br/>    a3 = sigmoid(a2 @ Theta2.T)<br/>    return np.argmax(a3, axis=1)+1</span><span id="a136" class="ju jv hi lc b fi mc lh l li lj">pred = predict(Theta1, Theta2, X)<br/>print(f"Accuracy = {np.mean(pred[:, np.newaxis]==y) * 100}%")</span></pre><p id="021b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它会显示95%左右的准确率。对手写数字进行分类是有好处的。</p><h1 id="9e8b" class="mf jv hi bd jw mg mh mi ka mj mk ml ke mm mn mo kh mp mq mr kk ms mt mu kn mv bi translated">结论</h1><p id="4007" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">今天，我们看到了神经网络的内幕以及它实际上是如何工作的。然后用python的numpy，pandas和matplotlib从头开始创建。数据集和最终代码上传到github。</p><p id="9a25" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">点击这里查看<a class="ae iu" href="https://github.com/jagajith23/Andrew-Ng-s-Machine-Learning-in-Python/tree/gh-pages/Neural%20Networks" rel="noopener ugc nofollow" target="_blank">神经网络。</a></p><h1 id="de7c" class="mf jv hi bd jw mg mh mi ka mj mk ml ke mm mn mo kh mp mq mr kk ms mt mu kn mv bi translated">如果你喜欢这篇文章，那么看看我在这个系列中的其他文章</h1><h2 id="2b51" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">1.<a class="ae iu" rel="noopener" href="/@jagajith23/what-is-machine-learning-daeac9a2ceca">什么是机器学习？</a></h2><h2 id="b8c4" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">2.<a class="ae iu" rel="noopener" href="/codex/what-are-the-types-of-machine-learning-53360b7db8b4">机器学习有哪些类型？</a></h2><h2 id="bb08" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">3.<a class="ae iu" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6">一元线性回归</a></h2><h2 id="9a0e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">4.<a class="ae iu" rel="noopener" href="/codex/linear-regression-on-multiple-variables-1893e4d940b1">多元线性回归</a></h2><h2 id="e946" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">5.<a class="ae iu" rel="noopener" href="/codex/logistic-regression-eee2fd028ffd">逻辑回归</a></h2><h2 id="a3cd" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">6.<a class="ae iu" rel="noopener" href="/@jagajith23/what-are-neural-networks-3a0965e2ebfb">什么是神经网络？</a></h2><h2 id="5cc3" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">7.<a class="ae iu" rel="noopener" href="/@jagajith23/image-compression-with-k-means-clustering-48e989055729">利用K均值聚类进行图像压缩</a></h2><h2 id="7004" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">8.<a class="ae iu" rel="noopener" href="/@jagajith23/dimensionality-reduction-on-face-using-pca-e3fec3bb4cee">使用PCA对人脸进行降维</a></h2><h2 id="6ea7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">9.<a class="ae iu" href="https://jagajith23.medium.com/detect-failing-servers-on-a-network-using-anomaly-detection-1c447bc8a46a" rel="noopener">使用异常检测来检测网络上的故障服务器</a></h2><h1 id="092b" class="mf jv hi bd jw mg mh mi ka mj mk ml ke mm mn mo kh mp mq mr kk ms mt mu kn mv bi translated">最后做的事</h1><p id="b7bd" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated"><em class="jt">如果你喜欢我的文章，鼓掌👏接下来是⚡neuralistic⚡和</em>媒体宣传这篇文章是有帮助的，这样其他人也可以阅读。<em class="jt">我是Jagajith，我会在下一集里抓住你。</em></p></div></div>    
</body>
</html>