<html>
<head>
<title>Introduction to Unsupervised Learning and K-Means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督学习和K-Means简介</h1>
<blockquote>原文：<a href="https://medium.com/codex/introduction-to-unsupervised-learning-and-k-means-ee8f103ac70a?source=collection_archive---------5-----------------------#2021-11-08">https://medium.com/codex/introduction-to-unsupervised-learning-and-k-means-ee8f103ac70a?source=collection_archive---------5-----------------------#2021-11-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6811" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">Python中K-Means算法的优化及弯头可视化</h2></div><p id="55e4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个故事中，我们将试图理解什么是无监督学习，我们将做一个关于无监督学习的例子。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/d0dcd6cd1173fad8a8308be37848e20c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v2CJkw8_8bUri2H018XMzA.jpeg"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">照片由<a class="ae kj" href="https://unsplash.com/@olloweb?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">通讯社在<a class="ae kj" href="https://unsplash.com/s/photos/learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的</a>跟踪拍摄</figcaption></figure><h1 id="11fd" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">无监督学习</h1><p id="e40f" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">基本上，我们可以说，如果因变量(目标变量)不在数据集中，那么这个问题就是一个无监督学习问题。无监督学习算法不需要因变量(目标变量)。我们可以使用无监督学习来解决以下问题:</p><ul class=""><li id="cf74" class="lh li hi iz b ja jb jd je jg lj jk lk jo ll js lm ln lo lp bi translated">使聚集</li><li id="4328" class="lh li hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated">联合</li><li id="2b50" class="lh li hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated">异常检测</li></ul><h1 id="4c96" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">k均值</h1><p id="3ea6" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">K-Means是无监督学习的一种基本算法。这是一种划分方法。基本上就是把<code class="du lv lw lx ly b">n</code>点分成<code class="du lv lw lx ly b">k</code>簇。K-Means使用数据点的距离来划分<code class="du lv lw lx ly b">k</code>聚类。由于使用了距离，该算法无疑会受到离群点的影响。为了避免这种情况，我们可以调整异常值并缩放数据。对K-Means使用非数值数据是不可能的。</p><p id="fa22" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们看到K-的输入意味着如下:</p><ul class=""><li id="ba62" class="lh li hi iz b ja jb jd je jg lj jk lk jo ll js lm ln lo lp bi translated">d:数据库，数据。{t1，t2，t3 … tn}</li><li id="fda3" class="lh li hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated">k:我们想要划分的集群数量</li></ul><p id="9b61" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">应用K-Means的步骤:</p><ul class=""><li id="644e" class="lh li hi iz b ja jb jd je jg lj jk lk jo ll js lm ln lo lp bi translated">分配随机{m1，m2 … mk}平均值</li><li id="7704" class="lh li hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated">为每个<code class="du lv lw lx ly b">tn</code>分配最近的平均聚类</li><li id="a256" class="lh li hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated">为每个聚类重新分配mk值(平均值)</li><li id="22f6" class="lh li hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated">从第一步开始重新申请</li><li id="df45" class="lh li hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated">如果值的分类没有变化，则返回分类</li></ul><p id="d9b4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">K-Means只给出一个输出:聚类。</p><h1 id="4553" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">在Python中应用K-Means</h1><p id="e265" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">我将在Google Colab上做例子。在做这些例子时，我们将使用这个数据集。该数据集包含美国各省的暴力统计数据。我们将把省份分成随机数量的群。在第二个示例中，我们将尝试找出划分的最佳集群数量。</p><h2 id="4583" class="lz kl hi bd km ma mb mc kq md me mf ku jg mg mh kw jk mi mj ky jo mk ml la mm bi translated">设置工作空间并获取数据</h2><p id="0031" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">首先，我将在Colab中执行这些行。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="7a63" class="lz kl hi ly b fi mr ms l mt mu">!pip install numpy<br/>!pip install pandas<br/>!pip install matplotlib<br/>!pip install sklearn<br/>!pip install yellowbrick</span></pre><p id="c124" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，我将导入所需的包。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="df7b" class="lz kl hi ly b fi mr ms l mt mu">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.cluster import KMeans<br/>from yellowbrick.cluster import KElbowVisualizer</span></pre><p id="51f8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我可以读取数据了。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="7661" class="lz kl hi ly b fi mr ms l mt mu">df = pd.read_csv('https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/USArrests.csv',index_col=0)</span><span id="c6a0" class="lz kl hi ly b fi mv ms l mt mu">df.head()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mw"><img src="../Images/f6c00ffb9e4dbdfd7a2ee22bdbef716e.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*W8PhO_-ERsKR03z-iSbPFg.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">作者图片</figcaption></figure><h2 id="cf60" class="lz kl hi bd km ma mb mc kq md me mf ku jg mg mh kw jk mi mj ky jo mk ml la mm bi translated">应用K-均值</h2><p id="2367" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">如前所述，K均值会受到异常值的影响。在这个例子中，我不会做详细的特征工程。我只是简单地缩放数据框。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="1f07" class="lz kl hi ly b fi mr ms l mt mu">scaler = MinMaxScaler((0,1))</span><span id="b2d7" class="lz kl hi ly b fi mv ms l mt mu">scaled_df = scaler.fit_transform(df)</span></pre><p id="1c2e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们可以创建一个K均值模型。我将创建模型并对其进行拟合。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="f332" class="lz kl hi ly b fi mr ms l mt mu">kmeans = KMeans(n_clusters=2)</span><span id="eb39" class="lz kl hi ly b fi mv ms l mt mu">kmeans.fit(scaled_df)</span></pre><p id="c704" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将数据分为两类。稍后我们将看到如何找到最佳的集群数。现在我想重点介绍一下K均值模型的一些属性。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="d5a5" class="lz kl hi ly b fi mr ms l mt mu">kmeans.n_clusters # total count of clusters</span><span id="ea21" class="lz kl hi ly b fi mv ms l mt mu">kmeans.cluster_centers_ # clusters' centers</span><span id="5079" class="lz kl hi ly b fi mv ms l mt mu">kmeans.labels_ # a label for each observation</span><span id="73b9" class="lz kl hi ly b fi mv ms l mt mu">kmeans.inertia_ # we can say sum of error</span></pre><p id="9bdb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以通过<code class="du lv lw lx ly b">inertia</code>值来测量误差。基本上，它代表了质心和观测值之间距离的总和。</p><p id="21b0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">应该注意的是，<code class="du lv lw lx ly b">cluster_centers_</code>属性将质心表示为观察值。我强调了<code class="du lv lw lx ly b">cluster_centers_</code>，因为我们将使用它们来可视化集群中心。</p><h2 id="50d2" class="lz kl hi bd km ma mb mc kq md me mf ku jg mg mh kw jk mi mj ky jo mk ml la mm bi translated">可视化集群</h2><p id="55f4" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">在本节中，我们将可视化数据。首先，我要将其转换为数据框。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="03a5" class="lz kl hi ly b fi mr ms l mt mu">scaled_df = pd.DataFrame(scaled_df)</span></pre><p id="a8ce" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可能会问，我们如何用4维来可视化数据？稍后，我会写什么是主成分分析，我们将知道如何可视化超过3维的数据。但是现在，我只想象二维空间。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="e3dc" class="lz kl hi ly b fi mr ms l mt mu">plt.scatter(scaled_df.iloc[:, 0],<br/>scaled_df.iloc[:, 1],<br/>c=kmeans.labels_,<br/>s=50,<br/>cmap="viridis")<br/>plt.show()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mx"><img src="../Images/4f9824125fe7b9a698512492c6a5f3cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*PqjpWSMv1Hyyw_aTwPKfcQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">作者图片</figcaption></figure><h2 id="3185" class="lz kl hi bd km ma mb mc kq md me mf ku jg mg mh kw jk mi mj ky jo mk ml la mm bi translated">可视化质心</h2><p id="159c" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">如果我们想要可视化质心，我们需要使用质心的点在我们上面画的第一个图的基础上创建另一个图。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="5888" class="lz kl hi ly b fi mr ms l mt mu"># getting centroids<br/>centroids = kmeans.cluster_centers_ </span><span id="08f0" class="lz kl hi ly b fi mv ms l mt mu"># plotting the data by 1st and 2nd dimensions<br/>plt.scatter(scaled_df.iloc[:, 0],<br/>scaled_df.iloc[:, 1],<br/>c=kmeans.labels_,<br/>s=50,<br/>cmap="viridis")</span><span id="c7a4" class="lz kl hi ly b fi mv ms l mt mu"># plotting the centroids<br/>plt.scatter(centroids[:, 0],<br/>centroids[:, 1],<br/>c="red",<br/>s=200,<br/>alpha=0.8)</span><span id="8b0a" class="lz kl hi ly b fi mv ms l mt mu">plt.show()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mx"><img src="../Images/9106aef825f7915f125703948da0f285.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*MgpbywtRAZEfClqGTHLooQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">作者图片</figcaption></figure><p id="85e4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我打印<code class="du lv lw lx ly b">centroids</code>变量，我会看到像观察一样的点。质心的值是每个变量聚类的最佳点。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es my"><img src="../Images/da758ff271cfc5d69848e54553c91713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KNiQuk1rXF1DGiBb1Z6W0w.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">作者图片</figcaption></figure><h2 id="4e4c" class="lz kl hi bd km ma mb mc kq md me mf ku jg mg mh kw jk mi mj ky jo mk ml la mm bi translated">手动方法查找最佳聚类数</h2><p id="a61f" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">我们可以使用<a class="ae kj" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)#:~:text=In%20cluster%20analysis%2C%20the%20elbow,number%20of%20clusters%20to%20use." rel="noopener ugc nofollow" target="_blank">肘</a>方法来找到最佳集群数。为此，我将创建两个变量:<code class="du lv lw lx ly b">inertias</code>和<code class="du lv lw lx ly b">cluster_range</code>。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="8d81" class="lz kl hi ly b fi mr ms l mt mu">inertias = []</span><span id="7e72" class="lz kl hi ly b fi mv ms l mt mu">cluster_range = range(1,20)</span></pre><p id="ecc7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du lv lw lx ly b">inertias</code>将存储每个聚类计数的惯性，<code class="du lv lw lx ly b">cluster_range</code>将存储聚类计数的范围。我将创建一个循环直到<code class="du lv lw lx ly b">cluster_range</code>并计算惯性，然后将簇计数的惯性附加到<code class="du lv lw lx ly b">inertias</code>。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="b1de" class="lz kl hi ly b fi mr ms l mt mu">for cluster_count in cluster_range:<br/>    kmeans = KMeans(n_clusters=cluster_count).fit(scaled_df)<br/>    inertias.append(kmeans.inertia_)</span></pre><p id="9879" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我已经准备好在同一个图上可视化误差和聚类数，以便通过Elbow方法确定最佳聚类数。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="aaa1" class="lz kl hi ly b fi mr ms l mt mu">plt.plot(cluster_range, inertias)<br/>plt.xlabel("Inertia Value for each Cluster Count")<br/>plt.title("Cluster Count")<br/>plt.show()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mz"><img src="../Images/f2014fdda299128f89e701e40c5e8617.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*2zdpz3CjuQq40lvvn53rrA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">作者图片</figcaption></figure><p id="8398" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">似乎3是集群的合理计数。现在，我可以通过向<code class="du lv lw lx ly b">n_clusters</code>属性传递3来创建一个新的K-Means模型。</p><h2 id="0bba" class="lz kl hi bd km ma mb mc kq md me mf ku jg mg mh kw jk mi mj ky jo mk ml la mm bi translated">使用包查找最佳聚类数</h2><p id="099b" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">此外，我们可以使用<code class="du lv lw lx ly b">yellowbrick</code>包来自动寻找最佳集群数。</p><p id="4a88" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我将创建一个空的K-Means模型，并将这个模型传递给<strong class="iz hj"> KElbowVisualizer </strong>函数。然后，我要通过<code class="du lv lw lx ly b">scaled_df</code>来拟合。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="606a" class="lz kl hi ly b fi mr ms l mt mu">model = KMeans()</span><span id="d4c0" class="lz kl hi ly b fi mv ms l mt mu">kelbow = KElbowVisualizer(model,k=(1,20))</span><span id="4794" class="lz kl hi ly b fi mv ms l mt mu">kelbow.fit(scaled_df)</span></pre><p id="c89f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我可以使用<code class="du lv lw lx ly b">kelbow.show()</code>方法绘制肘值。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es na"><img src="../Images/6d4bdd1e44fd0206b88c2dd61224821b.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*gLKc-AarP9My1UdSFvpS5A.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">作者图片</figcaption></figure><p id="7b3f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，我还可以获得集群数量的最佳值。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="30a9" class="lz kl hi ly b fi mr ms l mt mu">kelbow.elbow_value_</span><span id="193d" class="lz kl hi ly b fi mv ms l mt mu">&gt;&gt;&gt; 3</span></pre><h2 id="0798" class="lz kl hi bd km ma mb mc kq md me mf ku jg mg mh kw jk mi mj ky jo mk ml la mm bi translated">匹配标签和数据</h2><p id="248c" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">在这一节中，我将使用最佳聚类数创建一个K-Means。然后，我将观察结果分成3组。最后，我将合并标签和未缩放的原始数据。</p><pre class="ju jv jw jx fd mn ly mo mp aw mq bi"><span id="cafe" class="lz kl hi ly b fi mr ms l mt mu">model = KMeans(n_clusters=3)</span><span id="4cbe" class="lz kl hi ly b fi mv ms l mt mu">model.fit(scaled_df)</span><span id="f379" class="lz kl hi ly b fi mv ms l mt mu">df['Label'] = model.labels_</span><span id="c0d5" class="lz kl hi ly b fi mv ms l mt mu">df.head()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nb"><img src="../Images/85e4790b6dd586c7ab38793dd611e431.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*702v0gN4EqgiDLRfEZrj6A.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">作者图片</figcaption></figure><h1 id="bcb1" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">最后</h1><p id="ea6f" class="pw-post-body-paragraph ix iy hi iz b ja lc ij jc jd ld im jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">我写这篇文章的时候很享受。希望你也喜欢。此外，你可以查看我的个人资料，了解我关于机器学习和数据科学的其他内容。</p><p id="d342" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">亲切的问候。</p></div></div>    
</body>
</html>