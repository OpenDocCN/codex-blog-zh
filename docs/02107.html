<html>
<head>
<title>Web Scraping with Selenium in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中Selenium的Web抓取</h1>
<blockquote>原文：<a href="https://medium.com/codex/web-scraping-with-selenium-in-python-832cf4b827a4?source=collection_archive---------2-----------------------#2021-07-01">https://medium.com/codex/web-scraping-with-selenium-in-python-832cf4b827a4?source=collection_archive---------2-----------------------#2021-07-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/47b6285bd5c1eed94fb4c5cef78f40a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JN2CjA7dZUcA7adJ"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">由<a class="ae iu" href="https://unsplash.com/@clark_fransa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿诺·弗朗西斯卡</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="f237" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通常，数据对我们来说是公开的，但不是以一种容易使用的形式。这就是网络抓取的用武之地。网络抓取是从网站提取数据的过程。我们可以使用web抓取将我们想要的数据转换成一种方便的格式，然后可以使用。在本教程中，我将展示如何使用Python中的selenium包从网站中提取感兴趣的信息。硒是极其强大的。它允许我们驱动一个浏览器窗口，并以编程方式与网站进行交互。Selenium也有几种方法，使得提取数据非常容易。</p><p id="f90c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本教程中，我将在Windows 10上使用Python3在Jupyter笔记本上进行开发。</p><p id="11a3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们需要下载一个驱动程序。在本教程中，我将使用谷歌浏览器的ChromeDriver。有关支持的驱动程序和平台的完整列表，请参考<a class="ae iu" href="https://www.selenium.dev/downloads/" rel="noopener ugc nofollow" target="_blank">https://www.selenium.dev/downloads/</a>。如果你想使用谷歌浏览器，去https://chromedriver.chromium.org/<a class="ae iu" href="https://chromedriver.chromium.org/" rel="noopener ugc nofollow" target="_blank">下载与你当前版本谷歌浏览器相对应的驱动程序。</a></p><p id="989f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦你安装了驱动程序，你就可以开始写代码了。让我们从导入我们将使用的库开始:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="9e6a" class="kc kd hi jy b fi ke kf l kg kh">from selenium import webdriver<br/>import urllib3<br/>import re<br/>import time<br/>import pandas as pd</span></pre><p id="dbda" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经导入了库，我们需要初始化Chrome webdriver对象。您需要指定驱动程序的路径:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="79b8" class="kc kd hi jy b fi ke kf l kg kh"># Create driver object. Opens browser window.<br/>path = "C:/Users/Robpr/OneDrive/Documents/chromedriver.exe"<br/>driver = webdriver.Chrome(executable_path=path)</span></pre><p id="f6c6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您应该会看到一个空白的Chrome窗口出现，如下所示。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/d4346a0bf6f61f57a788501131c56760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TylV_Bpx_omMmQUJE_clpg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图selenium打开的Chrome浏览器。</figcaption></figure><p id="6c67" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们需要导航到我们感兴趣的网站。最近，我一直在收集insolvencyinsider.ca来归档数据，所以我将使用它。用<code class="du kj kk kl jy b">get()</code>方法导航到<a class="ae iu" href="https://insolvencyinsider.ca/filing/" rel="noopener ugc nofollow" target="_blank">https://insolvencyinsider.ca/filing/</a>:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a165" class="kc kd hi jy b fi ke kf l kg kh"># Navigates browser to insolvency insider.<br/>driver.get("<a class="ae iu" href="https://insolvencyinsider.ca/filing/" rel="noopener ugc nofollow" target="_blank">https://insolvencyinsider.ca/filing/</a>")</span></pre><p id="5ebd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你应该看到你的浏览器导航到破产内幕。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/23e1f7c6269daa616c4a53d1256d26ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xcgvcvMBVov2Q5HnFJ7K0A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图2:将浏览器导航到insolvencyinsider.ca/filing/</figcaption></figure><p id="1114" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你向下滚动到页面底部，你会注意到一个讨厌的“加载更多”按钮。如果没有selenium，我们将被限制在数据的第一页。</p><p id="ec26" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Selenium为在网页上定位元素提供了几种方法。我们将使用<code class="du kj kk kl jy b">find_element_by_xpath()</code>方法创建一个按钮对象，然后我们可以与之交互:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="05d0" class="kc kd hi jy b fi ke kf l kg kh"># Creates "load more" button object.<br/>loadMore = driver.find_element_by_xpath(xpath="/html/body/div[2]/div/main/div/div/div/button")</span></pre><p id="279b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们继续之前，我们需要知道有多少页，这样我们就知道我们需要点击按钮多少次。我们需要一种提取网站源代码的方法。幸运的是，使用<code class="du kj kk kl jy b">urllib3</code>和<code class="du kj kk kl jy b">re</code>库，这个过程相对来说没有什么痛苦。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="c444" class="kc kd hi jy b fi ke kf l kg kh">url = "<a class="ae iu" href="https://insolvencyinsider.ca/filing/" rel="noopener ugc nofollow" target="_blank">https://insolvencyinsider.ca/filing/</a>"<br/>http = urllib3.PoolManager()<br/>r = http.request("GET", url)<br/>text = str(r.data)</span></pre><p id="fec9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du kj kk kl jy b">text</code>现为一串。现在，我们需要一种从我们的<code class="du kj kk kl jy b">text</code>字符串中提取<code class="du kj kk kl jy b">total_pages</code>的方法。打印<code class="du kj kk kl jy b">text</code>来看看我们如何使用RegEx和<code class="du kj kk kl jy b">re</code>包来提取它。我们可以<code class="du kj kk kl jy b">total_pages</code>这样:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a526" class="kc kd hi jy b fi ke kf l kg kh">totalPagesObj = re.search(pattern='"total_pages":\d+', string=text)</span><span id="290c" class="kc kd hi jy b fi km kf l kg kh">totalPagesStr = totalPagesObj.group(0)</span><span id="4d9b" class="kc kd hi jy b fi km kf l kg kh">totalPages = int((re.search(pattern="\d+", string=totalPagesStr)).group(0))</span></pre><p id="b0ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du kj kk kl jy b">search</code>方法接受一个模式和一个字符串。在这种情况下我们的模式是<code class="du kj kk kl jy b">'"total_pages":\d+'</code>。如果您不熟悉RegEx，这意味着我们正在寻找冒号后有两位或更多数字的字符串<code class="du kj kk kl jy b">"total_pages":</code>。<code class="du kj kk kl jy b">\d</code>表示0到9之间的一个数字，而<code class="du kj kk kl jy b">+</code>表示Python应该寻找一个或多个前面的正则表达式。你可以在这里阅读更多关于<code class="du kj kk kl jy b">re</code>包<a class="ae iu" href="https://docs.python.org/3/library/re.html" rel="noopener ugc nofollow" target="_blank">的内容</a>。<code class="du kj kk kl jy b">search()</code>方法返回一个<code class="du kj kk kl jy b">Match</code>对象。<code class="du kj kk kl jy b">re</code>提供了<code class="du kj kk kl jy b">group()</code>方法，该方法返回一个或多个匹配的子组。我们将<code class="du kj kk kl jy b">0</code>作为参数传递，以表明我们想要整个补丁。第三行只是从字符串中提取对应于<code class="du kj kk kl jy b">total_pages</code>的整数。</p><p id="2aee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">完成后，我们现在可以加载破产内幕的每一页。我们可以通过访问对象的<code class="du kj kk kl jy b">click()</code>方法点击<em class="kn">加载更多的</em>按钮。我们在点击之间等待三秒钟，这样我们就不会淹没网站。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="8436" class="kc kd hi jy b fi ke kf l kg kh"># Clicks the Load more button (total pages - 1) times with a three second delay.<br/>for i in range(totalPages-1):<br/>    loadMore.click()<br/>    time.sleep(3)</span></pre><p id="a53d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦你运行这个，你应该看到<em class="kn">加载更多的</em>按钮被点击，剩下的页面被加载。</p><p id="dc9e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦每个页面都被加载，我们就可以开始抓取内容了。现在，抓取某些元素，比如文件名称、日期和超链接，非常简单。我们可以使用selenium的<code class="du kj kk kl jy b">find_elements_by_class_name()</code>和<code class="du kj kk kl jy b">find_elements_by_xpath()</code>方法(注意<code class="du kj kk kl jy b">element</code>后面多出来的<code class="du kj kk kl jy b">s</code>):</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4bcd" class="kc kd hi jy b fi ke kf l kg kh"># Creates a list of filing name elements and a list of filing date elements.<br/>filingNamesElements = driver.find_elements_by_class_name("filing-name")<br/>filingDateElements = driver.find_elements_by_class_name("filing-date")<br/>filingHrefElements = driver.find_elements_by_xpath("//*[<a class="ae iu" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>='content']/div[2]/div/div[1]/h3/a")</span></pre><p id="e620" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还需要备案元数据，即备案类型、备案公司的行业以及他们经营的省份。提取这些数据需要更多的工作。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="8639" class="kc kd hi jy b fi ke kf l kg kh">filingMetas = []<br/>for i in range(len(filingNamesElements) + 1):<br/>    filingMetai = driver.find_elements_by_xpath(("//*[<a class="ae iu" href="http://twitter.com/id" rel="noopener ugc nofollow" target="_blank">@id</a>='content']/div[2]/div[%d]/div[2]/div[1]" %(i)))<br/>    for element in filingMetai:<br/>        filingMetaTexti = element.text<br/>        filingMetas.append(filingMetaTexti)</span></pre><p id="25c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du kj kk kl jy b">text()</code>方法以字符串的形式返回元素的文本。上面的代码片段会产生如下列表:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="bf40" class="kc kd hi jy b fi ke kf l kg kh">['Filing Type: NOI\nCompany Counsel: Loopstra Nixon\nTrustee: EY\nTrustee Counsel: DLA Piper\nIndustry: Food &amp; Accommodation\nProvince: Alberta', ... ]</span></pre><p id="5bd5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从<code class="du kj kk kl jy b">filingMetas</code>的每个元素中，我们可以提取文件类型、行业和省份，如下所示:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="7378" class="kc kd hi jy b fi ke kf l kg kh">metaDict = {"Filing Type": [], "Industry": [], "Province": []}</span><span id="90f0" class="kc kd hi jy b fi km kf l kg kh">for filing in filingMetas:<br/>    filingSplit = filing.split("\n")<br/>    <br/>    for item in filingSplit:<br/>        itemSplit = item.split(": ")<br/>        <br/>        if itemSplit[0] == "Filing Type":<br/>            metaDict["Filing Type"].append(itemSplit[1])<br/>        elif itemSplit[0] == "Industry":<br/>            metaDict["Industry"].append(itemSplit[1])<br/>        elif itemSplit[0] == "Province":<br/>            metaDict["Province"].append(itemSplit[1])<br/>            <br/>    if "Filing Type" not in filing:<br/>        metaDict["Filing Type"].append("NA")<br/>    elif "Industry" not in filing:<br/>        metaDict["Industry"].append("NA")<br/>    elif "Province" not in filing:<br/>        metaDict["Province"].append("NA")</span></pre><p id="4370" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第二块<code class="du kj kk kl jy b">if</code>语句确保我们所有的键值都有相同的长度。如果我们想把这些数据放入熊猫的数据框架中，这是必要的。您可以验证情况是否如此:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="40bc" class="kc kd hi jy b fi ke kf l kg kh">for key in metaDict:<br/>    print(len(metaDict[key]))</span></pre><p id="7a67" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们仍然需要将我们的归档名称和日期放入列表中。我们通过使用前面的<code class="du kj kk kl jy b">text()</code>方法将每个元素文本追加到一个列表中来实现这一点:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="1bee" class="kc kd hi jy b fi ke kf l kg kh"># Initiates a list for filing names and a list for filing dates.<br/>filingName = []<br/>filingDate = []<br/>filingLink = []</span><span id="ff4d" class="kc kd hi jy b fi km kf l kg kh"># for each element in filing name elements list, appends the<br/># element's text to the filing names list.<br/>for element in filingNamesElements:<br/>    filingName.append(element.text)</span><span id="e932" class="kc kd hi jy b fi km kf l kg kh"># for each element in filing date elements list, appends the<br/># element's text to the filing dates list.<br/>for element in filingDateElements:<br/>    filingDate.append(element.text)</span><span id="0aa5" class="kc kd hi jy b fi km kf l kg kh">for link in filingHrefElements:<br/>    if link.get_attribute("href"):<br/>        filingLink.append(link.get_attribute("href"))</span></pre><p id="e035" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你也可以用两行列表理解来完成。</p><p id="8096" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们有了这些，我们就准备把所有的东西放进一个字典里，然后创建一个熊猫数据框架:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="cc14" class="kc kd hi jy b fi ke kf l kg kh"># Creates a final dictionary with filing names and dates.<br/>fullDict = {<br/>    "Filing Name": filingName,<br/>    "Filing Date": filingDate, <br/>    "Filing Type": metaDict["Filing Type"],<br/>    "Industry": metaDict["Industry"],<br/>    "Province": metaDict["Province"],<br/>    "Link": filingLink<br/>}</span><span id="d6eb" class="kc kd hi jy b fi km kf l kg kh"># Creates a DataFrame.<br/>df = pd.DataFrame(fullDict)<br/>df["Filing Date"] = pd.to_datetime(df["Filing Date"], infer_datetime_format=True)</span></pre><p id="0ac9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">瞧！现在我们有了各种破产申请的数据库。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/b5e9a895189a9ac51e2bd38aeb93750c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWQWJ2GHw1v1dKlj8qaIiQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3:破产申报数据表。</figcaption></figure><p id="8738" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我希望你发现这个教程是有用的。现在，您可以使用selenium从各种网站提取数据。</p></div></div>    
</body>
</html>