<html>
<head>
<title>LeNet-5 Complete Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LeNet-5完整架构</h1>
<blockquote>原文：<a href="https://medium.com/codex/lenet-5-complete-architecture-84c6d08215f9?source=collection_archive---------10-----------------------#2022-10-29">https://medium.com/codex/lenet-5-complete-architecture-84c6d08215f9?source=collection_archive---------10-----------------------#2022-10-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="468b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LeNet-5是一个非常有效的用于手写字符识别的卷积神经网络，来自应用于文档识别的基于纸张梯度的学习。</p><p id="53d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" rel="noopener ugc nofollow" target="_blank">论文:基于梯度的学习应用于文档识别</a></p><p id="a14b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">作者</strong> : Yann LeCun、Léon Bottou、Yoshua Bengio和Patrick Haffner<br/>T5，发表于:IEEE会议录(1998)</p><h1 id="823b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">LeNet网络的结构</h1><p id="bd59" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">LeNet5是一个小型网络，它包含深度学习的基本模块:卷积层、池化层和全链路层。它是其他深度学习模型的基础。这里我们深入分析LeNet5。同时，通过实例分析，加深对卷积层和池层的理解。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/2c54e476ad449e12bafcfc61f822551b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L4hn1DnMug-PeYD_.png"/></div></div></figure><p id="d400" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LeNet-5共有七层，不包含一个输入，每层包含一个可训练参数；每一层都有多个特征映射，每个输入特征映射的特征通过卷积滤波器提取，然后每个特征映射有多个神经元。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kt"><img src="../Images/0e431a0a24cc522161758b69fd284837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/0*7KwRVUJf0_H_T6oS.jpg"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">图2</figcaption></figure><p id="d87b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">各层参数的详细说明:</p><h1 id="ac39" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">输入层</h1><p id="01e1" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">首先是数据输入层。输入图像的大小被统一归一化为32 * 32。</p><blockquote class="ky kz la"><p id="94b2" class="if ig lb ih b ii ij ik il im in io ip lc ir is it ld iv iw ix le iz ja jb jc hb bi translated"><em class="hi">注:此层不算LeNet-5的网络结构。传统上，输入层不被认为是网络层次之一。</em></p></blockquote><h1 id="8c95" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">C1层-卷积层；</h1><ul class=""><li id="de29" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated"><strong class="ih hj">输入图片</strong> : 32 * 32</li><li id="b081" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">卷积核大小</strong> : 5 * 5</li><li id="8d52" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">卷积核类型</strong> : 6</li><li id="2b59" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">输出特性贴图尺寸</strong>:28 * 28(32–5+1)= 28</li><li id="0736" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">神经元数量</strong> : 28 * 28 * 6</li><li id="b6f8" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">可训练参数</strong> : (5 * 5 + 1) * 6 (5 * 5 =每个滤波器25个单位参数和一个偏置参数，共6个滤波器)</li><li id="0d52" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">连接数</strong> : (5 * 5 + 1) * 6 * 28 * 28 = 122304</li></ul><h2 id="f549" class="lt jf hi bd jg lu lv lw jk lx ly lz jo iq ma mb js iu mc md jw iy me mf ka mg bi translated">详细描述:</h2><ul class=""><li id="74b6" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated">对输入图像执行第一次卷积运算(使用大小为5 * 5的6个卷积核)以获得6个C1特征图(大小为28 * 28，32–5+1 = 28的6个特征图)。</li><li id="fa9c" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">我们来看看需要多少个参数。卷积核的大小是5 * 5，总共有6 * (5 * 5 + 1) = 156个参数，其中+ 1表示一个核有偏差。</li><li id="a4d0" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">对于卷积层C1，C1的每个像素连接到输入图像中的5 * 5个像素和1个偏置，所以总共有156 * 28 * 28 = 122304个连接。有122，304个连接，但是我们只需要学习156个参数，主要是通过权重共享。</li></ul><h1 id="365d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">S2图层-共用图层(缩减像素采样图层):</h1><ul class=""><li id="259b" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated"><strong class="ih hj">输入</strong> : 28 * 28</li><li id="5d24" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">采样面积</strong> : 2 * 2</li><li id="d960" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">采样方法</strong>:增加4个输入，乘以一个可训练参数，加上一个可训练偏移量。通过乙状结肠的结果</li><li id="fd13" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">取样类型</strong> : 6</li><li id="3b7b" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">输出特性地图尺寸</strong> : 14 * 14 (28/2)</li><li id="9f03" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">神经元数量</strong> : 14 * 14 * 6</li><li id="2d89" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">可训练参数</strong> : 2 * 6(总和的权重+偏移量)</li><li id="d84d" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">连接数</strong> : (2 * 2 + 1) * 6 * 14 * 14</li><li id="2774" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">S2每幅专题地图的尺寸是C1专题地图尺寸的1/4。</li></ul><h2 id="65de" class="lt jf hi bd jg lu lv lw jk lx ly lz jo iq ma mb js iu mc md jw iy me mf ka mg bi translated">详细描述:</h2><ul class=""><li id="f43f" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated">在第一次卷积之后，立即进行汇集操作。使用2 * 2个内核和S2来执行汇集，获得14 * 14 (28/2 = 14)的6个特征图。</li><li id="a938" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">S2的池图层是C1 2 * 2区域的像素之和乘以一个权重系数加上一个偏移量，然后结果被再次映射。</li><li id="67ce" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">所以每个池化核心有两个训练参数，所以有2×6 = 12个训练参数，但是有5×14×14×6 = 5880个连接。</li></ul><h1 id="a70c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">C3层-卷积层；</h1><ul class=""><li id="a634" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated"><strong class="ih hj">输入</strong>:S2所有6个或多个特征地图组合</li><li id="8451" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">卷积核大小</strong> : 5 * 5</li><li id="de73" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">卷积核类型</strong> : 16</li><li id="79fd" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">输出特征映射大小</strong>:10 * 10(14–5+1)= 10</li><li id="0dae" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">C3的每个特征地图与S2的所有6个或几个特征地图相连，表明该层的特征地图是从上一层提取的特征地图的不同组合。</li><li id="268d" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">一种方法是C3的前6个特征地图以S2的3个相邻特征地图子集作为输入。接下来的6个特征地图以S2的相邻特征地图的4个子集作为输入。接下来的三个以不相邻的4个特征地图子集作为输入。最后一个以S2所有的特征地图作为输入。</li><li id="4bb2" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">可训练参数为</strong>:6 *(3 * 5 * 5+1)+6 *(4 * 5 * 5+1)+3 *(4 * 5 * 5+1)+1 *(6 * 5 * 5+1)= 1516</li><li id="cd8f" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">连接数</strong> : 10 * 10 * 1516 = 151600</li></ul><h2 id="7f28" class="lt jf hi bd jg lu lv lw jk lx ly lz jo iq ma mb js iu mc md jw iy me mf ka mg bi translated">详细描述:</h2><ul class=""><li id="4002" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated">第一次汇集后，第二次卷积，第二次卷积的输出是C3，16个10×10的特征图，卷积核的大小是5 * 5。我们知道S2有6幅14 * 14的地形图，如何从6幅地形图得到16幅地形图？这是由S2的地形图特殊组合而成的16幅地形图。详情如下:</li><li id="5a68" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">C3的前6张要素地图(对应上图第一个红框的第6列)连接到与S2图层(上图第一个红框)连接的3张要素地图，接下来的6张要素地图连接到S2图层，4张要素地图连接(上图第二个红框)， 接下来的3个要素地图与未在S2层连接的4个要素地图相连接，最后一个要素地图与S2层的所有要素地图相连接。 卷积核大小还是5 * 5，所以有6 *(3 * 5 * 5+1)+6 *(4 * 5 * 5+1)+3 *(4 * 5 * 5+1)+1 *(6 * 5 * 5+1)= 1516个参数。图像大小是10 * 10，所以有151600个连接。</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mh"><img src="../Images/38a0e5ba320f7474dddeb4dfbc45e637.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/0*VNxefUk_VgazzhqV.png"/></div></figure><ul class=""><li id="7728" class="lf lg hi ih b ii ij im in iq mi iu mj iy mk jc lk ll lm ln bi translated">C3和S2前三个图的卷积结构如下所示:</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ml"><img src="../Images/573fa80cc5742488f30750c3de0cf713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*Z81o43o0vNgnz9dg.png"/></div></figure><h1 id="53e5" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">S4图层-共用图层(缩减像素采样图层)</h1><ul class=""><li id="102e" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated"><strong class="ih hj">输入</strong> : 10 * 10</li><li id="2c65" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">采样面积</strong> : 2 * 2</li><li id="918d" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">采样方法</strong>:增加4个输入，乘以一个可训练参数，加上一个可训练偏移量。通过乙状结肠的结果</li><li id="44fc" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">采样类型</strong> : 16</li><li id="711d" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">输出特性映射尺寸</strong> : 5 * 5 (10/2)</li><li id="77d5" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">神经元数量</strong> : 5 * 5 * 16 = 400</li><li id="3aa2" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">可训练参数</strong> : 2 * 16 = 32(总和的权重+偏移量)</li><li id="8ad4" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">连接数</strong> : 16 * (2 * 2 + 1) * 5 * 5 = 2000</li><li id="079c" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">S4每幅专题地图的尺寸是C3专题地图尺寸的1/4</li></ul><h2 id="eb6f" class="lt jf hi bd jg lu lv lw jk lx ly lz jo iq ma mb js iu mc md jw iy me mf ka mg bi translated">详细描述:</h2><ul class=""><li id="a58e" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated">S4为池化图层，窗口大小仍为2 * 2，共16幅特征地图，C3图层的16幅10x10地图以2x2为单位池化，得到16幅5x5特征地图。这一层共有32个训练参数2x16，5x5x5x16 = 2000个连接。</li></ul><blockquote class="ky kz la"><p id="89ed" class="if ig lb ih b ii ij ik il im in io ip lc ir is it ld iv iw ix le iz ja jb jc hb bi translated">该连接类似于S2层。</p></blockquote><h1 id="dcc6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">C5层-卷积层</h1><ul class=""><li id="f446" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated"><strong class="ih hj">输入</strong>:S4图层全部16个单元特征图(全部连接到S4)</li><li id="31de" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">卷积核大小</strong> : 5 * 5</li><li id="38ce" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">卷积核类型</strong> : 120</li><li id="a374" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">输出特性映射尺寸</strong>:1 * 1(5–5+1)</li><li id="73f8" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">可训练参数/连接</strong> : 120 * (16 * 5 * 5 + 1) = 48120</li></ul><h2 id="52a7" class="lt jf hi bd jg lu lv lw jk lx ly lz jo iq ma mb js iu mc md jw iy me mf ka mg bi translated">详细描述:</h2><ul class=""><li id="5f11" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated">C5层是卷积层。由于S4层的16个图像的大小是5×5，这与卷积核的大小相同，所以卷积后形成的图像的大小是1×1。这导致120个卷积结果。每一个都连接到上一层的16个地图。所以有(5x5x16 + 1) x120 = 48120个参数，也有48120个连接。C5层的网络结构如下:</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mm"><img src="../Images/914219bb6a4b831515fa9a7d65115aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*0iPC8Z5p6t8wwUgq.png"/></div></figure><h1 id="ca4a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">F6层-全连接层</h1><ul class=""><li id="dc58" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated"><strong class="ih hj">输入</strong> : c5 120维向量</li><li id="0bea" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">计算方法</strong>:计算输入向量和权重向量的点积，加上一个偏移量，结果通过sigmoid函数输出。</li><li id="5ce1" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">可训练参数</strong> : 84 * (120 + 1) = 10164</li></ul><h2 id="8a41" class="lt jf hi bd jg lu lv lw jk lx ly lz jo iq ma mb js iu mc md jw iy me mf ka mg bi translated">详细描述:</h2><ul class=""><li id="ebcc" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated">第6层是完全连接的层。F6层有84个节点，对应一个7×12的位图，-1表示白色，1表示黑色，所以每个符号位图的黑白对应一个代码。该层的训练参数和连接数为(120 + 1) x84 = 10164。ASCII编码图如下:</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mn"><img src="../Images/f2bfa2cd7866fc8793498f333b218926.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/0*3TRxYUXtWDJeGROH.png"/></div></figure><ul class=""><li id="2536" class="lf lg hi ih b ii ij im in iq mi iu mj iy mk jc lk ll lm ln bi translated">F6层的连接方法如下:</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mn"><img src="../Images/b59651bc96914dbb30bd126057e65748.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/0*O9nZH_rxExfVxHP3.png"/></div></figure><h1 id="af8b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">输出层-完全连接的层</h1><ul class=""><li id="5006" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated">输出层也是全连通层，共有10个节点，分别代表数字0到9，如果节点I的值为0，则网络识别的结果为数字I，采用径向基函数(RBF)网络连接。假设x是前一层的输入，y是RBF的输出，RBF输出的计算公式为:</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mo"><img src="../Images/2e229cdfa0b902670fc3d89329b8f73b.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/0*hLr-1s0y4_gxwoK7.png"/></div></figure><ul class=""><li id="5744" class="lf lg hi ih b ii ij im in iq mi iu mj iy mk jc lk ll lm ln bi translated">上述公式w_ij的值由I的位图编码确定，其中I的范围是0到9，j的范围是0到7 * 12–1。RBF输出的值越接近0，就越接近I，即越接近I的ASCII编码数字，就意味着当前网络输入的识别结果是字符I，这一层有84x10 = 840个参数和连接。</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mp"><img src="../Images/79c67416c44c60157c80cf362f6c44f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yUD50Yx9chu3cM5e.png"/></div></div></figure><h1 id="1fc9" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">摘要</h1><ul class=""><li id="a64a" class="lf lg hi ih b ii kc im kd iq lh iu li iy lj jc lk ll lm ln bi translated">LeNet-5是用于手写字符识别的非常有效的卷积神经网络。</li><li id="84aa" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">卷积神经网络可以很好地利用图像的结构信息。</li><li id="c7fc" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">卷积层的参数较少，这也是由卷积层的主要特性决定的，即本地连接和共享权重。</li></ul><p id="749d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代码实现:</strong></p><p id="fbc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用以手写数字闻名的MNIST数据集。</p><pre class="ki kj kk kl fd mq mr ms mt aw mu bi"><span id="b8b5" class="lt jf hi mr b fi mv mw l mx my">#If not available on your system<br/>pip install keras<br/>pip install tensorflow </span><span id="740e" class="lt jf hi mr b fi mz mw l mx my">import keras<br/>from keras.datasets import mnist<br/>from keras.layers import Conv2D, MaxPooling2D<br/>from keras.layers import Dense, Flatten<br/>from keras.models import Sequential</span><span id="f33b" class="lt jf hi mr b fi mz mw l mx my"># Loading the dataset and perform splitting<br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span id="e757" class="lt jf hi mr b fi mz mw l mx my"># Peforming reshaping operation<br/>x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)<br/>x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)</span><span id="9174" class="lt jf hi mr b fi mz mw l mx my"># Normalization<br/>x_train = x_train / 255 <br/>x_test = x_test / 255<br/>#As the pixel values range from 0 to 256, apart from 0 the range is #255. So dividing all the values by 255 will convert it to range #from 0 to 1</span><span id="d5ea" class="lt jf hi mr b fi mz mw l mx my"># One Hot Encoding<br/>y_train = keras.utils.to_categorical(y_train, 10)<br/>y_test = keras.utils.to_categorical(y_test, 10)<br/>#Here we are taking 10 because we have 10 different digits ranging from 0-9.</span><span id="e166" class="lt jf hi mr b fi mz mw l mx my"><br/># Building the Model Architecture<br/>model = Sequential()<br/># Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 66 feature maps. The size of each feature map is 32−5 + 1 = 2832−5 + 1 = 28.<br/># That is, the number of neurons has been reduced from 10241024 to 28 ∗ 28 = 784 28 ∗ 28 = 784.<br/># Parameters between input layer and C1 layer: 6 ∗ (5 ∗ 5 + 1)<br/>model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))<br/># The input of this layer is the output of the first layer, which is a 28 * 28 * 6 node matrix.<br/># The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 14 * 14 * 6.<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/># The input matrix size of this layer is 14 * 14 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.<br/># The output matrix size of this layer is 10 * 10 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters<br/>model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))<br/># The input matrix size of this layer is 10 * 10 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2, so the output matrix size of this layer is 5 * 5 * 16.<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/># The input matrix size of this layer is 5 * 5 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #<br/># So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.<br/># The number of output nodes in this layer is 120, with a total of 5 * 5 * 16 * 120 + 120 = 48120 parameters.<br/>model.add(Flatten())<br/>model.add(Dense(120, activation='relu'))<br/># The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)<br/>model.add(Dense(84, activation='relu'))<br/># The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850<br/>model.add(Dense(10, activation='softmax'))</span></pre><p id="db00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你在图2中看到的。研究人员将激活函数视为输出为<strong class="ih hj"> (-1到1) </strong>的<strong class="ih hj"> tanh </strong>，但在代码块中，我们将其视为输出为<strong class="ih hj"> max(0，x)的<strong class="ih hj">relu</strong>:x为输入。</strong>2011年发现，与2011年之前广泛使用的激活函数相比，它能够更好地训练更深的网络，例如，逻辑sigmoid(受概率论启发)及其更实用的对应物双曲正切。</p><pre class="ki kj kk kl fd mq mr ms mt aw mu bi"><span id="a6a4" class="lt jf hi mr b fi mv mw l mx my">model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])</span><span id="dbdd" class="lt jf hi mr b fi mz mw l mx my"><br/>model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))</span><span id="6ce5" class="lt jf hi mr b fi mz mw l mx my"><br/>score = model.evaluate(x_test, y_test)<br/>print('Test Loss:', score[0])<br/>print('Test accuracy:', score[1])</span></pre><p id="2743" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，研究人员再次将损失函数作为MSE(均方误差),但我们采用了分类交叉熵和20个时期，所以这是一个可以做的实验，所以你可以用更高的时期和不同的损失函数进行训练。</p></div></div>    
</body>
</html>