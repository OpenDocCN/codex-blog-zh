<html>
<head>
<title>Reliable Kubernetes on a Raspberry Pi Cluster: Storage</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">树莓Pi集群上的可靠Kubernetes:存储</h1>
<blockquote>原文：<a href="https://medium.com/codex/reliable-kubernetes-on-a-raspberry-pi-cluster-storage-ff2848d331df?source=collection_archive---------1-----------------------#2021-01-15">https://medium.com/codex/reliable-kubernetes-on-a-raspberry-pi-cluster-storage-ff2848d331df?source=collection_archive---------1-----------------------#2021-01-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="f0a3" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">药典</h2><div class=""/><figure class="ev ex ip iq ir is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es io"><img src="../Images/6bae5b584ec882a9c74d42d778ed20f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NYtvzwC-8pYF_NX0"/></div></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">哈里森·布罗德本特在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="2b70" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">对于大型集群，功能带来了巨大的存储需求。但是我们如何迎合他们呢？我想保留自己数据的所有权，所以云存储立即被淘汰了。由于成本原因，拥有一个花哨的NAS设置也是不可能的。这让我的选择非常有限。</p><p id="01b0" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><a class="ae jd" href="https://scott-jones4k.medium.com/reliable-kubernetes-on-a-raspberry-pi-cluster-introduction-cbdca4e759fb" rel="noopener">第1部分:简介</a> <br/> <a class="ae jd" href="https://scott-jones4k.medium.com/reliable-kubernetes-on-a-raspberry-pi-cluster-the-foundations-d9c792c27b75" rel="noopener">第2部分:基础</a> <br/>第3部分:存储<br/> <a class="ae jd" href="https://scott-jones4k.medium.com/reliable-kubernetes-on-a-raspberry-pi-cluster-monitoring-a771b497d4d3" rel="noopener">第4部分:监控</a> <br/> <a class="ae jd" href="https://scott-jones4k.medium.com/reliable-kubernetes-on-a-raspberry-pi-cluster-security-ef62cca74d78" rel="noopener">第5部分:安全</a></p><h1 id="d79f" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">本地路径存储</h1><p id="009e" class="pw-post-body-paragraph je jf hi jg b jh la jj jk jl lb jn jo jp lc jr js jt ld jv jw jx le jz ka kb hb bi translated">K3s附带本地路径存储功能。这意味着持久卷将存储在为其部署的节点上。这显然会导致一些意想不到的结果，因为对象在集群中移动。为了解决这个问题，K3s要求您指定节点亲缘关系，这样您就可以确保它总是部署在同一个地方。</p><h1 id="4737" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">NFS存储</h1><p id="022d" class="pw-post-body-paragraph je jf hi jg b jh la jj jk jl lb jn jo jp lc jr js jt ld jv jw jx le jz ka kb hb bi translated">也许RPi群集中使用最广泛的选项是NFS存储。这依赖于位于某处的NFS服务器向网络提供文件。这意味着您有一个负责与文件交互的集中式服务，因此您的所有节点都可以与它对话，并获得相同的结果，无论它们在哪里运行。</p><h1 id="e7cd" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Longhorn等人。</h1><p id="6741" class="pw-post-body-paragraph je jf hi jg b jh la jj jk jl lb jn jo jp lc jr js jt ld jv jw jx le jz ka kb hb bi translated">还有其他更复杂的集群存储机制，但在撰写本文时，对ARM架构的支持非常少。尤其是32位和64位ARM。我可能会考虑在某个时候升级到一个更复杂的机制，但是现在，这些已经超出了范围。</p><h1 id="7f61" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">我的解决方案</h1><p id="d387" class="pw-post-body-paragraph je jf hi jg b jh la jj jk jl lb jn jo jp lc jr js jt ld jv jw jx le jz ka kb hb bi translated">我不希望我的集群有任何外部依赖性。所以依靠外部NFS服务器是不可能的。但是把一个旋转起来作为我的集群的一部分是肯定的。我决定使用本地路径存储来运行NFS服务器。我不得不将它固定在一个特定的节点上，但这没关系——我想使用的硬盘被插入到一个Pi中。我创建了我的NFS服务器. yaml如下</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="d4cc" class="lo kd hi lk b fi lp lq l lr ls">apiVersion: v1<br/>kind: Namespace<br/>metadata:<br/>  name: storage<br/>  labels:<br/>    app: storage<br/>---<br/>apiVersion: v1<br/>kind: PersistentVolume<br/>metadata:<br/>  name: local-pv<br/>  namespace: storage<br/>spec:<br/>  capacity:<br/>    storage: 500Gi<br/>  accessModes:<br/>  - ReadWriteOnce<br/>  persistentVolumeReclaimPolicy: Retain<br/>  storageClassName: local-storage<br/>  local:<br/>    path: &lt;&lt;PATH-TO-SHARE&gt;&gt;<br/>  nodeAffinity:<br/>    required:<br/>      nodeSelectorTerms:<br/>      - matchExpressions:<br/>        - key: hdd<br/>          operator: In<br/>          values:<br/>          - enabled<br/>---<br/>kind: PersistentVolumeClaim<br/>apiVersion: v1<br/>metadata:<br/>  name: local-claim<br/>  namespace: storage<br/>spec:<br/>  accessModes:<br/>  - ReadWriteOnce<br/>  storageClassName: local-storage<br/>  resources:<br/>    requests:<br/>      storage: 500Gi<br/>---<br/>apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: nfs-server<br/>  namespace: storage<br/>  labels:<br/>    app: nfs-server<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      app: nfs-server<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: nfs-server<br/>        name: nfs-server<br/>    spec:<br/>      containers:<br/>      - name: nfs-server<br/>        image: itsthenetwork/nfs-server-alpine:11-arm<br/>        env:<br/>          - name: SHARED_DIRECTORY<br/>            value: /exports<br/>        ports:<br/>          - name: nfs<br/>            containerPort: 2049<br/>          - name: mountd<br/>            containerPort: 20048<br/>          - name: rpcbind<br/>            containerPort: 111<br/>        securityContext:<br/>          privileged: true<br/>        volumeMounts:<br/>          - mountPath: /exports<br/>            name: mypvc<br/>      volumes:<br/>        - name: mypvc<br/>          persistentVolumeClaim:<br/>            claimName: local-claim<br/>      nodeSelector:<br/>        hdd: enabled<br/>---<br/>kind: Service<br/>apiVersion: v1<br/>metadata:<br/>  name: nfs-server<br/>  namespace: storage<br/>spec:<br/>  ports:<br/>    - name: nfs<br/>      port: 2049<br/>    - name: mountd<br/>      port: 20048<br/>    - name: rpcbind<br/>      port: 111<br/>  clusterIP: 10.43.184.230 # This is optional, but you can guarantee its IP if you set this.<br/>  selector:<br/>    app: nfs-server</span></pre><p id="70d1" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">以通常的方式应用它</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="6e80" class="lo kd hi lk b fi lp lq l lr ls">$ sudo kubectl apply -f nfs-server.yaml</span></pre><p id="2e37" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">不过，现在还不能实际部署它。我们的节点选择器说它必须部署在一个标签为HDD: enabled的节点上。因此，让我们继续并标记包含您希望共享的路径的节点。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="1259" class="lo kd hi lk b fi lp lq l lr ls">$ sudo kubectl label node k3s-master hdd=enabled</span></pre><p id="5fab" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">完成后，继续检查以确保NFS服务器已经启动</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="b6ee" class="lo kd hi lk b fi lp lq l lr ls">$ sudo kubectl get pods -n storage</span></pre><figure class="lf lg lh li fd is er es paragraph-image"><div class="er es lt"><img src="../Images/e943b118f948756323d3064a8c786ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*FvM53AWS_FtNgyHCUGLvOw.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">正在运行的NFS服务器</figcaption></figure><p id="c34d" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">一旦启动并运行，您就可以在其他持久卷中引用它，并对群集中的所有其他内容使用NFS，这意味着所有其他内容都可以在任何地方运行。作为一个例子，你可以有这样的yaml(剧透一下，这就是我们下一次将触及的确切存储！)</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="46ba" class="lo kd hi lk b fi lp lq l lr ls">apiVersion: v1<br/>kind: PersistentVolume<br/>metadata:<br/>  name: grafana-nfs-volume<br/>  namespace: monitoring<br/>  labels:<br/>    directory: grafana<br/>spec:<br/>  capacity:<br/>    storage: 1Gi<br/>  volumeMode: Filesystem<br/>  accessModes:<br/>    - ReadWriteOnce<br/>  persistentVolumeReclaimPolicy: Retain<br/>  storageClassName: slow<br/>  nfs:<br/>    path: /grafana<br/>    server: 10.43.184.230<br/>---<br/>apiVersion: v1<br/>kind: PersistentVolumeClaim<br/>metadata:<br/>  name: grafana-nfs-claim<br/>  namespace: monitoring<br/>spec:<br/>  storageClassName: slow<br/>  accessModes:<br/>  - ReadWriteOnce<br/>  resources:<br/>    requests:<br/>      storage: 1Gi<br/>  selector:<br/>    matchLabels:<br/>      directory: grafana</span></pre><p id="0d75" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">需要注意的一点是，上面的设置没有将NFS服务器暴露在集群之外。对我来说，这是一个有意识的决定，但对你来说，可能有你想这样做的原因。要公开它，您所要做的就是创建一个负载平衡器服务(就像我们上次看到的那样),将所有相关端口公开给该服务。</p><p id="ff65" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">所以我们有它！本地持久存储，全部包含在您的集群中。全部仍归你所有，对可靠性影响有限。当然，我们仍然有单点故障，但这是通过将它放在单个主节点(另一个单点)上来管理的，因此我们仍然有一个故障点，而不是两个。下次当我们研究如何有效地监控我们的集群时，我们再见！</p></div></div>    
</body>
</html>