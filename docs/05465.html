<html>
<head>
<title>Feature Selection with a Random Variable</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有随机变量的特征选择</h1>
<blockquote>原文：<a href="https://medium.com/codex/feature-selection-with-a-random-variable-5b9bb6558a66?source=collection_archive---------1-----------------------#2022-03-02">https://medium.com/codex/feature-selection-with-a-random-variable-5b9bb6558a66?source=collection_archive---------1-----------------------#2022-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="475c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">一个让你选择前n个最重要特性的决定不那么武断的技巧</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/c8a96d80ad86dc650218361512d44e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HvBRlkBv2zEAzZjg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://unsplash.com/@victoriano?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">维多利亚诺·伊斯基耶多</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><h1 id="9813" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">问题</h1><p id="b671" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">特征选择主要集中在从模型中移除冗余或不相关的特征。该过程是可取的，因为它可以帮助减少模型训练的计算成本，并且在某些情况下，提高模型的预测性能。特征选择中的一个常见类别是嵌入式方法，它由具有自己的特征选择过程的算法实现。一些常见的算法是随机森林，XGBoost或额外的树。这些基于树的模型根据特征在所有树中提高节点纯度的程度来对特征进行排序。在树的顶端选择的特征通常比在树的底端选择的特征更重要。</p><p id="d5b2" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">然而，<strong class="ki hj">我们如何选择使用这些嵌入方法的最终重要特性列表的截止点仍然是任意的</strong>:</p><ul class=""><li id="efa4" class="lh li hi ki b kj lc km ld kp lj kt lk kx ll lb lm ln lo lp bi translated">我们选择前n个特征吗？如果是，为什么是n而不是其他数字？</li><li id="0f8b" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb lm ln lo lp bi translated">我们是否选择重要性值(例如杂质的平均减少量)大于某个阈值的特征？如果有，这个门槛从何而来？</li></ul><p id="f41d" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">本文关注一个简单的技术，它有助于减少决策的随意性:</p><blockquote class="lv"><p id="af82" class="lw lx hi bd ly lz ma mb mc md me lb dx translated"><strong class="ak">向候选特征集合附加随机变量(RV) </strong>。</p></blockquote><h1 id="69dc" class="jo jp hi bd jq jr js jt ju jv jw jx jy io mf ip ka ir mg is kc iu mh iv ke kf bi translated">建议</h1><p id="8fb0" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">假设RV通过基于树的算法以类似于其他候选特征的方式被排序，我们<strong class="ki hj">移除排序低于该RV </strong>的所有特征。通俗地说，我们不想要预测能力低于RV的特征。我们想要比随机性更好的特征。</p><p id="d146" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">为了说明这个想法，我将该技术应用于来自PMLB的两个不同的数据集。</p><h2 id="507a" class="mi jp hi bd jq mj mk ml ju mm mn mo jy kp mp mq ka kt mr ms kc kx mt mu ke mv bi translated">演示</h2><p id="eacd" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">首先，让我们加载数据集并初始化占位符，以存储我们想要监控的所有模型信息和指标，包括:特征集大小、训练时间、AUROC和AUPRC。</p><pre class="iy iz ja jb fd mw mx my mz aw na bi"><span id="87bf" class="mi jp hi mx b fi nb nc l nd ne">import numpy as np<br/>import pandas as pd<br/><br/>from sklearn.metrics import roc_auc_score, precision_recall_curve, auc<br/>from sklearn.model_selection import train_test_split</span><span id="b7ae" class="mi jp hi mx b fi nf nc l nd ne">from xgboost import XGBClassifier<br/>from time import time<br/>from pmlb import fetch_data</span><span id="93c2" class="mi jp hi mx b fi nf nc l nd ne"><strong class="mx hj"><em class="ng"># Fetch data &amp; convert them into appropriate format</em></strong><em class="ng"><br/></em>X, y = fetch_data(dataset_name, return_X_y=True)<br/>X = pd.DataFrame(X)<br/>y = pd.Series(y)</span><span id="40db" class="mi jp hi mx b fi nf nc l nd ne"><strong class="mx hj"><em class="ng"># Placeholder for model info &amp; metrics    <br/></em></strong>model_names = []    <br/>training_times = []    <br/>features_set_size = []    <br/>aurocs = []    <br/>auprcs = []</span></pre><p id="9cb2" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在，我们的功能选择过程可以分为三个主要步骤:</p><ol class=""><li id="8b78" class="lh li hi ki b kj lc km ld kp lj kt lk kx ll lb nh ln lo lp bi translated">根据均匀分布生成一个随机变量，然后将其追加到要素列表中。</li><li id="a9ac" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb nh ln lo lp bi translated">在这个包括RV的特征集上训练一个估计器(例如XGBoost)。</li><li id="4ae8" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb nh ln lo lp bi translated">丢弃所有低于RV的特征。</li></ol><pre class="iy iz ja jb fd mw mx my mz aw na bi"><span id="abdf" class="mi jp hi mx b fi nb nc l nd ne"><strong class="mx hj"><em class="ng"># Train-test split<br/></em></strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)<br/><br/><strong class="mx hj"><em class="ng"># Generate random variable from uniform distribution<br/></em></strong>X_train["random"] = np.random.uniform(0, 1, len(X_train))</span><span id="3d96" class="mi jp hi mx b fi nf nc l nd ne"><strong class="mx hj"><em class="ng"># Use XGBoost as base estimator for feature selector<br/></em></strong>selector = XGBClassifier()<br/>selector.fit(X_train, y_train)</span><span id="61a4" class="mi jp hi mx b fi nf nc l nd ne"><strong class="mx hj"><em class="ng"># Select features ranked above RV<br/></em></strong>df = pd.DataFrame({"feature": X_train.columns,<br/>                   "importance": selector.feature_importances_})<br/>rv_importance = df[df["feature"] == "random"]["importance"].iloc[0]<br/>sel_cols = df[df["importance"] &gt; rv_importance]["feature"].tolist()</span><span id="1458" class="mi jp hi mx b fi nf nc l nd ne"><strong class="mx hj"><em class="ng"># For 2 sets of models: one w/ all candidate features and one w/     selected features<br/></em></strong>for model_name in ["all_features", "selected_features"]:<br/>    if model_name == "all_features":<br/>       feature_set = [i for i in X_train.columns if i != "random"]<br/>    else:<br/>       feature_set = sel_cols<br/>    new_X_train = X_train.filter(feature_set)<br/>    new_X_test = X_test.filter(feature_set)<br/>    num_features = len(feature_set)<br/><br/>    start = time()</span><span id="a651" class="mi jp hi mx b fi nf nc l nd ne"><strong class="mx hj"><em class="ng">    # Fit estimator on training set<br/>    </em></strong>model = XGBClassifier()<br/>    model.fit(new_X_train, y_train)<br/>        <br/>   <strong class="mx hj"><em class="ng"># Validate on test set<br/>   </em></strong>y_scores = model.predict_proba(new_X_test)[:, 1]<br/><br/>   <strong class="mx hj"><em class="ng"># Append metrics results to placeholder<br/>   </em></strong>training_times.append(time() - start)  <br/>   model_names.append(model_name)<br/>   features_set_size.append(num_features)<br/>   aurocs.append(roc_auc_score(y_test, y_scores))<br/>   p, r, _ = precision_recall_curve(y_test, y_scores)<br/>   auprcs.append(auc(r, p))<br/><br/>result = pd.DataFrame({"model_name": model_names, <br/>                       "features_size": features_set_size, <br/>                       "auroc": aurocs, <br/>                       "auprc": auprcs,<br/>                       "training_time": training_times})</span></pre><h2 id="fc0e" class="mi jp hi bd jq mj mk ml ju mm mn mo jy kp mp mq ka kt mr ms kc kx mt mu ke mv bi translated">结果</h2><p id="6a2e" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">用<code class="du ni nj nk mx b">dataset_name = "tokyo1"</code> (n_features = 44，n_observations = 959):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/d7573a129823c9e2ba5a39ff450fe3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*TBfQO9cN-5v0nP8uY2Xn7g.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">将要素选择应用于PMLB中的“tokyo1”数据集时的结果数据框</figcaption></figure><p id="ee47" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">鉴于XGBoost的随机性，在多次运行中，所选特性的数量会略有不同。运行相同的代码片段时，结果可能会有所不同。然而，该技术总体上有助于减少特征的数量，从而减少训练时间，同时仍然保持类似的预测性能。</p><p id="70b0" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">让我们将这个过程应用于一个高维数据集(n_features = 1000，n_observations = 1600)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/152f0f3998ebfbf8ec22ec477c323283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*bBjQpSkbcjjP2uHWv8ujdw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">PMLB“配子_上位性_ 2 _ Way _ 1000 atts _ 0.4H _ EDM _ 1 _ EDM _ 1 _ 1”结果数据帧</figcaption></figure><p id="e47b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">由于该数据集最初具有更大的要素集，因此要素选择过程的效果更加明显。特征数量减少到原来的30%。训练时间显著减少，而预测性能保持不变。</p><p id="ad70" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这就是你要做的，又一个添加到你的特性选择工具箱的巧妙技巧:)。</p></div></div>    
</body>
</html>