<html>
<head>
<title>Understanding Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/codex/understanding-principal-component-analysis-533a78150907?source=collection_archive---------19-----------------------#2021-08-20">https://medium.com/codex/understanding-principal-component-analysis-533a78150907?source=collection_archive---------19-----------------------#2021-08-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e654" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇博客的主要目的是强调这样一个事实，即学习新事物时遇到的障碍应该是学习更多先决条件的借口，而不是逃避你首先想学的东西的借口。我想做的是通过问一些自然的问题来建立一种自然的主成分分析方法，这将引导我们对这个话题的理解不需要微积分。然而，线性代数将被大量使用；我会尽我所能涵盖一切。首先，我们从目标开始。我们给出一些数据"<strong class="ih hj"> X" </strong>也就是<strong class="ih hj"> </strong> a <em class="jd"> f x m(特征X实例)</em>矩阵；为了简单起见，假设我们有两个特性。为了向前推进，让我定义一些我将使用的符号:@是矩阵乘法(它在一些矩阵乘法不言而喻的地方被跳过，这只是使方程简洁)，一个点(。)用于点积。我们想要开发的是帮助我们更好地表达数据的工具。我这么说是什么意思？看看下面绘制的数据:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/bc4074ba6a716c064ad5f3ccb1b0fb42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*UBVwqjEZQ_cRnoslxcmjKg.jpeg"/></div></figure><p id="9d70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一条线。对于一个清晰的一维数据使用二维难道不是多余的吗？这本身并不是一个特别好的表现。因为有很多冗余。数据可能只是在x轴上，不会有太大的不同。为什么我们想要数据的低维表示？在机器学习的背景下，较少的特征有助于加快训练。但总的来说，这可能是对数据去噪的一个典型主题:如果首先生成这个的函数实际上是一条线的方程，而数据点与这条线的偏差是随机噪声，那会怎样？将数据转换到较低的维度，即将其投影到x轴上，然后通过旋转x轴将其转换回2d，以匹配原始线的斜率，这将消除噪声。记住用途，让我们从目标开始:我们想要做的是去除多余的维度。我们如何做到这一点？在这种情况下，答案似乎很明显:首先我们旋转我们的轴，使x轴与数据所在的线对齐，然后我们移除每个数据点的y分量，因为旋转后每个数据点的y分量几乎为0。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/65600a3621c1463eb7d59e4b794e7ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-TskvdpYC4sqLkDGaUppgA.jpeg"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">首先旋转轴，然后将数据点投影到x轴上。</figcaption></figure><p id="88c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在的问题是，我们想用数学来描述它，所以我们需要一些数学上可定义的目标。首先，很明显，我们希望从数据中删除冗余，这涉及到数据在2d中位于一条线上，在3d中位于一个平面上，等等(我们稍后会考虑丢弃额外的维度，这里我们只考虑线性低维可提取表面)。我们考虑这些低维线性表面的原因是，比方说，数据在2d中的一条线上，那么我们可以将x轴旋转到这条线上，并放弃其他维度；如果它在3d平面上，那么我们可以旋转x，y平面以匹配数据平面，并丢弃其他维度。这是PCA做出的一个假设:重要数据位于一个较低维度的<strong class="ih hj">线性</strong>表面；在更实际的情况下，例如图像，这种假设不一定是正确的，并且开发了更复杂的工具和算法来解决这些问题。现在坚持使用PCA，让我们试着想出一个更数学的方法。如果数据在2d中的一条线上，这意味着数据的协方差不为零，对吗？这来自协方差的定义:它测量数据的趋势。现在，这是一个特别伟大的观察，因为我们有测量协方差的方法。本质上计算起来很简单，如果你想回忆起这些概念，我推荐你看<a class="ae jq" href="https://www.youtube.com/watch?v=WBlnwvjfMtQ&amp;ab_channel=LuisSerrano" rel="noopener ugc nofollow" target="_blank">这个视频</a>，因为它比我能更好地解释这个话题。如果我的意思是将数据<strong class="ih hj"> X </strong>沿每个特征维度居中，并将其称为<strong class="ih hj"> Xbar，</strong>，那么协方差就是<strong class="ih hj"> (1/m)(Xbar@Xbar^T) </strong>的非对角元素(对2d数据执行矩阵乘法如果这让你感到困惑，我还会跳过1/m因子，以避免在进一步提及协方差矩阵时出现混乱。)现在我们要的是0协方差，为什么？我们知道协方差是我们对冗余的度量(记住我们所说的冗余是指位于低维线性表面上的数据:2d中的一条线，3d中的一个平面，3d中的一条线，等等)。)，并且开发建模数据的方法以使其具有0协方差，这将迫使数据所在的线或通常的超平面变平(首先我们移除冗余，然后我们移除额外的维度，在上面的示例中，移除冗余是轴的旋转，移除冗余维度将在稍后解释)。现在，这给了我们一个数学上可定义的目标:我们想要的是一个变换<strong class="ih hj"> P </strong>，它将<strong class="ih hj"> Xbar </strong>变换到一个新的空间，称之为<strong class="ih hj">“y”</strong>，<strong class="ih hj">，使得Y@Y^T的非对角元素(协方差)为0</strong>；这一切只是来自于我们所定义的目标。所以让我们把它写下来:<strong class="ih hj"> Y=P@Xbar </strong>这样<strong class="ih hj">y@y^t =(p@xbar)@(p@xbar)^t</strong>就是一个对角矩阵；简化:</p><p id="c1d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">(pxbar)(xbar)^t(p^t)</strong>=<strong class="ih hj">p(xbar)(xbar)^t(p)^t</strong></p><p id="88d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">let’s write <strong class="ih hj">(Xbar)(Xbar)^T as S</strong>, we find that <strong class="ih hj">P(S)P^T must be a diagonal matrix;</strong> if you have some knowledge of the concept of diagonalization in linear algebra this equation might have ringed some bells; because you must have seen that <strong class="ih hj">P^(-1)SP</strong> , where P is a matrix containing the eigenvectors of S, gives us a diagonal matrix where the diagonal entries are the eigenvalues of S. Again if you’re not familiar with the topic, or if you want to review it, I recommend checking out 3Blue1Brown’s <a class="ae jq" href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&amp;ab_channel=3Blue1Brown" rel="noopener ugc nofollow" target="_blank">awesome series</a> on Linear Algebra. We have <strong class="ih hj">P(S)P^T</strong> instead of the other way around(kind of ignoring the fact that we have a transpose in place of an inverse at the moment, this will be explained in a bit); this can be circumvented: just use<strong class="ih hj"> Y=(P^T)Xbar </strong>which gives us <strong class="ih hj">YY^T = (P^TXbar)(P^TXbar)^T</strong> which is <strong class="ih hj">P^T(Xbar.Xbar^T)P</strong> or <strong class="ih hj">P^T(S)P</strong>. Moving on, this is where we hit our first hurdle: there’s a P^(T) instead of P^(-1). What I want do first is figure out for which matrices P, P^T = P^(-1). We’ll then move forward with our analysis. To do so I want to delve deeper into what transposes do, which will make this a bit easier to understand: When we imagine a linear transformation <strong class="ih hj">“A” </strong>we imagine taking the linear combinations of the vectors in its columns; so for instance, if we have a 3x2 transformation you may imagine it as taking the linear combination of 2 vectors in 3d, which gives us a plane. To visualize transposes, first let’s give names to some stuff; <strong class="ih hj">A </strong>has two 3d vectors let’s call them <strong class="ih hj">v </strong>and <strong class="ih hj">w</strong>. Now, for A^T, we imagine the same two vectors but this time to see where a 3d vector <strong class="ih hj">z</strong> lands after A^T has been applied to it (A^T is now a transformation from 3 to 2 dimensions) we imagine dotting the vector <strong class="ih hj">z </strong>with <strong class="ih hj">v </strong>to get the x component of the transformed vector, and we dot <strong class="ih hj">z </strong>with <strong class="ih hj">w </strong>to get the y component of the transformed vector(If this is not obvious, write it on a piece of paper and think about what the matrix multiplication is doing). This, by the way, is also a way of understanding why the column space of A must be perpendicular(orthogonal) to the null space of A^T; I suggest you think about it a bit, if you want to. (if you’re not familiar with these concepts of column spaces, just forget about what I said.) Say z has components <strong class="ih hj">z_x, z_y </strong>and, say,<strong class="ih hj"> </strong>we applied <strong class="ih hj">A</strong> to <strong class="ih hj">z </strong>to get the the transformed vector <strong class="ih hj">Az</strong>,<strong class="ih hj"> </strong>call it<strong class="ih hj"> z_t. </strong>Recall that <strong class="ih hj">v</strong> is where the unit vector <strong class="ih hj">ihat</strong> lands after <strong class="ih hj">A</strong> is applied to it and <strong class="ih hj">w</strong> is where <strong class="ih hj">jhat</strong> lands if <strong class="ih hj">A</strong> is applied to it; Now if A^T = A^-1 this means that dotting <strong class="ih hj">z_t</strong> with <strong class="ih hj">v </strong>gives us <strong class="ih hj">z_x</strong> and dotting<strong class="ih hj"> z_t</strong> with <strong class="ih hj">w</strong> gives is <strong class="ih hj">z_y</strong>; this should be obvious given the above explanation of the transpose. Note that <strong class="ih hj">z</strong> dotted with <strong class="ih hj">ihat</strong> is <strong class="ih hj">z_x</strong> too. What we now have is <strong class="ih hj">z . ihat = z_x</strong> and, if we say <strong class="ih hj">v = ihat_t = A.ihat</strong>(remember v is where ihat lands when A is applied to z), then <strong class="ih hj">z_t . ihat_t= z_x.</strong> This means than A preserves dot products because <strong class="ih hj">z</strong> before or after the transformation dotted with ihat(pristine and transformed) gives its x component. Preserving dot products means 2 things: angles b/w vectors are preserved and the lengths of the vectors are preserved. This suggests the transformations to be rotations. So for rotation matrices A^T = A^-1. Now for a rotational matrix <strong class="ih hj">“R” </strong>all the vectors in it are orthogonal to each other, for they are the rotated basis vectors which were orthogonal to each other. Moreover they are of unit length(perpendicular normalized vectors are called orthonormal vectors). In our case we have the matrix S and its eigenvectors are in the matrix P; what we wanted to show was that P^T = P^-1, because if that is true then we can almost diagonalize, there’s more stuff left to prove. This asks the question: <strong class="ih hj">Are the eigenvectors of S orthonormal?</strong> S is xbar.xbar^T, the only algebraic property noticeable should be that it is symmetric<strong class="ih hj"> S^T = S</strong>. <strong class="ih hj">Do symmetric matrices have orthogonal eigenvectors?</strong> S also needs to have a full set of eigenvectors for us to be able to decompose it like that. <strong class="ih hj">Does it fulfill that requirement ? </strong>We also do not want complex eigenvectors, so <strong class="ih hj">are the eigenvectors of symmetric matrices real?</strong> These, to the best of my knowledge, can be answered algebraically. I, sadly, lack a convincing geometrical argument. I encourage you to stop and look these things up; this was the primary theme of the blog, remember? A symmetric matrix does, as a matter of fact, satisfy all of these properties. Isn’t it surprising that having just this one symmetric property translates into having a bunch of other very useful properties? So there we go, we have our matrix P that will remove any redundancy in the data for us. The matrix is the eigenvectors of <strong class="ih hj">xbar.xbar^T </strong>stacked horizontally. We now come to the topic of dimensionality reduction which is a just a little addition on top of this. Take a look at this picture below.</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/8407025caf6341218c30b986eeebdc2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*B-PWR_iN5FK0Jv2sYVaL1w.jpeg"/></div></figure><p id="b90e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你不认为y轴有点多余吗？你是如何着手解决这个问题的？我们如何从数学上确定维度的重要性？也许暂时停止浏览博客，看看你能否得出一个合理的结论。如果你仔细看看上面的图片，你应该可以看到数据在x轴上更加分散，而在y轴上被压缩。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/4b78f1deac1c16bb37c0b59a0416840b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ZZrgV5BOE2AcRDGsS2A4HQ.jpeg"/></div></figure><p id="709e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，衡量一个维度的重要性的问题的一个自然结论是衡量数据在该维度上的分布，对吗？即数据在该维度上的方差(为什么我们不测量平均值呢？).如果你对所做的声明有疑问，考虑几个案例。现在我们也知道协方差矩阵已经包含了每个维度上的方差:<strong class="ih hj"> xbar@xbar^T </strong>的对角线值代表了每个轴上的方差。现在，既然我们已经将<strong class="ih hj">“x”</strong>转换为<strong class="ih hj">“y”，</strong>我们想要查看<strong class="ih hj"> YY^T </strong>的对角线条目，每个条目都给出了每个维度的方差，这是我们对该维度重要性的度量。<strong class="ih hj"> YY^T </strong>是<strong class="ih hj"> P^TSP </strong>，(s是Xbar。Xbar^T)和P^TSP的对角线条目是s的特征值，因此s的特征值给我们每个维度的重要性，即沿着每个维度的方差。现在通常是这样做的:我们需要把x转换成y，通过对x应用P^T，p是X.X^T的特征向量的矩阵，我们能做的是得到X.X^T的特征值；如上所述，每个维度的重要性是什么，这意味着什么？这意味着，如果我们，比方说，在将P^T应用于x并得到y之后，从p中取两个具有特征值e1和e2的向量，(<strong class="ih hj"> Y=P^TX </strong>)沿每个变换轴的方差将等于e1和e2。如果e2足够低，我们可以用X乘以P的第一个本征向量和本征值e1，这样我们就不会得到多余的维数。如果<strong class="ih hj"> XX^T </strong>具有带f个特征值的f个特征向量，那么我们丢弃具有低特征值的特征向量，然后将特征向量堆叠到矩阵p中，并将其应用于x，从而获得不相关的、维数减少的数据。通过将P应用于Y，数据被转换回原始空间；X_t = PY。井P是非方阵；理想情况下，我们会想要应用(P^T)^(-1)Y是(P^T)^T(Y)或者只是PY(如果p是一个旋转矩阵，那么P^T也是一个旋转矩阵，当然因为P^T是P^(-1)这只是对角的旋转)；但是对于非方阵，没有定义逆矩阵。仍然使用P进行逆变换的正当性来自于矩阵的奇异值分解；我现在不会深入探讨这个问题。当我们在已经转换到较低维度空间之后转换回原始空间时，数据的一些变化将会丢失；这个方差是我们丢弃的特征向量的特征值之和，因为它们太低了。如果我们假设这个额外的方差是噪声，这对于一些数据集来说是正确的，那么在执行这些步骤并转换回原始空间之后，事实上，为我们提供了无噪声的数据。这些仅保留高方差维度的想法本身就很重要，如果你在机器学习中学习自动编码器，它模糊地建立在PCA提供的脚手架上。</p></div></div>    
</body>
</html>