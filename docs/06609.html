<html>
<head>
<title>Advantages of training a custom NER against pre-trained models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">针对预训练模型训练自定义NER的优势</h1>
<blockquote>原文：<a href="https://medium.com/codex/advantages-of-training-a-custom-ner-against-pre-trained-models-1d32bd70883d?source=collection_archive---------4-----------------------#2022-05-04">https://medium.com/codex/advantages-of-training-a-custom-ner-against-pre-trained-models-1d32bd70883d?source=collection_archive---------4-----------------------#2022-05-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3c33" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">为什么在处理特定于主题的数据时，自定义NER可能是最佳选择，因为它在性能和效率之间进行了权衡</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b90bc27d37f1bf8f2f6dc6d5d8b4266d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ur5S54BCI5DN5ehY.jpg"/></div></div></figure><h2 id="c9f2" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">目录</h2><ul class=""><li id="6daa" class="kh ki hi kj b kk kl km kn ju ko jy kp kc kq kr ks kt ku kv bi translated"><strong class="kj hj"> <em class="kw">简介</em> </strong></li><li id="8937" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated"><strong class="kj hj"> <em class="kw">数据采集</em> </strong></li><li id="9d2c" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated"><strong class="kj hj"> <em class="kw">型号选择</em> </strong></li><li id="8f81" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated"><strong class="kj hj"> <em class="kw">模特培训</em> </strong></li><li id="2437" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated"><strong class="kj hj"> <em class="kw">车型性能</em> </strong></li><li id="2b95" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated"><strong class="kj hj"> <em class="kw">计算成本</em> </strong></li></ul><h1 id="ae25" class="lc jk hi bd jl ld le lf jp lg lh li jt io lj ip jx ir lk is kb iu ll iv kf lm bi translated">介绍</h1><p id="16f6" class="pw-post-body-paragraph ln lo hi kj b kk kl ij lp km kn im lq ju lr ls lt jy lu lv lw kc lx ly lz kr hb bi translated">命名实体识别，也称为标记分类，是属于自然语言处理的一个子任务，它试图定位非结构化文本中提到的命名实体并将其分类为预定义的类别，如组织、位置、人名、时间表达式、数量、货币价值等。它有许多应用和用例，如内容分类、自动摘要、推荐系统改进或搜索引擎算法优化。</p><p id="1ac0" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">NER模型是分类器，它将文本编码到一个n维空间中(其中相似的单词将彼此靠近出现)，然后应用分类层将每个编码的单词与其类别匹配。目前有许多预训练的NER模型，具有不同的编码步骤架构，如<a class="ae ma" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> word2vec </a>、<a class="ae ma" href="https://spacy.io/api/tok2vec" rel="noopener ugc nofollow" target="_blank"> tok2vec </a>或<a class="ae ma" href="https://towardsdatascience.com/transformers-89034557de14" rel="noopener" target="_blank"> transformer </a>。预训练模型易于使用的界面的两个例子是SpaCy和🤗拥抱脸。然而，大多数模型都是用从网络上提取的通用语料库(例如维基百科)进行训练的，这意味着如果你处理特定主题的数据，这些模型可能会表现不佳。此外，随着模型复杂性的增加，会出现新的限制，例如对GPU等计算资源的需求。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/07878d016bd153a2bdec084d51c3a014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OMksd-OzO5p4sYK4"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">图像:空间和🤗拥抱脸</figcaption></figure><p id="aeba" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">在Clarity AI中，我们每天处理超过10万条商业新闻和公司报告，以便从中提取有意义的数据，并将其映射到我们影响力投资技术平台中的3万多家不同公司。为了做到这一点，我们利用NER模型来识别新闻中的公司，并将它们映射到Clarity的投资组合中。</p><p id="65d8" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">开始时，我们使用SpaCy预训练模型，如“en_core_web_{size}”来完成这些任务，但我们意识到，由于我们的数据相对于训练语料库的复杂性，它们没有达到预期的性能。其中一个主要问题是，在预测一些实体时，非常依赖于大写字母，比如那些与人名或位置混淆的组织。例如:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/990d66ee6865baad293775b29750cc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-m_Yw0VjCgkmPvLoInnmA.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">图:使用“en_core_web_sm”模型的组织预测</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/108e1dad380eca429101477fc68bce6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bAgo10DRIPEB1EkA3d1AQg.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">图:具有自定义NER的组织预测</figcaption></figure><p id="33d6" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">为此，我们决定训练我们自己的NER模型，下面我会告诉你为什么这是值得的。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><h1 id="1c5f" class="lc jk hi bd jl ld mt lf jp lg mu li jt io mv ip jx ir mw is kb iu mx iv kf lm bi translated"><strong class="ak">数据收集</strong></h1><p id="a460" class="pw-post-body-paragraph ln lo hi kj b kk kl ij lp km kn im lq ju lr ls lt jy lu lv lw kc lx ly lz kr hb bi translated">数据集由我们的DS团队使用来自AWS的<a class="ae ma" href="https://aws.amazon.com/augmented-ai/" rel="noopener ugc nofollow" target="_blank">增强人工智能</a>手动标记的1k商业新闻组成:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es my"><img src="../Images/2c5063376692c15b07d40726e71ff5c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLWJPpnR68WWsOIB5_CT2w.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">图像:A2I数据注释工具</figcaption></figure><p id="27e4" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">还有其他好的注释工具，如Prodigy、LightTag等。这也是一个很好的选择。</p><p id="4f31" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">为了测量模型的性能，标签被转换成IOB2格式(其他有效的格式是罗比或IOB)。IOB2格式是inside-outside-beginning的缩写，如果实体是第一个令牌，则在实体前添加前缀“B-”,如果不是实体的第一个令牌，则添加前缀“I-”,如果没有实体，则添加“0”。以下是原始数据的一个示例:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mz"><img src="../Images/b370a1a5df45d9c39d6275eeb942bfc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pNjXNsNHHd41OFAS5lpFfQ.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">表格:原始数据示例</figcaption></figure><p id="2298" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">最后，数据集被随机分为训练(80%)、验证(10%)和测试(10%)。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><h1 id="a7e7" class="lc jk hi bd jl ld mt lf jp lg mu li jt io mv ip jx ir mw is kb iu mx iv kf lm bi translated"><strong class="ak">型号选择</strong></h1><p id="744f" class="pw-post-body-paragraph ln lo hi kj b kk kl ij lp km kn im lq ju lr ls lt jy lu lv lw kc lx ly lz kr hb bi translated">由于本文的目的是证明训练一个定制的NER优于预训练的模型，我将从头开始采用一个SpaCy tok2vec + NER架构，并用我们标记的数据训练它。然后，我会将它的性能与作为零射击学习者的<a class="ae ma" href="https://spacy.io/models/en" rel="noopener ugc nofollow" target="_blank"> SpaCy预训练模型</a>以及变形金刚BERT和RoBERTa进行比较。</p><p id="819f" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">根据其架构，这些模型可分为两类:</p><p id="9933" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated"><strong class="kj hj"> Tok2Vec + NER </strong></p><ul class=""><li id="1cf3" class="kh ki hi kj b kk mb km mc ju na jy nb kc nc kr ks kt ku kv bi translated">空间的核心网页(13 MB)</li><li id="6646" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated">空间的en_core_web_md (45 MB)</li><li id="6349" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated">SpaCy的en_core_web_lg (774 MB)</li><li id="3a46" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated">自定义NER (831 MB)</li></ul><p id="a39d" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated"><strong class="kj hj">变压器+ NER </strong></p><ul class=""><li id="6d9e" class="kh ki hi kj b kk mb km mc ju na jy nb kc nc kr ks kt ku kv bi translated">空间的en_core_web_trf (460 MB)</li><li id="af7a" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated">伯特(450兆字节)</li><li id="49d4" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated">罗伯塔(470兆字节)</li></ul><p id="b9f2" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">目前，还有其他更接近SOTA的NER模型，如XLM-罗伯塔，但由于缺乏计算资源而被丢弃(众所周知，随着你越来越接近SOTA，模型越来越大)。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><h1 id="1d85" class="lc jk hi bd jl ld mt lf jp lg mu li jt io mv ip jx ir mw is kb iu mx iv kf lm bi translated"><strong class="ak">模特培训</strong></h1><p id="5e78" class="pw-post-body-paragraph ln lo hi kj b kk kl ij lp km kn im lq ju lr ls lt jy lu lv lw kc lx ly lz kr hb bi translated">使用SpaCy v3对模型进行训练，这使得用不到5行代码训练/微调NER模型，并从<a class="ae ma" href="https://huggingface.co/models?pipeline_tag=token-classification&amp;sort=downloads" rel="noopener ugc nofollow" target="_blank">导入模型🤗拥抱Face Hub </a>在这里可以找到最新的NLP架构。下面是做这件事的步骤:</p><ol class=""><li id="c0b6" class="kh ki hi kj b kk mb km mc ju na jy nb kc nc kr nd kt ku kv bi translated"><strong class="kj hj">将训练数据转换成SpaCy的二进制格式(。空间)</strong></li></ol><p id="9042" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">这种新格式由一个序列化的<a class="ae ma" href="https://spacy.io/api/docbin" rel="noopener ugc nofollow" target="_blank"> DocBin </a>对象组成。它在存储方面效率极高，尤其是在将多个文档打包在一起时。</p><p id="450d" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">为了转换数据，SpaCy v3有<a class="ae ma" href="https://spacy.io/api/cli" rel="noopener ugc nofollow" target="_blank">转换</a>功能，让我们从SpaCy v2 json获得二进制格式。iob或者。conllu格式(点击此<a class="ae ma" href="https://github.com/explosion/spaCy/tree/master/extra/example_data/ner_example_data" rel="noopener ugc nofollow" target="_blank">链接</a>了解更多信息):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/542a869d8e0a116c8decf7c6411dfbf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HNULaKAmYPdEUT1q"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">代码:将数据转换为空间二进制格式的CLI命令</figcaption></figure><p id="b38a" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">2.<strong class="kj hj">在config.cfg文件中定义所有设置和超参数</strong></p><p id="30be" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">要创建它，可以从<a class="ae ma" href="https://spacy.io/usage/training" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/training</a>下载一个模板，在那里你可以选择训练数据的语言、管道的组件、你的硬件(CPU | GPU)和优化目标。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nf"><img src="../Images/2b6c2504da303bb5bdbd4f67616331f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cGFQ0NIqNmkQO86z"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">图:创建config.cfg模板的用户界面</figcaption></figure><p id="da25" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">请注意，如果您选择CPU，NER任务的默认管道将是tok2vec + NER，如果您选择GPU，tok2vec将被替换为transformer架构。这里的主要区别是transformer模型已经预先训练好了(默认情况下会加载一个RoBERTa模型),作为一个零起点的学习者，它会有很好的表现。然而，tok2vec需要经过训练才能达到良好的效果。</p><p id="a1d8" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">该模板下载名为base_config.cfg的配置文件的简化版本，其中包含基本设置。一旦您用适合您的任务的值覆盖了它，您就可以通过这个命令用默认的高级设置完成该文件:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/bcf59b29896bfb5b0fd1bfe42f0fe714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/0*FUdAyEnpSLO9ddKP"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">代码:将base_config.cfg转换为config.cfg的CLI命令</figcaption></figure><p id="32bc" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">3.<strong class="kj hj">从CLI启动培训</strong></p><p id="52ef" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">这里是魔法出现的地方。使用train命令，只需指定config.cfg位置和输出路径，我们就可以训练/微调任何模型:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nh"><img src="../Images/204fec7cc9ab2d3787fa94e6cffc60dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ON6_iS2M6lFR5NtD"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">代码:使用CLI微调RoBERTa模型的示例</figcaption></figure><p id="fa86" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">在这篇文章中，你可以一步一步地找到如何达到目标。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><h1 id="1196" class="lc jk hi bd jl ld mt lf jp lg mu li jt io mv ip jx ir mw is kb iu mx iv kf lm bi translated">模型性能</h1><p id="5b01" class="pw-post-body-paragraph ln lo hi kj b kk kl ij lp km kn im lq ju lr ls lt jy lu lv lw kc lx ly lz kr hb bi translated">由于NER任务是分类问题，我们可以使用与定量模型中相同的度量标准。然而，精度和召回的定义有一点变化:</p><ul class=""><li id="03a5" class="kh ki hi kj b kk mb km mc ju na jy nb kc nc kr ks kt ku kv bi translated"><strong class="kj hj">精度</strong>:预测好的实体的比率。</li><li id="e748" class="kh ki hi kj b kk kx km ky ju kz jy la kc lb kr ks kt ku kv bi translated"><strong class="kj hj">召回</strong>:预测的真实实体比率。</li></ul><p id="bb89" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">例如，如果我们有80个真实实体，并且我们的模型总共预测了60个实体，其中40个被很好地预测，我们的精度将是66% (40/60)，我们的召回率是50% (40/80)。</p><p id="7d56" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">为了测量模型的性能，我选择了包<a class="ae ma" href="https://github.com/chakki-works/seqeval" rel="noopener ugc nofollow" target="_blank"> <em class="kw"> seqeval </em> </a>，因为它比较完整的实体，而不是单独的令牌。此外，它允许不同的数据格式，如IOB，IOB2或罗比。下一个例子展示了它的工作原理:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ni"><img src="../Images/4f05228a210601e19876c6d9bf998546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dNqwmbWY59lrBQyPt7eTGw.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">代码:seqeval包的示例</figcaption></figure><p id="2b03" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">尽管MISC和DATE实体的预测对于某些令牌是正确的，但是由于完整的实体不一致，因此它得到的分数是0。</p><p id="5169" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">以下是获得的结果:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nj"><img src="../Images/be55cd595fee90403cc433056fe1f48c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpRAZCXkVEz9155l5U4QQA.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">图表:每个型号的精确度与召回率</figcaption></figure><p id="c9f1" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">从图表中可以看出，采用tok2vec架构的SpaCy预训练模型远没有良好的性能。它们的召回率和准确率都没有超过50%。然而，定制NER，尽管也有tok2vec架构，也达到了与变压器模型相似的性能。表现最好的是精确的BERT和回忆的RoBERTa。</p><p id="5114" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">此时此刻，你可能会猜测:如果我已经预先训练好的变形金刚表现超过了它的效果，我为什么还要训练一个定制的NER呢？嗯，答案是计算资源和计算成本。像BERT或RoBERTa这样的变压器模型需要一个GPU来微调它们或进行预测。此外，随着模型复杂性和性能的提高，它们的执行时间也会增加。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><h1 id="ff23" class="lc jk hi bd jl ld mt lf jp lg mu li jt io mv ip jx ir mw is kb iu mx iv kf lm bi translated"><strong class="ak">计算成本</strong></h1><p id="99d5" class="pw-post-body-paragraph ln lo hi kj b kk kl ij lp km kn im lq ju lr ls lt jy lu lv lw kc lx ly lz kr hb bi translated">除了分类分数之外，执行时间在衡量模型性能时也有重要作用。由于我们要处理大量的文档，因此有必要选择一个计算成本低的模型。</p><p id="6520" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">下面显示了测量预测我们的标记数据所花费的执行时间的结果:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/0d5152ea266bfcf0425d9c1f153f00d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7Wv_dWZzyDoS4FEelGafw.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">图表:每个模型的执行时间</figcaption></figure><p id="711b" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">这张图表阐明了复杂性和计算成本之间的关系。尽管具有非常好的性能，transformer架构在预测实体上花费的时间比tok2vec架构多2.5倍。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nl"><img src="../Images/201d9ae7ca51c1ae8953642eda39e562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qkfgm0reJjvLUTygnokNYw.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">图表:召回与每个型号的执行时间</figcaption></figure><p id="dea9" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">由于这个原因，当我们处理大量数据时，使用自定义NER而不是更复杂的架构是很有趣的，因为它在性能和计算成本之间有最佳的权衡。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><h1 id="17e5" class="lc jk hi bd jl ld mt lf jp lg mu li jt io mv ip jx ir mw is kb iu mx iv kf lm bi translated"><strong class="ak">结论</strong></h1><p id="2932" class="pw-post-body-paragraph ln lo hi kj b kk kl ij lp km kn im lq ju lr ls lt jy lu lv lw kc lx ly lz kr hb bi translated">如果你面临一个NER的任务，你必须问自己你的模型是否如它应该的那样运行。由于大多数预训练模型是用从网络(如维基百科)中提取的一般语料库训练的，如果你处理特定主题的数据，你的NER可能表现不佳。因此，培养自己的NER是解决之道。</p><p id="ee83" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">在本文中，我将展示自定义NER如何超越SpaCy的预训练模型，以及当您处理大量数据时，为什么它是一个很好的选择，因为与BERT或RoBERTa等复杂模型相比，它的计算成本较低。</p><p id="c7c5" class="pw-post-body-paragraph ln lo hi kj b kk mb ij lp km mc im lq ju md ls lt jy me lv lw kc mf ly lz kr hb bi translated">我希望你喜欢这篇文章，如果你有任何意见或建议，请随时与我联系。</p></div></div>    
</body>
</html>