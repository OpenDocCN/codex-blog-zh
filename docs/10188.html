<html>
<head>
<title>ADF — Do You Know You Can Split a Large File Into Multiple Small Files During Copy?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ADF —您知道在复制过程中可以将一个大文件拆分成多个小文件吗？</h1>
<blockquote>原文：<a href="https://medium.com/codex/adf-do-you-know-you-can-split-a-large-file-into-multiple-small-files-b423d518e54e?source=collection_archive---------7-----------------------#2022-12-11">https://medium.com/codex/adf-do-you-know-you-can-split-a-large-file-into-multiple-small-files-b423d518e54e?source=collection_archive---------7-----------------------#2022-12-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1c52" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">本文描述了如何使用Azure Data Factory的复制活动将数据导出到较小的数据文件中，而不是一个大文件中。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/57204cc423765a7fe03ad8e64716e528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i1yBdCXlq67Va4S5n2vb2Q.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">pic credit—<a class="ae jn" href="https://blogs.sas.com/content/sgf/2020/07/23/splitting-a-data-set-into-smaller-data-sets/" rel="noopener ugc nofollow" target="_blank">https://blogs . SAS . com/content/sgf/2020/07/23/splitting-a-dataset-into-smaller-dataset/</a></figcaption></figure><h1 id="14e7" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">介绍</h1><p id="9e16" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">Azure Data Factory的复制活动有很多漂亮的特性，但有时会被忽略。在本文中，让我们使用其中的一个特性，我将演示如何在ADF中使用<em class="lc"> max rows per file </em>属性。这允许用户将大数据集分割成小文件。</p><h2 id="f009" class="ld jp hi bd jq le lf lg ju lh li lj jy kp lk ll ka kt lm ln kc kx lo lp ke lq bi translated">源系统</h2><p id="01b9" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">Azure SQL数据库。我创建了一个名为<em class="lc"> mock_data </em>的表，数据量为1，024，000行。为了练习，你可以去<a class="ae jn" href="https://generatedata.com/" rel="noopener ugc nofollow" target="_blank">https://generatedata.com/</a>为自己下载一份测试数据。我的数据看起来如下—</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/c5a2624030f09021bc18b8a9a7a4adbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*mb5W7VYU-H6DIxQTakKZxQ.png"/></div></figure><h2 id="8e09" class="ld jp hi bd jq le lf lg ju lh li lj jy kp lk ll ka kt lm ln kc kx lo lp ke lq bi translated">我需要做什么？</h2><blockquote class="ls lt lu"><p id="758c" class="kg kh lc ki b kj lv ij kl km lw im ko lx ly kr ks lz ma kv kw mb mc kz la lb hb bi translated">我需要将这个源系统的数据提取到一个文件中。由于SQL表包含100多万行数据，我想将大数据集分割成较小的块，并将这些小块导出到ADLSG2上一个文件夹中的单独文件中。</p></blockquote><h2 id="3b72" class="ld jp hi bd jq le lf lg ju lh li lj jy kp lk ll ka kt lm ln kc kx lo lp ke lq bi translated">怎么做？</h2><p id="f469" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">让我们创建一个ADF管道来完成这项工作。这将是一个简单的管道，只有一个用于演示目的的复制活动。</p><p id="0e68" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">来源看起来如下—</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/b2acde6249f913b86d8618cfa42bc9c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*hMdvxciwW5SmiWqeypWenQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">(ADF复印活动)</figcaption></figure><p id="9453" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">水槽的配置如图所示—</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/3d7a7cd2832e34869c56f368ea49f3eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ps5QKZCRnYy15xRqHV4ntQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">(每个文件的ADF最大行数)</figcaption></figure><p id="13cd" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">需要记住的要点是—</p><p id="b62b" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">👉我们可以配置每个文件可以存储的最大行数。在这种情况下，我告诉ADF在每个文件中最多分配100，000条记录。<br/>👉使用<em class="lc">文件名前缀</em>，我们可以为通过这个过程创建的每个较小的文件分配一个前缀。在上面的例子中，我让ADF在开头用SQL_Data标记每个parquet文件名。</p><h2 id="a64d" class="ld jp hi bd jq le lf lg ju lh li lj jy kp lk ll ka kt lm ln kc kx lo lp ke lq bi translated">输出</h2><p id="f242" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">我们最初的SQL表包含1，024，000行。我们要求每个文件最多有100，000行。由此，<br/> <code class="du mf mg mh mi b">1024000 / 100000 = 10.24</code></p><p id="adf6" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">ADF将在我们指定的位置创建11个较小的文件，每个文件最多可容纳100，000行。ADF中的输出告诉我们同样的故事—</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/cb6870eeaee8cb1361c950e8701b43b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdBRhQ_AOsl8B_TeQr7buQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">(ADF每个文件复印的最大行数)</figcaption></figure><p id="ff4c" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">我们可以从activity输出中看到，在这个过程中，ADF生成了11个新文件。</p><p id="db65" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">因为我们将sink数据集指定为一个parquet文件，所以在管道运行后，它们在我们的ADLSG2文件夹中是这样的——</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/49e451998029196627901e43239a47d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQScJ6PD3bFKM_OqdFp2QA.png"/></div></div></figure><p id="c619" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">正如我们所看到的，ADF还为导出的每个文件分配了一个自动增量编号。</p><h1 id="4dd9" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">结尾部分</h1><p id="142c" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">如我们所见，我们能够将1，024，000行的原始数据提取到11个单独的文件中。本例中的接收器是parquet文件，但是该属性也适用于csv文件。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/316354bb818aecff903d81e880e0ba28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7m-wsKos9mAZjN3jWhX0Ow.png"/></div></div></figure><p id="b040" class="pw-post-body-paragraph kg kh hi ki b kj lv ij kl km lw im ko kp ly kr ks kt ma kv kw kx mc kz la lb hb bi translated">到目前为止，我已经通过每个文件最多导出一百万行来测试了这个属性，它工作正常。</p></div></div>    
</body>
</html>