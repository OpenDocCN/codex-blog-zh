# 准确性和辨别力之间的权衡

> 原文：<https://medium.com/codex/the-trade-off-between-accuracy-and-discrimination-887a324b3267?source=collection_archive---------7----------------------->

## 如何在某个领域中导航，在该领域中，某些特征以歧视为代价使您的模型更加准确

![](img/6c48d4e41f62edc7a08b7e86ec6f1ca1.png)

图片由 [DevClass](https://devclass.com/2021/05/20/kotlindl-0-2-deep-learning-framework-for-jvm-progresses/) 提供

人工智能(AI)旨在模仿人类特质，不受情绪波动和有偏见的假设等偏见的影响；它的计算只与数据有关。换句话说，机器学习(ML)模型从未有过糟糕的一天，也不会因为加班而疲惫。由于人工智能不受这些因素的影响，因此有理由想象 ML 模型无法得出歧视性或不合理的输出。也就是说，人工智能的使用本身应该导致遵守公平原则。

为了说明人工智能模型可能会如何进行歧视，让我们回到 2013 年，当时 20 岁的迪伦·富格特和 21 岁的伯纳德·帕克在美国佛罗里达州被捕。他们之前都犯过同样性质的罪行，这次因为持有毒品而被捕。基于他们的犯罪背景、年龄、州和其他特征，他们非常相似。使用风险评估算法，ML 模型通过给出 1 到 10 之间的分数(其中 10 非常可能再次犯罪)来计算再次犯罪的概率，或者换句话说，计算未来犯罪的可能性。尽管有相似之处，伯纳德得了 10 分，迪伦得了 3 分。一段时间后，迪伦因持有毒品被捕，而伯纳德没有任何后续犯罪行为。唯一明显的区别是伯纳德是黑人，而迪伦是白人(安格温等人，2016)。

![](img/6a12f3133ef0f2b9ec33b9083e8d29e9.png)

两次因持有毒品被捕(安格温等人，2016 年)

## 代理歧视

伯纳德和迪伦的故事并不独特，但这并没有限制算法风险评估工具的使用；相反，包括俄克拉荷马州、科罗拉多州、华盛顿州、路易斯安那州、肯塔基州、亚利桑那州、弗吉尼亚州、特拉华州和威斯康星州在内的几个州使用该算法对罪犯进行分类，并在刑事量刑时将结果交给法官。对非裔美国人的偏见是显而易见的。

> 删除种族，但保留教育和父母是否被送进监狱的数据，被批评为种族的替身

虽然新工具的引入试图克服固有的偏见，但我们仍然会通过与其他功能的相关性来发现问题。删除种族但保留教育和父母是否被送进监狱的数据被批评为'*种族【T1 '(Jobes，2018)的替身。这种现象被称为代理歧视或红线。这意味着，即使删除了某些信息，模型也能在相似的数据中找到模式。举个简单的例子，假设有两个特性，一个关于年龄，另一个关于生日。两个特征都充当另一个的代理，因此删除其中一个不会影响模型性能。*

还有一个法律方面，禁止包含某些敏感信息。例如，瑞典禁止在特征分析中使用性别，美国禁止使用年龄、性别和种族等变量(Desiere et al .，2019)。但是，即使是这些国家也没有禁止使用邮政编码、信用分数、语言技能、教育和许多其他变量来代表被忽略的特征。

## 丹麦劳动力市场

一个类似的例子是丹麦劳动力市场，这是非常性别隔离。大多数男子在私营部门工作，而大多数妇女在公共部门工作。假设 ML 模型应该推荐具体的工作岗位，它可能最终会强化这种分化，因为它根据历史数据预测公民进入劳动力市场的路径。另一个例子是貌相，对求职者的统计歧视使移民、残疾人和老年人处于不利地位(Desiere & Struyven，2020 年)。一些国家包括揭示失业者背景的特征。这些特征被标注为出生国、国籍或原籍。虽然包含这些特性的目的是为了提高性能，但这通常是以歧视为代价的。

# 能否实现公平？

这一领域的科学研究可以大致分为两组:一组研究人员试图通过修改基础数据来实现公平。另一组旨在通过修改分类器来实现公平性。这两组的例子如下所示。

## 修改分类器

Pope 和 Sydnor (2009 年)的一项研究提出了处理这种情况的一种方法，其中介绍了在统计特征模型中实施反歧视政策的框架。相对于完全消除特征，它们提供了一种保持模型效率的方法。而不是允许或禁止敏感属性(如血统、种族、性别、年龄等。)，他们建议在训练模型时使用变量，但在执行实际预测时调整变量。他们使用普通最小二乘法(OLS)提出了他们的概念框架，但其逻辑和方法也适用于其他模型。OLS 模型由两种类型的变量组成，一组是“社会可接受的预测者”(SAPs)，另一组是“社会不可接受的预测者”(SUPs)。我们现在有以下场景:

![](img/25afc344167d3353a46ea47827440f8c.png)

这里我们有一个标记为 *y_i* 的输出，我们有 *alpha* ，这是基线，紧挨着它的是一组 sap(*x^sap*)，以及它们各自的系数 *beta* ，还有一组 sup(*x^sup*)，以及它们各自的系数 *theta* 。在正常情况下，我们会删除所有的 sup，这可能会导致遗漏变量偏倚(OVB)。如果一个 SUP 与因变量 *y_i* 和至少一个 SAPs 相关，从方程中移除它将影响*β*系数，使它们不可靠(Pope & Sydnor，2009)。

根据 Pope 和 Sydnor (2009 年)的说法，我们可以通过估计包含 sup 时的系数和使用单个预测发生时的平均值来避免 OVB。例如，我们可能将年龄作为构建模型时使用的 SUP，但是在执行实际预测时，我们使用人口的平均年龄作为值。如果 SUP 是一个分类变量，那么我们将使用虚拟变量，但不是乘以二进制数，而是使用少数民族的人口比例。预测能力将会降低，但不会达到我们在训练模型之前删除变量的程度。这项研究的一个实证例子表明，使用这种方法时，高风险群体中黑人求职者的比例从 22%下降到 16%。

## 修改基础数据

Kamiran 和 Calders (2009)的研究中概述了修改基础数据的一个示例，该研究介绍了一种新的分类方案，用于在有偏训练数据上学习无偏模型。他们提出了一个案例，从“按摩”数据开始；换句话说，他们试图通过对训练数据尽可能少的改变来消除歧视。在文章中，他们使用朴素贝叶斯分类器来计算所有样本的分类概率；然后，他们修改目标变量，直到消除歧视。因此，他们将对数据的更改保持在尽可能最小和无干扰的程度。最后一步是根据校正后的数据训练一个模型，称为无歧视分类(CND)。现在，该模型应该可以在不引入歧视的情况下对个人进行分类。

# 结论

准确性和区分度之间的权衡是应该在模型开发的早期阶段处理的主题。特别是考虑到代理歧视的存在，代理歧视是指基于与歧视性特征相关的特征的可能的间接歧视。

我介绍了研究人员如何试图通过修改算法或基础数据来实现公平，从而解决歧视问题。无论采用哪种方法，了解权衡应该是一个强有力的指标，表明某些模型在没有监督的情况下不能使用。

我希望你像我喜欢写这篇文章一样喜欢它。如果您有任何意见或问题，请随时发表评论。数据科学社区给了我很多，所以我总是乐于回馈。

请随时在 Linkedin 上与我联系，并在 Medium 上关注我以接收更多文章。

# **消息来源**

Angwin，j .，Larson，j .，Mattu，s .和 Kirchner，L. (2016 年)。机器偏差。Propublica。[https://www . propublica . org/article/machine-bias-risk-assessments-in-criminal-pending](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

乔贝斯河(2018)。客观性的诱惑之歌:风险评估工具和种族差异。中等。[https://nacdl . medium . com/from-the-president-the-siren-song-of-objectivity-risk-assessment-tools-and racial-disparity-fa 5 CCB 0698 a5](https://nacdl.medium.com/from-the-president-the-siren-song-of-objectivity-risk-assessment-tools-and-racial-disparity-fa5ccb0698a5)

Desiere，s .，Langenbucher，K. & Struyven，L. (2019 年 2 月 20 日)。公共就业服务中的统计特征:国际比较。经合组织。[https://pdf。语义学者。org/1081/e 2 f 90 fef 96 a 02593171 fb5f 45152326 DC 17 f . pdf](https://www.semanticscholar.org/paper/Statistical-profiling-in-public-employment-services-Desiere-Langenbucher/5d513d82f7a6e32ef5163718afbaac2758cc06bb?p2df)

Desiere，s .和 Struyven，L. (2020 年)。使用人工智能对求职者进行分类:准确性与公平性的权衡。社会政策杂志，1–19。

Pope，D. G .和 Sydnor，J. R. (2009 年)。在统计特征模型中实施反歧视政策，28。

茨韦塔纳·神岛、阿卡霍、阿索和佐久法史(2012 年)。带有偏见消除器正则化器的公平感知分类器[系列标题:计算机科学讲义]。在 P. A. Flach，T. De Bie 和 N. Cristianini(编辑).D. Hutchison、T. Kanade、J. Kittler、J. M. Kleinberg、F. Mattern、J. C. Mitchell、M. Naor、O. Nierstrasz、C. Pandu Rangan、B. Steffen、M. Sudan、D. Terzopoulos、D. Tygar、M. Y. Vardi 和 G. Weikum(类型编辑)，数据库中的机器学习和知识发现(第 35-50 页)。施普林格柏林海德堡。