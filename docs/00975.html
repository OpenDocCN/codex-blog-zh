<html>
<head>
<title>Review — CoGAN: Coupled Generative Adversarial Networks (GAN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述——CoGAN:耦合生成对抗网络(GAN)</h1>
<blockquote>原文：<a href="https://medium.com/codex/review-cogan-coupled-generative-adversarial-networks-gan-273f70b340af?source=collection_archive---------8-----------------------#2021-03-28">https://medium.com/codex/review-cogan-coupled-generative-adversarial-networks-gan-273f70b340af?source=collection_archive---------8-----------------------#2021-03-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="17e9" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph"><a class="ae ge" href="http://medium.com/codex" rel="noopener">法典</a></h2><div class=""/><div class=""><h2 id="a06e" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">通过权重共享，为相同的输入生成不同域中的相关输出，优于<a class="ae jg" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a></h2></div><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es jh"><img src="../Images/e671cc4ee9779db325bc6a573fdb756f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*OmHcX82ah8iNB38fbv4nJw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jt">脸代带笑和不带笑</strong></figcaption></figure><p id="14eb" class="pw-post-body-paragraph ju jv hi jw b jx jy is jz ka kb iv kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi kq translated"><span class="l kr ks kt bm ku kv kw kx ky di">在</span>这个故事中，回顾了三菱电机研究实验室(MERL)的<strong class="jw hs">耦合生成对抗网络(CoGAN) </strong>。</p><blockquote class="kz la lb"><p id="cf17" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">本文研究从数据中学习多域图像的联合分布的问题。</p></blockquote><p id="4dbb" class="pw-post-body-paragraph ju jv hi jw b jx jy is jz ka kb iv kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在本文中:</p><ul class=""><li id="dee9" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated"><strong class="jw hs">单个输入向量可以通过多个</strong><a class="ae jg" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"><strong class="jw hs"/></a><strong class="jw hs">加权共享产生不同域的相关输出。</strong></li><li id="e813" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">可能的应用:产生彩色图像和深度图像，其中这两个图像高度相关，即描述相同的场景，或者具有不同属性(微笑和非微笑)的相同面部的图像。</li></ul><p id="e92d" class="pw-post-body-paragraph ju jv hi jw b jx jy is jz ka kb iv kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这是一篇发表在<strong class="jw hs"> 2016 NIPS </strong>的论文，被引用超过<strong class="jw hs"> 1100次</strong>。(<a class="lu lv ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----273f70b340af--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><h1 id="a8f9" class="md me hi bd jt mf mg mh mi mj mk ml mm ix mn iy mo ja mp jb mq jd mr je ms mt bi translated">概述</h1><ol class=""><li id="556f" class="lg lh hi jw b jx mu ka mv kd mw kh mx kl my kp mz lm ln lo bi translated"><strong class="jw hs">耦合生成对抗网络</strong></li><li id="270b" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp mz lm ln lo bi translated"><strong class="jw hs">实验结果</strong></li></ol></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><h1 id="816c" class="md me hi bd jt mf mg mh mi mj mk ml mm ix mn iy mo ja mp jb mq jd mr je ms mt bi translated"><strong class="ak"> 1。耦合生成对抗网络(CoGAN) </strong></h1><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es na"><img src="../Images/4fb9306a0061ecd75eaf80fe07553cc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JY9G6jB3CRpjQ2fN3yBugA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jt">耦合生成对抗网络</strong></figcaption></figure><ul class=""><li id="a103" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">上图所示的CoGAN是为<strong class="jw hs">学习两个不同域中图像的联合分布而设计的。</strong></li><li id="d793" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated"><strong class="jw hs">它由一对</strong><a class="ae jg" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"><strong class="jw hs">GAN</strong></a><strong class="jw hs">s—</strong><a class="ae jg" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"><strong class="jw hs">GAN</strong></a><strong class="jw hs">1和</strong><a class="ae jg" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"><strong class="jw hs">GAN</strong></a><strong class="jw hs">2</strong>组成；每个人负责合成一个领域的图像。</li></ul><blockquote class="kz la lb"><p id="018d" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">通过权重共享，经过训练的CoGAN可以用于合成成对的对应图像——成对的图像共享相同的高级抽象，但具有不同的低级实现。</p></blockquote><h2 id="bde0" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">1.1.发电机</h2><ul class=""><li id="57ce" class="lg lh hi jw b jx mu ka mv kd mw kh mx kl my kp ll lm ln lo bi translated"><em class="lc"> g </em> 1和<em class="lc"> g </em> 2均被实现为多层感知器(MLP):</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es ns"><img src="../Images/0c951e2ad6381b727d564aa8f84aebe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dc3SW4ZfWxnUWQcyDEoSdg.png"/></div></div></figure><ul class=""><li id="2cb6" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">其中<em class="lc"> g </em> ( <em class="lc"> i </em> )1、<em class="lc"> g </em> ( <em class="lc"> i </em> )2为<em class="lc"> g </em> 1、<em class="lc"> g </em> 2、<em class="lc"> m </em> 1、<em class="lc"> m </em> 2为<em class="lc"> g </em> 1、<em class="lc"> g </em> 2中的层数。</li><li id="bded" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">通过多层感知器操作，生成模型逐渐将信息从更抽象的概念解码为更具体的细节。</li><li id="2007" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated"><strong class="jw hs">第一层解码高层语义</strong> <strong class="jw hs">最后一层解码低层细节。</strong></li><li id="41b1" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">最后一层没有强制约束。</li></ul><blockquote class="kz la lb"><p id="c469" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">想法是<strong class="jw hs">强制<em class="hi"> g </em> 1和<em class="hi"> g </em> 2的第一层具有相同的结构并分担重量。</strong></p><p id="96bc" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">通过权重共享，这对图像可以共享相同的高级抽象，但是具有不同的低级实现。</p></blockquote><h2 id="c235" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">1.2.鉴别器</h2><ul class=""><li id="be10" class="lg lh hi jw b jx mu ka mv kd mw kh mx kl my kp ll lm ln lo bi translated">判别模型将输入图像映射到概率得分，估计输入来自真实数据分布的可能性。</li><li id="e941" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated"><strong class="jw hs">判别模型的第一层提取低级特征，而最后一层提取高级特征。</strong></li><li id="9205" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">与生成器类似，<strong class="jw hs">最后一层是重量共享的。</strong></li></ul><blockquote class="kz la lb"><p id="d621" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">但是后来发现它对合成图像的质量没有多大帮助。但是仍然使用重量分配。</p><p id="8187" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">这是因为<strong class="jw hs">鉴别器中的权重共享约束有助于减少网络</strong>中的参数总数，尽管这对于学习联合分布并不重要。</p></blockquote><h2 id="4024" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">1.3.学问</h2><ul class=""><li id="81e5" class="lg lh hi jw b jx mu ka mv kd mw kh mx kl my kp ll lm ln lo bi translated">在游戏中，有两个队，每个队有两名球员。</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es nt"><img src="../Images/721eeb7d3cb0fc8676ef477278158297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldl9LYZdeFCt8LJ0useeIg.png"/></div></div></figure><ul class=""><li id="9e1c" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">类似于<a class="ae jg" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a>，<strong class="jw hs"> CoGAN可以通过具有交替梯度更新步骤的反向传播来训练。</strong></li></ul><blockquote class="kz la lb"><p id="3a32" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">基本上，交替梯度更新步骤是逐个训练2个鉴别器，然后交替地逐个训练2个生成器。</p></blockquote><ul class=""><li id="d453" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">对于不同的应用，例如数字生成和面部生成，网络架构是不同的，如下所述。</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es et"><img src="../Images/c815582984467b2b4cfa042cdfdf3461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4VtQJ45GJ-UCdAv1J6SszA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jt">数字生成的网络架构</strong></figcaption></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es nu"><img src="../Images/53da39966c7c0b0a13ce3a16af033033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IBXNi1-OgQhCs-Eom1-USQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jt">人脸生成的网络架构</strong></figcaption></figure><ul class=""><li id="762c" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">(网络体系结构和培训细节在论文的补充材料中。请随意访问报纸。)</li></ul></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><h1 id="d05c" class="md me hi bd jt mf mg mh mi mj mk ml mm ix mn iy mo ja mp jb mq jd mr je ms mt bi translated">2.<strong class="ak">实验结果</strong></h1><h2 id="018e" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">2.1.数字生成</h2><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es nv"><img src="../Images/ed4140565725c578e408feef9cbbf079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVf03UHG2zIxGlS51K64Nw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jt">左:边MNIST，右:负MNIST </strong></figcaption></figure><ul class=""><li id="fbdf" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated"><strong class="jw hs">左(任务A) </strong>:如图所示，使用相同的输入向量，CoGAN可以生成具有正常和基于边缘形式的相同数字图像。</li><li id="5c0d" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated"><strong class="jw hs">右(任务B) </strong>:阳性和阴性MNIST的结果相似。</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es nv"><img src="../Images/05c510903a77ad8ac36b67163bc6010e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vmq5-DBmHX3lZIG0NoY8Nw.png"/></div></div></figure><ul class=""><li id="664d" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">这些图绘制了任务A和任务b的具有不同权重分配配置的CoGANs的平均像素一致率<strong class="jw hs">。像素一致率越大，配对生成性能越好。</strong></li></ul><blockquote class="kz la lb"><p id="2512" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">发现<strong class="jw hs">性能与生成模型中的权重共享层数正相关</strong>，但与判别模型中的权重共享层数不相关。</p></blockquote><ul class=""><li id="91f0" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">作为比较，实现了有条件的GAN ( <a class="ae jg" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a>)。以0输入作为进入<a class="ae jg" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a>的条件，发生器类似于第一域中的图像；否则，它在第二域中生成图像。</li><li id="219e" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">对于任务A，CoGAN获得了0.952的平均比率，超过了由<a class="ae jg" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a>获得的0.909。</li><li id="889b" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">对于任务B，科根取得了0.967的成绩，比<a class="ae jg" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a>取得的0.778好得多。</li></ul><h2 id="794b" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">2.2.人脸生成</h2><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es nw"><img src="../Images/924f02d01c7dc151f665ca44daf73ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbxseUy6WvqmvOJoh8PDWQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jt">使用CoGAN生成不同属性的人脸图像。</strong></figcaption></figure><ul class=""><li id="d1ec" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">从上到下，该图显示了金发、微笑和眼镜属性的人脸生成结果。</li><li id="0f78" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">对于每一对，第一行包含具有属性的面，而第二行包含相应的不具有属性的面。</li></ul><blockquote class="kz la lb"><p id="face" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated"><strong class="jw hs">随着在空间中旅行，人脸逐渐从一个人变成另一个人。这种变形在两个区域都是一致的。</strong></p><p id="002b" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">请注意<strong class="jw hs">很难为某些属性(如金发)创建具有相应图像的数据集，因为受试者必须对头发进行染色。</strong></p></blockquote><h2 id="17ac" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">2.3.颜色和深度图像生成</h2><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="er es nx"><img src="../Images/e02c09d8ef5532d1b11a4e1b6cb61f39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ag-bYI_La86tQrPkpAWvgw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jt">使用CoGAN生成彩色和深度图像。</strong></figcaption></figure><ul class=""><li id="d02e" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated">上图显示了RGBD数据集的结果:第一行包含彩色图像，第二行包含深度图像，第三和第四行显示了不同视点下的深度剖面。</li><li id="58d8" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">下图显示了NYU数据集的结果。</li></ul><blockquote class="kz la lb"><p id="efa5" class="ju jv lc jw b jx jy is jz ka kb iv kc ld ke kf kg le ki kj kk lf km kn ko kp hb bi translated">CoGAN不知不觉地恢复了外观与深度的对应关系。</p></blockquote><h2 id="b7cf" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">2.4.潜在应用</h2><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ny"><img src="../Images/b2c663c62b8fbe4513a00248e3da0353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*JzQ9AAgL749eu48Ells60Q.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jt">无监督领域适应性能比较。</strong></figcaption></figure><ul class=""><li id="98a6" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated"><strong class="jw hs">无监督的域适应(UDA) </strong> : UDA涉及适应在一个域中训练的分类器，以在新的域中对样本进行分类，其中在新的域中没有用于重新训练分类器的标记样本。</li><li id="c07d" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">(领域适配不是本文的主要贡献。所以，我不深究。如果感兴趣，请随意阅读该文件。)</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es nz"><img src="../Images/aa2a75a28bcd584ab9633db283b4af84.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*Ltga6mcJQmlsg9J2dHlhsQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">跨域图像变换。</figcaption></figure><ul class=""><li id="9777" class="lg lh hi jw b jx jy ka kb kd li kh lj kl lk kp ll lm ln lo bi translated"><strong class="jw hs">跨域图像变换</strong>:对于每一对，左为输入；右图是变换后的图像。</li><li id="c2ea" class="lg lh hi jw b jx lp ka lq kd lr kh ls kl lt kp ll lm ln lo bi translated">(作者只是想在这部分介绍潜在的应用。如果对细节感兴趣，请参考论文。)</li></ul></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><p id="45e9" class="pw-post-body-paragraph ju jv hi jw b jx jy is jz ka kb iv kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">后来，作者扩展了CoGAN，使其具有图像到图像的翻译，并在2017年NIPS上发表。希望我能在未来的日子里回顾它。</p></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><h2 id="9de4" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">参考</h2><p id="5050" class="pw-post-body-paragraph ju jv hi jw b jx mu is jz ka mv iv kc kd oa kf kg kh ob kj kk kl oc kn ko kp hb bi translated">【2016 NIPS】【科根】<br/> <a class="ae jg" href="https://arxiv.org/abs/1606.07536" rel="noopener ugc nofollow" target="_blank">耦合生成对抗网络</a></p><h2 id="3e03" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated">生成对抗网络</h2><p id="907a" class="pw-post-body-paragraph ju jv hi jw b jx mu is jz ka mv iv kc kd oa kf kg kh ob kj kk kl oc kn ko kp hb bi translated"><strong class="jw hs">图像合成</strong> [ <a class="ae jg" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a> ] [ <a class="ae jg" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a> ] [ <a class="ae jg" rel="noopener" href="/@sh.tsang/review-lapgan-laplacian-generative-adversarial-network-gan-e87200bbd827">拉普甘</a>[<a class="ae jg" rel="noopener" href="/@sh.tsang/review-dcgan-deep-convolutional-generative-adversarial-network-gan-ec390cded63c">DCGAN</a>][<a class="ae jg" href="https://sh-tsang.medium.com/review-cogan-coupled-generative-adversarial-networks-gan-273f70b340af" rel="noopener">CoGAN</a>]<br/><strong class="jw hs">图像到图像的平移</strong>[<a class="ae jg" href="https://sh-tsang.medium.com/review-pix2pix-image-to-image-translation-with-conditional-adversarial-networks-gan-ac85d8ecead2" rel="noopener">pix 2 pix</a>]<br/><strong class="jw hs">超分辨率</strong>[<a class="ae jg" rel="noopener" href="/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490">SRGAN&amp;SRResNet</a>[<a class="ae jg" rel="noopener" href="/@sh.tsang/reading-enhancenet-automated-texture-synthesis-super-resolution-8429635aa75e"> <br/> <strong class="jw hs">摄像头篡改检测</strong></a><a class="ae jg" href="https://sh-tsang.medium.com/review-mantinis-visapp-19-generative-reference-model-and-deep-learned-features-camera-f608371c9854" rel="noopener">曼蒂尼的VISAPP’19</a><strong class="jw hs"><br/>视频编码</strong><a class="ae jg" rel="noopener" href="/@sh.tsang/reading-vc-lapgan-video-coding-oriented-laplacian-pyramid-of-generative-adversarial-networks-74daa2d23d3c">VC-lap gan</a><a class="ae jg" href="https://sh-tsang.medium.com/review-zhu-tmm20-generative-adversarial-network-based-intra-prediction-for-video-coding-c8a217c564ea" rel="noopener">朱TMM’20</a><a class="ae jg" href="https://sh-tsang.medium.com/review-zhong-elecgj21-a-gan-based-video-intra-coding-hevc-intra-9e3486dbca78" rel="noopener">钟elec gj’21</a></p><h2 id="67d2" class="nf me hi bd jt ng nh ni mi nj nk nl mm kd nm nn mo kh no np mq kl nq nr ms ho bi translated"><a class="ae jg" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>