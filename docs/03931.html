<html>
<head>
<title>Understanding Variational Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解可变自动编码器</h1>
<blockquote>原文：<a href="https://medium.com/codex/understanding-variational-autoencoders-72e56c3e03d?source=collection_archive---------10-----------------------#2021-10-08">https://medium.com/codex/understanding-variational-autoencoders-72e56c3e03d?source=collection_archive---------10-----------------------#2021-10-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ab66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个博客不打算关注理解变型自动编码器背后的严格数学，在互联网上有大量的资源可供参考(<a class="ae jd" href="https://youtu.be/uaaqyVS9-rM" rel="noopener ugc nofollow" target="_blank"> Ali Ghodsi的讲座</a>是一个很好的资源)。我想做的是，希望提供一些有用的直觉，变分自动编码器是什么，它的损失函数实际上直观地定义了什么(这是我理解它的方式)。我看过的视频和文章主要是深入事物的本质细节，而不是看事物的平易近人的观点。</p><p id="5c6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可变自动编码器的目标是生成性的:这意味着在我们完成模型训练后，我们希望从中抽取与我们训练模型的内容模糊相似的东西。想想普通的自动编码器会做什么:假设我们有一个256维的图像；首先，我们将其编码到某个较低的维度，比如2维空间。然后，解码器对图像的低维表示进行解码。现在，我想让你为MNIST数据想象这个过程:也许编码器在二维空间的不同区域编码不同的数字。想象在2d空间中每个数字有一个不同的区域。解码器然后学习解码图像的这些较低维度的表示。比如说，数字6出现了。编码器将其编码为数字6区域中某处的较低维度表示；解码器然后解码这个较低维度的表示。现在，只要我们知道每个数字区域在哪里，我们就可以从这些区域中随机抽取一个2d矢量，并将其传递给解码器，以获得新的图像。也许我们可以从数字9或其他什么的区域采样一个2d矢量。问题是，二维空间是巨大的，选择一个随机的二维向量是不会削减它的，所以我们需要以某种方式强制编码器在某种我们可以采样的有限空间中对表示进行编码。这就是可变自动编码器的聪明之处。</p><p id="8383" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们假设数据(MNIST)是这样生成的:2d中的数字有不同的区域(概率分布),我们用某种概率分布选取某个数字的区域。选择区域后，我们假设它有某种自己的概率分布来生成样本，比如说，正态分布。生成图像后，再将其馈送给解码器以生成图像。现在的想法是把这些不同数字的区域限制在某个封闭的空间里，这样我们就可以采样了。</p><p id="ef21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是一个常规的自动编码器(一个隐藏层)的管道看起来是什么样子:我们有一个训练示例x；它被映射到n维的隐藏状态z(较低的维表示)，然后解码器解码这个隐藏状态。维数n很大，隐藏态可以在任何地方。换句话说，状态z有一些概率分布p(z|x)。我们不知道这个概率分布，这是编码器内部创建的分布。为了从n维隐藏状态z的分布中取样，我们必须得到p(z|x)。你可以读到p(z|x)的难处理性，但主要思想是，由于我们不知道p(z|x)-模型创建的分布-我们将强制模型具有我们想要的分布，比如说，标准高斯分布(0均值，恒等协方差矩阵)。那是什么意思？这意味着，如果我们设法做到这一点，那么p(z|x)，底层编码器的分布，将是一个标准的高斯分布，这意味着，每个类的状态z，将在两个2d的单位圆上的某处，在3d的单位球上，等等，因为这是标准高斯分布的形状。</p><p id="8d3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">问题是我们如何做到这一点？我们最小化模型分布和单位高斯分布之间的KL散度:我们让神经网络输出n维空间中的平均值和方差，该平均值和方差将参数化模型的高斯分布，并且将由此采样随机隐藏状态z。模型的这种分布和单位高斯之间的KL散度将对损失有贡献。问题是，如果我们只是这样做，那么模型将只是为每个类输出单位高斯，这会给解码器带来麻烦，因为它将为每个类获得(几乎)相同的隐藏状态z。因此，一个简单的解决方案是将重建误差添加到损失中。该模型将只输出每个类的0均值和身份协方差，但是它不能这样做，因为这样重构误差将会很高，所以它必须找到一个中间点来保持KL散度损失和重构损失之间的平衡。直观地说，它最终将做的事情如下:编码器将输出接近0的均值和接近单位矩阵的协方差，以便最小化KL散度损失，但是这个均值和方差对于每个类将是不同的，以便最小化重构误差。想象一下MNIST数据集通过这条管道。假设隐藏状态z是二维的，然后假设对于数字6，模型输出一些均值和方差，它将接近单位高斯的均值和方差，即它将在单位圆中，但是它将远离其他数字的均值和方差(为了最小化重建误差)。所以你可以想象我们会在单位圆上刻上不同的高斯曲线，训练后每个数字代表一个高斯曲线。现在，为了在训练后生成数字，我们简单地从单元高斯a状态z中采样，并将其传递给解码器。因为编码器的基本分布被迫接近单位高斯，所以所有数字的隐藏状态将在2d的单位圆中。从高斯单位取样会给我们一个数为z的状态。我们设法把隐藏态的空间从无限的二维限制在二维的单位圆内，这难道不是一个非常聪明的方法吗？第一次看的时候，这背后的数学有点微妙和不直观(至少对我来说是这样)，希望带着一点直觉去看会有帮助。</p></div></div>    
</body>
</html>