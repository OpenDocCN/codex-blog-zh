<html>
<head>
<title>GRUs and LSTMs for Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于自然语言处理的GRUs和LSTMs</h1>
<blockquote>原文：<a href="https://medium.com/codex/grus-and-lstms-for-natural-language-processing-c858010f6d8c?source=collection_archive---------11-----------------------#2021-11-06">https://medium.com/codex/grus-and-lstms-for-natural-language-processing-c858010f6d8c?source=collection_archive---------11-----------------------#2021-11-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3570" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">门控递归单元(GRU)和长短期记忆(LSTM)是递归神经网络(RNN)，它们提供了对传统RNNS的改进，并已被证明在自然语言处理的学习任务中非常有用。</p><p id="1d92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章建立在我的自然语言处理的RNNs文章的基础上，推荐作为先决条件阅读。</p><div class="je jf ez fb jg jh"><a rel="noopener follow" target="_blank" href="/@matthewkramer_20144/rnns-for-natural-language-processing-d7242e6e0842"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">自然语言处理的RNNs</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">rnn对于理解BERT、T5或GPT-3等架构的当前状态至关重要</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">medium.com</p></div></div><div class="jq l"><div class="jr l js jt ju jq jv jw jh"/></div></div></a></div><h1 id="48e7" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">香草味RNNs的不足之处</h1><p id="57dc" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">正如在<a class="ae jd" rel="noopener" href="/@matthewkramer_20144/rnns-for-natural-language-processing-d7242e6e0842">自然语言处理的RNNs</a>中所描述的，传统的RNNs有一个缺点，即在较长的序列中会丢失早期单词的上下文。这通常被称为消失梯度。例如，考虑《低俗小说》中的这个剧本:</p><p id="e551" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">《T4》朱尔斯:我认为她最大的成就是出演了一部试播剧。</p><p id="eb85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文森特:什么是飞行员？</p><p id="fef4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">朱尔斯:嗯，你知道电视上的节目吗？</p><p id="af30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文森特:我不看电视。</p><p id="9104" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">朱尔斯:是的，但是你知道有一项发明叫做电视，在这项发明上他们播放节目？</p><p id="eae3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文森特:是啊。</p><p id="381f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">朱尔斯:他们挑选电视节目的方式是制作一个节目，这个节目叫做试播集。他们向挑选节目的人展示一个节目，根据这个节目的实力，他们决定是否要制作更多的节目。有的被接受成为电视节目，有的没有，什么都不是。她主演了一部一无是处的电影。"</p><p id="463e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们希望构建一个NLP系统来提供语法和拼写建议。该系统将接受低俗小说脚本，并突出显示不符合语法约定的单词，就像微软的Word一样。</p><p id="e464" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑一下剧本中的最后一句话:<em class="la">她出演了其中一部变得一无是处。</em>系统如何确定<em class="la">‘她’</em>是正确的代词？读剧本时，你可以从第一句话中辨别出<em class="la">她</em>是正确的代词:<em class="la">我认为她最大的收获是出演了一部试播剧。</em></p><p id="e3e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，正如您所看到的，确定正确代词所需的第一句话在脚本中出现得更早。一个普通的RNN会冲淡这个上下文，因为它必须通过每个输入单元传递它的隐藏状态，在这个例子中，大约是100个单词。这100个单词对确定正确的代词没有用，而且淹没了第一句话的信号。</p><p id="13b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GRUs和LSTMs试图通过向网络添加专门的门来解决这个问题，以便更容易地“记住”序列的较早部分，并且“忘记”序列的不相关部分。</p><h1 id="0469" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">门控循环网络</h1><p id="a49b" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">回想一下自然语言处理的RNNs，一个普通RNN的基本单元可以是这样的:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lb"><img src="../Images/6e5582b892e572a9ef7ca180bb1c38ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S6NNEjRKv7aEvV9T1Jg_Ig.png"/></div></div></figure><p id="5619" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">隐藏状态h^t从一个递归单元传递到另一个递归单元，每一步都产生一个输出ŷ。在每个单元中，隐藏状态总是根据网络中学习到的权重和偏差更新为新值。GRU在递归网络中增加了一些额外的步骤，以使隐藏状态能够<em class="la">可选地</em>在每个单元更新隐藏状态。这意味着隐藏状态可以几乎不加改变地直接通过。</p><p id="684c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">门控循环单元的简化版本可以概括为:</p><ol class=""><li id="8925" class="lm ln hi ih b ii ij im in iq lo iu lp iy lq jc lr ls lt lu bi translated">类似于我们的普通RNN，连接隐藏状态向量h和输入向量x来创建:[x^t，h^t]</li><li id="ecec" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">制作两个连接向量的副本</li><li id="6ea6" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">类似于我们的普通RNN，乘以权重(Wh)，加上偏差(b_h)，并在级联向量的一个副本上使用双曲正切激活函数——这将是我们的候选值(c^n ),我们可以用它来更新隐藏状态。</li><li id="a4bc" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">引入一组新的权重(Wu)和偏差(b_u ),称为更新权重/偏差，并将连接向量的第二个副本相乘，并将该结果通过sigmoid函数。因为我们使用sigmoid函数，所以结果将是一个非常接近1或0的数字。这个结果被称为更新值，<em class="la"> u </em></li><li id="f62b" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">然后，我们使用这个更新值<em class="la"> u </em>，来确定我们是否应该将先前的隐藏状态传播为新的隐藏状态，或者使用我们的候选值来更新隐藏状态。这可以由以下等式表示，如果<em class="la"> u </em>为0，则保持先前的隐藏状态，或者如果<em class="la"> u </em>为1，则用候选值更新隐藏状态。</li></ol><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ma"><img src="../Images/2c580ed032f576e0b4c19e3bcfe40791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ACgbZXS-vQGySH2Tb4TmA.png"/></div></div></figure><p id="c2f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.最后，要获得当前输出值(ŷ),请将结果输入softmax函数。</p><p id="ac0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图为简单GRU中的每个步骤提供了直观的参考</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mb"><img src="../Images/632f9d16d46abb844ed8501ce2ddd413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CCm4Vu2F-ydpPrgPLjxFw.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">简单GRU视觉图</figcaption></figure><p id="6015" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个简化的GRU也可以用一系列等式来表示(*表示为逐元素乘法):</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mg"><img src="../Images/73bdadf3e55479a0ae21621282b80701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-6cx4k6NzaLF6fSLT5EvkQ.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">简化的GRU方程</figcaption></figure><p id="0fbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GRU的完整版本使用了一个额外的“相关性”门，允许网络学习隐藏状态与每个单元的相关性。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mh"><img src="../Images/bdd06b50b99bd8f0942673f48bf7665c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7G90hd5OkzSRjDw7swAnqA.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">全GRU方程</figcaption></figure><p id="7071" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，我们允许网络学习一些附加参数，以便让网络忽略序列中的某些输入，并且对具有更多噪声的更长序列更有弹性。</p><h1 id="a1e8" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">长短期记忆(LSTM)</h1><p id="7323" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">与GRU类似，LSTMs使网络更容易忽略序列中不相关的部分。然而，LSTMs稍微复杂一点，使用额外的门来实现与GRUs类似的效果。使用单独的权重和偏置来计算三个门:更新门、<em class="la"> u、</em>遗忘门、<em class="la"> f </em>和输出门、<em class="la"> o. </em>不是将先前的隐藏状态乘以更新门的相反值(1 - <em class="la"> u </em>，而是使用遗忘门。此外，输出门用于对来自更新门和遗忘门的结果输出进行加权。这可以用以下等式表示:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mi"><img src="../Images/350281329d563b0a4d6f66d9a33faa15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1RIxmfIjlxVNqr2LjfKoqg.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">LSTM方程</figcaption></figure><h1 id="95cf" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">有用的资源</h1><ul class=""><li id="599d" class="lm ln hi ih b ii kv im kw iq mj iu mk iy ml jc mm ls lt lu bi translated">https://www.coursera.org/learn/nlp-sequence-models<a class="ae jd" href="https://www.coursera.org/learn/nlp-sequence-models" rel="noopener ugc nofollow" target="_blank">吴恩达关于序列模型的课程(点击‘审计课程’可免费观看所有视频)</a></li></ul></div></div>    
</body>
</html>