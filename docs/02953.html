<html>
<head>
<title>How Scrapy Makes Web Crawling Easy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scrapy如何让网页抓取变得简单</h1>
<blockquote>原文：<a href="https://medium.com/codex/how-scrapy-makes-web-crawling-easy-3ed1fc84d7a1?source=collection_archive---------16-----------------------#2021-08-12">https://medium.com/codex/how-scrapy-makes-web-crawling-easy-3ed1fc84d7a1?source=collection_archive---------16-----------------------#2021-08-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f31b73dde702ecbeb293e6c6bc8cabe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BmjJLk_-QYeJzoGx.png"/></div></div></figure><p id="cbed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你对网络抓取感兴趣，或者你可能已经有一些提取数据的脚本，但是不熟悉<a class="ae jo" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>，那么这篇文章就是为你准备的。我将快速浏览Scrapy的基本原理，以及为什么我认为它是大规模刮除的正确选择。我希望你能很快看到这个令人敬畏的框架的价值，并有兴趣了解更多，并考虑将其用于你的下一个大项目。</p><h1 id="4931" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是Scrapy？</h1><p id="91d7" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">Scrapy是一个用Python写的网络抓取框架。您可以利用Python丰富的数据科学生态系统和Scrapy，这使得开发更加容易。</p><p id="a116" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然导言公正，这篇短文旨在向您展示您可以从Scrapy中获得多少价值，并向您介绍它的一些基本概念。这不是对web抓取或Python的介绍，所以我假设您对这种语言有基本的了解，并且至少了解HTTP请求是如何工作的。</p><h1 id="d02d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">Scrapy与其他流行选项相比如何？</h1><p id="50b1" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">如果你以前学过Python web抓取教程，你很可能会遇到BeautifulSoup和requests库。这些提供了一种从网页中提取数据的快速方法，但没有提供Scrapy用于大多数任务的项目结构和合理的默认值。你必须自己处理重定向、重试、cookies等等，而Scrapy可以开箱即用。</p><p id="f717" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可能认为你可以用Selenium或Puppeteer这样的无头浏览器逃脱，毕竟，那会更难阻止。事实是，你可以少花很多资源，但如果你有成百上千的刮刀，那就要付出代价了。</p><h1 id="2a74" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">你如何设置Scrapy？</h1><p id="6f11" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">Scrapy是一个Python包，和其他包一样。您可以在virtualenv中安装pip，如下所示:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ed7c" class="lb jq hi kx b fi lc ld l le lf">$ pip install scrapy</span></pre><p id="d890" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你需要理解的两个概念是Scrapy项目和蜘蛛。一个项目包装了多个蜘蛛，你可以把一个蜘蛛想象成一个特定网站的抓取配置。安装后，您可以像这样启动一个项目:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1cd1" class="lb jq hi kx b fi lc ld l le lf">$ scrapy startproject myprojectname</span></pre><p id="3843" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一个项目将封装所有的蜘蛛、实用程序，甚至部署配置。</p><h1 id="0eae" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">如何抓取一个简单的网页？</h1><p id="ac64" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">蜘蛛处理特定网站所需的一切。它将向网页发出请求，并接收返回的响应。它的职责是处理这些响应并产生更多的请求或数据。</p><p id="7377" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在实际的Python代码中，蜘蛛只不过是一个继承自<code class="du lg lh li kx b">scrapy.Spider</code>的Python类。这里有一个基本的例子:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="4d32" class="lb jq hi kx b fi lc ld l le lf">import scrapy <br/>class MySpider(scrapy.Spider): <br/>name = 'zyte_blog' </span><span id="4ef8" class="lb jq hi kx b fi lj ld l le lf">start_urls = ['https://zyte.com/blog'] </span><span id="007d" class="lb jq hi kx b fi lj ld l le lf">def parse(self, response): <br/>for href in response.css('div.post-header h2 a::attr(href)').getall(): <br/>yield scrapy.Request(href) </span><span id="dc45" class="lb jq hi kx b fi lj ld l le lf">yield scrapy.Request( <br/>url=response.css('a.next-posts-link::attr(href)').get(), callback=self.parse_blog_post, <br/>) </span><span id="9c29" class="lb jq hi kx b fi lj ld l le lf">def parse_blog_post(self, response): <br/>yield { <br/>'url': response.url, <br/>'title': response.css('span#hs_cos_wrapper_name::text').get(), <br/>}</span></pre><p id="c18f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lg lh li kx b">start_urls</code>是开始抓取的URL列表。每个都将产生一个请求，其响应将在回调中接收。默认回调为<code class="du lg lh li kx b">parse</code>。如您所见，回调只是处理响应并产生更多请求或数据点的类方法。</p><h1 id="3cd5" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">如何用Scrapy从HTML中提取数据点？</h1><p id="4ddd" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">你可以用Scrapy的选择器！在<code class="du lg lh li kx b">response</code>对象上有CSS选择器可以直接实现这个功能:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="665a" class="lb jq hi kx b fi lc ld l le lf">link = response.css('a.next-posts-link::attr(href)').get() # extract using class <br/>title = response.css('span#hs_cos_wrapper_name::text').get() # extract using id</span></pre><p id="4ae3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还有XPath选择器，它提供了您最可能需要的更强大的选项。下面是使用XPath的相同选择器:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="840a" class="lb jq hi kx b fi lc ld l le lf">link = response.xpath('//a[contains(@class, "next-posts-link")]/a/@href').get() # extract using class <br/>title = response.xpath('//span[@id="hs_cos_wrapper_name"]/text()').get() # extract using id</span></pre><p id="f58c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，您需要一种将数据返回到可解析格式的方法。有一些功能强大的实用程序，如项目和项目加载器，但最简单的形式是将数据存储到Python字典中:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a7ed" class="lb jq hi kx b fi lc ld l le lf">yield { <br/>'url': response.url, <br/>'title': response.css('span#hs_cos_wrapper_name::text').get(), <br/>}</span></pre><h1 id="e5c8" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">你如何运行一个刺痒蜘蛛？</h1><p id="afc7" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在您的项目目录中，使用上面的示例项目，您可以运行:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1973" class="lb jq hi kx b fi lc ld l le lf">$ scrapy crawl zyte_blog</span></pre><p id="2056" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这将把抓取的数据显示到标准输出中，并记录大量日志，但是您可以通过添加几个选项轻松地只将实际数据重定向到CSV或JSON格式:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="261e" class="lb jq hi kx b fi lc ld l le lf">$ scrapy crawl zyte_blog -o blog_posts.csv</span></pre><p id="e24b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CSV文件的内容:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="2333" class="lb jq hi kx b fi lc ld l le lf">url,title <br/><a class="ae jo" href="https://zyte.com/blog/how-to-get-high-success-rates-with-proxies-3-steps-to-scale-up,How" rel="noopener ugc nofollow" target="_blank">https://zyte.com/blog/how-to-get-high-success-rates-with-proxies-3-steps-to-scale-up,How</a> to Get High Success Rates With Proxies: 3 Steps to Scale Up <br/><a class="ae jo" href="https://zyte.com/blog/data-center-proxies-vs.-residential-proxies,Data" rel="noopener ugc nofollow" target="_blank">https://zyte.com/blog/data-center-proxies-vs.-residential-proxies,Data</a> Center Proxies vs. Residential Proxies <a class="ae jo" href="https://zyte.com/blog/price-intelligence-questions-answered,Your" rel="noopener ugc nofollow" target="_blank">https://zyte.com/blog/price-intelligence-questions-answered,Your</a> Price Intelligence Questions Answered <br/>...</span></pre><h1 id="7ea0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">怎么处理被屏蔽？</h1><p id="4693" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">Scrapy使得管理复杂的会话逻辑变得容易。随着你添加更多的蜘蛛和你的项目变得更加复杂，Scrapy允许你以各种方式阻止禁令。</p><p id="5249" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">调整请求的最基本方法是设置标题。例如，您可以像这样添加一个<code class="du lg lh li kx b">Accept</code>头:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1c9c" class="lb jq hi kx b fi lc ld l le lf">scrapy.Request(url, headers={'accept': '*/*', 'user-agent': 'some user-agent value'})</span></pre><p id="dbcb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可能已经想到，一定有更好的方法来设置它，而不是为每个单独的请求设置它，你是对的！Scrapy允许你为每个蜘蛛设置默认的标题和选项，如下所示:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="8877" class="lb jq hi kx b fi lc ld l le lf">custom_settings = { <br/>'DEFAULT_REQUEST_HEADERS': {'accept': '*/*'}, <br/>'USER_AGENT': 'some user-agent value', <br/>}</span></pre><p id="698e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这既可以在单个蜘蛛上设置，也可以在Scrapy为你定义的<code class="du lg lh li kx b">settings.py</code>文件中设置。</p><p id="fdf2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">但是等等……还有更多！</strong></p><p id="ae08" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你也可以使用<a class="ae jo" href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html" rel="noopener ugc nofollow" target="_blank">中间件</a>来做到这一点。这些可以用于跨蜘蛛添加标题和更多。</p><p id="b762" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">中间件是Scrapy的另一个强大的特性，因为它们允许你处理重定向、重试、cookies等等。这正是Scrapy开箱即用的东西！</p><p id="7a17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用中间件，你可以<a class="ae jo" href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.robotstxt" rel="noopener ugc nofollow" target="_blank">尊重特定网站的robots.txt配置</a>,以确保你不会抓取你不应该抓取的内容。</p><h1 id="2589" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">刮的时候如何做到“善良”？</h1><p id="618e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">网络抓取会对网站产生负面影响，这不是你想要的。为了更好地节省时间，你需要在你的请求之间增加合理的延迟。您可以使用现有的<a class="ae jo" href="https://docs.scrapy.org/en/latest/topics/autothrottle.html" rel="noopener ugc nofollow" target="_blank">自动节流中间件</a>轻松做到这一点。</p><p id="6e18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你也可以设置一个时间间隔，这样你就不会看起来像一个机器人，每2秒请求一次，但产生1到5秒的请求！</p><h1 id="85a3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">Scrapy如何处理代理？</h1><p id="ccff" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">Scrapy中有很多方法可以使用代理。您可以为单个请求设置它们，如下所示:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="123f" class="lb jq hi kx b fi lc ld l le lf">scrapy.Request( <br/>url, <br/>meta={'proxy': 'host:port'}, <br/>)</span></pre><p id="7cfd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">或者使用现有的<a class="ae jo" href="https://docs.scrapy.org/en/latest/_modules/scrapy/downloadermiddlewares/httpproxy.html" rel="noopener ugc nofollow" target="_blank"> http代理中间件</a>，为每个单独的请求设置它。</p><p id="9bce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你正在使用智能代理管理器(<a class="ae jo" href="https://www.zyte.com/smart-proxy-manager/" rel="noopener ugc nofollow" target="_blank">或者想要</a>)，你可以使用<a class="ae jo" href="https://scrapy-crawlera.readthedocs.io/en/latest" rel="noopener ugc nofollow" target="_blank">官方中间件</a>来设置它。</p><h1 id="8f8f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">Scrapy如何帮助你处理数据？</h1><p id="036b" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">Scrapy还为您提供项目，以帮助定义您的数据结构。下面是一个简单的定义:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="37cb" class="lb jq hi kx b fi lc ld l le lf">import scrapy </span><span id="f6d9" class="lb jq hi kx b fi lj ld l le lf">class BlogItem(scrapy.Item): <br/>title = scrapy.Field() <br/>url = scrapy.Field()</span></pre><p id="8ddf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您也可以使用数据类！</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="20bd" class="lb jq hi kx b fi lc ld l le lf">from dataclasses import dataclass </span><span id="d7a9" class="lb jq hi kx b fi lj ld l le lf">@dataclass <br/>class BlogItem: <br/>title: str <br/>url: str</span></pre><p id="c8f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://docs.scrapy.org/en/latest/topics/loaders.html" rel="noopener ugc nofollow" target="_blank">项目加载器</a>是数据格式化的下一步。为了理解它们在哪里变得有用，您可以想象多个蜘蛛使用同一个项目并需要相同的格式。例如，去掉“描述”字段的空格并合并字符串列表。他们可以做一些非常复杂的事情！</p><p id="ceb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用于处理项目的管道也是一种选择。它们可以用于根据某些字段过滤重复的项目，或者添加/验证计算值(例如根据时间戳删除项目)。</p><h1 id="44cd" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">阅读更多</h1><p id="63cd" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">这只是一个概述，Scrapy中直接包含了许多其他特性，以及社区创建的许多扩展、中间件和管道。以下是您可能感兴趣的资源列表:</p><p id="42f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Scrapy是一个成熟的开源项目，有许多活跃的贡献者，已经存在多年了。它得到了很好的支持，所以你会发现几乎所有你能想到的东西都有<a class="ae jo" href="https://docs.scrapy.org/en/latest/" rel="noopener ugc nofollow" target="_blank">文档</a>和<a class="ae jo" href="https://docs.scrapy.org/en/latest/intro/tutorial.html" rel="noopener ugc nofollow" target="_blank">教程</a>，还有很多社区开发的插件。</p></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><p id="dec3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lr">最初发表于</em><a class="ae jo" href="https://www.zyte.com/blog/how-scrapy-makes-web-crawling-easy/" rel="noopener ugc nofollow" target="_blank"><em class="lr">【https://www.zyte.com】</em></a><em class="lr">。</em></p></div></div>    
</body>
</html>