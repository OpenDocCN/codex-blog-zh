<html>
<head>
<title>Save Time and Money Using Parquet and Feather in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python中的拼花地板和羽毛节省时间和金钱</h1>
<blockquote>原文：<a href="https://medium.com/codex/save-time-and-money-using-parquet-and-feather-in-python-d5d6c0b93899?source=collection_archive---------3-----------------------#2021-09-27">https://medium.com/codex/save-time-and-money-using-parquet-and-feather-in-python-d5d6c0b93899?source=collection_archive---------3-----------------------#2021-09-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/9b50fa2631d10ed00bd1e6d2e7ff7996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6yxYcArA5GQY_S7fP5dR9A.png"/></div></div></figure><div class=""/><p id="33d1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我花了<strong class="is hu"> <em class="jo">几十年</em> </strong>处理数据，大部分都是好的ole CSV格式。数据库导出使用CSV。Excel使用CSV。日志文件可以是CSV格式，也可以是普通的分隔格式(没有太大的区别)。传感器输出CSV数据。在过去的几年里，我注意到了Parquet和Cassandra数据库，甚至将Cassandra集成到了我们构建的系统中。我们还使用Parquet加载数据图表。这两者都支持数据的列存储:</p><blockquote class="jp"><p id="1e72" class="jq jr ht bd js jt ju jv jw jx jy jn dx translated">面向列的存储意味着存储每一列的所有值的向量，反之面向行的存储意味着每一行是每一列的单个值的向量。</p></blockquote><p id="3070" class="pw-post-body-paragraph iq ir ht is b it jz iv iw ix ka iz ja jb kb jd je jf kc jh ji jj kd jl jm jn hb bi translated">这里有一个例子说明为什么这真的很有用。假设您只需要对单个列的值进行操作来计算平均值。在一个常规的面向行的SQL数据库中，你必须读取每一行来得到你需要的值并挑选出值。在列式存储中，您只需读取所需的列，所有数据都在那里。更简单快捷。</p><p id="85ca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在结合另一个概念-数据框架。数据帧表示二维(行和列)表格数据的内存格式。您可以将表格数据接收到dataframe中，这为执行操作提供了一种中性的格式。也就是说，无论原始数据文件的格式如何，数据帧都是相同的。然而，这并不意味着所有文件格式对于读/写操作都执行相同的操作。数据帧从拼花和羽毛文件中读取数据比从CSV中读取数据快得多。数据帧大量用于分析和机器学习(意味着处理大型数据集)，因此更快地进出数据是一件大事。</p><p id="ffc4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在有一种更新的格式叫做Apache Arrow。Parquet和Arrow存在于多个系统中，并且有多种语言的API，所以虽然我的示例代码是Python，<strong class="is hu">但是你可以在任何支持的语言中使用这些方法。</strong>拼花8岁，阿罗快5了。顺便说一下，Arrow产生的二进制格式在nVidia GPU上与CUDA配合得很好。我将把那个话题留到以后再说。</p><div class="hh hi ez fb hj ke"><a href="https://parquet.apache.org/" rel="noopener  ugc nofollow" target="_blank"><div class="kf ab dw"><div class="kg ab kh cl cj ki"><h2 class="bd hu fi z dy kj ea eb kk ed ef hs bi translated">阿帕奇拼花地板</h2><div class="kl l"><h3 class="bd b fi z dy kj ea eb kk ed ef dx translated">Apache Parquet是一种柱状存储格式，适用于Hadoop生态系统中的任何项目，无论选择什么…</h3></div><div class="km l"><p class="bd b fp z dy kj ea eb kk ed ef dx translated">parquet.apache.org</p></div></div><div class="kn l"><div class="ko l kp kq kr kn ks hp ke"/></div></div></a></div><div class="hh hi ez fb hj ke"><a href="https://arrow.apache.org/" rel="noopener  ugc nofollow" target="_blank"><div class="kf ab dw"><div class="kg ab kh cl cj ki"><h2 class="bd hu fi z dy kj ea eb kk ed ef hs bi translated">阿帕奇箭头</h2><div class="kl l"><h3 class="bd b fi z dy kj ea eb kk ed ef dx translated">用于内存分析的跨语言开发平台</h3></div><div class="km l"><p class="bd b fp z dy kj ea eb kk ed ef dx translated">arrow.apache.org</p></div></div><div class="kn l"><div class="kt l kp kq kr kn ks hp ke"/></div></div></a></div><p id="29b0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意，我并不是说这些是等价的数据格式。它们存在于不同的地方/系统，可以做或用于不同的事情。我想说的是，它们都比使用CSV文件要好。请继续阅读，了解原因。</p><h1 id="8579" class="ku kv ht bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">需要Python模块</h1><p id="848b" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">我下面的代码需要的额外的python模块是pandas和pyarrow。Pandas是用于数据帧操作的，版本1.3.3。支持拼花地板和羽毛。</p><p id="8bdb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Pyarrow增加了拼花和羽毛数据格式的功能。在这些例子中，我用它来读取元数据，但是它可以做更多的事情(比如多线程读取JSON文件。Python API真的很远。参见<a class="ae lx" href="https://arrow.apache.org/docs/python/index.html" rel="noopener ugc nofollow" target="_blank">https://arrow.apache.org/docs/python/index.html</a></p><h1 id="4835" class="ku kv ht bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">测试平台</h1><p id="7956" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">我在以下系统上运行简单的基准测试:</p><ul class=""><li id="2ed6" class="ly lz ht is b it iu ix iy jb ma jf mb jj mc jn md me mf mg bi translated">运行Big Sur版本11.6的MacBook Pro (2016)</li><li id="a588" class="ly lz ht is b it mh ix mi jb mj jf mk jj ml jn md me mf mg bi translated">四核英特尔酷睿i7(2.7 GHz)</li><li id="ce88" class="ly lz ht is b it mh ix mi jb mj jf mk jj ml jn md me mf mg bi translated">16 GB内存，2133 MHz LPDDR3</li><li id="06f3" class="ly lz ht is b it mh ix mi jb mj jf mk jj ml jn md me mf mg bi translated">500 GB固态硬盘</li></ul><p id="a6b4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我用的是Python 3.8。<em class="jo">见最后的源代码。您需要提供自己的csv文件/路径作为参数，或者在代码中更改变量。不难。</em></p><h1 id="07e7" class="ku kv ht bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">大型CSV文件测试</h1><p id="d820" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">在这种情况下，large意味着大约1.8 GB，超过60列，大多数是数字，但也有一些字符串数据类型。这是我的测试程序的典型输出。</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="d771" class="mv kv ht mr b fi mw mx l my mz">using csv file data/Dataset-Unicauca-Version2-87Atts.csv<br/>reading CSV file into dataframe took 43.982755350999994<br/>csv file size is 1.767404086 GB<br/>writing csv file from dataframe took 186.065215698<br/>writing dataframe to parquet file took 13.231199089<br/>parquet file size is 0.682483215 GB<br/>reading parquet file into dataframe took 10.189509552999994</span><span id="c642" class="mv kv ht mr b fi na mx l my mz">writing dataframe to feather file took 10.413064489000021<br/>feather file size is 0.85888809 GB<br/>reading feather file into dataframe took 7.5954691249999655</span><span id="e346" class="mv kv ht mr b fi na mx l my mz">writing dataframe to json file took 131.16764677999998<br/>json file size is 5.006533096 GB<br/>reading json file into dataframe took 1107.9316589139999</span></pre><p id="4f56" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我已经运行了多次，结果是一致的，就像你在上面看到的那样。在大文件上读写拼花和羽毛格式要快得多。JSON比CSV写得快，但是比其他的慢一个数量级。从JSON文件中读取数据是很可怕的，因为文件太大了(本例中是x3 over CSV)。</p><p id="2408" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">顺便说一下，不管源文件是什么类型，得到的dataframe对象都是相同的。您可以使用dataframe对象的equals方法来确认这一点，即df.equals(other_df)。还有一个dataframe memory_usage方法，它按数据类型打印每列使用的内存量。</p><h1 id="ebab" class="ku kv ht bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">小型CSV文件</h1><p id="2e22" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">虽然随着文件变大，这些新格式可以很好地扩展，但对于小文件就不太适用了。下面是对一个只有4列的1.8 MB文件运行的输出。我确实在测试运行中看到了可变性，但是重要的一点是<strong class="is hu">小文件在读/写操作</strong>中给你带来的收益更小，在文件存储中的收益(文件大小的减少)也更少。</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="83c7" class="mv kv ht mr b fi mw mx l my mz">reading CSV file into dataframe took 0.03396918999999998<br/>csv file size is 0.001835543 GB<br/>writing csv file from dataframe took 0.2350556009999999<br/>writing dataframe to parquet file took 0.11766440999999994<br/>parquet file size is 0.000894606 GB<br/>reading parquet file into dataframe took 0.026598200000000016</span><span id="ab6b" class="mv kv ht mr b fi na mx l my mz">writing dataframe to feather file took 0.19203748499999995<br/>feather file size is 0.000952714 GB<br/>reading feather file into dataframe took 0.008075714000000067</span><span id="65f4" class="mv kv ht mr b fi na mx l my mz">writing dataframe to json file took 0.8354935490000002<br/>json file size is 0.002195646 GB<br/>reading json file into dataframe took 0.03366627099999997</span></pre><p id="a5a1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“镶木地板”和“羽毛”文件大约是CSV文件的一半大小。正如所料，JSON更大。有趣的是，feather文件读/写操作在小文件上比parquet提供了更好的性能。没有哪一项收益比大文件更令人印象深刻。</p><h1 id="953f" class="ku kv ht bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">[计]元数据</h1><p id="c927" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">无论你在做什么，元数据都是至关重要的。事实证明，Parquet和Feather给出了我没有预料到的关于起始CSV文件的额外元数据。下面是一个示例，使用一个小文件显示pyarrow的输出。请注意，feather和parquet推断数据类型。他们提供的信息不是来自CSV文件(它只有列名)。</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="de11" class="mv kv ht mr b fi mw mx l my mz">Info from Parquet file<br/>Column names: [‘question’, ‘product_description’, ‘image_url’, ‘label’]<br/>Schema: question: string<br/>product_description: string<br/>image_url: string<br/>label: int64<br/> — schema metadata — <br/>pandas: ‘{“index_columns”: [{“kind”: “range”, “name”: null, “start”: 0, “‘ + 740</span><span id="a285" class="mv kv ht mr b fi na mx l my mz">Info from CSV file<br/>[‘question’, ‘product_description’, ‘image_url’, ‘label’]</span><span id="e651" class="mv kv ht mr b fi na mx l my mz">Info from Feather file<br/>Column names: [‘question’, ‘product_description’, ‘image_url’, ‘label’]<br/>Schema: question: string<br/>product_description: string<br/>image_url: string<br/>label: int64<br/> — schema metadata — <br/>pandas: ‘{“index_columns”: [], “column_indexes”: [], “columns”: [{“name”:’ + 553</span><span id="5611" class="mv kv ht mr b fi na mx l my mz">Info from JSON file<br/>getting metadata from JSON requires either more code or libraries I do not know.</span></pre><h1 id="97a9" class="ku kv ht bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">但是MongoDB呢？</h1><p id="e96a" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">MongoDB非常受欢迎是有原因的——它为您提供了一个灵活的模式，您可以轻松地修改(尤其是添加列)。然而，Mongo将所有内容都存储为JSON文档。正如你在上面看到的，JSON很难处理——文件越大，性能越慢。你需要一个中间格式和已经做好的分析工具(为什么要重新发明轮子？)，所以我建议你使用数据框架和其他格式进行分析。Pandas数据帧可以加载到Mongo中，但是需要使用to_dict()函数进行转换或者转换到JSON。</p><p id="68d7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我对dataframe to_dict函数做了一个非常简单的检查。在我对大文件和小文件的测试中，我发现不应该使用to_dict()函数，因为它比简单地将数据帧直接写入JSON文件以导入Mongo要慢得多(50–100%)。</p><h1 id="6f21" class="ku kv ht bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">摘要</h1><p id="a402" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">对于大型昂贵的系统，你需要采取整体的方法，包括系统工程的元素。你不能只写有用的代码——它必须运行良好，并有助于降低成本。我的第一个建议是在你的云账单中找一个大的开销，并且这个开销看起来不太难处理，然后应用这些新的格式。这些都是使用CSV文件的很好的替代方法，但是你选择哪一个取决于你的系统的其他部分和你想要减少的开销。您可能有一些非常重要的东西，比如Apache Spark，可以很好地使用parquet数据。或者，您可能需要Arrow API来处理存储在feather中的数据集。或者您想将拼花地板数据读入feather。或者您在AWS中存储了大量CSV文件，并希望减少这一费用。您可能有在多种语言中运行的代码和一些共享的数据库，这些数据库将feather和IPC可以替代的所有东西绑定在一起(feather文件在任何语言中都是一样的)。</p><h1 id="879a" class="ku kv ht bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">使用源，卢克</h1><p id="27d0" class="pw-post-body-paragraph iq ir ht is b it ls iv iw ix lt iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">您需要传入一个指向CSV文件的命令行参数，或者您可以将它放在data_file变量的代码中。</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="ec20" class="mv kv ht mr b fi mw mx l my mz">import csv<br/>import os<br/>import pandas as pd<br/>import pyarrow.parquet as pq<br/>import pyarrow.feather as pf<br/>import sys<br/>import time<br/><br/>'''<br/>This is is some code to show key comparisons between csv, feather, parquet, and<br/>JSON data formats. This will do conversion time, size and performance <br/>comparisons for common operations.<br/>'''<br/><br/># change this one parameter and use whatever csv file you want as a start point<br/>data_file = 'data/Dataset-Unicauca-Version2-87Atts.csv'<br/><br/>def analyze(csv_file = data_file):<br/>    start = time.perf_counter()<br/>    print(f'using csv file {csv_file}')<br/>    df = pd.read_csv(csv_file)<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'reading CSV file into dataframe took {elapsed}')<br/>    #look at first 5 rows of dataframe: print(df.head())<br/>    print_size('csv', csv_file)<br/>    # write data frame to new csv<br/>    start = time.perf_counter()<br/>    df.to_csv('data/new.csv', index=False)<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'writing csv file from dataframe took {elapsed}')<br/><br/>    #parquet<br/>    start = time.perf_counter()<br/>    df.to_parquet('data/test.parquet')<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'writing dataframe to parquet file took {elapsed}')<br/>    print_size('parquet', 'data/test.parquet')<br/><br/>    start = time.perf_counter()<br/>    df_parquet = pd.read_parquet('data/test.parquet')<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'reading parquet file into dataframe took {elapsed}\n')<br/><br/>    #feather<br/>    start = time.perf_counter()<br/>    df.to_feather('data/test.feather')<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'writing dataframe to feather file took {elapsed}')<br/>    print_size('feather', 'data/test.feather')<br/><br/>    start = time.perf_counter()<br/>    df_feather = pd.read_feather('data/test.feather')<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'reading feather file into dataframe took {elapsed}\n')<br/><br/>    #json<br/>    start = time.perf_counter()<br/>    df.to_json('data/test.json')<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'writing dataframe to json file took {elapsed}')<br/>    print_size('json', 'data/test.json')<br/><br/>    start = time.perf_counter()<br/>    df_json = pd.read_json('data/test.json')<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'reading json file into dataframe took {elapsed}\n')<br/><br/>    # memory usage (same for all dataframes, since equal<br/>    if df.equals(df_json):<br/>        print('csv and json dataframes are equivalent')<br/>        print(df.memory_usage())<br/><br/># read file metadata<br/>def read_metadata(csv_file = data_file):<br/>    print('Info from Parquet file')<br/>    pfile = pq.read_table('data/test.parquet')<br/>    print("Column names: {}".format(pfile.column_names))<br/>    # Parquet infers the data type of each column<br/>    print("Schema: {}".format(pfile.schema))<br/><br/>    print('Info from CSV file')<br/>    # just column names- no inference of data types<br/>    with open(csv_file, newline='') as f:<br/>        reader = csv.reader(f)<br/>        row1 = next(reader)<br/>        print (row1)<br/><br/>    # note the symmetry within the pyarrow API<br/>    print('Info from Feather file')<br/>    ffile = pf.read_table(('data/test.feather'))<br/>    print("Column names: {}".format(ffile.column_names))<br/>    # Parquet infers the data type of each column<br/>    print("Schema: {}".format(ffile.schema))<br/><br/>    print('Info from JSON file')<br/>    print('getting metadata from JSON requires either more code or libraries I do not know.')<br/><br/><br/># look at file sizes for different storage technology (feather, parquet, csv)<br/>def print_size(name, file):<br/>    file_size = os.path.getsize(file) / 1000000000<br/>    print(f"{name} file size is {file_size} GB")<br/><br/>def mongo_conversion(csv_file = data_file):<br/>    df = pd.read_csv(csv_file)<br/><br/>    start = time.perf_counter()<br/>    for_mongo = df.to_dict()<br/>    stop = time.perf_counter()<br/>    elapsed = stop - start<br/>    print(f'converted dataframe to dict for Mongo import  took {elapsed}')<br/><br/># run the tests<br/>if __name__ == '__main__':<br/><br/>    if len(sys.argv) &lt; 2:<br/>        # uses default value<br/>        analyze()<br/>        read_metadata()<br/>        mongo_conversion()<br/>    else:<br/>        # use csv file and path from command line<br/>        analyze(sys.argv[1])<br/>        read_metadata(sys.argv[1])<br/>        mongo_conversion(sys.argv[1])</span></pre><p id="9705" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以用这个资源做任何你喜欢的事情。</p></div></div>    
</body>
</html>