<html>
<head>
<title>Demystifying Text Analytics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘文本分析</h1>
<blockquote>原文：<a href="https://medium.com/codex/demystifying-text-analytics-b550e7941d?source=collection_archive---------18-----------------------#2021-08-22">https://medium.com/codex/demystifying-text-analytics-b550e7941d?source=collection_archive---------18-----------------------#2021-08-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d714a771a389a455828b09c5887cdcf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ERrPqBOTa8Bximy9"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">帕特里克·托马索在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="4321" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">用余弦相似度算法查找相似文档</h1><p id="cde9" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">去年我在做一项审计工作，当时我正在查看由两个团队在两个独立的系统中记录的任务列表。这两个系统中的任务<em class="kr">(随后导出到excel) </em>是相同的，但由不同的人在不同的时间使用相似但不准确的词语编写。</p><p id="0c42" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">这促使我写了关于余弦相似性的文章，并分享了一些代码来帮助读者在几分钟内完成类似的挑战(我花了几个小时才弄明白)。</p><h1 id="1d7b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">简介</strong></h1><p id="a75f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><em class="kr">余弦相似度是一种度量标准，用于衡量文本的相似程度，而不考虑其大小。在数学上，它测量的是在多维空间中投影的两个向量之间的角度余弦。使用这种方法很有用，因为即使两个相似的文档相距欧几里德距离很远(由于文档的大小)，它们仍有可能靠得更近。角度越小，余弦相似度越高。</em></p><p id="29d7" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">在任何人进入第3步(余弦相似性)之前，这是我们在这里要讨论的，有两个更重要的初步步骤，例如:</p><ol class=""><li id="c407" class="kx ky hi jv b jw ks ka kt ke kz ki la km lb kq lc ld le lf bi translated">通过将句子标记成单词、去除停用词、对单词进行词干处理以及构造n元语法等来准备文本数据。</li><li id="7be5" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq lc ld le lf bi translated">通过使用一种称为<a class="ae iu" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>(术语频率—逆文档频率)的技术，为文档内部的术语(或标记)赋予权重，从而量化文档。计算出的TF-IDF表示每个术语(或标记)对给定文档的重要性。为了使这篇文章简短，我将直接跳到第3部分(余弦相似性)，你可以在以后向下滚动时阅读第1步和第2步。</li></ol><p id="1a92" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">余弦相似性在数学上被描述为向量的点积和每个向量的欧几里德范数或幅度的乘积之间的除法。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/240815d109e5961764cac3458ea0df00.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/0*p1laW5PfG-wJORnB.png"/></div></figure><p id="32f7" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">图片来自https://en.wikipedia.org/wiki/Cosine_similarity</a></p><p id="8cb8" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">我们的场景是恰当的，因为它需要识别文档对之间的相似性，作为两个对象之间相似性度量的量化。为了量化相似性，我们将把文档或句子中的单词或短语转换成矢量化的表示形式。</p><p id="a5aa" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">然后可以在余弦相似性公式中使用文档的矢量表示来获得相似性的量化。余弦相似度为1意味着这两个文档完全相似，余弦相似度为0意味着这两个文档之间没有相似性。</p><p id="53a6" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">这是我使用的代码。我减少了项目这个词，并将其屏蔽:</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="cf2f" class="lv iw hi lr b fi lw lx l ly lz">def word2vec(word):<br/>    from collections import Counter<br/>    from math import sqrt<br/><br/>    # count the characters in word<br/>    cw = Counter(word)<br/>    # precomputes a set of the different characters<br/>    sw = set(cw)<br/>    # precomputes the "length" of the word vector<br/>    lw = sqrt(sum(c*c for c in cw.values()))<br/><br/>    # return a tuple<br/>    return cw, sw, lw<br/><br/>def append_str(item): <br/>    <br/>    return item<br/><br/>def cosdis(v1, v2):<br/>    # which characters are common to the two words?<br/>    common = v1[1].intersection(v2[1])<br/>    # by definition of cosine distance we have<br/>    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]<br/><br/>S= []<br/>C = []<br/>w = []<br/>list_of_keywords = ["Benefit Term Reclass",<br/>"Annuityxxxxxxxx",<br/>"Final xxxxxxxxxxxxxx",<br/>"Accrualxxxxxx",<br/>"Seg xxxxxxReclass",<br/>"HIxxxxxxxxx",<br/>"Hxxxxxxxxxxxxxs",<br/>"Gxx xxxxReserve xxxxxxxAnalysis",<br/>"xxxxxRollforward Analysis",<br/>"Negative xxxxxreclass xxxxxxxx",<br/>"RCG's xxxxx",<br/>"xxxxFair Value xxxxxxxxxx",<br/>"GAAP xxxxxxxxxxxxxxxxxxxxxx",<br/>"INSxxxxx"]<br/><br/>Sentence = ["12b1 fees xxxxxxxxxxxx)",<br/>"Accountxxxxxxxxxxx",<br/>"Accrue xxxxxxx",<br/>"Analyze xxxxxxxxto roll forward",<br/>"Analyze xxxxxxxxto roll forward",<br/>"BB xxxxxxxxxxxxxxxxxxxxxxx)",<br/>"BB xxxxxxxxxxxxxxxxx)",<br/>"",<br/>]<br/>threshold = 0.80<br/>for key in list_of_keywords:<br/>    for word in Sentence:<br/>        try:<br/>           <br/>            res = cosdis(word2vec(word), word2vec(key))<br/>            {}".format(word, key, res*100))<br/>            #print("Key is:{}".format(key))<br/>         <br/>            if res &gt; threshold:<br/>             with original word: {}".format(word, key))<br/>               print(key , "^",  word)<br/>        except IndexError:<br/>            pass</span></pre><blockquote class="ma mb mc"><p id="748c" class="jt ju kr jv b jw ks jy jz ka kt kc kd md ku kg kh me kv kk kl mf kw ko kp kq hb bi translated">上面的代码解决了我的目的，但对于有兴趣了解更多概念的人，请继续阅读。</p></blockquote><p id="9eca" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">假设我们有:</p><p id="37b7" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><em class="kr">文档1:深度学习可能很难</em></p><p id="2237" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><em class="kr">文件二:深度学习可以简单</em></p><p id="9b2e" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj">第一步:首先我们获得文本的矢量化表示</strong></p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/8b2f879dda406f4f95ee346530bcb8a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*Q_wjaOEVLdECCdsm"/></div></figure><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/df9f6bc8ac2b4d095cc5a9c3f20fd168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/0*jiKIX_VlNt8tcigS.png"/></div></figure><p id="9a57" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><em class="kr">文档1: [1，1，1，1，1，0]姑且称之为</em></p><p id="7637" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><em class="kr">文件2: [1，1，1，1，0，1]我们把这个称为B </em></p><p id="6b54" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">上面我们有两个向量(A和B ),它们在一个6维向量空间中</p><p id="83cc" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj">第二步:寻找余弦相似度</strong></p><h1 id="9a21" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">两个非零向量之间的余弦相似性的数学公式为:</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/3c3cf3bc4c51ab6e662d38f0c2fa7e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/0*xIneuQUzRi-C2r32.png"/></div></figure><p id="f598" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">让我们看看如何计算两个文本文档之间的余弦相似度的例子。</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="211b" class="lv iw hi lr b fi lw lx l ly lz">doc_1 = "Data is the oil of the digital economy" <br/>doc_2 = "Data is a new oil" </span><span id="2037" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj"># Vector representation of the document</strong><br/>doc_1_vector = [1, 1, 1, 1, 0, 1, 1, 2]<br/>doc_2_vector = [1, 0, 0, 1, 1, 0, 1, 0]</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/04ac83d921c63350f6aaab2a65ddbd24.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*dVxeY17x6jjwPH4f.png"/></div></figure><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/3f4ac756187a6c2199f179548e2c93cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2VyCYbopzj4lIutN.png"/></div></div></figure><p id="a2ce" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj">余弦相似度</strong>是比<strong class="jv hj"> <em class="kr">欧几里德距离</em> </strong>更好的度量，因为如果两个文本文档相隔<em class="kr">欧几里德距离</em>，它们仍然有可能在上下文方面彼此接近。</p><h1 id="96d6" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">定义数据</h1><p id="a355" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">让我们定义样本文本文档并对其应用CountVectorizer。</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="eeb2" class="lv iw hi lr b fi lw lx l ly lz">doc_1 = "Data is the oil of the digital economy"<br/>doc_2 = "Data is a new oil"</span><span id="a20d" class="lv iw hi lr b fi mj lx l ly lz">data = [doc_1, doc_2]</span></pre><h1 id="53ff" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">调用<a class="ae iu" href="https://studymachinelearning.com/countvectorizer-for-text-classification/" rel="noopener ugc nofollow" target="_blank">计数矢量器</a></h1><p id="2b9d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">机器只能理解数字，不能理解文本。所以有必要将文本数据编码成数字。将每个唯一的数字分配给每个单词的过程称为标记化。</p><p id="6e08" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">Scikit-Learn库提供了一个CountVectorizer，它将一组文本文档转换成一个令牌计数矩阵。</p><p id="6737" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"># count vectorizer提供了一种简单的方法来标记文本数据和构建已知单词的词汇表。</p><p id="d404" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">#它还使用构建的词汇表对新的文本数据进行编码。编码向量是一个稀疏矩阵，因为它包含许多零。使用计数矢量器的步骤如下:</p><ul class=""><li id="38dc" class="kx ky hi jv b jw ks ka kt ke kz ki la km lb kq mm ld le lf bi translated"><em class="kr">创建一个对象的</em><strong class="jv hj"><em class="kr">count vectorizer</em></strong><em class="kr">类。</em></li><li id="5a93" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><em class="kr">调用</em><strong class="jv hj"><em class="kr">fit()</em></strong><em class="kr">函数，以便从文本数据中构建单词的词汇表。</em></li><li id="e323" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><em class="kr">调用</em><strong class="jv hj"><em class="kr">transform()</em></strong><em class="kr">函数，使用构建的词汇对文本数据进行分词。</em></li></ul><p id="a10b" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj"> <em class="kr">参数:</em> </strong></p><ul class=""><li id="1ea9" class="kx ky hi jv b jw ks ka kt ke kz ki la km lb kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr">输入:</em> </strong> <em class="kr">序列字符串</em></li><li id="d4f9" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr">小写:</em> </strong> <em class="kr"> bool(默认-真)。在标记前将所有字符转换成小写。</em></li><li id="0206" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr"> stop_words : </em> </strong> <em class="kr">从结果词汇中删除已定义的单词。</em></li><li id="e9e2" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"><em class="kr">ngram _ range:</em></strong><em class="kr">要提取的不同n元文法的n值范围的上下边界。</em></li><li id="53d7" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr"> max_df : </em> </strong> <em class="kr">忽略文档频率高于阈值的术语。</em></li><li id="91be" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr"> min_df : </em> </strong> <em class="kr">忽略文档频率低于阈值的术语。</em></li><li id="6080" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"><em class="kr">max _ features:</em></strong><em class="kr">构建一个只考虑按单词出现次数排序的top max_features的词汇表。</em></li></ul><p id="7732" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj">例如:</strong></p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="6789" class="lv iw hi lr b fi lw lx l ly lz"><strong class="lr hj">In [1]:<br/></strong>from sklearn.feature_extraction.text import CountVectorizer<br/>text = ["Do not limit your challenges,challenge your limits"]<br/>vect = CountVectorizer()  # create an object <br/>vect.fit(text)            # build vocabulary<br/>tokenize_text = vect.transform(text)  # encode the text data</span><span id="61ca" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj"># Let's print vocabulary</strong><br/><strong class="lr hj">In [2]: </strong>vect.vocabulary_<br/><strong class="lr hj">Out[2]:</strong> {'limit': 3, 'do': 2, 'challenge': 0, 'challenges': 1, 'not': 5, 'your': 6, 'limits': 4}</span><span id="1ca8" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj">In [3]:</strong> vect.get_feature_names()<br/><strong class="lr hj">Out[3]:</strong> ['challenge', 'challenges', 'do', 'limit', 'limits', 'not', 'your']</span><span id="7c6b" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj"># Let's see the encoded vector, which showing a count of 1 occurrence for each word except the last word(index=6) that has an occurrence of 2.</strong> </span><span id="5272" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj">In [4]: </strong>tokenize_text.toarray() <br/><strong class="lr hj">Out[4]: </strong>[[1, 1, 1, 1, 1, 1, 2]]</span></pre><p id="4fa6" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">让我们应用这个词汇表来编码新的文本数据。</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="d396" class="lv iw hi lr b fi lw lx l ly lz"><strong class="lr hj">In [5]: <br/></strong>new_text = ["push yourself to your limit"]<br/>new_txt_encode = vect.transform(new_text)</span><span id="14c5" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj">In [6]: </strong>new_txt_encode.toarray()<strong class="lr hj"><br/>Out[6]: </strong>[[0, 0, 0, 1, 0, 0, 1]]</span></pre><p id="f697" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">上面的结果表明，索引3和6处的单词(' limit '，' your ')在测试文档中出现过一次，其余的单词不在词汇表中，因此，它将0指定为没有出现。</p><p id="b674" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">回到我们的问题:</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="d21a" class="lv iw hi lr b fi lw lx l ly lz">from sklearn.feature_extraction.text import CountVectorizer</span><span id="3a28" class="lv iw hi lr b fi mj lx l ly lz">count_vectorizer = CountVectorizer()<br/>vector_matrix = count_vectorizer.fit_transform(data)<br/>vector_matrix</span><span id="026b" class="lv iw hi lr b fi mj lx l ly lz">&lt;2x8 sparse matrix of type '&lt;class 'numpy.int64'&gt;'<br/>	with 11 stored elements in Compressed Sparse Row format&gt;</span></pre><p id="46a7" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">生成的向量矩阵是稀疏矩阵，这里不打印。我们把它转换成numpy数组，用令牌字显示。</p><p id="c6ca" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">这里是在数据中找到的唯一令牌列表。</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="4286" class="lv iw hi lr b fi lw lx l ly lz">tokens = count_vectorizer.get_feature_names()<br/>tokens</span><span id="216d" class="lv iw hi lr b fi mj lx l ly lz">['data', 'digital', 'economy', 'is', 'new', 'of', 'oil', 'the']</span></pre><p id="570e" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">将稀疏向量矩阵转换为numpy数组，可视化doc_1和doc_2的矢量化数据。</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="009a" class="lv iw hi lr b fi lw lx l ly lz">vector_matrix.toarray()</span><span id="b84b" class="lv iw hi lr b fi mj lx l ly lz">array([[1, 1, 1, 1, 0, 1, 1, 2],<br/>       [1, 0, 0, 1, 1, 0, 1, 0]])</span></pre><p id="0e76" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">让我们创建pandas数据帧，使矢量化数据和令牌清晰可见。</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="1a67" class="lv iw hi lr b fi lw lx l ly lz">import pandas as pd</span><span id="a428" class="lv iw hi lr b fi mj lx l ly lz">def create_dataframe(matrix, tokens):</span><span id="858c" class="lv iw hi lr b fi mj lx l ly lz">    doc_names = [f'doc_{i+1}' for i, _ in enumerate(matrix)]<br/>    df = pd.DataFrame(data=matrix, index=doc_names, columns=tokens)<br/>    return(df)</span><span id="6e76" class="lv iw hi lr b fi mj lx l ly lz">create_dataframe(vector_matrix.toarray(),tokens)</span><span id="b033" class="lv iw hi lr b fi mj lx l ly lz">data  digital  economy  is  new  of  oil  the<br/>doc_1     1        1        1   1    0   1    1    2<br/>doc_2     1        0        0   1    1   0    1    0</span></pre><h1 id="ec23" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">查找余弦相似性</h1><p id="8a3a" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">Scikit-Learn提供了计算<a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>的功能。让我们计算<strong class="jv hj"> doc_1 </strong>和<strong class="jv hj"> doc_2 </strong>之间的余弦相似度。</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="17cc" class="lv iw hi lr b fi lw lx l ly lz">from sklearn.metrics.pairwise import cosine_similarity</span><span id="6bd6" class="lv iw hi lr b fi mj lx l ly lz">cosine_similarity_matrix = cosine_similarity(vector_matrix)<br/>create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])</span><span id="fa52" class="lv iw hi lr b fi mj lx l ly lz">doc_1     doc_2<br/>doc_1  1.000000  0.474342<br/>doc_2  0.474342  1.000000</span></pre><p id="6f84" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">通过观察上表，我们可以说<strong class="jv hj"> doc_1 </strong>和<strong class="jv hj"> doc_2 </strong>的余弦相似度为<strong class="jv hj"> 0.47 </strong></p><p id="c4da" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">让我们用<strong class="jv hj"> TfidfVectorizer、</strong>检查余弦相似度，看看它在<strong class="jv hj"> CountVectorizer上是如何变化的。</strong></p><h1 id="1f77" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">调用<a class="ae iu" href="https://studymachinelearning.com/tfidfvectorizer-for-text-classification/" rel="noopener ugc nofollow" target="_blank">tfidf矢量器</a></h1><p id="22c1" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">从文本文档中统计单词是非常基础的。然而，简单的字数统计对于文本处理是不够的，因为存在诸如“the”、“an”、“your”等词。经常出现在文本文档中。频繁出现的单词会妨碍我们的分析，因此<strong class="jv hj"> Tf-idf </strong>可以成功地用于过滤掉文本文档中的停用词。</p><p id="1981" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">解决这个问题的另一个方法是词频。这种方法叫做<strong class="jv hj"> TF-IDF </strong>代表<strong class="jv hj"><em class="kr">词频—逆文档频率</em></strong>。TF-IDF是一个数字统计量，用于衡量单词在文档中的重要性。</p><ul class=""><li id="bbd6" class="kx ky hi jv b jw ks ka kt ke kz ki la km lb kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr">词频:</em> </strong> <em class="kr">一个词在文本文档中出现的次数。</em></li><li id="4de2" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr">逆文档频率:</em> </strong> <em class="kr">衡量该词是文档中的稀有词还是常用词。</em></li></ul><p id="e438" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><em class="kr">用来计算</em><strong class="jv hj"><em class="kr">IF-IDF</em></strong><em class="kr">的公式是</em> <strong class="jv hj"> <em class="kr"> : </em> </strong></p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="6d86" class="lv iw hi lr b fi lw lx l ly lz"><strong class="lr hj">tf(t,d) = (Number of times term t appears in a document) / (Total number of terms in the document)</strong></span><span id="9014" class="lv iw hi lr b fi mj lx l ly lz">Where, <br/>tf(t,d) - Term Frequency<br/>t = term <br/>d = document</span><span id="4a2b" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj">idf(t) = log [ n / df(t) ] + 1</strong></span><span id="1ac7" class="lv iw hi lr b fi mj lx l ly lz">where,<br/>idf(t) - Inverse Document Frequency<br/>n - Total number of documents<br/>df(t) is the document frequency of term t;</span><span id="4d23" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj">tf-idf(t, d) = tf(t, d) * idf(t)</strong></span></pre><p id="f351" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">示例:</p><p id="c648" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">考虑一个总共有100个单词的文档，单词<strong class="jv hj">“book”</strong>在文档中出现了5次。</p><p id="9fc8" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj">词频(tf) </strong> = 5 / 100 = 0.05</p><p id="c452" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">假设我们有10，000个文档，单词<strong class="jv hj">“book”</strong>在其中的1000个文档中出现过。那么idf就是:</p><p id="1d50" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj">逆文档频率(IDF) </strong> = log[10000/1000] + 1 = 2</p><p id="9a45" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">TF-IDF = 0.05 * 2 = 0.1</p><p id="902d" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj">。。。</strong></p><p id="a30b" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">Scikit-Learn提供了tfidf矢量器的实现。</p><p id="e081" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj"> <em class="kr">参数:</em> </strong></p><ul class=""><li id="5619" class="kx ky hi jv b jw ks ka kt ke kz ki la km lb kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr">输入:</em> </strong> <em class="kr">文本文档</em></li><li id="97ea" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr">小写:</em> </strong> <em class="kr"> bool(默认-True)。在标记前将所有字符转换成小写。</em></li><li id="ffa3" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"><em class="kr">stop _ words:</em></strong><em class="kr">从结果词汇中删除已定义的单词。</em></li><li id="ea94" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"><em class="kr">ngram _ range:</em></strong><em class="kr">要提取的不同n元文法的n值范围的上下边界。</em></li><li id="8a6b" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr"> max_df : </em> </strong> <em class="kr">忽略文档频率高于阈值的术语。</em></li><li id="8b01" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr"> min_df : </em> </strong> <em class="kr">忽略文档频率低于阈值的术语。</em></li><li id="33eb" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"><em class="kr">max_features:</em></strong><em class="kr">构建一个只考虑按单词出现次数排序的顶级max _ features的词汇表。</em></li><li id="af7d" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"> <em class="kr">规范:</em></strong><em class="kr">‘L1’，‘L2’或‘无’(默认——‘L2’)</em></li><li id="4274" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"><em class="kr">use _ IDF:</em></strong><em class="kr">boolean(默认=真)。启用逆文档频率重新加权。</em></li><li id="4454" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"><em class="kr">smooth _ IDF:</em></strong><em class="kr">布尔型(默认=真)。通过给文档频率加1来平滑idf权重，就好像一个额外的文档恰好包含集合中的每个术语一次。防止零次分割。</em></li><li id="509f" class="kx ky hi jv b jw lg ka lh ke li ki lj km lk kq mm ld le lf bi translated"><strong class="jv hj"><em class="kr">sublinear _ TF:</em></strong><em class="kr">布尔(默认=假)。应用次线性tf标度，即用1 + log(tf)代替tf。</em></li></ul><p id="0f17" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated"><strong class="jv hj">例子</strong></p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="9283" class="lv iw hi lr b fi lw lx l ly lz"><strong class="lr hj">In [1]:<br/></strong>from sklearn.feature_extraction.text import TfidfVectorizer<br/>text = ["Do not limit your challenges,challenge your limits",<br/>        "your challenges",<br/>        "their limits"]</span><span id="4c37" class="lv iw hi lr b fi mj lx l ly lz">vect = TfidfVectorizer()  # create an object <br/>vect.fit(text)            # build vocabulary<br/>tokenize_text = vect.transform(text)  # encode the text data</span><span id="a7fd" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj"># Let's print vocabulary<br/>In [2]: </strong>vect.vocabulary_ <br/><strong class="lr hj">Out[2]:</strong> {'your': 7, 'limits': 4, 'challenge': 0, 'limit': 3, 'do': 2, 'not': 5, 'their': 6, 'challenges': 1}</span><span id="f781" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj">In [3]: </strong>vect.get_feature_names()<br/><strong class="lr hj">Out[3]: </strong>['challenge', 'challenges', 'do', 'limit', 'limits', 'not', 'their', 'your']</span><span id="1a58" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj">In [4]: </strong>tokenize_text.shape<strong class="lr hj"><br/>Out[4]: </strong>(3, 8)</span><span id="373a" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj"># Let's print idf score of terms</strong><br/><strong class="lr hj">In [5]: </strong>vect.idf_<br/><strong class="lr hj">Out[5]: </strong>[1.69314718, 1.28768207, 1.69314718, 1.69314718, 1.28768207,<br/>       1.69314718, 1.69314718, 1.28768207]</span><span id="d0db" class="lv iw hi lr b fi mj lx l ly lz"><strong class="lr hj"># Let’s apply this vocabulary to encode new text data.</strong><br/><strong class="lr hj">In [6]: </strong><br/>new_text = ["push yourself to your limit"]<br/>new_txt_encode = vect.transform(new_text)<br/><strong class="lr hj">In [7]:</strong> new_txt_encode.toarray()<br/><strong class="lr hj">Out[7]: </strong>[[0.        , 0.        , 0.        , 0.79596054, 0.        ,<br/>        0.        , 0.        , 0.60534851]]</span></pre><p id="dc70" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">回到我们的问题:</p><pre class="lm ln lo lp fd lq lr ls lt aw lu bi"><span id="4a64" class="lv iw hi lr b fi lw lx l ly lz">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="b793" class="lv iw hi lr b fi mj lx l ly lz">Tfidf_vect = TfidfVectorizer()<br/>vector_matrix = Tfidf_vect.fit_transform(data)</span><span id="cff6" class="lv iw hi lr b fi mj lx l ly lz">tokens = Tfidf_vect.get_feature_names()<br/>create_dataframe(vector_matrix.toarray(),tokens)</span><span id="17f3" class="lv iw hi lr b fi mj lx l ly lz">data  digital  economy        is       new       of       oil      the<br/>doc_1  0.243777  0.34262  0.34262  0.243777  0.000000  0.34262  0.243777  0.68524   <br/>doc_2  0.448321  0.00000  0.00000  0.448321  0.630099  0.00000  0.448321  0.00000</span><span id="d402" class="lv iw hi lr b fi mj lx l ly lz">cosine_similarity_matrix = cosine_similarity(vector_matrix)<br/>create_dataframe(cosine_similarity_matrix,['doc_1','doc_2'])</span><span id="e468" class="lv iw hi lr b fi mj lx l ly lz">doc_1     doc_2<br/>doc_1  1.000000  0.327871<br/>doc_2  0.327871  1.000000</span></pre><p id="c78d" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">这里，使用<strong class="jv hj"> TfidfVectorizer </strong>我们得到doc_1和doc_2之间的余弦相似度是<strong class="jv hj"> 0.32。</strong>其中<strong class="jv hj">计数矢量器</strong>返回doc_1和doc_2的余弦相似度为<strong class="jv hj"> 0.47 </strong>。<strong class="jv hj"> TfidfVectorizer </strong>惩罚文档中最频繁出现的单词，如停用词。</p></div></div>    
</body>
</html>