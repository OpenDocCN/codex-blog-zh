<html>
<head>
<title>Autograd in PyTorch — How to Apply it on a Customised Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch亲笔签名——如何将其应用于定制功能</h1>
<blockquote>原文：<a href="https://medium.com/codex/autograd-in-pytorch-how-to-apply-it-on-a-customised-function-4f0033430755?source=collection_archive---------8-----------------------#2021-07-12">https://medium.com/codex/autograd-in-pytorch-how-to-apply-it-on-a-customised-function-4f0033430755?source=collection_archive---------8-----------------------#2021-07-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="bf80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PyTorch中的亲笔签名包使我们能够以友好的方式有效地实施梯度。</p><p id="4349" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">微分是几乎所有深度学习优化算法中的关键步骤。PyTorch等深度学习框架可以轻松有效地计算梯度。它主要基于<em class="jd">计算图</em>，跟踪哪些数据通过哪些运算组合产生输出。这可以通过<em class="jd">反向传播程序实现。</em>这里，<em class="jd"> backpropagate </em>简单地说就是通过计算图进行追踪，填入关于每个参数的偏导数。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/939717e8befdc3444e6c0338f90719c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*RjcEPn6QVlzx_wP7uo_eMQ.jpeg"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><a class="ae jq" href="https://arstechnica.com/science/2019/11/limits-schlimits-its-time-to-rethink-how-we-teach-calculus/" rel="noopener ugc nofollow" target="_blank">微积分</a></figcaption></figure><h1 id="7fe0" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">为什么是渐变？</h1><p id="fe40" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我们大多数人在高中都学过微积分。不管我们喜欢与否，我们都认为微积分在我们的日常决策中起着至关重要的作用。我们观察我们周围的环境，看看环境是如何以及在什么基础上变化的。这是微积分的一部分，导数主要用于描述动态环境。在机器学习算法中，特别是深度学习，我们处理大量的参数集，这些参数控制从输入到输出的映射。如果我们遵循直觉或一些随机选择，调整这些参数可能会很麻烦。这正是微积分，尤其是梯度计算拯救我们的地方。我们使用梯度来优化用于评估模型的客观指标。梯度指导我们设置模型的参数，以实现输入到输出映射的最小差异。</p><h1 id="d548" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">亲笔签名</h1><p id="edb2" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">Autograd是集成在PyTorch中的一个包，用于方便任何类型的输入输出关系的梯度计算。这种关系甚至可以用于包含<code class="du ku kv kw kx b">for</code>和<code class="du ku kv kw kx b">if</code>语句的控制流类型函数。PyTorch通过计算图实现自动签名功能。计算图只是一个使我们能够从输出反向传播到输入的图，输出通常是我们要计算其导数的函数，输入通常是模型中的参数。先说什么是计算图。</p><h1 id="26ac" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">计算图形</h1><p id="a940" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">假设我们想计算一个非常简单的增量函数的导数</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/f504fea8a1ae52c477754eace9135b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_lL8ze0Qf69EEhpqDtVaA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">递增函数</figcaption></figure><p id="d07c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了计算Y相对于X的导数(在特定点)，PyTorch构建了一个计算图，如下图所示</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ld"><img src="../Images/7c18554cf1d2bead70ca27532ed8a8f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7Gm_n77hl7gehH_3LssTA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">计算图形</figcaption></figure><p id="528c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个计算图是在python中通过将变量定义为张量自动构建的，张量是PyToch框架的基本构建块。下面的代码构建了前面的计算图</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="fcaa" class="li js hi kx b fi lj lk l ll lm">#import pytorch library<br/>import torch<br/></span><span id="44c5" class="li js hi kx b fi ln lk l ll lm"># we want to calculate the derivative at point 3<br/>X = torch.tensor(3, requires_grad=True)</span><span id="e658" class="li js hi kx b fi ln lk l ll lm"># define the output<br/>Y = X + 10<br/>print(Y)</span></pre><p id="d9e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出将是</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="e068" class="li js hi kx b fi lj lk l ll lm">tensor(5., grad_fn=&lt;AddBackward0&gt;)</span></pre><p id="00ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看出，Y变量是设置了<code class="du ku kv kw kx b">grad_fn</code>的张量。为了激活给定变量的计算图，我们应该使用<code class="du ku kv kw kx b">requires_grad=True</code>，默认为<code class="du ku kv kw kx b">false</code>。</p><p id="20f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经建立了PyTorch计算图，我们可以应用反向传播到propgate来计算Y相对于x的导数。我们可以通过在PyTorch中调用<code class="du ku kv kw kx b">backward</code>来完成此操作。</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="f2f1" class="li js hi kx b fi lj lk l ll lm">#call backpropagation<br/>y.backward()</span></pre><p id="d981" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了获得特定点的导数的值，我们应该将注意力转向推导导数所基于的预期变量。</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="b6e7" class="li js hi kx b fi lj lk l ll lm">#access the derivative value <br/>print(x.grad)</span></pre><p id="7c58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其输出为</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="5326" class="li js hi kx b fi lj lk l ll lm">[output]: tensor(1.)</span></pre><p id="6562" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在点3处，Y相对于X变量的导数的值是1(实际上，在任意点，它都是1，因为直线的斜率是常数)。就是这样。我们得到一个函数相对于某个特定点的一个单一值的导数。我们可以将相同的概念扩展到函数和参数的广泛范围。例如，下面的代码将渐变扩展到一个具有四个不同变量的函数。我们想计算Y相对于每个变量在特定点的梯度，这里是一个向量。</p><blockquote class="lo lp lq"><p id="7d62" class="if ig jd ih b ii ij ik il im in io ip lr ir is it ls iv iw ix lt iz ja jb jc hb bi translated">在使用带有向量输入的梯度时有一个小的注意事项。你应该给反向函数提供单位向量，以便将梯度作为一个向量来访问。</p></blockquote><p id="8ea1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于输入是大小为3的向量的情况，我们有</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="d041" class="li js hi kx b fi lj lk l ll lm">#a random vector <br/>X = torch.randn(3, requires_grad=True)</span><span id="7c5e" class="li js hi kx b fi ln lk l ll lm">#the output is also a vector <br/>Y = X + 10</span><span id="aa11" class="li js hi kx b fi ln lk l ll lm"># a vector to be fed into backward<br/>v = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32)</span><span id="d9fc" class="li js hi kx b fi ln lk l ll lm">Y.backward(v)<br/>X.grad</span></pre><p id="6d77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出将是</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="d535" class="li js hi kx b fi lj lk l ll lm">tensor([1., 1., 1.])</span></pre><p id="fd70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在梯度计算中包括单位向量是雅可比乘法的结果，其中在梯度中使用。向量-雅可比乘积计算如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lu"><img src="../Images/6c49e09a7505c71c8f0bc2ff02b5fa84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sTyi7e5FnP2wXrLHQWP5rA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">向量雅可比乘积</figcaption></figure><p id="31c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果不支持反向传播中的<code class="du ku kv kw kx b">v</code>向量，PyTorch就不能产生渐变。</p><h1 id="61de" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">定制功能上的亲笔签名</h1><p id="c13d" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我们可以将反向传播应用于定制的函数，该函数甚至没有被直接定义为现成的函数，例如加法和乘法。</p><p id="a18b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个函数，形式为<a class="ae jq" href="https://d2l.ai/chapter_preliminaries/autograd.html" rel="noopener ugc nofollow" target="_blank">【1】</a>:</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="db8a" class="li js hi kx b fi lj lk l ll lm"><strong class="kx hj">def</strong> f(a):<br/>    b = a * 2<br/>    <strong class="kx hj">while</strong> b.norm() &lt; 1000:<br/>        b = b * 2<br/>    <strong class="kx hj">if</strong> b.sum() &gt; 0:<br/>        c = b<br/>    <strong class="kx hj">else</strong>:<br/>        c = 100 * b<br/>    <strong class="kx hj">return</strong> c</span></pre><p id="de94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，这个函数包含了许多循环和if语句。但是PyTorch中的自动签名功能可以轻松处理这个功能。我们可以像以前一样应用梯度计算</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="a2b4" class="li js hi kx b fi lj lk l ll lm">a = torch.randn(size=(), requires_grad=True)<br/>d = f(a)<br/>d.backward()<br/>a.grad</span></pre><p id="1fb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将给出给定随机点中定制函数的梯度。</p><h1 id="a8a6" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">脱离计算图形</h1><p id="f72c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">有时我们希望运算不是计算图的一部分。例如，在深度学习范式中，在应用反向传播后，我们希望基于某种优化算法(如梯度下降)来更新参数。然而，我们不希望更新过程成为梯度计算的一部分，因为它显然不是目标函数的一部分。那么我们如何从计算图中排除更新过程呢？</p><p id="196e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PyTorch为我们提供了一种从计算图中分离给定操作的能力。有三种不同的方式来满足这一愿望，如下所示:</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="4aa6" class="li js hi kx b fi lj lk l ll lm">#now x is part of computational graph<br/>x = torch.randn(3, requires_grad=True)</span><span id="afe1" class="li js hi kx b fi ln lk l ll lm">#first option<br/>x.requires_grad_(False)</span><span id="fedc" class="li js hi kx b fi ln lk l ll lm">#second option<br/>y = x.detach()</span><span id="b263" class="li js hi kx b fi ln lk l ll lm">#last option<br/>with torch.no_grad():<br/>    y = x + 2</span></pre><p id="c43a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的三个选项中，给定的变量和涉及的任何操作都与计算图分离，因此不是反向传播的一部分。</p><h1 id="5bd5" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">将梯度归零</h1><p id="b70b" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在PyTorch中，默认情况下，渐变是随着更多的渐变被调用而累积的。换句话说，当前梯度的结果被加到先前调用的梯度的结果上。让我们通过一个例子来阐明这一点:</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="6551" class="li js hi kx b fi lj lk l ll lm">#the grad is accumulated<br/>weight = torch.ones(3, requires_grad=True)<br/>for epoch in range(4):<br/>    model_out = (weight*3).sum()<br/>    <br/>    model_out.backward()<br/>    print(f"The {epoch}-th round of gradeint applciation",weight.grad)</span></pre><p id="8176" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出将是</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="07ee" class="li js hi kx b fi lj lk l ll lm">[output]:</span><span id="b3c2" class="li js hi kx b fi ln lk l ll lm">The 0-th round of gradeint applciation tensor([3., 3., 3.])<br/>The 1-th round of gradeint applciation tensor([6., 6., 6.])<br/>The 2-th round of gradeint applciation tensor([9., 9., 9.])<br/>The 3-th round of gradeint applciation tensor([12., 12., 12.])</span></pre><p id="ac08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型输出是3和<code class="du ku kv kw kx b">weight.</code>的乘积。通过在模型输出上应用梯度，每次我们应用梯度时，结果将是3(斜率是固定的)。我们期望看到，不管时间如何，我们应用梯度，结果将是3。然而，由于PyTorch的累积，结果是累积的，因此取决于之前梯度应用的次数。为了解决这个问题，我们可以每次将梯度归零，如下所示:</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="506d" class="li js hi kx b fi lj lk l ll lm">#the grad is NOT accumulated<br/>weight = torch.ones(3, requires_grad=True)<br/>for epoch in range(4):<br/>    model_out = (weight*3).sum()<br/>    <br/>    model_out.backward()<br/>    print(f"The {epoch}-th round of gradeint applciation", weight.grad)<br/>    <br/>    #set the grad to zero<br/>    weight.grad.zero_()</span></pre><p id="6f62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出将是</p><pre class="jf jg jh ji fd le kx lf lg aw lh bi"><span id="f4bd" class="li js hi kx b fi lj lk l ll lm">The 0-th round of gradeint applciation tensor([3., 3., 3.])<br/>The 1-th round of gradeint applciation tensor([3., 3., 3.])<br/>The 2-th round of gradeint applciation tensor([3., 3., 3.])<br/>The 3-th round of gradeint applciation tensor([3., 3., 3.])</span></pre><p id="06d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的归零，我们将得到相同的梯度，每次我们应用梯度。</p><p id="31b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样:)</p><h1 id="e1ae" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">结论</h1><p id="cc06" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在这个简短的教程中，我们接触了PyTorch框架中的自签名包和渐变的概念。我们讨论了计算图以及如何将它应用于带有向量输入的函数。我们还研究了PyTorch中梯度的分离和累积特性以及如何解决它！</p></div></div>    
</body>
</html>