<html>
<head>
<title>Mastering Web Scraping in Python: From Zero to Hero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">掌握Python中的Web抓取:从零到英雄</h1>
<blockquote>原文：<a href="https://medium.com/codex/mastering-web-scraping-in-python-from-zero-to-hero-51e27705b51b?source=collection_archive---------5-----------------------#2021-07-22">https://medium.com/codex/mastering-web-scraping-in-python-from-zero-to-hero-51e27705b51b?source=collection_archive---------5-----------------------#2021-07-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/d9a913b0fb14b20ce938b4b7c22a2c64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CGySd9PnS3B-rXIZazq00w.png"/></div></div></figure><div class=""/><p id="02fe" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">网站抓取不仅仅是用一些CSS选择器提取内容。我们在本指南中总结了多年的专业知识。有了这些新的技巧和想法，您将能够更可靠、更快速、更高效地收集数据。并获得一些您认为不存在的额外字段。</p><h2 id="723e" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">先决条件</h2><p id="484e" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">为了让代码工作，你需要安装的<a class="ae ko" href="https://www.python.org/downloads/" rel="noopener ugc nofollow" target="_blank"> python3。有些系统已经预装了它。之后，通过运行<code class="du kp kq kr ks b">pip install</code>安装所有必要的库。</a></p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="844b" class="jo jp ht ks b fi lb lc l ld le">pip install requests beautifulsoup4 pandas</span></pre><p id="b0de" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用请求库可以很容易地从URL获取HTML。然后将内容传递给BeautifulSoup，我们就可以开始用选择器获取数据和查询了。我们就不多赘述了。简而言之，你可以使用<a class="ae ko" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors" rel="noopener ugc nofollow" target="_blank"> CSS选择器</a>来获取页面元素和内容。有些需要不同的语法，但是我们稍后会发现。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="e558" class="jo jp ht ks b fi lb lc l ld le">import requests<br/>from bs4 import BeautifulSoup<br/><br/>response = requests.get("https://zenrows.com")<br/>soup = BeautifulSoup(response.content, 'html.parser')<br/><br/>print(soup.title.string) <em class="lf"># Web Data Automation Made Easy - ZenRows</em></span></pre><p id="e6c9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了避免每次都请求HTML，我们可以将它存储在一个HTML文件中，并从那里加载BeautifulSoup。对于一个简单的演示，我们可以手动完成。一个简单的方法是查看页面源代码，复制并粘贴到一个文件中。像爬虫一样，不登录就访问页面是很重要的。</p><p id="9beb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里获取HTML可能看起来是一个简单的任务，但事实并非如此。我们不会在这篇博文中涉及它，但它值得一个完整的指南。我们的建议是使用这种静态方法，因为许多网站会在几次请求后将你重定向到登录页面。有些人会显示验证码，在最坏的情况下，你的IP将被禁止。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="9fdc" class="jo jp ht ks b fi lb lc l ld le">with open("test.html") as fp:<br/>	soup = BeautifulSoup(fp, "html.parser")<br/><br/>print(soup.title.string) <em class="lf"># Web Data Automation Made Easy - ZenRows</em></span></pre><p id="5618" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们从一个文件中静态加载，我们就可以尽可能多的测试，而不会出现任何网络或阻塞问题。</p><h1 id="62b6" class="lg jp ht bd jq lh li lj ju lk ll lm jy ln lo lp kb lq lr ls ke lt lu lv kh lw bi translated">编码前探索</h1><p id="a451" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">在我们开始编码之前，我们必须理解页面的内容和结构。为此，我们知道的更简单的方法是使用浏览器检查目标页面。<em class="lf">我们将使用Chrome的DevTools，但其他浏览器也有类似的工具。</em></p><p id="3788" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，我们可以打开亚马逊上的任何产品页面，快速浏览就会显示产品的名称、价格、可用性和许多其他字段。在复制所有这些选择器之前，我们建议花几分钟时间寻找隐藏的输入、元数据和网络请求。</p><p id="550d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">小心不要用Chrome DevTools或类似的工具来做这件事。一旦Javascript和网络请求(可能)修改了内容，您就会看到它。这很烦人，但有时我们必须探索原始的HTML以避免运行Javascript。如果我们找到了所有的东西，就不需要运行一个无头浏览器——也就是木偶师——节省了时间和内存消耗。</p><p id="8a8f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">免责声明:我们不会在每个例子的代码片段中包含URL请求。他们看起来都像第一个。请记住，如果您要多次测试HTML文件，请将其存储在本地。</p><h2 id="3cb3" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">隐藏输入</h2><p id="5469" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">隐藏输入允许开发人员包含终端用户看不到也不能修改的输入字段。许多表单使用这些来包含内部id或安全令牌。</p><p id="6ed3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<a class="ae ko" href="https://www.amazon.com/dp/B08F7PTF54" rel="noopener ugc nofollow" target="_blank">亚马逊产品</a>，我们可以看到更多。有些将在其他地方或格式，但有时他们是独一无二的。不管怎样，隐藏输入的名字比类更稳定。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es lx"><img src="../Images/7daf3741db53b8c35f76a6682e533281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*BG6echd5_izVp50OaSsV2A.png"/></div></figure><h2 id="2c3a" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">[计]元数据</h2><p id="aea6" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">虽然有些内容可以通过用户界面看到，但使用元数据提取可能更容易。你可以在<a class="ae ko" href="https://www.youtube.com/watch?v=tmNXKqeUtJM" rel="noopener ugc nofollow" target="_blank"> YouTube视频</a>中获得数字格式的观看次数和YYYY-mm-dd格式的发布日期。这两个可以从可见部分通过手段获得，但没有必要。花几分钟做这些技术是值得的。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="2d93" class="jo jp ht ks b fi lb lc l ld le">interactionCount = soup.find('meta', itemprop="interactionCount")<br/>print(interactionCount['content']) <em class="lf"># 8566042</em><br/> <br/>datePublished = soup.find('meta', itemprop="datePublished")<br/>print(datePublished['content']) <em class="lf"># 2014-01-09</em></span></pre><h2 id="67fa" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">XHR请求</h2><p id="5580" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">其他一些网站决定加载一个空模板，并通过XHR请求带来所有数据。在这种情况下，只检查原始HTML是不够的。我们需要检查网络，特别是XHR请求。</p><p id="6fca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ko" href="https://www.auction.com/" rel="noopener ugc nofollow" target="_blank">拍卖</a>就是这种情况。在表格中填入任意一个城市并进行搜索。这将把您重定向到一个带有框架页面的结果页面，同时它对您输入的城市执行一些查询。</p><p id="1905" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这迫使我们使用可以执行Javascript和拦截网络请求的无头浏览器，但我们也会看到它的好处。有时您可以直接调用XHR端点，但它们通常需要cookies或其他身份验证方法。或者他们可以立即禁止你，因为这不是一个常规的用户路径。小心点。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ly"><img src="../Images/9870b6511e21a289826288ad6b69ceb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ScE04SNd8tWGTAryoNdNw.jpeg"/></div></div></figure><p id="236b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们挖到了金子。再看一下图像。</p><p id="a7f5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以拥有的所有数据都已经过清理和格式化，可以提取了。然后更多。地理位置、内部id、没有格式的数字价格、制造年份等等。</p><h1 id="98ad" class="lg jp ht bd jq lh li lj ju lk ll lm jy ln lo lp kb lq lr ls ke lt lu lv kh lw bi translated">提取可靠内容的方法和技巧</h1><p id="8ed4" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">暂时抛开你的冲动。用CSS选择器获取一切是一种选择，但还有更多。看看这些，直接用选择器之前再想一想。我们并没有说这些是不好的，而我们的是好的。不要误解我们。</p><p id="09ce" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们试图给你更多的工具和想法。那每次都会是你的决定。</p><h2 id="ae65" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">获取内部链接</h2><p id="266f" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">现在，我们将开始使用BeautifulSoup来获取有意义的内容。这个库允许我们通过id、类、伪选择器等等获取内容。我们将只介绍它的一小部分功能。</p><p id="b4a8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个例子将从页面中提取所有的内部链接。为了简单起见，只有以斜杠开头的链接才被认为是内部链接。在完整的情况下，我们应该检查域和子域。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="64e6" class="jo jp ht ks b fi lb lc l ld le">internalLinks = [<br/>	a.get('href') for a in soup.find_all('a')<br/>	if a.get('href') and a.get('href').startswith('/')]<br/>print(internalLinks)</span></pre><p id="c847" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们拥有了所有这些链接，我们就可以对它们进行重复数据删除和排队，以备将来使用。通过这样做，我们将建立一个完整的网站爬虫，而不仅仅是一个页面。由于这是一个完全不同的问题，我们想提到它并准备一篇博文来处理它的使用和可伸缩性。要抓取的页面数量可以滚雪球。</p><p id="c191" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lf">需要注意的是:在自动运行时要谨慎。你可以在几秒钟内获得数百个链接，这将导致对同一个网站的太多请求。如果不小心处理，验证码或禁令可能会适用。</em></p><h2 id="c870" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">提取社交链接和电子邮件</h2><p id="9ee0" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">另一个常见的抓取任务是提取社交链接和电子邮件。“社交链接”没有确切的定义，所以我们将基于域来获取它们。至于电子邮件，有两种选择:“mailto”链接和检查全文。</p><p id="06b0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将在这个演示中使用一个<a class="ae ko" href="https://www.webscraper.io/test-sites/e-commerce/allinone" rel="noopener ugc nofollow" target="_blank">刮擦测试站点</a>。</p><p id="a0a9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一个代码片段将获得所有的链接，与上一个类似。然后遍历所有邮件，检查是否存在任何社交域或“mailto”。在这种情况下，将该URL添加到列表中，最后打印出来。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="f158" class="jo jp ht ks b fi lb lc l ld le">links = [a.get('href') for a in soup.find_all('a')]<br/>to_extract = ["facebook.com", "twitter.com", "mailto:"]<br/>social_links = []<br/>for link in links:<br/>	for social in to_extract:<br/>		if link and social in link:<br/>			social_links.append(link)<br/>print(social_links)<br/><em class="lf"># ['mailto:****@webscraper.io',</em><br/><em class="lf"># 'https://www.facebook.com/webscraperio/',</em><br/><em class="lf"># 'https://twitter.com/webscraperio']</em></span></pre><p id="8618" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您不熟悉正则表达式，那么第二个问题就有点棘手了。简而言之，它们将尝试匹配给定搜索模式的任何文本。</p><p id="8867" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种情况下，它会尝试匹配一些字符(主要是字母和数字)，然后是[@]，然后是字符—域名—[点]，最后是两到四个字符—互联网顶级域名或TLD。例如，它会找到<code class="du kp kq kr ks b">test@example.com</code>。</p><p id="d17d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lf">注意，这个正则表达式不是一个完整的正则表达式，因为它不会匹配像</em> <code class="du kp kq kr ks b"><em class="lf">co.uk</em></code> <em class="lf">这样的组合TLD。</em></p><p id="d1cb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以在整个内容(HTML)中运行这个表达式，也可以只在文本中运行。我们使用HTML来完成，尽管我们将复制电子邮件，因为它显示在文本和href中。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="46ad" class="jo jp ht ks b fi lb lc l ld le">emails = re.findall(<br/>	r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,4}",<br/>	str(soup))<br/>print(emails) <em class="lf"># ['****@webscraper.io', '****@webscraper.io']</em></span></pre><h2 id="fcaa" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">自动解析表格</h2><p id="86e6" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">HTML表格已经存在很久了，但是它们仍然被用来显示表格内容。我们可以利用这一点，因为它们通常是结构化的和格式良好的。</p><p id="fb17" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以<a class="ae ko" href="https://en.wikipedia.org/wiki/List_of_best-selling_albums" rel="noopener ugc nofollow" target="_blank">维基百科的畅销专辑列表</a>为例，我们将把所有的值提取到一个数组和一个熊猫数据帧中。这是一个简单的例子，但是您应该像处理数据集一样处理所有的数据。</p><p id="a7a0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们首先找到一个表并遍历所有的行(“tr”)。对于每个单元格，查找单元格(“td”或“th”)。下面几行将删除维基百科表格中的注释和可折叠内容，并非绝对必要。然后，将单元格的剥离文本追加到行中，并将行追加到最终输出中。打印结果以检查一切正常。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="bae0" class="jo jp ht ks b fi lb lc l ld le">table = soup.find("table", class_="sortable")<br/>output = []<br/>for row in table.findAll("tr"):<br/>	new_row = []<br/>	for cell in row.findAll(["td", "th"]):<br/>		for sup in cell.findAll('sup'):<br/>			sup.extract()<br/>		for collapsible in cell.findAll(<br/>				class_="mw-collapsible-content"):<br/>			collapsible.extract()<br/>		new_row.append(cell.get_text().strip())<br/>	output.append(new_row)<br/><br/>print(output)<br/><em class="lf"># [</em><br/><em class="lf">#     ['Artist', 'Album', 'Released', ...],</em><br/><em class="lf">#     ['Michael Jackson', 'Thriller', '1982', ...]</em><br/><em class="lf"># ]</em></span></pre><p id="4640" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一种方法是使用<code class="du kp kq kr ks b">pandas</code>并直接导入HTML，如下所示。它将为我们处理一切:第一行将匹配标题，其余的将以正确的类型作为内容插入。<code class="du kp kq kr ks b">read_html</code>返回一个数组，所以我们取第一项，然后删除一个没有内容的列。</p><p id="16e3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦进入数据框架，我们就可以做任何操作，比如按销售额排序，因为pandas将一些列转换成了数字。或者所有索赔销售的总和。在这里并不真正有用，但你得到的想法。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="951e" class="jo jp ht ks b fi lb lc l ld le">import pandas as pd<br/><br/>table_df = pd.read_html(str(table))[0]<br/>table_df = table_df.drop('Ref(s)', 1)<br/>print(table_df.columns) <em class="lf"># ['Artist', 'Album', 'Released' ...</em><br/>print(table_df.dtypes) <em class="lf"># ... Released int64 ...</em><br/>print(table_df['Claimed sales*'].sum()) <em class="lf"># 422</em><br/>print(table_df.loc[3])<br/><em class="lf"># Artist			Pink Floyd</em><br/><em class="lf"># Album				The Dark Side of the Moon</em><br/><em class="lf"># Released			1973</em><br/><em class="lf"># Genre				Progressive rock</em><br/><em class="lf"># Total certified copies...	24.4</em><br/><em class="lf"># Claimed sales*		45</em></span></pre><h2 id="ad0e" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">从元数据而不是HTML中提取</h2><p id="cb46" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">如前所述，有很多方法可以不依赖于视觉内容而获得基本数据。让我们看一个使用<a class="ae ko" href="https://www.netflix.com/title/80189685" rel="noopener ugc nofollow" target="_blank">网飞的《巫师</a>》的例子。我们会尽力找到演员。很简单，对吧？一句俏皮话就够了。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="51b7" class="jo jp ht ks b fi lb lc l ld le">actors = soup.find(class_="item-starring").find(<br/>	class_="title-data-info-item-list")<br/>print(actors.text.split(','))<br/><em class="lf"># ['Henry Cavill', 'Anya Chalotra', 'Freya Allan']</em></span></pre><p id="a5ae" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我告诉你有十四个男女演员呢？你会试着把它们都拿走吗？如果您想自己尝试，请不要继续滚动。我会等的。</p><p id="7a71" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还没有吗？记住，有比看起来更多的东西。你知道其中的三个；在原始HTML中搜索。老实说，下面还有一个地方可以展示全部演员阵容，但要尽量避开。</p><p id="68cb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">《网飞》包括一段Schema.org片段，以及男女演员名单和许多其他数据。与YouTube的例子一样，有时使用这种方法更方便。例如，日期通常以“类似机器”的格式显示，这在抓取时更有帮助。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lz"><img src="../Images/889681efffdfee2bfae28750ae9eded9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fas6i2vdu3e7TIlsDOGi7A.jpeg"/></div></div></figure><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="6ebe" class="jo jp ht ks b fi lb lc l ld le">import json <br/> <br/>ldJson = soup.find("script", type="application/ld+json") <br/>parsedJson = json.loads(ldJson.contents[0]) <br/>print([actor['name'] for actor in parsedJson['actors']]) <br/><em class="lf"># [... 'Jodhi May', 'MyAnna Buring', 'Joey Batey' ...]</em></span></pre><p id="d19a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有时，如果我们不想呈现Javascript，这是一种实用的方法。我们将使用Instagram Billie Eilish的简介展示一个例子。他们是众所周知的阻挡者。访问几个页面后，您将被重定向到登录页面。抓取Instagram时要小心，使用本地HTML进行测试。</p><p id="ffbe" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将在以后的文章中讨论如何避免这些阻塞或重定向。敬请期待！</p><p id="6748" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通常的方法是搜索一个类，在我们的例子中是“Y8-fY”。我们建议不要使用这些类，因为它们可能会改变。它们看起来是自动生成的。许多现代网站都使用这种CSS，并且每次改变都会生成，这意味着我们不能依赖它们。</p><p id="6412" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">B计划:<code class="du kp kq kr ks b">"header ul &gt; li"</code>，对吗？会有用的。但是我们需要Javascript渲染，因为它在第一次加载时不存在。如前所述，我们应该尽量避免这种情况。</p><p id="1ddb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">看一下源HTML:标题和描述包括关注者、跟随者和帖子号。这可能是一个问题，因为它们是字符串格式，但我们可以克服它。如果我们只想要那些数据，我们就不需要一个无头浏览器。太好了！</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="34d6" class="jo jp ht ks b fi lb lc l ld le">metaDescription = soup.find("meta", {'name': 'description'})<br/>print(metaDescription['content'])<br/><em class="lf"># 87.9m Followers, 0 Following, 493 Posts ...</em></span></pre><h2 id="0396" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">隐藏的电子商务产品信息</h2><p id="daf9" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">结合一些已经看到的技术，我们的目标是提取不可见的产品信息。我们的第一个例子是Shopify电子商务，<a class="ae ko" href="https://www.spigen.com/collections/tesla/products/tesla-model-3-ta100-sticker?variant=39270568230959" rel="noopener ugc nofollow" target="_blank"> Spigen </a>。</p><p id="fb2e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你想的话，可以先自己看看。</p><p id="485a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">提示:寻找品牌🤐。</p><p id="425d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将能够可靠地提取它，而不是从产品名称或面包屑中，因为我们不能说它们是否总是相关的。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lz"><img src="../Images/f96c392380ac11711c071c3fce23de4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6nho1iPKfplBhPEprtygg.jpeg"/></div></div></figure><p id="0f23" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你找到他们了吗？在这种情况下，他们使用“itemprop”并包括来自schema.org的产品和报价。我们可以通过查看表单或“添加到购物车”按钮来判断产品是否有货。但是没有必要，我们可以相信<code class="du kp kq kr ks b">itemprop="availability"</code>。至于brand，与用于YouTube的代码片段相同，但是将属性名改为“brand”</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="3cca" class="jo jp ht ks b fi lb lc l ld le">brand = soup.find('meta', itemprop="brand")<br/>print(brand['content']) <em class="lf"># Tesla</em></span></pre><p id="5195" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一个Shopify例子:<a class="ae ko" href="https://nomz.com/collections/energy-bites/products/gift-box?variant=31459597090948" rel="noopener ugc nofollow" target="_blank"> nomz </a>。我们希望提取评分计数和平均值，可以在HTML中访问，但有些隐藏。使用CSS隐藏平均评级。</p><p id="029e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">附近有一个屏幕阅读器专用标签，上面有平均值和计数。那两个包括文字，没什么大不了的。但是我们知道我们可以做得更好。</p><p id="b6bc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你检查源代码，这是一个简单的问题。产品模式将是您看到的第一个东西。应用网飞例子中的相同知识，获得第一个“ld+json”块，解析json，所有内容都将可用！</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="0118" class="jo jp ht ks b fi lb lc l ld le">import json<br/><br/>ldJson = soup.find("script", type="application/ld+json")<br/>parsedJson = json.loads(ldJson.contents[0])<br/>print(parsedJson["aggregateRating"]["ratingValue"]) <em class="lf"># 4.9</em><br/>print(parsedJson["aggregateRating"]["reviewCount"]) <em class="lf"># 57</em><br/>print(parsedJson["weight"]) <em class="lf"># 0.492kg -&gt; extra, not visible in UI</em></span></pre><p id="75bc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后但同样重要的是，我们将利用数据属性，这在电子商务中也很常见。在考察<a class="ae ko" href="https://maruccisports.com/wood-bats/" rel="noopener ugc nofollow" target="_blank"> Marucci运动木棒</a>的时候，我们可以看到每一款产品都有几个数据点会派上用场。数字格式的价格、ID、产品名称和类别。我们那里有我们可能需要的所有数据。</p><pre class="kt ku kv kw fd kx ks ky kz aw la bi"><span id="91de" class="jo jp ht ks b fi lb lc l ld le">products = []<br/>cards = soup.find_all(class_="card")<br/>for card in cards:<br/>	products.append({<br/>		'id': card.get('data-entity-id'),<br/>		'name': card.get('data-name'),<br/>		'category': card.get('data-product-category'),<br/>		'price': card.get('data-product-price')<br/>	})<br/>print(products)<br/><em class="lf"># [</em><br/><em class="lf">#    {</em><br/><em class="lf">#	"category": "Wood Bats, Wood Bats/Professional Cuts",</em><br/><em class="lf">#	"id": "1945",</em><br/><em class="lf">#	"name": "6 Bat USA Professional Cut Bundle",</em><br/><em class="lf">#	"price": "579.99"</em><br/><em class="lf">#    },</em><br/><em class="lf">#    {</em><br/><em class="lf">#	"category": "Wood Bats, Wood Bats/Pro Model",</em><br/><em class="lf">#	"id": "1804",</em><br/><em class="lf">#	"name": "M-71 Pro Model",</em><br/><em class="lf">#	"price": "159.99"</em><br/><em class="lf">#    },</em><br/><em class="lf">#    ...</em><br/><em class="lf"># ]</em></span></pre><h1 id="cf8c" class="lg jp ht bd jq lh li lj ju lk ll lm jy ln lo lp kb lq lr ls ke lt lu lv kh lw bi translated">剩余的障碍</h1><p id="88fb" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">好吧！你从那一页得到了所有的数据。现在你必须把它复制到第二个，然后第三个。规模很重要。所以不会被禁赛。</p><p id="2d19" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是你也必须转换和存储这些数据:CSV文件或数据库，无论你需要什么。嵌套字段不容易导出为任何一种格式。</p><p id="04b5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经占用了你足够的时间。吸收所有这些新信息，在日常工作中使用它们。同时，我们将致力于以下指南，以克服所有这些障碍！</p><h1 id="c958" class="lg jp ht bd jq lh li lj ju lk ll lm jy ln lo lp kb lq lr ls ke lt lu lv kh lw bi translated">结论</h1><p id="75ab" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">我们希望你参加三节课:</p><ol class=""><li id="2d0f" class="ma mb ht is b it iu ix iy jb mc jf md jj me jn mf mg mh mi bi translated">CSS选择器很好，但是还有其他选择。</li><li id="6b83" class="ma mb ht is b it mj ix mk jb ml jf mm jj mn jn mf mg mh mi bi translated">一些内容是隐藏的，或者不存在，但是可以通过元数据访问。</li><li id="0717" class="ma mb ht is b it mj ix mk jb ml jf mm jj mn jn mf mg mh mi bi translated">尽量避免加载Javascript和无头浏览器来提升性能。</li></ol><p id="2f5a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每一种都有优缺点，不同的方法，以及很多很多的选择。写一本完整的指南会是一本很长的书，而不是一篇博客文章。</p><p id="c8d0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你知道更多的网站抓取技巧或者对应用它们有疑问，请联系我们。</p><p id="398b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">记住，我们讨论了抓取，但是还有更多:爬行、避免被阻塞、转换和存储内容、扩展基础设施等等。敬请期待！</p><p id="45fb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不要忘记看看本系列的其他文章。<br/> + <a class="ae ko" href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-scaling-to-distributed-crawling?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=mastering_scraping" rel="noopener ugc nofollow" target="_blank">缩放到分布式爬行</a> (4/4) <br/> + <a class="ae ko" href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-crawling-from-scratch?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=mastering_scraping" rel="noopener ugc nofollow" target="_blank">从零开始爬行</a> (3/4) <br/> + <a class="ae ko" href="https://www.zenrows.com/blog/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=mastering_scraping" rel="noopener ugc nofollow" target="_blank">像忍者一样躲避阻挡</a> (2/4)</p><p id="675d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你喜欢这个内容，请分享。👇</p></div><div class="ab cl mo mp gp mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hb hc hd he hf"><p id="c496" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lf">原载于</em><a class="ae ko" href="https://www.zenrows.com/blog/mastering-web-scraping-in-python-from-zero-to-hero?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=mastering_scraping" rel="noopener ugc nofollow" target="_blank"><em class="lf">https://www.zenrows.com</em></a></p></div></div>    
</body>
</html>