<html>
<head>
<title>A Comprehensive Tutorial to Pytorch DistributedDataParallel</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch的综合教程</h1>
<blockquote>原文：<a href="https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51?source=collection_archive---------0-----------------------#2021-08-16">https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51?source=collection_archive---------0-----------------------#2021-08-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8d1ab0911e8c5da6cbe26be3b510a8ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_ATYVia3tYuo0E-5"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@xps?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> XPS </a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="2c4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">学校有限的计算资源阻碍了跨多个GPU的分布式训练。我第一次开始学是在我加入微软做实习生的时候。用DDP(distributed data parallel的缩写)包装模型基本上是一件容易的工作。令我沮丧的是，我无法正确调整我的多gpu工作流程，包括<code class="du jt ju jv jw b">DataLoader</code>、<code class="du jt ju jv jw b">Sampler</code>、训练和评估。网上的教程和博客几乎不包括所有这些东西。在解决了我遇到的这么多bug之后，我提出了目前为止的最佳实践。</p><p id="0a90" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个博客中，我想与所有DDP初学者分享我的代码和见解。我希望这个博客能帮助他们避免可怕的错误。我不打算详细解释DDP是如何工作的，相反，我提供了让模型在多个GPU中运行所需的最基本的知识。注意<strong class="ix hj">我只在一台有多个GPU的机器上介绍DDP，这是最一般的情况(</strong>不然就用<a class="ae iu" href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html" rel="noopener ugc nofollow" target="_blank"> <em class="jx">官博</em> </a> <strong class="ix hj">里说的模型并行)。</strong>本博客组织如下:</p><ul class=""><li id="569c" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated"><a class="ae iu" href="#e774" rel="noopener ugc nofollow">DDP概述</a></li><li id="57db" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated"><a class="ae iu" href="#854f" rel="noopener ugc nofollow">实施DDP工作流程(步骤1-6)</a></li><li id="c864" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated"><a class="ae iu" href="#63e5" rel="noopener ugc nofollow">关于dist.barrier()的问题</a></li></ul><p id="41d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">顺便说一句，我正在使用<code class="du jt ju jv jw b">torch==1.7.1</code>，但我认为它在<code class="du jt ju jv jw b">torch&gt;=1.7.1</code>中会工作得很好。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="e774" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">DDP概述</h1><p id="c88c" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">首先，我们必须了解分布式培训中使用的几个术语:</p><ul class=""><li id="3aa8" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated"><strong class="ix hj"> <em class="jx">主节点</em> </strong> <em class="jx">:负责同步、制作副本、加载模型、写日志的主gpu</em></li><li id="6f34" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated"><strong class="ix hj"> <em class="jx">进程组:</em> </strong> <em class="jx">如果你想在K个GPU上训练/测试模型，那么K个进程组成一个组，它由一个</em> <strong class="ix hj"> <em class="jx">后台支持(</em> </strong> <em class="jx"> pytorch替你管理，根据</em><a class="ae iu" href="https://pytorch.org/docs/1.9.0/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel" rel="noopener ugc nofollow" target="_blank"><em class="jx"/></a><em class="jx">，</em><strong class="ix hj"><em class="jx">nccl</em></strong><em class="jx"/><strong class="ix hj"><em class="jx">是</em>T29】</strong></li><li id="3664" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated"><strong class="ix hj"> <em class="jx">等级:</em> </strong> <em class="jx">在流程组内，每个流程都是通过其等级来标识的，从0到K-1；</em></li><li id="e85f" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated"><strong class="ix hj"> <em class="jx">世界大小:</em> </strong> <em class="jx">组内进程数即gpu数——K. </em></li></ul><p id="be0c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Pytorch为分布式训练提供了两个设置:torch.nn.DataParallel (DP)和torch . nn . parallel . distributeddataparallel(DDP)，其中后者是<a class="ae iu" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html?highlight=distributed" rel="noopener ugc nofollow" target="_blank">官方推荐的</a>。简而言之，DDP比DP更快，更灵活。<strong class="ix hj">DDP做的基本事情是将模型复制到多个GPU，从它们那里收集梯度，平均梯度以更新模型，然后在所有K个进程上同步模型。</strong>我们还可以通过<code class="du jt ju jv jw b">torch.distributed.gather/scatter/reduce</code>聚集/分散除梯度以外的张量/物体。</p><p id="eff5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果模型可以放在一个gpu上(可以用<code class="du jt ju jv jw b">batch_size=1</code>在一个gpu上训练它),并且我们想要在K个GPU上训练/测试它，DDP的最佳实践是将模型复制到K个GPU上(DDP类会自动为您完成这项工作),并将数据加载器分成K个不重叠的组，分别提供给K个模型。</p><p id="435e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，事情对我们来说是清楚的。我们必须做以下事情:</p><ol class=""><li id="1b13" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js lw ke kf kg bi translated"><strong class="ix hj">设置流程组</strong>，三行代码，无需修改；</li><li id="326c" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js lw ke kf kg bi translated"><strong class="ix hj">将数据加载器拆分到组中的每个进程，</strong>这可以通过torch . utils . data . distributed sampler或任何定制的采样器轻松实现；</li><li id="1cad" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js lw ke kf kg bi translated"><strong class="ix hj">用DDP包装我们的模型，</strong>这是一行代码，几乎不需要修改；</li><li id="63b8" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js lw ke kf kg bi translated"><strong class="ix hj">训练/测试我们的模型</strong>，和在1个gpu上一样；</li><li id="3128" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js lw ke kf kg bi translated"><strong class="ix hj">清理进程组(像C中的free)<em class="jx">、</em>、</strong>这是一行代码。</li><li id="cf20" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js lw ke kf kg bi translated"><em class="jx">可选:在进程间收集额外的数据(分布式测试可能需要)，基本上就是一行代码；</em></li></ol><p id="7195" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">很简单，对吧？事实上的确如此。让我们一步一步来。</p><h1 id="854f" class="kt ku hi bd kv kw lx ky kz la ly lc ld le lz lg lh li ma lk ll lm mb lo lp lq bi translated">1.设置流程组</h1><p id="2c12" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">在这里，没有额外的步骤。</p><pre class="mc md me mf fd mg jw mh mi aw mj bi"><span id="f494" class="mk ku hi jw b fi ml mm l mn mo">import torch.distributed as dist</span><span id="a095" class="mk ku hi jw b fi mp mm l mn mo">def setup(rank, world_size):</span><span id="3c21" class="mk ku hi jw b fi mp mm l mn mo">    os.environ['MASTER_ADDR'] = 'localhost'<br/>    os.environ['MASTER_PORT'] = '12355'</span><span id="8ab4" class="mk ku hi jw b fi mp mm l mn mo">    dist.init_process_group("nccl", rank=rank, world_size=world_size)</span></pre><h1 id="40b6" class="kt ku hi bd kv kw lx ky kz la ly lc ld le lz lg lh li ma lk ll lm mb lo lp lq bi translated">2.拆分数据加载器</h1><p id="0dd0" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">我们可以很容易地通过torch . utils . data . distributed . distributed sampler来拆分我们的dataloader<strong class="ix hj">采样器返回一个遍历索引的迭代器，这些索引被送入data loader进行bachify。</strong></p><p id="cd8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">分布式采样器将数据集的总索引拆分成<em class="jx"> world_size </em>个部分，并在每个进程中均匀地分配给数据加载器，没有重复。</p><pre class="mc md me mf fd mg jw mh mi aw mj bi"><span id="0972" class="mk ku hi jw b fi ml mm l mn mo">from torch.utils.data.distributed import DistributedSampler</span><span id="f5a4" class="mk ku hi jw b fi mp mm l mn mo">def prepare(rank, world_size, batch_size=32, pin_memory=False, num_workers=0):<br/>    dataset = Your_Dataset()<br/>    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)<br/>    <br/>    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)<br/>    <br/>    return dataloader</span></pre><p id="ce19" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设K=3，数据集长度为10。我们必须理解<strong class="ix hj">分布式采样器对指数进行均匀划分。</strong></p><ul class=""><li id="cfa2" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated">如果我们在定义分布式采样器时设置了<code class="du jt ju jv jw b">drop_last=False</code>，它将自动填充<strong class="ix hj">。</strong>例如，当<em class="jx">排名</em> =1时，它将索引[0，1，2，3，4，5，6，7，8，9]拆分为[0，3，6，9]；<em class="jx">排名</em> =2时，它将索引[0，4，7，0]拆分为T17排名 =3时，它将索引[2，5，8，0]拆分为[2，5，8，0]。如您所见，这种填充可能会导致问题，因为填充的0 <strong class="ix hj">是数据记录</strong>。</li><li id="4caf" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">否则，<strong class="ix hj">会剥离拖尾元件</strong>。例如，它在<em class="jx">等级</em> =1处将索引拆分为[0，3，6]，在<em class="jx">等级</em> =2处将索引拆分为[1，4，7]，在<em class="jx">等级</em> =3处将索引拆分为[2，5，8]。在这种情况下，它修改了9，使索引数可以被world_size整除。</li></ul><p id="b37e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">定制我们的采样器非常简单。我们只需要创建一个类，然后定义它的<code class="du jt ju jv jw b">__iter__()</code>和<code class="du jt ju jv jw b">__len__()</code>函数。更多细节请参考<a class="ae iu" href="https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler" rel="noopener ugc nofollow" target="_blank">官方文件</a>。</p><p id="5eb8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">顺便说一下，<strong class="ix hj">分布式训练时最好设置</strong> <code class="du jt ju jv jw b"><strong class="ix hj">num_workers=0</strong></code> <strong class="ix hj">，因为在子进程中创建额外的线程可能会有问题。我还发现</strong> <code class="du jt ju jv jw b"><strong class="ix hj">pin_memory=False</strong></code> <strong class="ix hj">避免了许多可怕的错误，也许这种事情是机器特有的，如果你的读者探索了更多的细节，请给我发电子邮件。</strong></p><h1 id="a21e" class="kt ku hi bd kv kw lx ky kz la ly lc ld le lz lg lh li ma lk ll lm mb lo lp lq bi translated">3.用DDP包裹模型</h1><p id="fbd5" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">我们应该首先将我们的模型移动到特定的gpu(回想一下，一个模型副本驻留在一个gpu中)，然后我们用DDP类包装它。下面的函数接受一个参数<em class="jx"> rank </em>，我们很快会介绍它。现在，我们只需记住<em class="jx">等级</em>等于gpu id <em class="jx">。</em></p><pre class="mc md me mf fd mg jw mh mi aw mj bi"><span id="d90f" class="mk ku hi jw b fi ml mm l mn mo">from torch.nn.parallel import DistributedDataParallel as DDP</span><span id="71a5" class="mk ku hi jw b fi mp mm l mn mo">def main(rank, world_size):<br/>    # setup the process groups<br/>    setup(rank, world_size)</span><span id="4a58" class="mk ku hi jw b fi mp mm l mn mo">    # prepare the dataloader<br/>    dataloader = prepare(rank, world_size)<br/>    <br/>    # instantiate the model(it's your own model) and move it to the right device<br/>    model = Model().to(rank)<br/>    <br/>    # wrap the model with DDP<br/>    # device_ids tell DDP where is your model<br/>    # output_device tells DDP where to output, in our case, it is rank<br/>    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model</span><span id="75e7" class="mk ku hi jw b fi mp mm l mn mo">    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)</span></pre><p id="e3d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里有一些棘手的事情:</p><ul class=""><li id="bdd8" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated">当我们想访问DDP包装模型的一些<strong class="ix hj">定制的</strong>属性时，我们必须引用<code class="du jt ju jv jw b">model.module</code>。也就是说，我们的模型实例被保存为DDP模型的一个<code class="du jt ju jv jw b">module</code>属性。如果我们分配一些属性<code class="du jt ju jv jw b">xxx</code>而不是内置属性或函数，我们必须通过<code class="du jt ju jv jw b">model.module.xxx</code>来访问它们。</li><li id="02cf" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">当我们保存DDP模型时，我们的<code class="du jt ju jv jw b">state_dict</code>会给所有参数添加一个<em class="jx">模块前缀。</em></li><li id="7d6b" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">因此，如果我们想要将一个DDP保存的模型加载到一个非DDP模型中，我们必须手工去掉额外的前缀。我在下面提供我的代码:</li></ul><pre class="mc md me mf fd mg jw mh mi aw mj bi"><span id="bd06" class="mk ku hi jw b fi ml mm l mn mo"># in case we load a DDP model checkpoint to a non-DDP model</span><span id="294b" class="mk ku hi jw b fi mp mm l mn mo">model_dict = OrderedDict()<br/>pattern = re.compile('module.')</span><span id="6f2d" class="mk ku hi jw b fi mp mm l mn mo">for k,v in state_dict.items():<br/>    if re.search("module", k):<br/>        model_dict[re.sub(pattern, '', k)] = v</span><span id="d11d" class="mk ku hi jw b fi mp mm l mn mo">    else:<br/>        model_dict = state_dict</span><span id="7dd0" class="mk ku hi jw b fi mp mm l mn mo">model.load_state_dict(model_dict)</span></pre><h1 id="11c8" class="kt ku hi bd kv kw lx ky kz la ly lc ld le lz lg lh li ma lk ll lm mb lo lp lq bi translated">4.训练/测试我们的模型</h1><p id="93dd" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">这部分是实现DDP的关键。首先我们需要知道多进程的基础:<strong class="ix hj">所有子进程和父进程一起运行相同的代码</strong>。</p><p id="aa72" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在PyTorch中，torch.multiprocessing提供了创建并行进程的便捷方式。正如官方文件所说，</p><blockquote class="mq mr ms"><p id="70b5" class="iv iw jx ix b iy iz ja jb jc jd je jf mt jh ji jj mu jl jm jn mv jp jq jr js hb bi translated">下面的<code class="du jt ju jv jw b">spawn</code>函数解决了这些问题，负责错误传播、无序终止，并在检测到其中一个进程出错时主动终止进程。</p></blockquote><p id="e1b4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以，用<code class="du jt ju jv jw b">spawn</code>是个不错的选择。</p><p id="0404" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们的脚本中，我们应该在生成并行流程之前定义一个训练/测试函数:</p><pre class="mc md me mf fd mg jw mh mi aw mj bi"><span id="4482" class="mk ku hi jw b fi ml mm l mn mo">def main(rank, world_size):<br/>    # setup the process groups<br/>    setup(rank, world_size)</span><span id="68a5" class="mk ku hi jw b fi mp mm l mn mo">    # prepare the dataloader<br/>    dataloader = prepare(rank, world_size)<br/>    <br/>    # instantiate the model(it's your own model) and move it to the right device<br/>    model = Your_Model().to(rank)<br/>    <br/>    # wrap the model with DDP<br/>    # device_ids tell DDP where is your model<br/>    # output_device tells DDP where to output, in our case, it is rank<br/>    # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model</span><span id="7642" class="mk ku hi jw b fi mp mm l mn mo">    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)</span><span id="1f1b" class="mk ku hi jw b fi mp mm l mn mo">    #################### The above is defined previously<br/>   <br/>    optimizer = Your_Optimizer()<br/>    loss_fn = Your_Loss()</span><span id="b5cd" class="mk ku hi jw b fi mp mm l mn mo">    for epoch in epochs:<br/>        # if we are using DistributedSampler, we have to tell it which epoch this is<br/>        dataloader.sampler.set_epoch(epoch)       <br/>        <br/>        for step, x in enumerate(dataloader):<br/>            optimizer.zero_grad(set_to_none=True)<br/>            <br/>            pred = model(x)<br/>            label = x['label']<br/>            <br/>            loss = loss_fn(pred, label)<br/>            loss.backward()<br/>            optimizer.step()</span><span id="7580" class="mk ku hi jw b fi mp mm l mn mo">    cleanup()</span></pre><p id="f5d3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个<code class="du jt ju jv jw b">main</code>函数运行在每个并行进程中。我们现在需要通过<code class="du jt ju jv jw b">spawn</code>方法调用它。在我们的<code class="du jt ju jv jw b">.py</code>脚本中，我们写道:</p><pre class="mc md me mf fd mg jw mh mi aw mj bi"><span id="feed" class="mk ku hi jw b fi ml mm l mn mo">import torch.multiprocessing as mp<br/>if __name__ == '__main__':<br/>    # suppose we have 3 gpus<br/>    world_size = 3    </span><span id="1e36" class="mk ku hi jw b fi mp mm l mn mo">    mp.spawn(<br/>        main,<br/>        args=(world_size),<br/>        nprocs=world_size<br/>    )</span></pre><p id="7057" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还记得<code class="du jt ju jv jw b">main</code>的第一个论点是<em class="jx">排位吗？</em>由<code class="du jt ju jv jw b">mp.spawn</code>自动传递给各个流程，我们不需要显式传递。<code class="du jt ju jv jw b"><strong class="ix hj">rank=0</strong></code>默认情况下<strong class="ix hj">是主节点。等级的范围是从0到K-1(在我们的例子中是2)。</strong></p><h1 id="2593" class="kt ku hi bd kv kw lx ky kz la ly lc ld le lz lg lh li ma lk ll lm mb lo lp lq bi translated">5.清理进程组</h1><p id="9b4b" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated"><code class="du jt ju jv jw b">main</code>功能的最后一行是清理功能，它是:</p><pre class="mc md me mf fd mg jw mh mi aw mj bi"><span id="ba88" class="mk ku hi jw b fi ml mm l mn mo">def cleanup():<br/>    dist.destroy_process_group()</span></pre><p id="ecf1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">太棒了。我们已经完成了分布式培训/测试的基本工作流程！</p><h1 id="44f7" class="kt ku hi bd kv kw lx ky kz la ly lc ld le lz lg lh li ma lk ll lm mb lo lp lq bi translated">6.可选:G <em class="mw">其他进程间的额外数据</em></h1><p id="6950" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">有时我们需要从所有过程中收集一些数据，比如测试结果。我们可以很容易地通过<code class="du jt ju jv jw b">dist.all_gather</code>聚集张量，通过<code class="du jt ju jv jw b">dist.all_gather_object</code>聚集物体。</p><p id="6935" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不失一般性，我假设我们想要收集python对象。对象的唯一约束是它必须是可序列化的，这基本上是python中的一切。在使用 <code class="du jt ju jv jw b"><strong class="ix hj">all_gather_xxx</strong></code> <strong class="ix hj">之前，应该先赋值</strong> <code class="du jt ju jv jw b"><strong class="ix hj">torch.cuda.set_device(rank)</strong></code> <strong class="ix hj">。和</strong>、<strong class="ix hj">一样，如果我们要在物体中存储一个张量，它必须位于</strong>、<code class="du jt ju jv jw b"><strong class="ix hj">output_device</strong></code>、<strong class="ix hj">。</strong></p><pre class="mc md me mf fd mg jw mh mi aw mj bi"><span id="2710" class="mk ku hi jw b fi ml mm l mn mo">def main(rank, world_size):<br/>    torch.cuda.set_device(rank)<br/>    data = {<br/>        'tensor': torch.ones(3,device=rank) + rank,<br/>        'list': [1,2,3] + rank,<br/>        'dict': {'rank':rank}   <br/>    }<br/>    <br/>    # we have to create enough room to store the collected objects<br/>    outputs = [None for _ in range(world_size)]<br/>    # the first argument is the collected lists, the second argument is the data unique in each process<br/>    dist.all_gather_object(outputs, data)</span><span id="7e7e" class="mk ku hi jw b fi mp mm l mn mo">    # we only want to operate on the collected objects at master node<br/>    if rank == 0:<br/>        print(outputs)</span></pre><h1 id="63e5" class="kt ku hi bd kv kw lx ky kz la ly lc ld le lz lg lh li ma lk ll lm mb lo lp lq bi translated">关于dist.barrier()的问题</h1><p id="0378" class="pw-post-body-paragraph iv iw hi ix b iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js hb bi translated">最让我困惑的是<strong class="ix hj">什么时候用</strong> <code class="du jt ju jv jw b"><strong class="ix hj">dist.barrier()</strong></code> <strong class="ix hj">。</strong>如文档所说，<strong class="ix hj">它同步进程</strong>。换句话说，<strong class="ix hj">它阻塞进程，直到所有进程都到达同一行代码:</strong> <code class="du jt ju jv jw b"><strong class="ix hj">dist.barrier()</strong></code>。我把它的用法总结如下:</p><ol class=""><li id="3211" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js lw ke kf kg bi translated">训练时我们<strong class="ix hj">不需要</strong>，因为DDP自动为我们做了(在<code class="du jt ju jv jw b">loss.backward()</code>)；</li><li id="541c" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js lw ke kf kg bi translated">我们<strong class="ix hj">在收集数据时不需要它，因为<code class="du jt ju jv jw b">dist.all_gather_object</code>为我们做了；</strong></li><li id="7dd9" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js lw ke kf kg bi translated">当强制执行代码的执行顺序时，我们需要它，<a class="ae iu" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="noopener ugc nofollow" target="_blank">比如一个进程加载另一个进程保存的模型</a>(我很难想象这种场景是需要的)。</li></ol></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><p id="6911" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，我们从头开始学习如何在我们的模型中实现DDP。希望每个读到这篇文章的人都能从中受益。谢谢你。</p></div></div>    
</body>
</html>