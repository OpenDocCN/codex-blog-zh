<html>
<head>
<title>Review — EfficientDet: Scalable and Efficient Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾— EfficientDet:可扩展且高效的对象检测</h1>
<blockquote>原文：<a href="https://medium.com/codex/review-efficientdet-scalable-and-efficient-object-detection-ed9ebc70f873?source=collection_archive---------10-----------------------#2021-07-29">https://medium.com/codex/review-efficientdet-scalable-and-efficient-object-detection-ed9ebc70f873?source=collection_archive---------10-----------------------#2021-07-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0848" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">胜过<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-amoebanet-regularized-evolution-for-image-classifier-architecture-search-image-278f5c077a4a">阿米巴内特</a> + <a class="ae ix" href="https://sh-tsang.medium.com/review-nas-fpn-learning-scalable-feature-pyramid-architecture-for-object-detection-f29039f94373" rel="noopener"> NAS-FPN </a> + <a class="ae ix" href="https://sh-tsang.medium.com/review-autoaugment-learning-augmentation-strategies-from-data-image-classification-af27dea9a839" rel="noopener"> AA </a>，<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank">ResNet</a>+<a class="ae ix" href="https://sh-tsang.medium.com/review-nas-fpn-learning-scalable-feature-pyramid-architecture-for-object-detection-f29039f94373" rel="noopener">NAS-FPN</a>+<a class="ae ix" href="https://sh-tsang.medium.com/review-autoaugment-learning-augmentation-strategies-from-data-image-classification-af27dea9a839" rel="noopener">AA</a>，<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"> RetinaNet </a>，<a class="ae ix" rel="noopener" href="/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4"> Mask R-CNN </a>，以及<a class="ae ix" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank"> YOLOv3 </a></h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es iy"><img src="../Images/6bba73a9f710ea0e4e78b02bf1ce3885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*7jYXbBTj3MMd607NY8EdrA.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">型号FLOPs与COCO精度</strong></figcaption></figure><p id="0c90" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di">在</span>这个故事中，<strong class="jn hj"> EfficientDet:可扩展和高效的对象检测</strong>，(EfficientDet)，由谷歌研究，大脑团队，进行评论。在本文中:</p><ul class=""><li id="5111" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">首先，<strong class="jn hj">提出了一种加权双向特征金字塔网络(BiFPN) </strong>，它允许简单和快速的多尺度特征融合。</li><li id="9efd" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">然后，<strong class="jn hj">还提出了一种复合缩放方法</strong>，其可以同时统一缩放所有主干、特征网络和盒/类预测网络的分辨率、深度和宽度。</li><li id="9728" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">最后，以<a class="ae ix" href="https://sh-tsang.medium.com/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-image-classification-ef67b0f14a4d" rel="noopener"><strong class="jn hj">efficient net</strong></a><strong class="jn hj">为骨干</strong>的<strong class="jn hj">形成了一族对象检测器<strong class="jn hj"> EfficientDet </strong>，始终实现比现有技术好得多的效率，如上所示。</strong></li></ul><p id="acfc" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">这是一篇发表在<strong class="jn hj"> 2020年CVPR </strong>的论文，被<strong class="jn hj">引用超过600次</strong>。(<a class="le lf ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----ed9ebc70f873--------------------------------" rel="noopener" target="_blank">曾植和</a> @中)</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="325c" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">概述</h1><ol class=""><li id="6a09" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg mj kw kx ky bi translated"><strong class="jn hj">先前技术为</strong> <a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> <strong class="jn hj"> FPN </strong> </a></li><li id="77c7" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">双向特征金字塔网络</strong></li><li id="619a" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">加权bip pn</strong></li><li id="4b83" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj"> EfficientDet:网络架构</strong></li><li id="65d8" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">复合缩放法</strong></li><li id="6498" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj"> SOTA比较</strong></li><li id="9a97" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">消融研究</strong></li></ol></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="3f7f" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 1。</strong>先前技术为<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mk"><img src="../Images/1203141a05fb938b54aebb5f45c05337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RSIrs-OGvsUzaH36Qqb-YQ.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">最先进的</strong><a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="bd jk">FPN</strong></a><strong class="bd jk">s</strong></figcaption></figure><ul class=""><li id="a680" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">给定一个多尺度特征列表:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mp"><img src="../Images/e04a2d15e70b859fd08ee0ab4d8f84ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*eLffFPOff44r_EEupCrpKQ.png"/></div></figure><ul class=""><li id="9c8d" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">其中<em class="mq"> Pin_li </em>代表级别<em class="mq"> li </em>的特征。</li><li id="bb99" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">目标是找到一个转换<em class="mq"> f </em>，它可以有效地聚合不同的特性并输出一系列新特性:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mr"><img src="../Images/37026fceb62c4a56eeac8014b180e71c.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*-oPfvKLA-0yLIWB5n309og.png"/></div></figure><h2 id="ebf5" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">1.1.<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a></h2><ul class=""><li id="23df" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated"><a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>采用3-7级输入特性:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ng"><img src="../Images/5947ab45d169eb5332e2631a89afda74.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*-C38J7OnyyfIbLAUq-lsbg.png"/></div></figure><ul class=""><li id="1259" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">例如，如果输入分辨率为640×640，那么<em class="mq"> Pin_ </em> 3表示分辨率为80×80 (640/ = 80)的特征级别3，而<em class="mq"> Pin_ </em> 7表示分辨率为5×5的特征级别7。</li><li id="c97d" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">传统的<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>自顶向下聚合多尺度特征；</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nh"><img src="../Images/ca9b65595acdee6c37890f8c09f1b2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*t47Ny6dwuEzArn10YwxuYA.png"/></div></figure><ul class=""><li id="17f9" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">其中Resize通常是用于分辨率匹配的上采样或下采样操作。</li></ul><blockquote class="ni nj nk"><p id="d96d" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated"><strong class="jn hj">常规的自上而下</strong><a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jn hj"/></a><strong class="jn hj"/>本来就受到<strong class="jn hj">单向信息流的限制。</strong></p></blockquote><h2 id="40d3" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">1.2.<a class="ae ix" href="https://sh-tsang.medium.com/review-nas-fpn-learning-scalable-feature-pyramid-architecture-for-object-detection-f29039f94373" rel="noopener">FPN国家科学院</a></h2><ul class=""><li id="8e51" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">最近，<a class="ae ix" href="https://sh-tsang.medium.com/review-nas-fpn-learning-scalable-feature-pyramid-architecture-for-object-detection-f29039f94373" rel="noopener"> NAS-FPN </a>采用神经架构搜索(NAS)来搜索更好的跨尺度特征网络拓扑。</li></ul><blockquote class="ni nj nk"><p id="c5ab" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated">但是在搜索期间需要<strong class="jn hj">数千个GPU小时，并且发现的<strong class="jn hj">网络不规则</strong>和<strong class="jn hj">难以解释或修改</strong>，如上图所示。</strong></p></blockquote><h2 id="1512" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">1.3.帕内特</h2><ul class=""><li id="4d07" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">为了解决<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>问题，<a class="ae ix" href="https://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-segmentation-fe4c985cad1b" rel="noopener ugc nofollow" target="_blank"> PANet </a>增加了<strong class="jn hj">一个额外的自底向上的路径聚合网络</strong>，如上面的(b)所示。而且它还得到了一个比<a class="ae ix" href="https://sh-tsang.medium.com/review-nas-fpn-learning-scalable-feature-pyramid-architecture-for-object-detection-f29039f94373" rel="noopener"> NAS-FPN </a>更<strong class="jn hj">正规的网络</strong>。</li></ul><blockquote class="ni nj nk"><p id="6286" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated"><a class="ae ix" href="https://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-segmentation-fe4c985cad1b" rel="noopener ugc nofollow" target="_blank"> PANet </a>比<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jn hj">【FPN】</strong></a><strong class="jn hj">和</strong> <a class="ae ix" href="https://sh-tsang.medium.com/review-nas-fpn-learning-scalable-feature-pyramid-architecture-for-object-detection-f29039f94373" rel="noopener"> <strong class="jn hj"> NAS-FPN </strong> </a>达到了更好的精度，但代价是更多的参数和计算。<a class="ae ix" href="https://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-segmentation-fe4c985cad1b" rel="noopener ugc nofollow" target="_blank"> PANet </a>说明<strong class="jn hj">只有一条自顶向下和一条自底向上的路径。</strong></p></blockquote></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="312d" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">2.双向特征金字塔网络</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es no"><img src="../Images/9a05b3e48d3578cef7629cc292f3842a.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*0cZUDetJfGHAgrMg7208lQ.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">提议的BiFPN </strong></figcaption></figure><ul class=""><li id="547f" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">我们可以从1.3版本修改BiFPN。(b)在<a class="ae ix" href="https://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-segmentation-fe4c985cad1b" rel="noopener ugc nofollow" target="_blank"> PANet </a>中使用的FPN。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es np"><img src="../Images/3dd1847adecbeb376c9cf7402ace11b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*U8mTYVHh2C7qEiFlXMD0BA.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk"> 1)只有一条输入边的节点被删除(红色圆圈)</strong></figcaption></figure><ul class=""><li id="05c7" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj">首先，去掉只有一条输入边的节点。直觉很简单:如果一个节点只有一条没有特征融合的输入边，那么它对旨在融合不同特征的特征网络的贡献就较小。这导致简化的双向网络。</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nq"><img src="../Images/127fe6398fb14d9dc4c8ad5ebc0dde9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0yyayPSUYw7zhVfc7F80dQ.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk"> 2)多余的边缘(紫色箭头)</strong></figcaption></figure><ul class=""><li id="f3ac" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj">第二，从原来的输入到输出节点如果在同一个级别就增加一条额外的边</strong>，以便在不增加太多成本的情况下融合更多的特征。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nr"><img src="../Images/ea056d7a16a7db4947b37062696ca731.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*kyykLhiLc7hlMF45nV1LFQ.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk"> 3)可重复多次</strong></figcaption></figure><ul class=""><li id="240c" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj">第三</strong>，与<a class="ae ix" href="https://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-segmentation-fe4c985cad1b" rel="noopener ugc nofollow" target="_blank"> PANet </a>只有一条自上而下和一条自下而上的路径不同，每条双向(自上而下&amp;自下而上)路径被视为一个特征网络层，<strong class="jn hj">多次重复同一层</strong>以实现更高级的特征融合。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="6c68" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 3。加权BiFPN </strong></h1><ul class=""><li id="552a" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">由于不同的输入特征在<strong class="jn hj">有不同的分辨率</strong>，它们通常<strong class="jn hj">对输出特征的贡献不相等。</strong></li><li id="13a2" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">为了解决这个问题，<strong class="jn hj">为每个输入增加了一个额外的权重</strong>，并让网络学习<strong class="jn hj">每个输入特征</strong>的重要性。</li><li id="0619" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">考虑了3种权重融合方法。</li></ul><h2 id="d219" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated"><strong class="ak"> 3.1。无界融合</strong></h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ns"><img src="../Images/e86ba64d9d88ebbe23b6e2dd9dcc9086.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*8XI-Yc-dUodUruZtTgM4qg.png"/></div></figure><ul class=""><li id="ae6a" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">其中<strong class="jn hj"> <em class="mq"> wi </em> </strong>是一个<strong class="jn hj">可学习权重</strong>，它可以是一个标量(每特征)、一个矢量(每通道)或一个多维张量(每像素)。</li><li id="2c70" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">然而，由于标量权重是<strong class="jn hj">无界的</strong>，它可能会<strong class="jn hj">导致训练不稳定。需要有界限的范围。</strong></li></ul><h2 id="508d" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">3.2.基于Softmax的融合</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nt"><img src="../Images/4eed0ecd1b72d86efadd8f91007a836e.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*GO6KVYwMq1JOCpxNupEGTg.png"/></div></figure><ul class=""><li id="c09b" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">一个直观的想法是将<strong class="jn hj"> softmax </strong>应用于每个权重，这样所有权重都被标准化为值<strong class="jn hj">范围从0到1的概率。</strong></li><li id="20e4" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">然而，额外的softmax导致GPU硬件上的<strong class="jn hj">显著减速</strong>。</li></ul><h2 id="738f" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">3.3.快速归一化融合</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nu"><img src="../Images/687daec3e956f56a2448c59448517ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*FncueLSBSa5eibWAEy4R9A.png"/></div></figure><ul class=""><li id="b83b" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">最后，<strong class="jn hj">采用快速归一化融合</strong>，通过在每个<em class="mq"> wi </em>后应用一个Relu来保证<em class="mq"> wi </em> ≥ 0。<em class="mq"> ε </em> = 0.0001是为了避免数值不稳定。</li><li id="1dcf" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">每个归一化权重的值也在0和1之间，但由于这里没有<strong class="jn hj">soft max</strong>运算，因此<strong class="jn hj">比</strong>高效得多，在GPU上运行速度比<strong class="jn hj">快30%。</strong></li><li id="caf9" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">例如，上面显示的BiFPN的第6级的两个融合特征是:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nv"><img src="../Images/399e284c9b85a6884809dc61ee69db38.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*KL2UiEp0W3doKTZDevFrQA.png"/></div></figure><ul class=""><li id="1ece" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">其中<em class="mq"> Ptd </em> _6是自上而下路径上第6级的中间特征，<em class="mq"> Pout </em> _6是自下而上路径上第6级的输出特征。</li><li id="89df" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">值得注意的是，为了进一步提高效率，<strong class="jn hj">深度方向可分离卷积</strong>用于特征融合。</li><li id="fb65" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">在每次卷积后添加批量标准化和激活。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="35aa" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 4。EfficientDet:网络架构</strong></h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es nw"><img src="../Images/6f3ff91f8c231ad148ecdf3982869299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4zOSLZ6Dfc-_rIJdGnvVQ.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">高效检测:网络架构</strong></figcaption></figure><ul class=""><li id="3571" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj">使用一级</strong>探测器范例。</li><li id="c4b4" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">采用ImageNet-pre trained<a class="ae ix" href="https://sh-tsang.medium.com/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-image-classification-ef67b0f14a4d" rel="noopener"><strong class="jn hj">efficient net</strong></a><strong class="jn hj">s</strong>作为主干网络。</li><li id="2e7a" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">提出的<strong class="jn hj"> BiFPN </strong>作为<strong class="jn hj">特征网络</strong>，从主干网络中提取3-7级特征{ <em class="mq"> P </em> 3，<em class="mq"> P </em> 4，<em class="mq"> P </em> 5，<em class="mq"> P </em> 6，<em class="mq"> P </em> 7}，反复应用自顶向下和自底向上的双向特征融合。</li><li id="73e9" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">这些融合的特征被<strong class="jn hj">馈送到一个类和盒网络</strong>以分别产生对象类和边界盒预测。与<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank"> RetinaNet </a>类似，类和盒网络权重在所有级别的要素中共享。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="88a3" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">5.复合标度法</h1><ul class=""><li id="58d7" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">先前的工作通过使用更大的主干、更大的输入图像或者堆叠更多的<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>层来放大探测器，即单因子缩放。</li><li id="bd23" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">最近，<a class="ae ix" href="https://sh-tsang.medium.com/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-image-classification-ef67b0f14a4d" rel="noopener"> EfficientNet </a>联合提升了网络宽度、深度和输入分辨率的所有维度。</li><li id="bb3b" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">在这里，EfficientDet提出了一种新的用于对象检测的复合缩放方法，该方法使用简单的复合系数<em class="mq"> φ </em>到<strong class="jn hj">联合放大主干网络、BiFPN网络、类/盒网络和分辨率的所有维度。</strong></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nx"><img src="../Images/72cb56522b8779cee8f74ab83aaee94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*hPpq2NCCr1nfswkj0-XOHw.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">效率检测D0-D6的缩放配置</strong></figcaption></figure><h2 id="3436" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">5.1.中枢网络</h2><ul class=""><li id="8ea2" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated"><strong class="jn hj">使用</strong><a class="ae ix" href="https://sh-tsang.medium.com/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-image-classification-ef67b0f14a4d" rel="noopener"><strong class="jn hj">efficient net</strong></a><strong class="jn hj">-B0到B6</strong>的相同宽度/深度缩放系数，从而可以使用ImageNet预训练的检查点。</li></ul><h2 id="3318" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">5.2.BiFPN网络</h2><ul class=""><li id="25eb" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated"><strong class="jn hj">BiFPN深度<em class="mq"> D_bifpn </em> (#layers)线性增加</strong>，因为深度需要四舍五入为小整数。</li><li id="3ea9" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj"> BiFPN宽度<em class="mq"> W_bifpn </em> (#channels)呈指数增长</strong>与<a class="ae ix" href="https://sh-tsang.medium.com/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-image-classification-ef67b0f14a4d" rel="noopener"> EfficientNet </a>类似。具体而言，对值列表{1.2，1.25，1.3，1.35，1.4，1.45}执行网格搜索，并挑选最佳值1.35作为BiFPN宽度比例因子。</li><li id="6d65" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">形式上，BiFPN的宽度和深度按下式缩放:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ny"><img src="../Images/0ef19fc1c2087a2c7dd4aae04c681dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*yKKtjznJ-ofKSuY9AgOv4w.png"/></div></figure><h2 id="4c11" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">5.3.盒/类预测网络</h2><ul class=""><li id="1f9c" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated"><strong class="jn hj">它们的宽度固定为始终与BiFPN </strong>相同(即<em class="mq">W _ pred</em>=<em class="mq">W _ BiFPN</em>)。</li><li id="52fa" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj">深度(层数)线性增加</strong>使用公式:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nz"><img src="../Images/4bbb6cfeef1b803b1ca7a7db41022370.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*tN4gEBKliYBz_OMvCTh4Bw.png"/></div></figure><h2 id="e0e0" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">5.4.输入图像分辨率</h2><ul class=""><li id="f231" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">由于在BiFPN中使用特征级别3-7，输入分辨率必须除以2⁷ =128，因此<strong class="jn hj">分辨率线性增加</strong>:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es oa"><img src="../Images/b398abc1368b210fa6d7dd59750364c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*ZVndEikYaSpoLKcUmn3-MA.png"/></div></figure><ul class=""><li id="472c" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">(阅读<a class="ae ix" href="https://sh-tsang.medium.com/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-image-classification-ef67b0f14a4d" rel="noopener"> EfficientNet </a>可以更容易理解这一部分。)</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="4295" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">6.SOTA比较</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es ob"><img src="../Images/1bcc8755ca8b9c032fe51c85f9830a0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPXR2S-YccCHwqcJbmsQjA.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">可可上的EfficientDet性能</strong></figcaption></figure><ul class=""><li id="5b84" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">上表比较了EfficientDet与其他物体探测器，在<strong class="jn hj">单模型单比例</strong>设置下，没有测试时间增加。</li><li id="42c7" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">测试了<strong class="jn hj">测试开发(20K测试图像)</strong>和<strong class="jn hj"> val (5K验证图像)</strong>的准确性。</li><li id="1381" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">EfficientDet实现了比以前的检测器更高的<strong class="jn hj">效率，比以前的检测器<strong class="jn hj">小4×— 9倍</strong>,并且使用了<strong class="jn hj"> 13× — 42倍的触发器</strong>,在很大的精度范围或资源限制范围内。</strong></li><li id="804f" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">在精度相对较低的情况下，<strong class="jn hj"> EfficientDet-D0可达到与</strong><a class="ae ix" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jn hj">yolov 3</strong></a><strong class="jn hj">相似的精度，但触发器数量减少了28倍。</strong></li><li id="e030" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">与<a class="ae ix" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------" rel="noopener" target="_blank">retina net</a>【21】和<a class="ae ix" rel="noopener" href="/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4">Mask R-CNN</a>【11】相比，我们的<strong class="jn hj"> EfficientDet-D1 </strong>实现了类似的精度，参数<strong class="jn hj">减少了8倍</strong>和<strong class="jn hj">减少了21倍FLOPs。</strong></li><li id="6452" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">在高精度状态下，EfficientDet也始终<strong class="jn hj">优于最近的</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-nas-fpn-learning-scalable-feature-pyramid-architecture-for-object-detection-f29039f94373" rel="noopener"><strong class="jn hj">NAS-FPN</strong></a>【8】<strong class="jn hj">和</strong>它在【42】中的增强版本(即带有<a class="ae ix" href="https://sh-tsang.medium.com/review-autoaugment-learning-augmentation-strategies-from-data-image-classification-af27dea9a839" rel="noopener"> <strong class="jn hj">自动增强</strong> </a>)具有<strong class="jn hj">少得多的参数和FLOPs。</strong></li><li id="c2d3" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">特别是，EfficientDet-D7在测试开发方面实现了新的最先进的52.2 AP，在单型号单秤方面实现了51.8 AP。</li></ul></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="6fb0" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">7.消融研究</h1><h2 id="e599" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">7.1.主干和BiFPN</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es oc"><img src="../Images/235a0a360efdbd028e0e60d8df503c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*HM-y99Db9QHzYfSxuSBn4A.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">解开主干和BiFPN </strong></figcaption></figure><ul class=""><li id="22f7" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">以<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------" rel="noopener" target="_blank"> ResNet </a> + <a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>为基线。</li><li id="2701" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">然后主干换成了<a class="ae ix" href="https://sh-tsang.medium.com/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-image-classification-ef67b0f14a4d" rel="noopener"> EfficientNet </a> -B3，用稍微少一点的参数和FLOPs提高了大概3 AP的精度。</li></ul><blockquote class="ni nj nk"><p id="fed0" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated">通过用提议的<strong class="jn hj"> BiFPN </strong>、<strong class="jn hj">进一步替换<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a>，用<strong class="jn hj">少得多的参数和触发器实现了额外的4 AP增益</strong>。</strong></p></blockquote><h2 id="d201" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">7.2.<strong class="ak">不同的特征网络</strong></h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es od"><img src="../Images/0f866b6d2bbb0ecf9d12ccf98a771fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*2nt2ZgfBomV8XWzEy-bgYQ.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">不同特征网络的比较</strong></figcaption></figure><ul class=""><li id="660d" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">虽然重复<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"> FPN </a> + <a class="ae ix" href="https://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-segmentation-fe4c985cad1b" rel="noopener ugc nofollow" target="_blank"> PANet </a>实现了比<a class="ae ix" href="https://sh-tsang.medium.com/review-nas-fpn-learning-scalable-feature-pyramid-architecture-for-object-detection-f29039f94373" rel="noopener"> NAS-FPN </a>稍好的精度，但它也需要更多的参数和FLOPs。</li></ul><blockquote class="ni nj nk"><p id="daf9" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated">提出的<strong class="jn hj"> BiFPN </strong>达到了与重复<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------" rel="noopener" target="_blank"><strong class="jn hj">FPN</strong></a><strong class="jn hj">+</strong><a class="ae ix" href="https://becominghuman.ai/reading-panet-path-aggregation-network-1st-place-in-coco-2017-challenge-instance-segmentation-fe4c985cad1b" rel="noopener ugc nofollow" target="_blank"><strong class="jn hj">PANet</strong></a>类似的精度，但是使用了<strong class="jn hj">少得多的参数和FLOPs。</strong></p><p id="c052" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated">通过额外的<strong class="jn hj">加权特征融合</strong>，提出的BiFPN <strong class="jn hj">进一步以<strong class="jn hj">更少的参数和触发器实现了最佳精度</strong>。</strong></p></blockquote><h2 id="385e" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">7.3.不同特征融合</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es oe"><img src="../Images/e7f28961f0ab7746a199e21cbbd779d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*1zaTgeonOMe76j_5fkKB_A.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">不同特征融合的比较</strong></figcaption></figure><blockquote class="ni nj nk"><p id="f331" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated">与基于softmax的融合相比，<strong class="jn hj">快速融合</strong>实现了<strong class="jn hj">相似的精度</strong>，但是运行速度<strong class="jn hj">快了28% — 31%。</strong></p></blockquote><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es of"><img src="../Images/83be960f2650aabdbea399e11c95cfe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lt8AKAB0Oc8qtIr51uMgxg.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk"> Softmax与快速归一化特征融合</strong></figcaption></figure><ul class=""><li id="97c1" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">在EfficientDet-D3的BiFPN中随机选择3个节点。</li></ul><blockquote class="ni nj nk"><p id="3bf8" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated">尽管变化很快，但对于所有三个节点，提出的<strong class="jn hj">快速归一化融合</strong>方法总是表现出与<strong class="jn hj">基于softmax的融合</strong>非常<strong class="jn hj">相似的学习行为</strong>。</p></blockquote><h2 id="3f49" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">7.4.复合缩放</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es og"><img src="../Images/090a84ce722c1dc78fc1f37a2052a44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*X7Rx-JMgsJsYykZ5VpoVHw.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx translated"><strong class="bd jk">不同缩放方法的比较</strong></figcaption></figure><ul class=""><li id="dc27" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">上图比较了建议的复合缩放与其他替代方法，这些方法在分辨率/深度/宽度的单一维度上进行缩放。</li></ul><blockquote class="ni nj nk"><p id="97a8" class="jl jm mq jn b jo jp ij jq jr js im jt nl jv jw jx nm jz ka kb nn kd ke kf kg hb bi translated"><strong class="jn hj">复合扩展</strong>比其他方法实现了<strong class="jn hj">更高的效率</strong>，表明了通过更好地平衡不同架构维度来联合扩展的好处。</p></blockquote></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="8ed9" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">参考</h2><p id="b9ae" class="pw-post-body-paragraph jl jm hi jn b jo me ij jq jr mf im jt ju oh jw jx jy oi ka kb kc oj ke kf kg hb bi translated">【2020 CVPR】【高效检测】<br/> <a class="ae ix" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank">高效检测:可扩展且高效的对象检测</a></p><h2 id="c8b1" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated">目标检测</h2><p id="5361" class="pw-post-body-paragraph jl jm hi jn b jo me ij jq jr mf im jt ju oh jw jx jy oi ka kb kc oj ke kf kg hb bi translated"><strong class="jn hj"> 2014 </strong> : [ <a class="ae ix" rel="noopener" href="/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------">过食</a>][<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------">R-CNN</a>]<br/><strong class="jn hj">2015</strong>:[<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------">快R-CNN </a> ] [ <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------" rel="noopener" target="_blank">更快R-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------" rel="noopener" target="_blank">MR-CNN&amp;S-CNN</a>][<a class="ae ix" href="https://towardsdatascience.com/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------" rel="noopener" target="_blank">DeepID-Net</a><br/><strong class="jn hj">2016<strong class="jn hj"> [<a class="ae ix" href="https://towardsdatascience.com/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac?source=post_page---------------------------" rel="noopener" target="_blank">GBD-网/GBD-v1&amp;GBD-v2</a>][<a class="ae ix" href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------" rel="noopener" target="_blank">SSD</a>][<a class="ae ix" href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------" rel="noopener" target="_blank">yolov 1</a><br/><strong class="jn hj">2017</strong>:[<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a?source=post_page---------------------------">NoC</a>][<a class="ae ix" href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------" rel="noopener" target="_blank">G-RMI</a>][<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------">TDM</a>[<a class="ae ix" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------" rel="noopener" target="_blank">DSSD</a>[<a class="ae ix" href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------" rel="noopener" target="_blank">yolov 2/yolo 900] [</a><a class="ae ix" href="https://sh-tsang.medium.com/review-couplenet-coupling-global-structure-with-local-parts-for-object-detection-object-d80150c5c850" rel="noopener">couple net</a>]<br/><strong class="jn hj">2018</strong>:[<a class="ae ix" href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------" rel="noopener" target="_blank">yolov 3</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-cascade-r-cnn-delving-into-high-quality-object-detection-object-detection-8c7901cc7864">Cascade R-CNN</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/reading-megdet-a-large-mini-batch-object-detector-1st-place-of-coco-2017-detection-challenge-e82072e9b7f">MegDet</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/reading-stairnet-top-down-semantic-aggregation-object-detection-de689a94fe7e">stair net</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-refinedet-single-shot-refinement-neural-network-for-object-detection-object-detection-5fc483449562" rel="noopener">refined et</a>][<a class="ae ix" href="https://sh-tsang.medium.com/review-cornernet-detecting-objects-as-paired-keypoints-object-detection-ffb23026291b" rel="noopener">corner net</a>]【T78</strong></strong></p><h2 id="84af" class="ms lo hi bd jk mt mu mv ls mw mx my lw ju mz na ly jy nb nc ma kc nd ne mc nf bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>