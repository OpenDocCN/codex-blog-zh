<html>
<head>
<title>Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/codex/logistic-regression-eee2fd028ffd?source=collection_archive---------2-----------------------#2021-09-19">https://medium.com/codex/logistic-regression-eee2fd028ffd?source=collection_archive---------2-----------------------#2021-09-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/e312f2a57156d3b721096bf55a635314.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/0*GXkyzZsgyVyags6w"/></div></figure><p id="fa4d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">学完回归的基础，就该学分类的基础了。还有什么比逻辑回归更简单的呢！</p><p id="166a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">建议您在开始本教程之前，先阅读本<a class="ae jk" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6"> <strong class="io hj">线性回归教程</strong> </a>。这是线性回归的完整指南，逻辑回归使用了几个与线性回归相关的思想。说完了，我们开始吧！</p><h2 id="8062" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">什么是逻辑回归？</h2><p id="afe7" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">这是一种分类算法，适用于输出变量为<em class="kl">分类</em>的情况。逻辑回归的目标是发现特征和特定结果的概率之间的关系。</p><p id="4966" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">举个例子，</p><p id="eac5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">邮件:垃圾邮件/不是垃圾邮件？</p><p id="4285" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在线交易:欺诈(是/否)</p><p id="c855" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里，</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es km"><img src="../Images/12dddd94a1dab741eeb9ecba0fe83ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*32m_H3rSyuGxE8NMumXU3Q.png"/></div></div></figure><p id="4f9f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇文章中，我们将建立一个逻辑回归模型来预测一个学生是否被大学录取。</p><h2 id="77fd" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">为什么是逻辑回归，而不是线性？</h2><p id="27cc" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">在简单线性回归中，将所有数据绘制到一个图表(x和y)上，将所有数据拟合到一条最佳拟合线，然后将输入预测为相应的y。另一方面，逻辑回归将所有数据拟合到一条S曲线，只有两种可能的输出(两种分类)，分别表示为顶线和底线。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/a85eaf3291d0b534aed14f18ddce8b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*siY_-85Df0_JAAEp"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">线性和逻辑回归</figcaption></figure><h2 id="698b" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">Sigmoid函数</h2><p id="42c7" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">S曲线不是指字母S的形状；相反，它代表的是乙状结肠函数。这是因为sigmoid函数完全符合我们将数据分为两组的目的。sigmoid公式如下，其中x是输入的数量。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es la"><img src="../Images/689eed2f77701631d8baf9ee5bdf20e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*f8Joedj1LoFjeyaj9HY9XQ.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">乙状结肠的</figcaption></figure><p id="cb3a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在英语中，sigmoid只是基于输入要素的加权和的概率计算。加权和的公式如下:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/24e36de6839968c02d2d7ffdf176b9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*8IWS6j3bu8chjPmo2zxh2A.png"/></div></figure><p id="1baa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在编写sigmoid函数之前，让我们初始化数据集。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="5980" class="jl jm hi lk b fi lo lp l lq lr">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="0c8c" class="jl jm hi lk b fi ls lp l lq lr">data = pd.read_csv("ex2data1.txt", header=None)<br/>data.head()</span></pre><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/d6c0e369b1930cb5acb6f466211725a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*PAf53ISye-tn_ESX5HGe_g.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">data.head()</figcaption></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="e5c1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里“0”代表学生在1ˢᵗ考试中的分数，“1”代表学生在2ⁿᵈ考试中的分数，“2”代表学生是否被录取(1)或不被录取(0)。</p><p id="3160" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们将数据可视化，</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="ec10" class="jl jm hi lk b fi lo lp l lq lr">X = data.values[:, :-1]<br/>y = data.values[:, -1]</span><span id="038b" class="jl jm hi lk b fi ls lp l lq lr">pos, neg = (y==1).reshape(100, 1), (y==0).reshape(100, 1)</span><span id="5c80" class="jl jm hi lk b fi ls lp l lq lr">plt.scatter(X[pos[:, 0], 0], X[pos[:, 0], 1], c='r', marker='+', label="Admitted")<br/>plt.scatter(X[neg[:, 0], 0], X[neg[:, 0], 1], marker='o', label="Not Admitted", s=10)<br/>plt.xlabel("Exam1 Score")<br/>plt.ylabel("Exam2 Score")<br/>plt.legend(loc=0)</span></pre><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/8344da9c8e9cbad491f7f5c65e408149.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*Ve2KbDvgljT4kdXkOXgpKA.png"/></div></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="8f49" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在逻辑回归的情况下，假设(h)由以下等式表示:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/f43e82e8183e3aa4e86dc4f93075dbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*vTr3uqttX5V9zodKQ7j_TQ.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">假设</figcaption></figure><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/6c18710d307dd089a487f057a8cc530d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*0iOQPstIOATn6QqgZ6nmuA.png"/></div></figure><p id="cb62" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">假设的图示:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lx"><img src="../Images/ed133a0fd797e726746c05357ecbe7b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*79fBK9tedP5O8hR1"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">假设</figcaption></figure><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="b7fe" class="jl jm hi lk b fi lo lp l lq lr">def sigmoid(z):<br/>    return 1 / (1 + np.exp(-z))</span><span id="a496" class="jl jm hi lk b fi ls lp l lq lr">print(sigmoid(0))<br/>print(sigmoid(10))<br/>print(sigmoid(1))</span></pre><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/d3fafcd18e5a34b17fc6a24da7b963fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*lHa9NTzDxchc4-ExFyhccg.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">Sigmoid结果</figcaption></figure><p id="c245" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从上面的数字我们可以看出，</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/b569b889b81e579eee9648ed32ba4f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*al80RVah4SVbEodQAicPBA.png"/></div></figure><h2 id="8e85" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">逻辑回归决策边界</h2><p id="5847" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">由于我们的数据集有两个特征:test1和test2，逻辑回归假设如下:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/3d868881ffaa31466e2c6c65369345f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*ApM46wpGWUIzf2NRLyg0tA.png"/></div></figure><p id="492c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">逻辑回归分类器将预测“入院”,如果:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/eae8ddb987a3d07372043b5d4928da1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*_1OraA5QhJM94RgGoWuZVA.png"/></div></figure><p id="f7da" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是因为逻辑回归“<em class="kl">阈值</em>被设置为g(z)=0.5，请参见上面的逻辑回归函数图进行验证。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es mc"><img src="../Images/fc3eef71d5c5ecf09daff2f2d03ce6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RChsgjQ13r_88HafaL1ZXw.png"/></div></div></figure><h2 id="005f" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">价值函数</h2><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es md"><img src="../Images/7cfd5b81db543229319f2ff729cecc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xxl7TukE1v5dAwesEz-j3w.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">记号</figcaption></figure><p id="278a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们先来定义两个兴趣点的logistic回归代价函数:y=1，y=0，也就是假设函数预测录取或不录取时，</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es me"><img src="../Images/273583ed1dd80cf0646a25e9805e65a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*Wx4ZnFfJl8y4gXp6Rr6vEw.png"/></div></figure><p id="95e6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">简化的成本函数如下:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/e235463de536c8ac6a5d1687d5ee1e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*7fRcA_1s48ac6TM_pxWkyQ.png"/></div></figure><p id="bb3d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然后，我们在这两项的<strong class="io hj"> y </strong>中取一个凸组合，得出逻辑回归成本函数:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/844900fccbe5bdfb793946fa1ba3ed57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*uAcyoE6Mf6IOzcTgLXZARA.png"/></div></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="d2a9" class="jl jm hi lk b fi lo lp l lq lr">def Costfunction(X, y, theta):<br/>    m=len(y)<br/>    <br/>    h_theta = sigmoid(X@theta)<br/>    y_pos = -y.T @ np.log(h_theta)<br/>    y_neg = (1-y).T @ np.log(1-h_theta)<br/>    error = y_pos - y_neg<br/>    <br/>    cost = 1/m * sum(error)<br/>    grad = 1/m * (X.T@(h_theta - y))<br/>    <br/>    return cost[0] , grad</span></pre></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="267c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在使用数据计算成本之前，我们应该将我们的数据标准化(要了解更多关于标准化的信息<a class="ae jk" rel="noopener" href="/codex/linear-regression-on-multiple-variables-1893e4d940b1"> <strong class="io hj"> <em class="kl">点击此处</em> </strong> </a> <strong class="io hj"> <em class="kl">)。</em> </strong>)</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="eff1" class="jl jm hi lk b fi lo lp l lq lr">def featureNormalization(X):<br/>    mu = np.mean(X, axis=0)<br/>    sigma = np.std(X, axis=0)<br/>    X_Norm = (X - mu)/sigma<br/>    return X_Norm, mu, sigma</span><span id="a87b" class="jl jm hi lk b fi ls lp l lq lr">m, n = X.shape<br/>X, mu, sigma = featureNormalization(X)<br/>X = np.column_stack((np.ones((m, 1)), X))<br/>y = y.reshape(m, 1)</span><span id="1f71" class="jl jm hi lk b fi ls lp l lq lr">initial_theta = np.zeros((n+1, 1))<br/>cost, grad= Costfunction(X, y, initial_theta)<br/>print("Cost of initial theta is", cost)<br/>print("Gradient at initial theta (zeros):", grad)</span></pre><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/8064f9315e726e9c7b82c9407edaeca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*D_ZFVISEPx9udNlyadnbfA.png"/></div></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><h2 id="7fa9" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">梯度下降</h2><p id="1ae8" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">逻辑回归的梯度下降算法看起来与线性回归的梯度下降算法相同。对于梯度下降的情况，搜索方向是逻辑回归成本函数相对于参数θ的负偏导数。在其最基本的形式中，梯度下降将沿着θ的负梯度方向迭代(称为<em class="kl">最小化序列</em>)，直到达到收敛。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/2f4cb740b19c227e13096b614fbb3767.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*xG91MW6vuUIUTc67x1ymvA.png"/></div></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="cff6" class="jl jm hi lk b fi lo lp l lq lr">def gradientDescent(X, y, theta, alpha, n_iters):<br/>    m=len(y)<br/>    J_history =[]<br/>    <br/>    for i in range(n_iters):<br/>        cost, grad = Costfunction(X, y, theta)<br/>        theta = theta - (alpha * grad)<br/>        J_history.append(cost)<br/>    return theta, J_history</span><span id="c57b" class="jl jm hi lk b fi ls lp l lq lr">theta, J_history = gradientDescent(X=X, y=y, theta=initial_theta, alpha=1, n_iters=400)</span></pre></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><h2 id="e6f7" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">绘制决策边界</h2><p id="6994" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">这里，决策边界如下:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/d4b453bd00ea1434612e9309ce7d9c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*Hp1ryCfZcSedMJYTxsCa4g.png"/></div></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="c919" class="jl jm hi lk b fi lo lp l lq lr">plt.scatter(X[pos[:,0],1],X[pos[:,0],2],c="r",marker="+",label="Admitted")<br/>plt.scatter(X[neg[:,0],1],X[neg[:,0],2],c="b",marker="x",label="Not admitted")<br/>x_value = np.array([np.min(X[:,1]),np.max(X[:,1])])<br/>y_value = -(theta[0] +theta[1]*x_value)/theta[2]<br/>plt.plot(x_value,y_value, "r")<br/>plt.xlabel("Exam 1 score")<br/>plt.ylabel("Exam 2 score")<br/>plt.legend(loc=0)</span></pre><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/c9ec608632b106406ab4e8b0ea48c359.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*NxHxF7YlwGmBAAyNhNurew.png"/></div></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><h2 id="d0b2" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">预言</h2><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/3c5dc5b8d5ee442fe63413296df3c4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*ILpj9uD3S6oOvprKHq-MJQ.png"/></div></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="062a" class="jl jm hi lk b fi lo lp l lq lr">x_sample = np.array([45, 85])<br/>x_sample = featureNormalization(x_sample)[0]<br/>x_sample = np.append(np.ones(1), x_sample)<br/>prob = sigmoid(x_sample.dot(theta))<br/>print("For a student with scores 45 and 85, we predict an admission probability of ",prob[0])</span></pre><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es mm"><img src="../Images/da7aa3fa7f1d794a300c2ff0cf2be9fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*euCt5_dr6BZJUmtf4M1G-Q.png"/></div></div></figure><p id="cf68" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从上面的输出可以看出，45分和85分的学生有80%的概率被大学录取。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><pre class="kn ko kp kq fd lj lk ll lm aw ln bi"><span id="b8f1" class="jl jm hi lk b fi lo lp l lq lr">def predict(X, theta):<br/>    p = sigmoid(X@theta) &gt;= 0.37#select your own threshold<br/>    return p</span></pre><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/a59c3d90475ec7ad0f9461c7cdc0215d.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*xAMaixrgbUZQGvEtnvA_Kg.png"/></div></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><h2 id="db0c" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">结论</h2><p id="393f" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">今天，我们看到了逻辑回归的假设、成本函数和梯度下降背后的概念。然后用python的numpy，pandas和matplotlib从头开始创建。数据集和最终代码上传到github。</p><p id="7dc6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">查看这里<a class="ae jk" href="https://github.com/jagajith23/Andrew-Ng-s-Machine-Learning-in-Python/tree/gh-pages/Logistic%20Regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>。</p><h1 id="8461" class="mo jm hi bd jn mp mq mr jr ms mt mu jv mv mw mx jy my mz na kb nb nc nd ke ne bi translated">如果你喜欢这篇文章，那么看看我在这个系列中的其他文章</h1><h2 id="9f54" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">1.<a class="ae jk" rel="noopener" href="/@jagajith23/what-is-machine-learning-daeac9a2ceca">什么是机器学习？</a></h2><h2 id="cc5b" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">2.<a class="ae jk" rel="noopener" href="/codex/what-are-the-types-of-machine-learning-53360b7db8b4">机器学习有哪些类型？</a></h2><h2 id="8d7a" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">3.<a class="ae jk" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6">一元线性回归</a></h2><h2 id="5a10" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">4.<a class="ae jk" rel="noopener" href="/codex/linear-regression-on-multiple-variables-1893e4d940b1">多元线性回归</a></h2><h2 id="50b3" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">5.<a class="ae jk" rel="noopener" href="/@jagajith23/what-are-neural-networks-3a0965e2ebfb">什么是神经网络？</a></h2><h2 id="e91b" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">6.<a class="ae jk" rel="noopener" href="/@jagajith23/digit-classifier-using-neural-networks-ad17749a8f00">使用神经网络的数字分类器</a></h2><h2 id="bee8" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">7.<a class="ae jk" rel="noopener" href="/@jagajith23/image-compression-with-k-means-clustering-48e989055729">利用K均值聚类进行图像压缩</a></h2><h2 id="1737" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">8.<a class="ae jk" rel="noopener" href="/@jagajith23/dimensionality-reduction-on-face-using-pca-e3fec3bb4cee">使用PCA对人脸进行降维</a></h2><h2 id="eefc" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">9.<a class="ae jk" href="https://jagajith23.medium.com/detect-failing-servers-on-a-network-using-anomaly-detection-1c447bc8a46a" rel="noopener">使用异常检测来检测网络上的故障服务器</a></h2><h1 id="df18" class="mo jm hi bd jn mp mq mr jr ms mt mu jv mv mw mx jy my mz na kb nb nc nd ke ne bi translated">最后做的事</h1><p id="cac7" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated"><em class="kl">如果你喜欢我的文章，鼓掌👏一个追随者将是绝对的坏蛋和</em>这是有益的媒体推广这篇文章，使其他人可以阅读它<em class="kl">。我是Jagajith，我会在下一个里抓住你。</em></p></div></div>    
</body>
</html>