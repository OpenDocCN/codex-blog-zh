<html>
<head>
<title>Year 2020: the Transformers Expansion in the CV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2020年:变形金刚资料片</h1>
<blockquote>原文：<a href="https://medium.com/codex/year-2020-the-transformers-expansion-in-the-cv-fed9e4c77b55?source=collection_archive---------16-----------------------#2021-07-30">https://medium.com/codex/year-2020-the-transformers-expansion-in-the-cv-fed9e4c77b55?source=collection_archive---------16-----------------------#2021-07-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="90a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在过去的几年中，变压器在自然语言处理领域表现出色。他们显著提高了语言处理模型的性能，其效果可与2012年以来通过卷积神经网络对图像理解所做的工作相媲美。现在，在2020年底，我们有变形金刚进入知名计算机视觉基准的前四分之一，如ImageNet上的<a class="ae jd" href="https://paperswithcode.com/sota/image-classification-on-imagenet" rel="noopener ugc nofollow" target="_blank">图像分类</a>和COCO上的<a class="ae jd" href="https://paperswithcode.com/sota/object-detection-on-coco-minival" rel="noopener ugc nofollow" target="_blank">物体检测</a>。</p><p id="e280" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">继续上一篇关于基于transformer的DETR和稀疏R-CNN的话题，在这篇文章中，我们将概述最近脸书人工智能和索邦大学的联合研究<a class="ae jd" href="https://arxiv.org/abs/2012.12877" rel="noopener ugc nofollow" target="_blank">“训练数据高效的图像转换器&amp;通过注意力的升华”</a>、他们的DeiT模型以及哪些科学成就预测了这项工作。</p><h1 id="3abf" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">变压器架构</h1><h2 id="69f5" class="kc jf hi bd jg kd ke kf jk kg kh ki jo iq kj kk js iu kl km jw iy kn ko ka kp bi translated">概观</h2><p id="73f1" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">用于图像分类的卷积网络的许多改进是受变压器的启发。例如，挤压和激励、选择性核心和分裂注意力网络利用类似于变形金刚<strong class="ih hj">自我注意力机制</strong>的机制。</p><p id="dc79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2017年推出<a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的全部”</a>用于机器翻译的论文目前是所有自然语言处理任务的参考模型。这样的任务是越来越常见的家庭人工智能助理和呼叫中心聊天机器人的核心。Transformer能够在任意长度的顺序数据中找到依赖关系，而不会使模型变得非常复杂。由此产生了它在序列任务中的出色表现，例如将文本从一种语言翻译成另一种语言，或者创建关于某个问题的答案。</p><p id="451f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在变压器出现之前，循环网络被用来完成这些任务。让我们来谈谈rnn与变压器相比的缺点。下面的幻灯片摘自Pascal Plupart的<a class="ae jd" href="https://www.youtube.com/watch?v=OyFJWRnt_AY&amp;t" rel="noopener ugc nofollow" target="_blank">讲座</a>。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/35be181af64e35767dced04ed0f01962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gXE-cUmACGrFY7EO5W2jgQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">RNNs与变压器特性的比较</figcaption></figure><p id="943f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们可以看到以下细节:</p><ol class=""><li id="ca54" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">为了处理长序列并发现数据中的相关性，需要利用具有大递归度的LSTM或GRU，这在实践中意味着具有非常深的神经网络。另一方面，变压器不必很深，以促进远程依赖</li><li id="f96b" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">在transformer的架构中没有渐变消失和爆炸，因为它不是像在RNNs中那样进行线性计算，而是同时对整个序列进行计算，实际上它的层数比RNN少得多。</li><li id="538b" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">由于其架构，变压器不会采取那么多的训练步骤，RNN和缺乏递归导致并行计算能力。这意味着它可以批量处理输入令牌。</li></ol><h2 id="681f" class="kc jf hi bd jg kd ke kf jk kg kh ki jo iq kj kk js iu kl km jw iy kn ko ka kp bi translated">结构</h2><p id="1d21" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">变压器有一个编码器-解码器结构</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lz"><img src="../Images/d41ab8dd18deecc4eaceb88f80ba06d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cN3BOujc2LPXNqs-.png"/></div></div></figure><p id="39da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它将令牌输入编码器，并从解码器输出令牌。两部分都由复合块组成，编码器和解码器之间略有不同。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es ma"><img src="../Images/86ba74ba2a5573edbd1dd9e3140a5ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HfM7oLyiN_zqFWTn.png"/></div></div></figure><p id="6906" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模块的第一部分是<strong class="ih hj">自我关注</strong>层，它帮助模块根据其他输入及其位置对输入进行编码。例如，输入句子中的一个单词。之后，输出进入<strong class="ih hj">前馈</strong>层，该层创建新的表示。解码器模块具有额外的<strong class="ih hj">掩码</strong>层，因为它对两个令牌序列都进行操作，并且它不应该看到下面尚未生成的令牌，即，它采用输入法语句子表示和迄今生成的英语输出单词的嵌入。</p><p id="f4a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解transformer架构，看一看transformer作者之一的视频和插图丰富的帖子<a class="ae jd" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"/></p><h2 id="6054" class="kc jf hi bd jg kd ke kf jk kg kh ki jo iq kj kk js iu kl km jw iy kn ko ka kp bi translated">自我关注</h2><p id="68e1" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">网络的这一部分对变压器非常重要，DeiT架构也高度依赖于此。</p><p id="27b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，我们可以将注意力看作是使用查询从(key，value)存储中检索值。其中结果值是基于<query key="">相似性的值的加权和。</query></p><p id="2af2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇论文的作者对此进行了更精确的定义。</p><blockquote class="mb mc md"><p id="73c1" class="if ig me ih b ii ij ik il im in io ip mf ir is it mg iv iw ix mh iz ja jb jc hb bi translated">注意机制基于具有(键，值)向量对的可训练联想记忆。使用内积将查询向量<em class="hi"> q </em>与一组<em class="hi"> k </em>关键向量(一起打包成矩阵<em class="hi"> K </em>)进行匹配。然后用softmax函数对这些内积进行缩放和归一化，以获得<em class="hi"> k </em>权重。一组<em class="hi"> k </em>值向量的输出加权和可以写成:</p></blockquote><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mi"><img src="../Images/52374323db0029dc52878c878ba62f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*myupJBpdkTevzM4h7C0F3A.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><a class="ae jd" href="https://arxiv.org/pdf/2012.12877.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="52a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中Softmax函数应用于输入矩阵的每一行，并且<br/><em class="me">d</em>项提供适当的归一化。</p><h1 id="fc91" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">DeiT建筑</h1><p id="210e" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">作者提出的神经网络是这篇<a class="ae jd" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">论文</a>中提出的<strong class="ih hj">视觉转换器</strong> (ViT)的后继者，事实上是其逻辑延续。研究人员提出的新颖性主要体现在以下三个方面。</p><ol class=""><li id="e56b" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">在没有大量精选数据的情况下，有效地训练可视转换器，而ViT的作者则相反。</li><li id="d410" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">他们进行了各种实验，研究如何从卷积神经网络中提取知识，这得益于近十年的调整和优化。</li><li id="a83c" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">并且显示了在准确性和吞吐量之间的折衷方面，提取的模型优于它的老师。</li></ol><p id="a4f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图中，您可以看到在ImageNet上的准确性以及建议的网络变化、EfficientNet (convnet)变化和ViT之间的推理速度方面的比较</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mj"><img src="../Images/570270280a84408010f4ab4502d26c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AmDNu8EAsa3YVoEJ6EHpdQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><a class="ae jd" href="https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="fb94" class="kc jf hi bd jg kd ke kf jk kg kh ki jo iq kj kk js iu kl km jw iy kn ko ka kp bi translated">视觉变压器</h2><p id="01cf" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">Visual transformer有一个简单而优雅的架构，将输入图像视为一系列固定空间大小为16 × 16像素的<em class="me"> N </em>个图像块。每个面片都投影有一个线性层，保持其总尺寸为3 × 16 × 16 = 768。</p><p id="a140" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Transformer block不知道补丁的顺序，这就是为什么在第一个编码器层之前，固定或可训练的<strong class="ih hj">位置编码</strong>被添加到补丁嵌入中。</p><p id="aa58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了从监督数据中学习，ViT使用了BERT <a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">论文</a>中提出的<strong class="ih hj">类标记</strong>。班级令牌类似于conv网训练中的班级标签。</p><p id="b82b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">DeiT有三种变体。它们的性能如下表所示</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mk"><img src="../Images/15272c903d6cbf42711b3bba68f319b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9LS80HuB9j7eGjnO7pkhiQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><a class="ae jd" href="https://arxiv.org/pdf/2012.12877.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="b72c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">DeiT-B是与ViT相同的型号，即它具有相同的架构，但经过不同的训练。DeiT-Ti和DeiT-S是较小的型号，不同型号之间唯一不同的参数是<strong class="ih hj">嵌入尺寸</strong>和<strong class="ih hj">多头自关注</strong>中的头数。较小的型号具有较低的参数数量和较快的吞吐量。吞吐量是针对分辨率为224×224的图像测量的。</p><h2 id="10e2" class="kc jf hi bd jg kd ke kf jk kg kh ki jo iq kj kk js iu kl km jw iy kn ko ka kp bi translated">不同分辨率下的微调</h2><p id="54e1" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">作者采用了本文中<a class="ae jd" href="https://arxiv.org/abs/1906.06423" rel="noopener ugc nofollow" target="_blank">的微调程序。结果表明，使用较低的训练分辨率并以较高的分辨率对网络进行微调是可取的。这种方法加快了全面训练的速度，并在流行的数据扩充方案下提高了<br/>的准确性，当您在ImageNet1k这样的相对较小的数据集上训练transformer时就存在这种情况。</a></p><p id="15f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当增加输入图像的分辨率时，作者保持块大小不变，因此块的序列长度改变。Transformer不需要固定长度的序列，但是需要调整面片的位置嵌入。为了做到这一点，作者采用了一种<br/>双三次插值，在微调网络之前，近似保持向量的范数。</p><h2 id="9b4e" class="kc jf hi bd jg kd ke kf jk kg kh ki jo iq kj kk js iu kl km jw iy kn ko ka kp bi translated">蒸馏</h2><p id="e7ae" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">除了在更大的分辨率下进行数据扩充和微调，作者建议使用与ViT相比显著提高DeiT性能的技术。他们对DeiT进行知识提炼，以转移<strong class="ih hj">的<a class="ae jd" href="https://arxiv.org/abs/2006.00555" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">感应偏置</strong></a></strong><strong class="ih hj">。</strong></p><p id="226e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在进入性能表之前，我想让你先了解一些情况。</p><ol class=""><li id="112e" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">作者提出了<strong class="ih hj">提取令牌，</strong>它与<strong class="ih hj">类令牌</strong>具有相同的性质，与分类令牌一样用于标签预测。提取令牌是一种嵌入，它作为类别令牌输入到网络中，并通过自我关注与其他输入交互。类和提取令牌之间的区别在于，它是从卷积网络的预测中获得的，而不是从基础真值标签中获得的。</li><li id="d611" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">使用蒸馏令牌，作者尝试了<strong class="ih hj">软蒸馏</strong>和<strong class="ih hj">硬蒸馏</strong>。简单介绍一下，<strong class="ih hj">软蒸馏</strong>试图最小化教师的softmax和学生模型的softmax之间的Kullback-Leibler分歧，而<strong class="ih hj">硬蒸馏</strong>暗示着将教师的硬决策作为真实标签来调整学生的损失函数。他们在消融研究中显示，标签上的硬蒸馏加上蒸馏标记比其他组合效果更好。</li></ol><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es ml"><img src="../Images/697c04037f4a02f23865a23b23f33b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*IRxAA-OhXmmtQN4E0SdhMA.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><a class="ae jd" href="https://arxiv.org/pdf/2012.12877.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="6591" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从论文中可以看出，使用convnet作为教师，DeiT-B获得了比教师更好的结果。例如，<a class="ae jd" href="https://paperswithcode.com/paper/designing-network-design-spaces" rel="noopener ugc nofollow" target="_blank"> RegNetY-8.0GF </a>(注意，由于不同扩充，这份报告和原始报告之间的准确性有所不同)的表现比其学生的表现好两分</p><p id="5003" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上表显示了在微调(第3和第4列)过程中，DeiT-B的准确度如何根据蒸馏所用的教师(按行)而变化。教师的初始准确度显示在第二列。</p><h1 id="b830" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">给我看看代码！</h1><p id="5843" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">像往常一样，在最后一部分，你可以找到一个链接，链接到论文的<a class="ae jd" href="https://github.com/facebookresearch/deit" rel="noopener ugc nofollow" target="_blank">代码</a>。但是如果你想知道这篇论文的主要建议是什么，那就是蒸馏，不幸的是它还没有提供，但是<a class="ae jd" href="https://github.com/facebookresearch/deit/issues/4" rel="noopener ugc nofollow" target="_blank">的一期</a>说它将很快发布。</p><h1 id="181d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">参考</h1><p id="bc12" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">[1]训练数据高效的图像转换器&amp;通过注意力进行提炼<a class="ae jd" href="https://arxiv.org/abs/2012.12877" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2012.12877</a></p><p id="a7b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2] CS480/680第19讲:注意力和变压器网络<a class="ae jd" href="https://www.youtube.com/watch?v=OyFJWRnt_AY" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=OyFJWRnt_AY</a></p><p id="cf35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]图解变压器<a class="ae jd" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a></p></div></div>    
</body>
</html>