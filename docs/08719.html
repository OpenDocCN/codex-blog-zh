<html>
<head>
<title>Data Science Interview Preparation Series: Naive Bayes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学面试准备系列:朴素贝叶斯</h1>
<blockquote>原文：<a href="https://medium.com/codex/data-science-interview-preparation-series-part-2-naive-bayes-9b93b1bef16e?source=collection_archive---------8-----------------------#2022-08-29">https://medium.com/codex/data-science-interview-preparation-series-part-2-naive-bayes-9b93b1bef16e?source=collection_archive---------8-----------------------#2022-08-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="4220" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">这是数据科学面试准备系列的第2部分。这是第一部分的链接。</p></blockquote><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es ji"><img src="../Images/8ae427526ba605580176c95c7f878677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*he-U6IcaiN6jacV78kWYvA.jpeg"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">摘自<a class="ae jh" href="https://unsplash.com/photos/eF7HN40WbAQ" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="c17a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这篇文章的结构与大多数文章不同。一般来说，文章包括副标题和描述。在本文中，我积累了关于朴素贝叶斯的最基本的问题，并以一种使文章详尽和自给自足的方式回答了这些问题。这些问题是有序的，因此每个后续问题都建立在前一个问题的基础上，模拟面试官如何测试你的朴素贝叶斯知识。为了更好的理解，我还在中间插入了图片，如果你还想满足你的好奇心，我还提供了链接参考。</p><p id="8dbf" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">那么让我们开始吧！！！:)))</strong></p><h1 id="3782" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">什么是朴素贝叶斯？能简单介绍一下算法吗？</h1><p id="26af" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">朴素贝叶斯是一种受监督的机器学习算法，可以通过训练将数据分类到多类类别中。朴素贝叶斯算法的核心是概率模型，它计算输入要素的条件概率，并将概率分布分配给每个可能的类。</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es le"><img src="../Images/62d37938894b0e729986951dd3309045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NUTtVANdt6XqwjEh"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图片取自<a class="ae jh" href="https://www.udacity.com/course/data-scientist-nanodegree--nd025" rel="noopener ugc nofollow" target="_blank"> Udacity数据科学纳米学位</a></figcaption></figure><h1 id="ae25" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">你谈到这个模型是一个概率模型。能否详细阐述一下背后的数学概念，朴素贝叶斯是一个什么样的模型？</h1><p id="e2bf" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">让我们把这个问题分成两部分:</p><ol class=""><li id="bcef" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lk ll lm ln bi translated"><strong class="il hj">朴素贝叶斯</strong>中使用的数学概念，<strong class="il hj">贝叶斯定理</strong></li><li id="018d" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated"><strong class="il hj">朴素贝叶斯属于被称为生成模型的模型类别</strong></li></ol><p id="4e02" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">巴耶定理:</strong></p><p id="8570" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">朴素贝叶斯基于统计学中的贝叶斯定理。它基于条件和无条件独立计算每一类的概率，然后预测结果。让我们更详细地解释一下:</p><p id="9cc6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">最初，我们从一个事件开始，这个事件可能是A或B。每个事件的概率显示为P(A)和P(B)。现在，我们观察第三个事件，对于A和B，该事件可能发生，也可能不发生。R将帮助我们通过以下方式找到A和B的更精确的概率。</p><p id="6634" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">计算给定a的r的概率(记为P(R|A))，以及给定a的r补的概率(记为P(Rᶜ|A)).)类似地，P(R|B)和P(Rᶜ|B).</p><p id="00ab" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这四组场景是:</p><ul class=""><li id="2ea4" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lt ll lm ln bi translated">P(R ∩ A)</li><li id="1db4" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">P(Rᶜ ∩ A)</li><li id="c2fa" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">P(R ∩ B)</li><li id="760f" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">p(rᶜb)</li></ul><p id="a884" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">但是既然我们知道R发生了，我们知道第二个和第四个事件是不可能的。我们的新宇宙由两个事件组成，P(R ∩ A)和P(R ∩ B)。</p><ul class=""><li id="3969" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lt ll lm ln bi translated">P(R ∩ A) = P(A)P(R|A)</li><li id="85da" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">P(R ∩ B) = P(B)P(R|B)</li></ul><p id="edc9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">因为这些概率加起来不等于1，所以我们将它们除以它们的和，这样新的归一化概率现在就等于1了。</p><p id="8fc2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">标准化概率:</p><p id="9a79" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">因此，我们得到了P(A|R)和P(B|R)的公式</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es lu"><img src="../Images/f22cb167bf415f0910f19c9732b8247e.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*CplaN6F7BsX3qAlZi0_hZw.png"/></div></figure><p id="72c2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">在我们知道R发生后，这些是我们新的和改进的A和B的概率。</p><p id="a0b9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">最后，我们有了贝叶定理的完整公式。</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es lu"><img src="../Images/a8cdd2665f13cb4755ef5ab18c5c8005.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*fnYIrY4xzgbonf987x9LYw.png"/></div></figure><p id="80cf" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">生成模型:</strong></p><p id="2d8e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">朴素贝叶斯是一种生成分类器。它通过对数据集执行操作来学习数据集的实际分布。它不像在判别模型中那样创建决策边界来对数据进行分类。这些模型使用<strong class="il hj">概率估计</strong>和<strong class="il hj">可能性</strong>来模拟数据点，并区分数据集中的不同类别标签。要了解更多关于生成和判别模型的信息，请参考我在Twitter上写的这条帖子:</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h1 id="ed31" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">你能从似然、先验和证据的角度定义贝叶斯定理吗？</h1><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es lx"><img src="../Images/5437dd56a1e7522ffcec8f114e5c7a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*wFYK-gP9IvJXQlCl.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图片描述了贝叶斯定理，摘自另一篇<a class="ae jh" rel="noopener" href="/analytics-vidhya/naïve-bayes-algorithm-5bf31e9032a2">媒体文章</a>。</figcaption></figure><h1 id="0914" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">朴素贝叶斯只对离散数据有效吗？如果不是，可以用于什么样的数据？</h1><p id="2828" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">朴素贝叶斯可以适用于离散的、连续的和不能用数字表示的数据。我们有不同类型的朴素贝叶斯分类器:</p><ol class=""><li id="ed01" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lk ll lm ln bi translated">高斯朴素贝叶斯</li><li id="457b" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">伯努利朴素贝叶斯</li><li id="0611" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">多项式朴素贝叶斯</li></ol><p id="2ad3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">让我们更深入地了解每一种类型:</p><p id="7c6f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">高斯朴素贝叶斯:</strong></p><p id="91e8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">高斯朴素贝叶斯是遵循高斯正态分布并支持连续数据的朴素贝叶斯的变体。</p><p id="89d7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">假设特征的可能性为:</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es ly"><img src="../Images/206234d73daefb157a3b665318798d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/0*qEsV_v9LLf4ibE_k"/></div></figure><p id="9b8a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">有时假设方差</p><ul class=""><li id="1087" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lt ll lm ln bi translated">独立于Y(即σi)，</li><li id="3d94" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">或者独立于Xi(即σk)</li><li id="2985" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">或者两者都有(即σ)</li></ul><p id="8153" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">高斯朴素贝叶斯支持连续值的要素和模型，每个要素和模型都符合高斯(正态)分布。</p><p id="37ca" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">一种创建简单模型的方法假设高斯分布描述了维度之间没有共方差(独立维度)的数据。该模型可以通过简单地找到每个标签内的点的平均值和标准偏差来拟合，这是定义这种分布所需要的。</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es lz"><img src="../Images/0c8fbadf9ae71ba60ec747b7874a3b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/0*9SZecKJj3coDoJZF"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图片摘自关于<a class="ae jh" href="https://iq.opengenus.org/gaussian-naive-bayes/" rel="noopener ugc nofollow" target="_blank">高斯NB </a>的文章</figcaption></figure><p id="3ea0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">上图显示了高斯朴素贝叶斯(GNB)分类器的工作原理。在每个数据点，计算该点与每个类平均值之间的z得分距离，即与类平均值的距离除以该类的标准差。</p><p id="d2a1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">因此，我们看到高斯朴素贝叶斯有一个稍微不同的方法，可以有效地使用。以上内容摘自这篇<a class="ae jh" href="https://iq.opengenus.org/gaussian-naive-bayes/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><p id="4c81" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">伯努利朴素贝叶斯:</strong></p><p id="083d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这用于离散数据，适用于伯努利分布。Bernoulli Naive Bayes的主要特征是，它只接受二进制值形式的特征，如真或假、是或否、成功或失败、0或1等等。所以当特征值是二进制的时候，我们知道我们必须使用伯努利朴素贝叶斯分类器。</p><p id="a1ae" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">当我们处理二进制值时，让我们考虑' p '是成功的概率，' q '是失败的概率，q=1-p .对于伯努利分布中的一个随机变量' X ',</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es ma"><img src="../Images/00fe037da00ef9a650d0f0f8db67fc80.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*7U5Nt7EylGrg3txR.jpg"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图片摘自关于<a class="ae jh" href="https://iq.opengenus.org/bernoulli-naive-bayes/" rel="noopener ugc nofollow" target="_blank">伯努利NB </a>的文章</figcaption></figure><p id="14f2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><em class="ik">其中“x”只能有两个值，要么是0，要么是1 </em></p><p id="0c71" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">伯努利朴素贝叶斯分类器基于以下规则(可能性):</strong></p><p id="5c81" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">p(xᵢ∣y)= p(∣y)xᵢ+(1 p(∣y))(1xᵢ)</p><p id="320b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">以上内容摘自<a class="ae jh" href="https://iq.opengenus.org/bernoulli-naive-bayes/" rel="noopener ugc nofollow" target="_blank">篇</a>。最初的文章也展示了它使用表格数据的工作方式。</p><p id="74c0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">多项朴素贝叶斯:</strong></p><p id="764a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">多项式模型提供了对无法用数字表示的数据进行分类的能力。它的主要优点是大大降低了复杂性。它提供了使用小训练集来执行分类的能力，而不需要连续地重新训练。</p><p id="1269" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">多项式朴素贝叶斯广泛用于根据文档内容的统计分析将文档分配到类别中。它为“繁重的”基于人工智能的语义分析提供了一种替代方案，并大大简化了文本数据分类。它被广泛用作基于距离的K-Means聚类和决策树森林的替代方法。它将概率视为数据属于特定类别的“可能性”。</p><p id="79b4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">算法:</p><p id="0a8e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">让𝑺 —一个输入字符串，𝑫 —一个𝒛-documents语料库，𝑪 —一个𝒎-classes:集</p><p id="176d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">计算样本</strong> 𝑺 <strong class="il hj">的类别</strong> 𝑪ₛ <strong class="il hj">如下:</strong></p><ol class=""><li id="b76b" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lk ll lm ln bi translated">将样本𝑺分割成一组𝒏-terms</li><li id="6f47" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">对于每个𝒌-th级𝑪ₖ 𝒌=𝟭..𝒎，请执行以下操作:</li></ol><ul class=""><li id="8d45" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lt ll lm ln bi translated">计算𝒏-features ∀𝒘ₖᵢ ∈ 𝑾的向量𝑾，其中𝒘ₖᵢ是相应的𝒊-th术语在𝑪ₖ.的文档中出现的频率</li><li id="2566" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">评估先前的𝒑(𝑪ₖ)作为文档出现在来自𝑪ₖ.类别的文档中的全概率</li><li id="6db6" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">计算后验𝐏𝐫(𝑪ₖ | 𝑾)的方法是将先验𝒑(𝑪ₖ)加到每项𝒘ᵢ的总和上，给定𝑪ₖ，概率𝒑(𝒘ᵢ | 𝑪ₖ):</li></ul><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es mb"><img src="../Images/887b30a8f251817353e5a8e8fa67963b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VB2efUkfP0WNmgCD.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">后验𝐏𝐫(𝑪ₖ | 𝑾)公式</figcaption></figure><p id="2afc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">3.确定𝑪ₛ为𝑺的一类，其中𝐏𝐫(𝑪ₖ | 𝑾) 𝒌=𝟭..𝒎是最高的:</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es mb"><img src="../Images/e3f2f54bbb5fdf17a1822b3a2393ab9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RR_837PPQimOHxsa.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">𝑺计算的𝑪ₛ类</figcaption></figure><p id="3314" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">该算法的复杂度<strong class="il hj"> 𝜎 </strong>被评估为:</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es mb"><img src="../Images/b6c3dc41175ab16769be5ec477db0655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8Aq09tkAs9V-4UAK.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">多项式贝叶斯分类器复杂度(<strong class="bd kd"> 𝜎 </strong></figcaption></figure><p id="4e7b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">𝒛—𝒏𝑫的文档总数—𝒎𝑺样本中的术语数—𝑪的类数</p><p id="c4ec" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">作者的上述表达方式在原文<a class="ae jh" href="https://towardsdatascience.com/multinomial-na%C3%AFve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6" rel="noopener" target="_blank">这里</a>中得到了精美的推导。:)</p><h1 id="f4e8" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">以上所有方法都是在监督下进行的。你认为朴素贝叶斯可以用其他方式训练吗？</h1><p id="1d7c" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">朴素贝叶斯算法可以调整为在半监督设置中进行训练。半监督学习提供了提高多项式模型性能的能力。此外，它允许通过基于已经分类的文档的语料库训练模型来提高分类的质量。</p><p id="75d4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">半监督学习算法相当直观简单且公式化，例如:</p><ol class=""><li id="ba61" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lk ll lm ln bi translated">使用上面讨论的多项式模型计算𝑺的𝑪ₛ等级。</li><li id="e816" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">将标有𝑪ₛ的𝑺加入𝐷.文献集</li><li id="7072" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">重新评估分类模型。</li></ol><p id="1a1c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">按照以下流程对每个新的𝑺.样本进行分类</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="jo jp di jq bf jr"><div class="er es mb"><img src="../Images/829d0297f9bc41059ce7d79676ce3aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iiWTNu2j_K4Ro9_S.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">半监督学习过程(<a class="ae jh" href="https://towardsdatascience.com/multinomial-na%C3%AFve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6" rel="noopener" target="_blank">来源</a>)</figcaption></figure><h1 id="7452" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">朴素贝叶斯算法有什么好处？</h1><p id="c4cc" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">它比逻辑回归等简单算法更有效。它还可以很好地处理分类数据和数值数据。此外，使用朴素贝叶斯分类器非常简单快捷。复杂的高维数据非常适合朴素贝叶斯分类器。它也可以使用带有半监督学习的小标签数据集来训练。</p><p id="68dc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">换句话说:</p><ul class=""><li id="bd23" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lt ll lm ln bi translated">它对干净和有噪声的数据都有很好的表现。除了一些实验观察，我找不到对这种行为的任何好的解释，但这里有一个关于quora<a class="ae jh" href="https://www.quora.com/Which-is-more-robust-to-noisy-data-a-Decision-Tree-or-Naive-Bayes" rel="noopener ugc nofollow" target="_blank">的好答案，可能会对此有所启发。</a></li><li id="284e" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">训练需要几个样本，但基本假设是训练数据集是总体的真实表示。</li><li id="e87b" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">获得预测的可能性很简单。</li></ul><h1 id="af50" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">朴素贝叶斯比逻辑回归这样的简单算法更好有什么确定的原因吗？它总是正确的吗，或者有什么特别的地方吗？</h1><p id="9e5a" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">吴恩达教授和迈克尔一世·乔丹教授的这篇论文提供了这两个模型的误差性质的数学证明。他们得出结论，当训练规模达到无穷大时，判别模型:逻辑回归比生成模型朴素贝叶斯表现更好。然而，生成模型比判别模型(O (n))更快地达到其渐近解(O(log n))，即，生成模型(朴素贝叶斯)比判别模型(逻辑回归)更少的训练集达到渐近解。</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es mc"><img src="../Images/2352f14b49f69808e1d1d7ad55feffd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/0*Fqf7xhEVrcvgAq80.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">显示NB和逻辑回归性能的图像(<a class="ae jh" rel="noopener" href="/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c">来源</a></figcaption></figure><p id="2fab" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">朴素贝叶斯还假设特征是条件独立的。真实的数据集从来都不是完全独立的，但是它们可以很接近。简而言之，<strong class="il hj">与逻辑回归相比，朴素贝叶斯具有更高的偏差但更低的方差。如果数据集遵循偏差，朴素贝叶斯将是更好的分类器。</strong>朴素贝叶斯和逻辑回归都是线性分类器。逻辑回归使用直接函数形式预测概率，而朴素贝叶斯则根据结果计算出数据是如何生成的。</p><h1 id="7aa1" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">你能详细阐述一下你对偏差和方差的理解吗？</h1><p id="b62b" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">要回答这个问题，最好把它分解成更基本的问题，比如:</p><ol class=""><li id="ecf3" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lk ll lm ln bi translated">什么是偏差和方差？</li><li id="2c23" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">欠拟合和过拟合与偏差和方差有什么关系？</li><li id="c701" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">从数学公式上来说，偏倚和方差是如何构成模型的总误差的？</li></ol><p id="e39f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">所有这些问题都可以在我写的关于Twitter⬇️的帖子中找到答案</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h1 id="d590" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">为什么认为复杂高维数据很适合朴素贝叶斯分类器？</h1><p id="ca56" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">朴素贝叶斯隐式地将所有特征视为彼此独立。因此，当处理高维数据时，各种维数灾难问题通常不会出现。</p><p id="834c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">如果您的数据具有k维，那么尝试学习这些特征之间所有可能的相关性的完全通用的ML算法必须处理2ᵏ可能的特征相互作用，因此需要2ᵏ数量级的许多数据点来执行。然而，因为朴素贝叶斯假设特征之间是独立的，所以它只需要k数量级的数据点，指数级地更少。</p><p id="f53e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">然而，这是以只能捕获输入变量和输出类之间更简单的映射为代价的，因此当涉及到图像识别等任务时，朴素贝叶斯永远无法与在大型数据集上训练的大型神经网络竞争，尽管它可能在非常小的数据集上表现更好(但具有更多功能)。</p><p id="2845" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这是我写的一篇关于维数灾难的Twitter帖子。随意参考。:)</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h1 id="55d6" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">朴素贝叶斯分类器有哪些缺点？</h1><p id="f0f2" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">朴素贝叶斯分类器遭受“<a class="ae jh" href="https://www.atoti.io/how-to-solve-the-zero-frequency-problem-in-naive-bayes/" rel="noopener ugc nofollow" target="_blank">零频率</a>问题。当某个类别不在训练集中时，就会发生这种情况。它会给它0的概率。</p><p id="2008" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">它最大的缺点是认为特征是相互独立的，因为在现实生活中，不可能得到独立的特征。所有的特征在某种程度上都是相互关联的。</p><h1 id="c6db" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">你知道解决“零频率”问题的方法吗？</h1><p id="1df3" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">克服这种“零频率问题”的一种方法是，当一个属性值不是与每个类值一起出现时，为每个属性值-类组合的计数加1。</p><p id="2f83" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这将导致从类中移除所有零值，同时不会影响类的整体相对频率。</p><p id="8694" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">在上面的例子中，我们将垃圾邮件和垃圾邮件的字数都增加了1。然后，我们计算一封带有“办公室”、“富人”、“富人”和“金钱”字样的新邮件是垃圾邮件或火腿的概率。</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es md"><img src="../Images/c5f8cc2551ef27e0e7d70e742c498e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MY-2xrV4-zhEJoSE"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">该图显示了添加一个常数来解决NB( <a class="ae jh" href="https://www.atoti.io/how-to-solve-the-zero-frequency-problem-in-naive-bayes/" rel="noopener ugc nofollow" target="_blank">源</a>)中的零频率问题的效果</figcaption></figure><p id="5888" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">当分子中特定情况的概率为零时，我们可能会遇到零除法误差。为了减轻这种情况，我们可以使用拉普拉斯平滑，即在分子中增加一个数字，在分母中增加另一个数字。</p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><p id="48c1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi ml translated">这篇文章的下两个问题不像其他问题那样笼统。他们专注于使用我们讨论的主要算法解决的问题。我故意这样做是为了指出探索和调查是好的。如果你用自己做过的例子来解释概念，面试官会很感激。  <em class="ik">会显示你喜欢深挖事物；基本上，你是个书呆子，会成为公司的资产。</em></p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="d18f" class="kb kc hi bd kd ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku my kw kx ky bi translated">你能举个例子解释一下朴素贝叶斯的整体分类过程吗？</h1><p id="04aa" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">我们以<strong class="il hj">垃圾邮件分类</strong>为例。特定单词在垃圾邮件和合法邮件中出现的概率特定。过滤器事先不知道这些概率，并且必须首先被训练，以便它可以建立它们。为了训练过滤器，用户必须手动指示新电子邮件是否是垃圾邮件。对于每个训练电子邮件中的所有单词，过滤器将调整每个单词在其数据库中出现在垃圾邮件或合法电子邮件中的概率。</p><p id="874f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">训练后，单词概率(也称为似然函数)用于计算包含特定单词集的电子邮件属于任一类别的概率。电子邮件中的每个单词都会影响垃圾邮件的概率，或者只影响最感兴趣的单词。这种贡献被称为后验概率，并使用贝叶斯定理计算。然后，计算电子邮件中所有单词的垃圾邮件概率，如果总数超过某个阈值(比如95%)，过滤器会将该电子邮件标记为垃圾邮件。</p><p id="e59c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">如果我们要总结将邮件分类为垃圾邮件或ham的整个过程，我们可以这样做:</p><ol class=""><li id="6b0e" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lk ll lm ln bi translated">计算单个单词的垃圾邮件数量。</li><li id="38e8" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">把所有的单词概率结合起来，得到一个分数。</li><li id="b963" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lk ll lm ln bi translated">将分数与阈值进行比较，并给出最终结论。</li></ol><p id="fa3c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">计算单个单词的垃圾信息</strong></p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es mz"><img src="../Images/77439937ed5ea8f646b2172c6df52f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*BHWA1YROK4Fmtmi6bSqDeg.png"/></div></figure><p id="8504" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">其中:</p><ul class=""><li id="bc20" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lt ll lm ln bi translated">Pr(S|W)是一条消息是垃圾邮件的概率，知道这个词在里面；</li><li id="08cf" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">Pr(S)是任何给定消息是垃圾邮件的总体概率；</li><li id="4dc5" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">Pr(W|S)是该词出现在垃圾短信中的概率；</li><li id="6d09" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">Pr(H)是任何给定消息不是垃圾邮件(是“ham”)的总概率；</li><li id="68d2" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">Pr(W|H)是该单词出现在ham消息中的概率。</li></ul><p id="8741" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">此公式中使用的数字Pr(W|S)近似于在学习阶段识别为垃圾邮件的消息中包含特定单词的消息的频率。类似地，Pr(W|H)近似于在学习阶段识别为ham的消息中包含特定单词的消息的频率。为了使这些近似有意义，学习到的信息集需要足够大和有代表性。</p><p id="d5b1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">当然，仅根据单个单词的存在来确定一条消息是垃圾邮件还是ham是容易出错的。贝叶斯垃圾邮件软件试图考虑几个词，并结合它们的垃圾邮件，以确定邮件是垃圾邮件的总体概率。</p><p id="e8e4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">综合所有单词概率得到一个分数</strong></p><p id="d54f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">消息中出现的单词是<a class="ae jh" href="https://en.wikipedia.org/wiki/Statistical_independence" rel="noopener ugc nofollow" target="_blank">独立事件</a>。这个条件一般不满足(例如，在英语这样的自然语言中，找到一个形容词的概率受到拥有一个名词的概率的影响)。尽管如此，这仍然是一种有用的理想化，尤其是因为单个单词之间的统计相关性通常是未知的。在此基础上，可以从贝叶斯定理推导出以下公式:</p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es na"><img src="../Images/b2a8f9609fcf967e7144708b79586db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*WXwu9CVvB8Zo0xHOdpFltA.png"/></div></figure><p id="ab4d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">其中:</p><ul class=""><li id="c108" class="lf lg hi il b im in iq ir jy lh jz li ka lj jg lt ll lm ln bi translated">p是可疑消息是垃圾邮件的概率；</li><li id="dadd" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">p₁是第一个单词(例如，“副本”)出现的概率p(W₁|S)，假定该消息是垃圾邮件；</li><li id="be8e" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">p₂是第二个单词(例如，“手表”)出现的概率p(W₂|S)，假定该消息是垃圾邮件；</li><li id="b3aa" class="lf lg hi il b im lo iq lp jy lq jz lr ka ls jg lt ll lm ln bi translated">等等..…</li></ul><p id="2216" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">通常将结果<em class="ik"> p </em>与给定的阈值进行比较，以决定该消息是否是垃圾邮件。<strong class="il hj">如果<em class="ik"> p </em>低于阈值，则认为该消息可能是ham否则，它被视为可能的垃圾邮件。</strong></p><h1 id="3a20" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">你能告诉我使用基于贝叶斯的垃圾邮件过滤模型的优点和缺点吗？</h1><p id="1430" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated"><strong class="il hj">优点:</strong></p><p id="87ce" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">可以在每个用户的基础上训练朴素贝叶斯垃圾邮件过滤模型。用户的垃圾邮件通常与在线用户的活动有关。例如，用户可能已经订阅了被用户视为垃圾邮件的在线时事通讯。该在线简讯可能包含所有简讯通用的词语，例如简讯的名称及其原始电子邮件地址。贝叶斯垃圾邮件过滤器最终会根据用户的特定模式分配更高的概率。</p><p id="1ffc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">单词概率对每个用户来说都是唯一的，并且每当过滤器对电子邮件进行错误分类时，单词概率会随着时间的推移而随着纠正训练而演变。因此，训练后的贝叶斯垃圾邮件过滤准确性通常优于预定义的规则。</p><p id="1c84" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">由于同样的原因，<strong class="il hj">它可以很好地避免误报</strong>，即合法的电子邮件被错误地归类为垃圾邮件。例如，如果电子邮件包含单词“Nigeria”，预定义的规则过滤器可能会直接拒绝它，这是预付费用欺诈垃圾邮件中常用的单词。贝叶斯过滤器会将单词“尼日利亚”标记为可能的垃圾邮件单词，但会考虑通常表示合法电子邮件的其他重要单词。例如，配偶的名字可能强烈表明该电子邮件不是垃圾邮件，这可能会克服“尼日利亚”一词的使用</p><p id="64ce" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">缺点:</strong></p><p id="effb" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">NB垃圾邮件过滤模型可能容易受到<a class="ae jh" href="https://en.wikipedia.org/wiki/Bayesian_poisoning" rel="noopener ugc nofollow" target="_blank">贝叶斯中毒</a>的影响，这是垃圾邮件制造者用来降低依赖贝叶斯过滤的垃圾邮件过滤器效率的一种技术。实践贝叶斯中毒的垃圾邮件发送者将发送带有大量合法文本(从合法新闻或文学来源收集)的电子邮件。<a class="ae jh" href="https://en.wikipedia.org/wiki/E-mail_spam" rel="noopener ugc nofollow" target="_blank">垃圾邮件制造者</a>的策略包括插入通常与垃圾邮件无关的随机无害单词，降低电子邮件的垃圾邮件分数，并使其更有可能逃过贝叶斯垃圾邮件过滤器。</p><p id="cafc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">解决这个问题的一个办法是使用保罗·格拉厄姆的方案，其中只使用最重要的概率。用非垃圾邮件相关的单词填充文本不会显著影响检测概率。</p><h1 id="0278" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">总结全文</h1><p id="5d2f" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">这篇文章到此为止！</p><p id="c36f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">当在面试中被问到上述所有问题时，这些问题足以解决任何关于朴素贝叶斯的讨论。当然，如果你要进入一家研究公司，你需要对算法有更深刻的理解，以及如何使用它来解决你试图解决的研究问题。</p><p id="b6c8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">作为结束语，我想宣布我是<strong class="il hj">专业书呆子</strong>。事实上，我想成为一个书呆子，并鼓励你也成为一个！！！！</p><p id="af40" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">对某件事成为一个书呆子表明你热爱你所做的事情，会让你过上更满意的生活。工作看起来不再像工作；这意味着，在某个时候，如果我们的手艺足够好，我们将会得到报酬去享受乐趣！但是成为一个书呆子并不容易。在开始的时候，你必须强迫自己利用你的休息时间来做你的手艺，并且痴迷于它！关于这个我已经在本系列的<a class="ae jh" rel="noopener" href="/@rohit18115/data-science-interview-preparation-series-part-1-perspective-shift-237e79ffd555"> <strong class="il hj">第一篇</strong> </a>中写的比较多了。请去看看！:)</p><p id="7985" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">最后但同样重要的是，你可能已经注意到，我把你们带到了各种Twitter帖子，这些帖子在文章的很多地方都涉及到了更基本的概念。我希望它能帮助你快速入门这些主题，如果你想查看更多这样的线程，这里有一个链接到我的🔥主线程🔥，这是我迄今为止编写的所有DS、ML和python线程的集合。</p><figure class="jj jk jl jm fd jn"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h1 id="f997" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">感谢</h1><p id="acb7" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">如果没有其他人写的大量关于朴素贝叶斯和其他基本概念的文章和博客，这篇文章是不可能完成的。我想衷心感谢各自的作者做了如此伟大的工作！</p><p id="cfe0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">我不得不承认，整理问题并找到这些问题的最佳答案需要花费大量时间。所以大部分答案都是直接从文章/博客中摘抄而来，稍加编辑或转述。</p><h1 id="76c1" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">参考</h1><p id="4b3e" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jy lb iw ix jz lc ja jb ka ld je jf jg hb bi translated">(排名不分先后)</p><p id="a4f6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><a class="ae jh" href="http://www.paulgraham.com/spam.html" rel="noopener ugc nofollow" target="_blank">保罗·格拉厄姆改进贝叶斯垃圾邮件过滤的方案</a></p><p id="ee66" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><a class="ae jh" href="https://towardsdatascience.com/whats-so-naive-about-naive-bayes-58166a6a9eba" rel="noopener" target="_blank">为什么朴素贝叶斯这么幼稚？</a></p><p id="c57b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><a class="ae jh" href="https://iq.opengenus.org/gaussian-naive-bayes/" rel="noopener ugc nofollow" target="_blank">高斯朴素贝叶斯</a></p><p id="c01d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><a class="ae jh" href="https://iq.opengenus.org/bernoulli-naive-bayes/" rel="noopener ugc nofollow" target="_blank">伯努利朴素贝叶斯</a></p><p id="fef7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><a class="ae jh" href="https://towardsdatascience.com/multinomial-na%C3%AFve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6" rel="noopener" target="_blank">多项式朴素贝叶斯</a></p><p id="30ae" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><a class="ae jh" rel="noopener" href="/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c">逻辑回归和朴素贝叶斯的比较</a></p><p id="5ee1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><a class="ae jh" href="https://towardsdatascience.com/multinomial-na%C3%AFve-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6#:~:text=number%20of%20classes%20%F0%9D%91%AA-,Semi%2DSupervised%20Learning,-Semi%2Dsupervised%20learning" rel="noopener" target="_blank">朴素贝叶斯中的半监督学习</a></p><p id="e615" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><a class="ae jh" href="https://www.notion.so/Naive-bayes-from-interview-perspective-f91b7a065c274f26925270e5826c397f" rel="noopener ugc nofollow" target="_blank">还有许多其他重要的环节，详见我对这篇文章的粗略注释。</a></p></div></div>    
</body>
</html>