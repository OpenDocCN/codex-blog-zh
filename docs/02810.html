<html>
<head>
<title>YOLOv5: Tiny Footprint &amp; GPU Results on CPUs via Sparsification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLOv5:通过稀疏化在CPU上实现微小的内存占用和GPU结果</h1>
<blockquote>原文：<a href="https://medium.com/codex/yolov5-tiny-footprint-gpu-results-on-cpus-neural-magic-18252946f85e?source=collection_archive---------11-----------------------#2021-08-06">https://medium.com/codex/yolov5-tiny-footprint-gpu-results-on-cpus-neural-magic-18252946f85e?source=collection_archive---------11-----------------------#2021-08-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="327d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">修剪和量化YOLOv5，以12倍的较小模型文件将性能提高10倍。然后轻松应用您的数据并部署到商用CPU上。</h1><p id="17d4" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Neural Magic通过使用最先进的修剪和量化技术结合<a class="ae kb" href="https://docs.neuralmagic.com/deepsparse/" rel="noopener ugc nofollow" target="_blank"> DeepSparse引擎</a>，提高了YOLOv5模型在CPU上的性能。在这篇博文中，我们将介绍我们的一般方法，并演示如何:</p><ol class=""><li id="249d" class="kc kd hi jf b jg ke jk kf jo kg js kh jw ki ka kj kk kl km bi translated">利用<a class="ae kb" href="https://github.com/ultralytics" rel="noopener ugc nofollow" target="_blank"> Ultralytics YOLOv5知识库</a>和<a class="ae kb" href="http://github.com/neuralmagic/sparseml" rel="noopener ugc nofollow" target="_blank"> SparseML的</a>稀疏化<a class="ae kb" href="https://github.com/neuralmagic/sparseml/blob/main/integrations/ultralytics-yolov5/tutorials/sparsifying_yolov5_using_recipes.md" rel="noopener ugc nofollow" target="_blank">配方</a>创建高度修剪和INT8量化的YOLOv5模型；</li><li id="824e" class="kc kd hi jf b jg kn jk ko jo kp js kq jw kr ka kj kk kl km bi translated">在新数据集上训练YOLOv5，利用<a class="ae kb" href="https://sparsezoo.neuralmagic.com/?repo=ultralytics&amp;page=1&amp;order_by=created&amp;descending=true" rel="noopener ugc nofollow" target="_blank"> SparseZoo </a>中的预稀疏化模型，用您自己的数据复制我们的性能；</li><li id="1624" class="kc kd hi jf b jg kn jk ko jo kp js kq jw kr ka kj kk kl km bi translated">使用前面提到的集成和从<a class="ae kb" href="http://neuralmagic.com/yolov5" rel="noopener ugc nofollow" target="_blank"> Neural Magic YOLOv5模型页面</a>链接的工具复制我们的基准。</li></ol><p id="55dd" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">我们在2021年8月31日围绕上述三个话题进行了现场讨论。这里 <strong class="jf hj">可以查看<a class="ae kb" href="https://neuralmagic.com/resources/on-demand-webinars/live-discussion-sparsifying-yolov5-for-better-performance-smaller-file-size-and-cheaper-deployment/" rel="noopener ugc nofollow" target="_blank">。</a></strong></p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/56fe239f9e5514940e92853ff4142dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IV6tf858L9PpuU0Y.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><em class="ll">图1:不同CPU实现的YOLOv5l(批量1)与常用GPU基准测试的实时性能对比</em>。</figcaption></figure><p id="bf8f" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">除了YOLOv5，我们还稀疏化了<a class="ae kb" href="https://neuralmagic.com/blog/pruning-hugging-face-bert-compound-sparsification/" rel="noopener ugc nofollow" target="_blank">拥抱脸BERT </a>(并为<a class="ae kb" href="https://neuralmagic.com/use-cases/sparse-question-answering/" rel="noopener ugc nofollow" target="_blank">问答</a>和<a class="ae kb" href="https://neuralmagic.com/use-cases/sparse-sentiment-analysis" rel="noopener ugc nofollow" target="_blank">情绪分析</a>)、<a class="ae kb" href="https://neuralmagic.com/blog/benchmark-resnet50-with-deepsparse/" rel="noopener ugc nofollow" target="_blank"> ResNet-50 </a>和<a class="ae kb" href="https://neuralmagic.com/blog/benchmark-yolov3-on-cpus-with-deepsparse/" rel="noopener ugc nofollow" target="_blank"> YOLOv3 </a>编写了具体的端到端教程，显示出比其他CPU实现显著的加速比。</p><h2 id="cc0a" class="lm ig hi bd ih ln lo lp il lq lr ls ip jo lt lu it js lv lw ix jw lx ly jb lz bi translated">在CPU上实现GPU级性能</h2><p id="5615" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">2020年6月，Ultralytics通过创建和发布YOLOv5 GitHub <a class="ae kb" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank">库</a>迭代了YOLO对象检测模型。新的迭代为非常成功的YOLO家族增加了新的贡献，如<a class="ae kb" href="https://github.com/ultralytics/yolov5/issues/804" rel="noopener ugc nofollow" target="_blank"> Focus </a>卷积块和更标准的现代实践，如复合缩放等。该迭代还标志着YOLO模型首次在PyTorch内部进行本地开发，从而在FP16和量化感知训练(QAT)上实现更快的训练。</p><p id="8750" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">YOLOv5的新发展带来了更快、更准确的GPU模型，但也增加了CPU部署的复杂性。复合缩放——同时改变网络的输入大小、深度和宽度——产生了小型的、受内存限制的网络，如YOLOv5s，以及更大的、受计算限制更多的网络，如YOLOv5l。此外，由于YOLOv5s的内存移动，后处理和焦点块花费了大量时间来执行，并降低了YOLOv5l的速度，尤其是在较大的输入大小时。因此，为了在CPU上实现YOLOv5型号的突破性性能，需要额外的ML和系统改进。</p><p id="33f4" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">直到今天，GPU和CPU之间的部署性能还是截然不同的。以YOLOv5l为例，在批处理大小为1且输入大小为640×640时，性能差距超过10倍:</p><ul class=""><li id="0a88" class="kc kd hi jf b jg ke jk kf jo kg js kh jw ki ka ma kk kl km bi translated">运行PyTorch的AWS上的一个T4 FP16 GPU实例实现了<strong class="jf hj"> 59.3项/秒</strong>。</li><li id="94b3" class="kc kd hi jf b jg kn jk ko jo kp js kq jw kr ka ma kk kl km bi translated">运行ONNX运行时的AWS上的一个24核C5 CPU实例达到了<strong class="jf hj"> 5.8项/秒</strong>。</li></ul><p id="2adf" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">好消息是CPU有惊人的能力和灵活性；我们只需要利用它来实现更好的性能。</p><p id="fc2d" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">为了展示不同的系统方法如何提高性能，我们用<a class="ae kb" href="https://docs.neuralmagic.com/deepsparse/" rel="noopener ugc nofollow" target="_blank"> DeepSparse引擎</a>替换了ONNX运行时。DeepSparse Engine拥有专有的改进，可以更好地将CPU硬件的优势融入YOLOv5模型架构。这些进步通过网络利用CPU上可用的大型缓存在深度方向上执行<a class="ae kb" href="https://neuralmagic.com/blog/how-neural-magics-deep-sparse-technology-works/" rel="noopener ugc nofollow" target="_blank">。使用我们在密集的FP32网络上使用ONNX运行时的相同24核设置，DeepSparse可以将基本性能提升到<strong class="jf hj"> 17.7项/秒</strong>，提高了3倍。这还不包括我们现在正在积极开发的新算法所能实现的额外性能提升。在接下来的几个版本中会有更多的消息— </a><a class="ae kb" href="https://neuralmagic.com/contact/" rel="noopener ugc nofollow" target="_blank">敬请期待</a>。</p><p id="2d47" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">DeepSparse引擎上的密集FP32结果是一个显著的改进，但它仍然比T4 GPU慢3倍以上。那么，我们如何缩小差距，在CPU上达到GPU级别的性能呢？由于网络现在很大程度上受计算限制，我们可以利用稀疏性来获得额外的性能改进。<a class="ae kb" href="https://docs.neuralmagic.com/sparseml/source/recipes.html" rel="noopener ugc nofollow" target="_blank">使用SparseML的配方驱动方法</a>进行模型稀疏化，加上大量的修剪深度学习网络的研究，我们成功地创建了高度稀疏和INT8量化的YOLOv5l和YOLOv5s模型。将稀疏量化的YOLOv5l模型插回到具有DeepSparse引擎的相同设置中，我们可以实现<strong class="jf hj"> 52.6项/秒</strong> —比ONNX运行时好9倍，并且几乎与最佳可用T4实现具有相同的性能水平。</p><figure class="kw kx ky kz fd la"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">视频1:在4核笔记本电脑上比较DeepSparse引擎和ONNX运行时的修剪-量化YOLOv5l。</figcaption></figure><h2 id="22cf" class="lm ig hi bd ih ln lo lp il lq lr ls ip jo lt lu it js lv lw ix jw lx ly jb lz bi translated">对数字的深入探究</h2><p id="a157" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">基准测试YOLOv5s和YOLOv5l有三种不同的变体:</p><ol class=""><li id="7d89" class="kc kd hi jf b jg ke jk kf jo kg js kh jw ki ka kj kk kl km bi translated">基线(密集FP32)；</li><li id="2643" class="kc kd hi jf b jg kn jk ko jo kp js kq jw kr ka kj kk kl km bi translated">修剪过的；</li><li id="b56f" class="kc kd hi jf b jg kn jk ko jo kp js kq jw kr ka kj kk kl km bi translated">修剪-量化(INT8)。</li></ol><p id="7959" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">在下面的表1中报告了所有这些模型在<a class="ae kb" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">可可</a>的验证集上的<a class="ae kb" href="https://towardsdatascience.com/intersection-over-union-iou-calculation-for-evaluating-an-image-segmentation-model-8b22e2e84686" rel="noopener" target="_blank"> IOU </a>为0.5时的mAP(值越高越好)。修剪和量化的另一个好处是它为部署创建了更小的文件。另外还测量了每个模型的压缩文件大小，也可以在表1中找到(值越低越好)。这些模型将在后面的章节中引用，并提供不同部署设置的完整基准。</p><p id="b71a" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">下面的基准测试数据是在<a class="ae kb" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank"> AWS </a>的现成服务器上运行的。基准测试和<a class="ae kb" href="https://github.com/neuralmagic/sparseml/tree/main/integrations/ultralytics-yolov5" rel="noopener ugc nofollow" target="_blank">创建模型</a>的代码<a class="ae kb" href="https://github.com/neuralmagic/deepsparse/tree/main/examples/ultralytics-yolo" rel="noopener ugc nofollow" target="_blank">分别在</a><a class="ae kb" href="https://github.com/neuralmagic/deepsparse" rel="noopener ugc nofollow" target="_blank">深度稀疏报告</a>和<a class="ae kb" href="https://github.com/neuralmagic/sparseml" rel="noopener ugc nofollow" target="_blank">稀疏报告</a>中开源。每个基准包括从预处理到模型执行再到后处理的端到端时间。为了生成每个系统的准确数字，运行了25次预热，报告了80次测量的平均值。结果以每秒项数(项数/秒)记录，数值越大越好。</p><p id="7de4" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">为每个使用案例选择的CPU服务器和核心数量确保了不同部署设置和定价之间的平衡。具体来说，使用<a class="ae kb" href="https://aws.amazon.com/ec2/instance-types/c5/" rel="noopener ugc nofollow" target="_blank"> AWS C5服务器</a>是因为它们是为计算密集型工作负载而设计的，并且包括<a class="ae kb" href="https://www.intel.com/content/dam/www/public/us/en/documents/product-overviews/dl-boost-product-overview.pdf" rel="noopener ugc nofollow" target="_blank"> AVX512和VNNI </a>指令集。由于CPU服务器的一般灵活性，核心的数量可以变化，以更好地适应确切的部署需求，使用户能够轻松平衡性能和成本。显而易见，CPU服务器更容易获得，模型可以部署在离最终用户更近的地方，节省了昂贵的网络时间。</p><p id="3b42" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">不幸的是，云中可用的通用GPU不支持使用非结构化稀疏性的加速。这是由于缺乏硬件和软件支持，是一个活跃的研究领域。在撰写本文时，新的<a class="ae kb" href="https://www.nvidia.com/en-us/data-center/a100/" rel="noopener ugc nofollow" target="_blank"> A100s </a>确实有对半结构化稀疏性的硬件支持，但还不容易获得。当支持变得可用时，我们将更新我们的基准，同时通过模型稀疏化继续发布准确、更便宜、更<a class="ae kb" href="https://thenextweb.com/neural/2021/03/16/solving-big-ais-big-energy-problem/" rel="noopener ugc nofollow" target="_blank">环保的</a>神经网络。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es md"><img src="../Images/bb349e5907c0d440ef0be24b0f97ccf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vj15VduCe6Fi_Xpv327DqQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><em class="ll">表1: YOLOv5模型稀疏化和验证结果</em>。</figcaption></figure><p id="0f12" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated"><strong class="jf hj">延迟性能</strong></p><p id="48c7" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">对于延迟测量，我们使用批量大小1来表示图像可以被检测和返回的最快时间。一个24核、单路AWS服务器用于测试CPU实现。下表2显示了测量值(以及图1的来源)。我们可以看到，将DeepSparse引擎与修剪和量化模型相结合，提高了下一个最佳CPU实现的性能。与PyTorch运行修剪量化模型相比，对于YOLOv5l和YOLOv5s，DeepSparse都要快6–7倍。与GPU相比，DeepSparse上修剪量化的YOLOv5l与T4匹配，DeepSparse上的YOLOv5s比V100快<strong class="jf hj">2.5倍</strong>，比T4快<strong class="jf hj">1.5倍</strong>。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es me"><img src="../Images/1852dfff53cfef64424792fe026298b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4mxnfkV-HKwCEYNu-HIJzg.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><em class="ll">表2:yolov 5</em>的延迟基准数(批量1)。</figcaption></figure><p id="0bd3" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated"><strong class="jf hj">吞吐量</strong> <strong class="jf hj">性能</strong></p><p id="73e8" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">对于吞吐量测量，我们使用批量大小64来表示吞吐量性能基准测试的正常、批量用例。此外，在我们的测试中，64的批处理大小足以使GPU和CPU的性能完全饱和。一个24核、单路AWS服务器也被用来测试CPU实现。下表3显示了测量值。我们可以看到V100的数字很难被击败；然而，修剪和量化结合深度稀疏打败了T4性能。这种组合还击败了下一个最好的CPU数量，YOLOv5l的CPU数量为<strong class="jf hj"> 16x </strong>，YOLOv5s的CPU数量为<strong class="jf hj"> 10x </strong>！</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es md"><img src="../Images/8ed61507917987a3af8198f63222748f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k5cV_yew2lRgCQtNZoKnzA.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><em class="ll">表3:yolov 5</em>的吞吐量性能基准数(批量64)。</figcaption></figure><h2 id="a356" class="lm ig hi bd ih ln lo lp il lq lr ls ip jo lt lu it js lv lw ix jw lx ly jb lz bi translated">用你自己的数据复制</h2><p id="aac5" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">虽然上述基准测试结果值得注意，但Neural Magic尚未看到许多在COCO数据集上训练的部署模型。此外，从私有云到多云设置，部署环境各不相同。下面，我们将介绍将稀疏模型转移到您自己的数据集以及在您自己的部署硬件上对模型进行基准测试时可以采用的其他资产和一般步骤。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/397e1e7eec483460fd3a1bd74d8b4869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WVD1_TRQLhbdeMo6.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><em class="ll">图2:yolov 5车型</em>的 <a class="ae kb" href="https://pjreddie.com/projects/pascal-voc-dataset-mirror/" rel="noopener ugc nofollow" target="_blank"> <em class="ll"> VOC数据集</em> </a> <em class="ll">上的转移学习结果。</em></figcaption></figure><p id="b80a" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated"><strong class="jf hj">稀疏迁移学习</strong></p><p id="a0c9" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">稀疏迁移学习研究仍在进行中；然而，在过去几年里，基于彩票假说的有趣结果已经发表。强调计算机视觉和自然语言处理结果的论文显示，稀疏迁移学习从<a class="ae kb" href="https://arxiv.org/pdf/2007.12223.pdf" rel="noopener ugc nofollow" target="_blank">到</a><a class="ae kb" href="https://arxiv.org/abs/1905.07785" rel="noopener ugc nofollow" target="_blank">在下游任务中从零开始修剪一样好，胜过密集迁移学习</a>。</p><p id="865f" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">本着同样的精神，我们已经发布了一个关于如何将稀疏YOLOv5模型中的学习转移到新数据集的教程。就像检查<a class="ae kb" href="https://github.com/neuralmagic/sparseml" rel="noopener ugc nofollow" target="_blank"> SparseML存储库</a>，运行<a class="ae kb" href="https://github.com/neuralmagic/sparseml/tree/main/integrations/ultralytics-yolov5" rel="noopener ugc nofollow" target="_blank"> SparseML和YOLOv5集成</a>的设置，然后使用您的数据启动命令行命令一样简单。该命令从SparseZoo 下载<a class="ae kb" href="https://sparsezoo.neuralmagic.com/?repo=ultralytics&amp;page=1&amp;order_by=created&amp;descending=true" rel="noopener ugc nofollow" target="_blank">预稀疏模型，并开始在数据集上训练。下面给出了从删减的量化YOLOv5l模型转移的示例:</a></p><pre class="kw kx ky kz fd mf mg mh mi aw mj bi"><span id="0c6a" class="lm ig hi mg b fi mk ml l mm mn">python train.py --data voc.yaml --cfg ../models/yolov5l.yaml --weights zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned_quant-aggressive_95?recipe_type=transfer --hyp data/hyp.finetune.yaml --recipe ../recipes/yolov5.transfer_learn_pruned_quantized.md</span></pre><p id="8f12" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated"><strong class="jf hj">标杆管理</strong></p><p id="4e90" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">为了在您自己的部署上重现我们的基准并检查DeepSparse的性能，在DeepSparse repo 中提供了代码作为<a class="ae kb" href="https://github.com/neuralmagic/deepsparse/tree/main/examples/ultralytics-yolo#benchmarking-example" rel="noopener ugc nofollow" target="_blank">示例。基准测试脚本支持使用DeepSparse、ONNX运行时(CPU)和PyTorch GPU的YOLOv5模型。</a></p><p id="3272" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">有关选项的完整列表，请运行:</p><pre class="kw kx ky kz fd mf mg mh mi aw mj bi"><span id="7193" class="lm ig hi mg b fi mk ml l mm mn">python benchmark.py --help.</span></pre><p id="e810" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">例如，要在支持VNNI的CPU上对DeepSparse的修剪量化YOLOv5l性能进行基准测试，请运行:</p><pre class="kw kx ky kz fd mf mg mh mi aw mj bi"><span id="dc0b" class="lm ig hi mg b fi mk ml l mm mn">python benchmark.py zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned_quant-aggressive_95 --batch-size 1 --quantized-inputs</span></pre><h2 id="387c" class="lm ig hi bd ih ln lo lp il lq lr ls ip jo lt lu it js lv lw ix jw lx ly jb lz bi translated">结论</h2><p id="4dc7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">DeepSparse引擎与SparseML的配方驱动方法相结合，为YOLOv5系列模型提供了GPU级的性能。与其他CPU推理引擎相比，YOLOv5l上的推理性能在延迟方面提高了6–7倍，在吞吐量方面提高了16倍。迁移学习教程和基准测试示例支持在您自己的数据集和部署上直接评估性能模型，因此您可以为自己的应用程序实现这些收益。</p><p id="b17c" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated">这些引人注目的胜利不止于YOLOv5。我们将通过更高的稀疏度、更好的高性能算法和尖端的多核编程开发，最大化稀疏化和CPU部署的可能性。这些进步的结果将被推送到我们的开源回购协议中，让所有人受益。请启动我们的<a class="ae kb" href="https://github.com/neuralmagic/deepsparse" rel="noopener ugc nofollow" target="_blank"> DeepSparse </a>和<a class="ae kb" href="https://github.com/neuralmagic/sparseml" rel="noopener ugc nofollow" target="_blank"> SparseML </a> GitHub repos来支持我们的努力。</p><p id="cdc4" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated"><strong class="jf hj">有疑问或需要支持？</strong>加入DeepSparse <a class="ae kb" href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ" rel="noopener ugc nofollow" target="_blank"> Slack </a>社区，与我们的产品和工程团队以及对模型稀疏化和加速深度学习推理性能感兴趣的更广泛的社区进行互动。</p></div><div class="ab cl mo mp gp mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hb hc hd he hf"><p id="5c06" class="pw-post-body-paragraph jd je hi jf b jg ke ji jj jk kf jm jn jo ks jq jr js kt ju jv jw ku jy jz ka hb bi translated"><em class="mv">原载于2021年8月6日https://neuralmagic.com</em><a class="ae kb" href="https://neuralmagic.com/blog/benchmark-yolov5-on-cpus-with-deepsparse/" rel="noopener ugc nofollow" target="_blank"><em class="mv"/></a><em class="mv">。</em></p></div></div>    
</body>
</html>