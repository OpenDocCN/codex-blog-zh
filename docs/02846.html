<html>
<head>
<title>The Heads Hypothesis: A Unifying Statistical Approach Towards Understanding Multi-Headed Attention in BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">头部假设:理解BERT中多头注意的统一统计方法</h1>
<blockquote>原文：<a href="https://medium.com/codex/the-heads-hypothesis-a-unifying-statistical-approach-towards-understanding-multi-headed-attention-15fd8b221637?source=collection_archive---------16-----------------------#2021-08-08">https://medium.com/codex/the-heads-hypothesis-a-unifying-statistical-approach-towards-understanding-multi-headed-attention-15fd8b221637?source=collection_archive---------16-----------------------#2021-08-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4e8e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在AAAI 21大会上发表的研究工作的高水平综述</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/ba41951e8e339f0c9669491c45550857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VosnASMMAJPBbXKsGSmxlQ.png"/></div></div></figure><h1 id="f9c1" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">目的</h1><p id="ba0c" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">BERT模型已经获得了巨大的欢迎，并且已经成为解决各种自然语言处理问题的默认选择，比如情感分类，问题回答，解释等等。它已经在多个NLP任务上展示了显著优于其任何前身模型的示范性性能。然而，它被批评为一个对其工作原理知之甚少的黑箱。BERT也是一个基于transformer的模型，该模型中有多个组件，下面描述了其中的两个主要组件(如果您是BERT的新手，可以考虑阅读这篇令人惊叹的<a class="ae kx" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">博客</a>，以了解基于transformer的模型的端到端架构)。为了理解这个博客，将理解抽象成这样就足够了:(1)BERT由多个内部成分组成，其中“自我关注”是非常重要的。(二)自我注意由多个注意头组成。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ky"><img src="../Images/e400e4b415b918345dbbf7b935e666af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JFAiDouLmKDqQyt2PJVmaw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">自我注意成分由多个注意头组成</figcaption></figure><p id="ec5a" class="pw-post-body-paragraph kb kc hi kd b ke ld ij kg kh le im kj kk lf km kn ko lg kq kr ks lh ku kv kw hb bi translated">因为有多个注意力头，所以很少有像“<em class="li">这样的问题，每个注意力头学习什么？</em>“起兵。我们的工作正是试图回答这个问题。我们试图确定BERT的不同注意头执行的各种功能角色，以及当我们进行微调时行为如何变化。</p><h1 id="85d5" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">与其他相关作品的区别</h1><p id="7bef" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">使我们的工作不同于其他当代方法的是我们提出的方法所需要的统计的严格性和形式主义。统计测试是许多科学领域研究的一个组成部分。然而，在自然语言处理模型的可解释性领域，大多数作品采用基于平均值的方法或者基于一组精选的句子得出结论。考虑到每个不同的输入句子都有很大的可变性，统计测试不仅仅是一个小时的需要。因此，我们从我们的方法中得出的推论是稳健的，并且是仔细得出的。</p><h1 id="e8a7" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">方法</h1><p id="cca0" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们考虑4个高级功能角色:<em class="li">局部、语法、块和分隔符。</em>我们建议采用三步程序来确定负责人的职能角色:</p><blockquote class="lj lk ll"><p id="71db" class="kb kc li kd b ke ld ij kg kh le im kj lm lf km kn ln lg kq kr lo lh ku kv kw hb bi translated">(I)我们首先定义一个用于分析注意力模式的模板，我们称之为“注意力筛子”。</p><p id="7cc0" class="kb kc li kd b ke ld ij kg kh le im kj lm lf km kn ln lg kq kr lo lh ku kv kw hb bi translated">(ii)我们计算所有注意头和所有输入序列的所有功能角色的“筛选偏差分数”的度量。</p><p id="6fad" class="kb kc li kd b ke ld ij kg kh le im kj lm lf km kn ln lg kq kr lo lh ku kv kw hb bi translated">(iii)我们应用假设检验，最终将职能角色分配给负责人。</p></blockquote><h2 id="ead8" class="lp jk hi bd jl lq lr ls jp lt lu lv jt kk lw lx jv ko ly lz jx ks ma mb jz mc bi translated">注意力筛子</h2><p id="e8d0" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">给定功能角色的令牌集被定义为当前令牌的注意力筛子。下面的数字会使它更清楚。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/be49dbbcb795b8b5799436a291442f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GMcqWBdb_r0IMNNqXWpByw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">为职能角色定义的各种注意力筛选。这些线指向框中单词的给定注意筛选中的单词。</figcaption></figure><h2 id="6d53" class="lp jk hi bd jl lq lr ls jp lt lu lv jt kk lw lx jv ko ly lz jx ks ma mb jz mc bi translated">筛偏分数</h2><p id="2562" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">通过输入序列的样本，我们为所有功能角色和所有注意力头计算这个度量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/fa9681d728d02e6750fce331673bd554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a7dJ4XyxCTR-yJ9pt7torg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">这表明在该示例中，给予该局部注意力筛子内的标记5倍以上(多于一致)的注意力</figcaption></figure><p id="ee1b" class="pw-post-body-paragraph kb kc hi kd b ke ld ij kg kh le im kj kk lf km kn ko lg kq kr ks lh ku kv kw hb bi translated">此指标量化了注意力筛选中的标记相对于其他标记的相对偏好。因此，在输入序列的大样本中，我们对每个功能角色、每个注意力头都有一个筛选偏差分数。</p><h2 id="826e" class="lp jk hi bd jl lq lr ls jp lt lu lv jt kk lw lx jv ko ly lz jx ks ma mb jz mc bi translated">假设检验</h2><p id="fc1f" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们需要汇总输入序列的筛选偏差分数。我们如何做到这一点？最简单的方法是对输入序列取平均值，这样我们就有一个“单一数字”来代表每个注意力头和每个功能角色。如果“单一数字”足够高，那么我们说这个注意力头可以被分配那个功能角色。然而，这种基于平均值的测量容易出错，因为筛偏分数的分布是不对称的。我们需要更准确的东西:假设检验。</p><p id="f163" class="pw-post-body-paragraph kb kc hi kd b ke ld ij kg kh le im kj kk lf km kn ko lg kq kr ks lh ku kv kw hb bi translated"><strong class="kd hj">备选假设:</strong>所有输入序列的筛选偏差得分的平均值<em class="li">大于或等于</em>某个阈值τ。</p><p id="dda5" class="pw-post-body-paragraph kb kc hi kd b ke ld ij kg kh le im kj kk lf km kn ko lg kq kr ks lh ku kv kw hb bi translated"><strong class="kd hj">零假设:</strong>对于某个阈值τ，所有输入序列的筛选偏差得分的平均值小于<em class="li">。</em></p><p id="8f93" class="pw-post-body-paragraph kb kc hi kd b ke ld ij kg kh le im kj kk lf km kn ko lg kq kr ks lh ku kv kw hb bi translated">为每个注意力头和每个功能角色设置零假设。只有当注意力头<strong class="kd hj">拒绝</strong>为<em class="li"> f </em>设置的无效假设时，我们才会将给定的功能角色<em class="li"> f </em>分配给注意力头。下图形象地描述了职能角色分配策略。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/1a984c32d9cfd32c4d35f322f86b55d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hBaMg-Y4q0ozjmyKAHHE0g.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">职能角色分配:表中的❌表示拒绝零假设，而✔表示接受零假设。</figcaption></figure><p id="1fb7" class="pw-post-body-paragraph kb kc hi kd b ke ld ij kg kh le im kj kk lf km kn ko lg kq kr ks lh ku kv kw hb bi translated">使用这个过程，我们给BERT的所有注意力头分配功能角色。接下来，我们讨论从作业中得出的结果和见解。</p><h1 id="ad02" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">结果和讨论</h1><p id="679f" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们讨论结果并回答三个主要的研究问题:(一)功能角色是互斥的吗？(ii)职能角色如何跨层分布？㈡微调对职能角色有什么影响？</p><h2 id="96fb" class="lp jk hi bd jl lq lr ls jp lt lu lv jt kk lw lx jv ko ly lz jx ks ma mb jz mc bi translated">实验装置</h2><p id="fd8c" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们在来自标准GLUE数据集的四个NLU任务上测试了我们的方法。任务是<em class="li">转述(MRPC和QQP) </em>，<em class="li">情感分类(SST-2) </em>，<em class="li">自然语言推理(QNLI) </em>。</p><h2 id="49c2" class="lp jk hi bd jl lq lr ls jp lt lu lv jt kk lw lx jv ko ly lz jx ks ma mb jz mc bi translated">1.职能角色是互斥的吗？</h2><p id="3d09" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">不，他们不是。单个注意头可以执行多种功能角色，例如，它可以是局部的也可以是句法的。下图显示了头部的集合(用圆圈表示),重叠部分表示多功能性的程度。</p><ul class=""><li id="7194" class="mg mh hi kd b ke ld kh le kk mi ko mj ks mk kw ml mm mn mo bi translated">有大量的定界符头(73%跨越4个粘合任务)</li><li id="0b03" class="mg mh hi kd b ke mp kh mq kk mr ko ms ks mt kw ml mm mn mo bi translated">局部标题和语法标题之间的重叠非常高，在所有任务中约为42%到89%。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/6c1d7796c1bd45b43792af3e5b9fe006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OGW4_T2YJbMf1BWdoMVh1A.png"/></div></div></figure><h2 id="b2a5" class="lp jk hi bd jl lq lr ls jp lt lu lv jt kk lw lx jv ko ly lz jx ks ma mb jz mc bi translated">2.职能角色是如何跨层分布的？</h2><p id="a337" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们将4个粗略的功能角色(局部、语法、块和分隔符)分成细粒度的角色。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/af5348dd4c135115ba016d459474477b.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*SglB_6rPSklBXYTHnPNRJA.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">句法角色进一步研究为<strong class="bd jl"> nsubj，dobj，amod，advmod </strong></figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/ab2713858b03e13d5fdeb2678630e09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*FjcDQCyCpIIl6Dfo1k9BNA.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">对于<strong class="bd jl"> CLS </strong>和<strong class="bd jl"> SEP </strong>令牌，分隔符的作用是分别研究的</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mx"><img src="../Images/3f9f768d402891057b005b55abe0395a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jd_WYNbd92Kgpf08QQtgLg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">分配给所有注意力头的功能角色。颜色表示各自的功能角色。</figcaption></figure><ul class=""><li id="c3b0" class="mg mh hi kd b ke ld kh le kk mi ko mj ks mk kw ml mm mn mo bi translated">MRPC、QNLI和SST-2的中间层(第5层至第9层)的多功能磁头数量较多。然而对于QQP来说，这些头更多的是在最初的层。</li><li id="7645" class="mg mh hi kd b ke mp kh mq kk mr ko ms ks mt kw ml mm mn mo bi translated">定界符头(注意SEP)在后面的层中。</li><li id="f96c" class="mg mh hi kd b ke mp kh mq kk mr ko ms ks mt kw ml mm mn mo bi translated">负责CLS令牌的头在初始层。</li><li id="8668" class="mg mh hi kd b ke mp kh mq kk mr ko ms ks mt kw ml mm mn mo bi translated">块头主要出现在第0层。它们在随后的层中的存在随着任务的不同而不同。</li><li id="f4b2" class="mg mh hi kd b ke mp kh mq kk mr ko ms ks mt kw ml mm mn mo bi translated">每层至少有3个句法头(除了极少数例外)。</li><li id="0174" class="mg mh hi kd b ke mp kh mq kk mr ko ms ks mt kw ml mm mn mo bi translated">最后，在这四项任务中，最后两层的多功能负责人很少。</li></ul><h2 id="5ca1" class="lp jk hi bd jl lq lr ls jp lt lu lv jt kk lw lx jv ko ly lz jx ks ma mb jz mc bi translated">3.微调对职能角色有什么影响？</h2><p id="1f7f" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">伯特经历了两个阶段的训练过程；首先是预训练阶段，其次是特定任务的微调。在本节中，我们将研究当我们微调BERT模型时，这些功能角色是如何变化的。</p><p id="e543" class="pw-post-body-paragraph kb kc hi kd b ke ld ij kg kh le im kj kk lf km kn ko lg kq kr ks lh ku kv kw hb bi translated">我们观察到，在微调过程中，功能角色分布在后面的层中。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es my"><img src="../Images/3f6be4e5160ad883374af05b9eaaa17f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SIYqi_WMz4GfBCEG4LOWMQ.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">作为微调一部分的功能行为变化</figcaption></figure><ul class=""><li id="3395" class="mg mh hi kd b ke ld kh le kk mi ko mj ks mk kw ml mm mn mo bi translated">作为微调的一部分，对SEP标记的关注减少，这被分配给输入序列的其他标记。</li><li id="b961" class="mg mh hi kd b ke mp kh mq kk mr ko ms ks mt kw ml mm mn mo bi translated">加强了Clark等人提出的假设，即[SEP]标记是无操作指示符，因此随着微调特殊化最终层，对这些标记的注意偏差正在减少。</li></ul><h1 id="7d3e" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">摘要</h1><p id="7272" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">这项工作提出了一种统计技术，以确定所扮演的角色的注意头伯特，其中所有不同的功能角色可以分析同一个统一的方法。这有助于阐明注意力头是如何工作的，并鼓励设计可解释的模型。</p><p id="d1fc" class="pw-post-body-paragraph kb kc hi kd b ke ld ij kg kh le im kj kk lf km kn ko lg kq kr ks lh ku kv kw hb bi translated">更多细节和细微差别，请考虑阅读我们的全文:<a class="ae kx" href="https://ojs.aaai.org/index.php/AAAI/article/view/17605" rel="noopener ugc nofollow" target="_blank">这里</a></p></div></div>    
</body>
</html>