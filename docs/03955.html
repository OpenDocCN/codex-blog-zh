<html>
<head>
<title>Image Compression with K-means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于K均值聚类的图像压缩</h1>
<blockquote>原文：<a href="https://medium.com/codex/image-compression-with-k-means-clustering-48e989055729?source=collection_archive---------2-----------------------#2021-10-11">https://medium.com/codex/image-compression-with-k-means-clustering-48e989055729?source=collection_archive---------2-----------------------#2021-10-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/78ba3fc6ca0207406f80d53d11e5ae70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_nMkWVGUC2w1pezC"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://www.vertica.com/blog/finding-the-k-in-k-means-clustering-with-a-udf/" rel="noopener ugc nofollow" target="_blank">https://www . vertica . com/blog/finding-the-k-in-k-means-clustering-with-a-UDF/</a></figcaption></figure><p id="5dfa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大家好。我们只在我以前的文章中看到过监督学习算法。但是今天，我们将进入我们的第一个无监督学习算法。在本帖中，我们将学习K-means，并从一个2D数据集的例子开始。接下来，我们将应用K-means算法，通过将图片中的颜色数量减少到该图片中常见的颜色来压缩图像。</p><p id="b130" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在无监督学习中，我们的算法将从未标记的数据中学习，而不是从标记的数据中学习。那么，什么是无监督学习呢？我在<a class="ae iu" rel="noopener" href="/codex/what-are-the-types-of-machine-learning-53360b7db8b4"> <strong class="ix hj"> <em class="jt">里简单讲过，机器学习的类型有哪些？</em> </strong> </a> <strong class="ix hj"> <em class="jt">。</em> </strong>但是对比监督学习还是很有用的。所以下面是监督学习的问题:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ju"><img src="../Images/ac1c38425a13e736126b3f1cd7e30c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jgz6pYeIFwHN79V84NIK1w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">监督学习</figcaption></figure><p id="f11f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的例子中，给定一组标签来拟合一个假设。相反，在无监督学习问题中，我们得到的数据没有任何与之相关的标签。所以我们得到了如下的数据集:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jz"><img src="../Images/4edc94d78922f65d4a5bf6c48f411afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xe8i4opOwhGyszd3nWWvwA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">无监督学习</figcaption></figure><p id="9477" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，数据集给定了一组点，但没有标签，因此我们的训练集只是写为x1，x2，…，xm，我们没有得到任何标签y。因此，在无监督学习中，我们只是要求算法在这些未标记的数据中找到一些结构或模式。上面画圈的叫做聚类。这是我们第一种无监督学习算法。</p><h2 id="5802" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">什么是K-means？</h2><p id="f045" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">K-means算法是一种将相似数据样本自动分组的技术。例如，假设您有一个训练集x1，…，xm，并希望将数据整理成几个单一的统一“集群”。</p><p id="1ea2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其使用的更具体的例子是:</p><ul class=""><li id="1a9b" class="la lb hi ix b iy iz jc jd jg lc jk ld jo le js lf lg lh li bi translated"><strong class="ix hj">市场细分:</strong>您可能有一个客户数据库，并希望将其归入不同的市场群体。因此，您可以独立地向他们销售，或者更好地服务于您的各种目标群体。</li><li id="7e55" class="la lb hi ix b iy lj jc lk jg ll jk lm jo ln js lf lg lh li bi translated"><strong class="ix hj">社交网络分析:</strong>像脸书、Instagram等东西。例如，关于你最经常给谁发电子邮件以及他们给谁发电子邮件最频繁的信息，以便发现一个有凝聚力的个人群体。另一种聚类算法，你想在社交网络中找到其他连贯的朋友群。</li><li id="ff9d" class="la lb hi ix b iy lj jc lk jg ll jk lm jo ln js lf lg lh li bi translated"><strong class="ix hj">组织数据中心:</strong>如果你知道数据中心的哪些计算机和哪个集群倾向于协同工作。您可以使用它来识别您的资源，以及您如何布局网络，如何设计您的数据中心和通信。</li><li id="f229" class="la lb hi ix b iy lj jc lk jg ll jk lm jo ln js lf lg lh li bi translated"><strong class="ix hj">探索星系:</strong>聚类算法了解星系形成，并利用它来了解如何理解天文数据。</li></ul><p id="e083" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们快速加载数据集:</p></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><pre class="jv jw jx jy fd lv lw lx ly aw lz bi"><span id="7e10" class="ka kb hi lw b fi ma mb l mc md">import numpy as np<br/>from scipy.io import loadmat<br/>import matplotlib.pyplot as plt<br/>import matplotlib.image as img</span><span id="69db" class="ka kb hi lw b fi me mb l mc md">mat = loadmat('ex7data2.mat')<br/>X = mat['X']</span></pre></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><h2 id="4991" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">k均值算法；</h2><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/319b08e9d02a34ba44e38077eb460cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_xr9A9EN_cKhzTikbgRFQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">k-均值算法</figcaption></figure><p id="718b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">蓝色分组的是簇分配步骤，红色分组的是移动质心步骤。</p><p id="753b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">聚类分配步骤:</strong>在这个步骤中，我们根据最接近的聚类质心将所有的训练样本涂成红色或蓝色。<br/> c⁽ⁱ⁾是范围从1到k的值，其指示训练样本是更接近红叉还是更接近蓝叉。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/9ae5792b5ab2fc6fb6a4252c78cfd321.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*PwcRU4rAEaCldgTofy3qtg.png"/></div></figure><p id="7ff3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的公式中，我们取训练样本x⁽ⁱ⁾，并测量x⁽ⁱ⁾和每个μₖ(质心)之间的距离。它会找到使x⁽ⁱ⁾和μₖ.之间的距离最小的k值请注意，这里使用的k是小写的k，它表示每个聚类的质心。大写的K用来表示质心的总数。</p><p id="edb4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看到下图可以获得直觉。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/ec34b9050ed3c3095bd40cff6a8ba9cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*Q5fhpe4N5cdnNJKllE2nFA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">集群分配步骤</figcaption></figure><p id="3e43" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">移动质心步骤:</strong>在该步骤中，对于从1到K的质心，我们平均分配给每个聚类质心K的点。对于更具体的例子，</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/53faf48745275037a5a094a39d8936db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uAxqt7EfTCq6lABZlUhjMg.png"/></div></div></figure><p id="7f16" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设，你取训练样本x⁽ ⁾、x⁽⁵⁾、x⁽⁸⁾、x⁽ ⁰⁾，并且它们被分配到第二聚类质心(即，k=2)，那么它被表示为c⁽ ⁾=2、c⁽⁵⁾=2、c⁽⁸⁾=2、c⁽ ⁰⁾=2(这里2表示聚类质心k=2，上标表示相应的训练示例)。在移动质心步骤中，我们执行以下操作:以第二个聚类质心为例，从上面我们可以看到4个样本(即，x⁽ ⁾、x⁽⁵⁾、x⁽⁸⁾、x⁽ ⁰⁾)被分配给它。通过取平均值，我们可以移动质心。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/24bcfba68d03b3dace007c2eae5f97bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*GEep1EH6tgzRInQL8vF7gQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">移动质心步长</figcaption></figure><p id="170b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过重复这两个步骤，我们可以将它们分组。下面的gif可以帮助你理解这个算法的迭代过程。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/f4149fdb632bbcd5a442e009c6d9400b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*UE_SIiaf-ThyVfK1"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://www.toptal.com/machine-learning/clustering-algorithms" rel="noopener ugc nofollow" target="_blank"> Topal </a></figcaption></figure><p id="118f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">随机初始化:</strong></p><ol class=""><li id="e91f" class="la lb hi ix b iy iz jc jd jg lc jk ld jo le js ml lg lh li bi translated">应该具有K &lt; m(即，质心的总数必须小于训练集中样本的总数)</li><li id="b8cb" class="la lb hi ix b iy lj jc lk jg ll jk lm jo ln js ml lg lh li bi translated">随机挑选K个训练例子。</li><li id="5709" class="la lb hi ix b iy lj jc lk jg ll jk lm jo ln js ml lg lh li bi translated">设定μ₁,….，μₖ等于这k个例子。我们需要在特定的训练例子上指定μₖ。</li></ol></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><p id="07ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随机初始化:</p><pre class="jv jw jx jy fd lv lw lx ly aw lz bi"><span id="d5d0" class="ka kb hi lw b fi ma mb l mc md"># Random Initialization</span><span id="26e7" class="ka kb hi lw b fi me mb l mc md">def kMeansInit(X, K):<br/>    <br/>    m, n = X.shape<br/>    centroids = np.zeros((K, n))<br/>    <br/>    for i in range(K):<br/>        centroids[i] = X[np.random.randint(0, m+1), :]<br/>    return centroids</span></pre><p id="638c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">集群分配步骤:</p><pre class="jv jw jx jy fd lv lw lx ly aw lz bi"><span id="348b" class="ka kb hi lw b fi ma mb l mc md"># 1. Cluster Assignment Step<br/># Finding closest centroid (i.e., c⁽ⁱ⁾)</span><span id="7f9a" class="ka kb hi lw b fi me mb l mc md">def findClosestCentroids(X, centroids):<br/>    K = centroids.shape[0]<br/>    m = X.shape[0]<br/>    <br/>    idx = np.zeros((m, 1))<br/>    temp = np.zeros((K, 1))<br/>    <br/>    for i in range(m):<br/>        for j in range(K):<br/>            temp[j] = np.sqrt(np.sum( (X[i, :] - centroids[j, :])**2 ))<br/>        idx[i] = np.argmin(temp)+1<br/>    return idx</span></pre><p id="8409" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">移动质心步长:</p><pre class="jv jw jx jy fd lv lw lx ly aw lz bi"><span id="ea28" class="ka kb hi lw b fi ma mb l mc md"># 2. Move Centroid Step<br/># Computing Centroids (i.e., averaging over assigned examples μₖ)</span><span id="90f0" class="ka kb hi lw b fi me mb l mc md">def computeCentroids(X, idx, K):<br/>    m, n = X.shape<br/>    centroids = np.zeros((K, n))<br/>    count = np.zeros((K, 1))<br/>    for i in range(m):<br/>        index = int((idx[i]-1)[0])<br/>        centroids[index, :] += X[i, :]<br/>        count[index] += 1<br/>    return centroids/count</span></pre><p id="3df6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">可视化:</p><pre class="jv jw jx jy fd lv lw lx ly aw lz bi"><span id="5616" class="ka kb hi lw b fi ma mb l mc md"># Visualizing k-means on each iteration</span><span id="5964" class="ka kb hi lw b fi me mb l mc md">def plotKmeans(X, idx, K, centroids, num_iters):<br/>    m, n = X.shape<br/>    fig, axis = plt.subplots(nrows=num_iters, ncols=1, figsize=(6, 36))<br/>    <br/>    for i in range(num_iters):<br/>        color = 'rgb'<br/>        for k in range(1, K+1):<br/>            grp = (idx==k).reshape(m, 1)<br/>            axis[i].scatter(X[grp[:, 0], 0], X[grp[:, 0], 1], c=color[k-1], s=15)<br/>        axis[i].scatter(centroids[:, 0], centroids[:, 1], s=120, marker="x", c="black", linewidth=3)<br/>        title = "Iteration Number: " + str(i)<br/>        axis[i].set_title(title)<br/>        centroids = computeCentroids(X, idx, K)<br/>        idx = findClosestCentroids(X, centroids)<br/>        plt.tight_layout()</span></pre><h2 id="e5af" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">图像压缩</h2><p id="fc25" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">加载我们的图像:</p><pre class="jv jw jx jy fd lv lw lx ly aw lz bi"><span id="b0db" class="ka kb hi lw b fi ma mb l mc md">mat2 = loadmat('bird_small.mat')<br/>A = mat2['A']</span></pre><p id="e110" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在给定图像上运行K-means:</p><pre class="jv jw jx jy fd lv lw lx ly aw lz bi"><span id="b20a" class="ka kb hi lw b fi ma mb l mc md">X2 = (A/255).reshape(128*128, 3)</span><span id="c1bf" class="ka kb hi lw b fi me mb l mc md">K2 = 16<br/>num_iters = 10</span><span id="6d83" class="ka kb hi lw b fi me mb l mc md">def runKMeans(X, initial_centroids, K, num_iters):<br/>    idx = findClosestCentroids(X, initial_centroids)<br/>    <br/>    for i in range(num_iters):<br/>        centroids = computeCentroids(X, idx, K)<br/>        idx = findClosestCentroids(X, centroids)<br/>    return centroids, idx<br/>initial_centroids = kMeansInit(X2, K2)<br/>centroids2, idx2 = runKMeans(X2, initial_centroids, K2, num_iters)</span></pre><p id="ec88" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">压缩和可视化:</p><pre class="jv jw jx jy fd lv lw lx ly aw lz bi"><span id="e52e" class="ka kb hi lw b fi ma mb l mc md">m2, n2 = X2.shape<br/>X2_recovered = X2.copy()<br/><br/>for i in range(1,K2+1):<br/>    X2_recovered[(idx2==i).ravel(),:] = centroids2[i-1]<br/>    <br/>X2_recovered = X2_recovered.reshape(128,128,3)<br/><br/>import matplotlib.image as mpimg<br/>fig, ax = plt.subplots(1,2)<br/>ax[0].imshow(X2.reshape(128,128,3))<br/>ax[1].imshow(X2_recovered)</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/4f8cf9b1c04e75f798e4b3b64c97a8b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*sieLbC5xMX8ef2pftn3cdQ.png"/></div></figure><p id="5ff5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面右边的图像被压缩成16种最主要的颜色。尝试更改K2的值。</p><h1 id="fba7" class="mn kb hi bd kc mo mp mq kg mr ms mt kk mu mv mw kn mx my mz kq na nb nc kt nd bi translated">结论</h1><p id="9120" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">今天，我们看到了K-means的内幕以及它实际上是如何工作的。然后用python的numpy，pandas和matplotlib从头开始创建。数据集和最终代码上传到github。尝试使用K2的值，您可以看到每个模块的输出。</p><p id="1f95" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里查看一下<a class="ae iu" href="https://github.com/jagajith23/Andrew-Ng-s-Machine-Learning-in-Python/tree/gh-pages/Unsupervised%20Machine%20Learning" rel="noopener ugc nofollow" target="_blank"> K的意思是</a>。</p><h1 id="7c98" class="mn kb hi bd kc mo mp mq kg mr ms mt kk mu mv mw kn mx my mz kq na nb nc kt nd bi translated">如果你喜欢这篇文章，那么看看我在这个系列中的其他文章</h1><h2 id="7b46" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">1.<a class="ae iu" rel="noopener" href="/@jagajith23/what-is-machine-learning-daeac9a2ceca">什么是机器学习？</a></h2><h2 id="a9fe" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">2.<a class="ae iu" rel="noopener" href="/codex/what-are-the-types-of-machine-learning-53360b7db8b4">机器学习有哪些类型？</a></h2><h2 id="72f6" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">3.<a class="ae iu" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6">一元线性回归</a></h2><h2 id="9c2a" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">4.<a class="ae iu" rel="noopener" href="/codex/linear-regression-on-multiple-variables-1893e4d940b1">多元线性回归</a></h2><h2 id="f394" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">5.<a class="ae iu" rel="noopener" href="/codex/logistic-regression-eee2fd028ffd">逻辑回归</a></h2><h2 id="a936" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">6.<a class="ae iu" rel="noopener" href="/@jagajith23/what-are-neural-networks-3a0965e2ebfb">什么是神经网络？</a></h2><h2 id="9244" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">7.<a class="ae iu" rel="noopener" href="/codex/digit-classifier-using-neural-networks-ad17749a8f00">使用神经网络的数字分类器</a></h2><h2 id="4c2d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">8.<a class="ae iu" rel="noopener" href="/@jagajith23/dimensionality-reduction-on-face-using-pca-e3fec3bb4cee">使用PCA对人脸进行降维</a></h2><h2 id="f170" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">9.<a class="ae iu" href="https://jagajith23.medium.com/detect-failing-servers-on-a-network-using-anomaly-detection-1c447bc8a46a" rel="noopener">使用异常检测来检测网络上的故障服务器</a></h2><h1 id="de7d" class="mn kb hi bd kc mo mp mq kg mr ms mt kk mu mv mw kn mx my mz kq na nb nc kt nd bi translated">最后做的事</h1><p id="1d17" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">如果你喜欢我的文章，请鼓掌👏一个追随者会是🤘非常感谢🤘而且有助于媒体推广这篇文章，让其他人也能阅读。<em class="jt">我是Jagajith，下一集再来抓你。</em></p></div></div>    
</body>
</html>