# ICF，TF-IDF 缺失的成分

> 原文：<https://medium.com/codex/icf-the-missing-ingredient-of-tf-idf-d9f715c9946f?source=collection_archive---------13----------------------->

## 针对文本分类任务进行优化

![](img/563f4eef50d35b295c4d3765aa35ea40.png)

图片来自[自由影像](https://freeimages.com)

许多人都知道 [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) 是一个基本的自然语言处理工具，通常用于简单的文本分类任务，如垃圾邮件过滤。没有多少人知道普通的 TF-IDF 不是为这种应用而优化的，并且通过应用简单的算法改变，TF-IDF 可以被显著地改进。也就是说，考虑改用 TF-ICF。

# TF-IDF，根据文档重要性对术语进行排序

TF-IDF 起源于文本搜索/索引领域，是一种在给定文档语料库的情况下评估术语对文档重要性的方法。首先，确定每个文档的关键术语，然后，给定一个搜索查询，根据查询术语 TF-IDF 得分对最相关的文档进行排序。TF-IDF 公式相当直观，是两个因素的产物:

*   TF —术语频率，术语在文档中出现的次数。一个术语越常见，它就越重要。但是如果这个术语在其他文档中也很常见呢？。以单词“the”为例，它可能在许多语料库文档中非常常见。这样的单词对于区分文档和其他文档是没有用的。TF 应该与其他文档的外观标准化，以使其成为术语对文档(在语料库中)重要性的真实指标。
*   IDF —逆文档频率，日志(文档数/包含该术语的文档数)。超级常见的术语将得到 log(1) = 0。词条越稀有，语料库中的文档越多，日志得到的响应就越高。

使用 IDF，我们可以将 TF 得分标准化；将 TF 乘以 IDF，我们考虑在给定其他文档的术语的情况下，一个术语对一个文档有多重要。一个术语在文档中的常见程度乘以该术语在语料库中的独特程度。对于分类任务，TF-IDF 通常与单词包一起出现。

# BOW —单词包，对文本进行分类

为了对文档进行分类，我们首先需要将其规范化为分类模型能够处理的格式。BOW 是一种向量化文本的技术；特征是语料库术语(单词)的子集，值是术语 TF，1/0(以二进制模式)或 TF-IDF 分数。

“词汇袋”因为缺乏语境；不知道**在哪里**每个单词出现，但**是否**它出现。为了选择要考虑的 X 个术语(从 N 个语料库可能的单词中),简单的方法将是在一些文本处理步骤(去除停用词、词干等)之后采取 k 个最常见的..).不太简单的方法使用诸如 TF-IDF 的试探法，假设 TF-IDF 突出语料库中最重要的术语。[这里的](/xeneta/boosting-sales-with-machine-learning-fbcf2e618be3#.33uhr93sw)就是这样一个使用 TF-IDF 和 BOW 的文本分类管道的例子。

# TF — ICF，以类而不是文档为目标

如前所述，TF-IDF 最初的动机之一是支持语料库的文档搜索；对于每个文档，在语料库中找到最能代表它的关键词。这就是为什么 TF-IDF 更喜欢使用**罕见术语**(假设它们可以更好地将文档与其他术语区分开来)而不是常见术语。问题是文本分类，**不总是最稀有的记号是最重要的记号**。例如，考虑一个二进制分类任务，其中一个术语出现在每个正面文档中，而在负面文档中则完全不出现。从概念上讲，这样一个术语对于文档分类非常有用，因此我们假设 TF-IDF 将其排在较高的位置。但是由于这个术语非常常见(出现在一半的语料库文档中)，TF-IDF 将降低其排名，并且很可能倾向于其他术语。内在的原因是 TF-IDF 没有考虑 documents **类**，因此(简单的)修复方法是让它知道这些类。王和张(2010，[此处](https://arxiv.org/abs/1012.2609))介绍了一种解决该问题的优雅方案——TF-ICF:

*   **TCF-ICF** —将每个类别的文档合并成一个主文档，然后计算 TF-IDF 分数。TCF-ICF；每个类别的术语计数*每个类别的术语重要性(关于其在其他类别的分布)。使 TF-IDF 考虑到该术语对于分类的重要性。但是要使它真正有用，我们需要重新添加每个文档的重要性；例如，考虑具有两个类的分类任务，每个类 100 个文档，以及在类合并的主文档中最常见但同时仅在少数文档中出现的术语。当 TCF 查看合并文档统计数据时，它会倾向于该术语，从而获得较高的 TCF-ICF 得分。问题是从类的角度来看，这样的术语没有用(与类的其他文档无关)。
*   **TF**-**ICF**——解决方案将是用 TF 替换 TCF，又名 TF-ICF；考虑每个文档术语频率(TF ),同时支持类别唯一术语(ICF)。使用 TF-ICF，我们支持文档和类中最重要的术语，使分数对分类更有用。

## 慎用，并考虑合奏

尽管 TF-ICF 似乎是文本分类任务中最直观的 TF-IDF，但在某些情况下，最好的解决方案是也考虑其他版本，使用这些方法的集合。主要原因是大多数数据集并不像我们在例子中描述的那样尖锐地分布，在许多情况下，我们可以从混合不同的方法中受益；TF-ICF 将突出与大部分类别文档相关的术语。在某些情况下，拥有一个特征来区分甚至是类的一部分，对于一个模型来说示例可能是有用的。TCF-ICF 将强调这一点。类似地，在某些情况下，TF-IDF 发现文档中的重要术语(相对于 TF-ICF 类术语)对分类模型很有用。底线是我们应该选择最合适的版本，同时也要记住其他选项。

并让最优秀的 TF-IDF 胜出；)