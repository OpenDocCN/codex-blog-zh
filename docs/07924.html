<html>
<head>
<title>How to Build a LinkedIn Scraper in Python [No Headless Browser Needed]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Python构建LinkedIn Scraper【不需要无头浏览器】</h1>
<blockquote>原文：<a href="https://medium.com/codex/how-to-build-a-linkedin-scraper-in-python-no-headless-browser-needed-2398b7c67645?source=collection_archive---------9-----------------------#2022-07-05">https://medium.com/codex/how-to-build-a-linkedin-scraper-in-python-no-headless-browser-needed-2398b7c67645?source=collection_archive---------9-----------------------#2022-07-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/70fade06aa562e77152fd689b0725377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Pv5zdxXVNCY7wa0AT5tkQ.jpeg"/></div></div></figure><p id="0fac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在ScraperAPI上找到的关于<a class="ae jo" href="https://www.scraperapi.com/blog/linkedin-scraper-python/" rel="noopener ugc nofollow" target="_blank"> Linkedin Scraper与Python </a>的原创文章。</p><p id="745c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">LinkedIn是一个巨大的数据来源，对用户和非用户都是公开的，而且在我写这篇文章的时候，抓取数据是合法的。然而，就像在<a class="ae jo" href="https://jeremiahtang.medium.com/scraping-linkedin-in-2021-is-it-legal-f6aafc93ba41" rel="noopener"> 2019 LinkedIn诉HiQ案</a>中显示的那样，这并不意味着LinkedIn对此感到舒服。</p><p id="f7ee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在本文中，我们将向您展示如何构建一个不违反任何隐私政策或不需要无头浏览器来访问登录墙后的任何数据的web scraper这虽然不违法，但可能被视为不道德。</p><p id="a304" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">相反，我们将使用Requests和Beautiful Soup提取职位名称、公司招聘、位置和到职位列表的链接，并将数据导出到CSV文件，供以后分析或使用。</p><p id="fb05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果你对JavaScript更精通，我们有一个关于<a class="ae jo" href="https://www.scraperapi.com/uncategorized/how-to-build-a-linkedin-scraper/%5C" rel="noopener ugc nofollow" target="_blank">使用Node.js和Cheerio </a>构建LinkedIn scraper的教程，你可以查看一下。</p><h1 id="205e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">1.设置我们的项目</h1><p id="81f2" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我们将从安装我们将用于这个项目的所有依赖项开始。假设您已经安装了Python 3，打开vs code——或者您最喜欢的文本编辑器——并打开一个新的终端窗口。在那里，使用以下命令安装库:</p><ul class=""><li id="4417" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn kx ky kz la bi translated">请求:<code class="du lb lc ld le b">pip3 install requests</code></li><li id="ee0e" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">美汤:<code class="du lb lc ld le b">pip3 install beautifulsoup4</code></li><li id="e2e3" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">CSV: Python附带了一个随时可用的CSV模块</li></ul><p id="077a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">安装完依赖项后，让我们创建一个新文件，命名为<code class="du lb lc ld le b">linkedin_python.py</code>，并在顶部导入库:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="de53" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们的文件已经准备好了，让我们先来探索我们的目标网站。从InPrivate浏览器窗口(在Chrome中匿名)导航到位于<a class="ae jo" href="https://www.linkedin.com/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/</a>的主页，然后点击页面顶部的<em class="lq">工作</em>。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/82c825822a45e33dfe7ab62c6d9724c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iTkJXkhAbTtVz0v4.png"/></div></div></figure><p id="e36c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它会将我们直接发送到职务搜索结果页面，在那里我们可以创建新的搜索。对于这个例子，假设我们试图在旧金山建立一个产品管理工作列表。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/ce2d2a23d5a6a583a04822691ff1a0fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AMFcsdjthUEYRT2c.png"/></div></div></figure><p id="feae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">乍一看，似乎每个作业数据都在一个类似卡片的容器中，果然，在检查页面(右键单击&gt; inspect)后，我们可以看到每个作业结果都被包装在一个<code class="du lb lc ld le b">&lt;ul&gt;</code>元素中的<code class="du lb lc ld le b">&lt;li&gt;</code>标记之间。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/1cd45fa2d5844402a16fddfdc56eb3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2mq1qkvKlub1AFh7.png"/></div></div></figure><p id="d077" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，第一种方法是获取<code class="du lb lc ld le b">&lt;ul&gt;</code>元素并遍历其中的每个<code class="du lb lc ld le b">&lt;li&gt;</code>标签，以提取我们正在寻找的数据。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/75ae0e44a5a05b594cc1629cec02d4e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NyIlLbi4JuTdg9fP.png"/></div></div></figure><p id="49f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但有一个问题:为了访问新的工作，LinkedIn使用无限滚动分页，这意味着没有“下一页”按钮来获取下一页URL链接，URL本身也不会改变。</p><p id="0399" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种情况下，我们可以使用类似于<a class="ae jo" href="https://www.selenium.dev/" rel="noopener ugc nofollow" target="_blank"> Selenium </a>的无头浏览器来访问站点，提取数据，然后向下滚动以显示新数据。</p><p id="7d7f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当然，正如我们之前所说的，我们不会那样做。相反，让我们通过使用DevTools中的<em class="lq">网络选项卡</em>来智胜网站。</p><h1 id="b96b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">3.使用DevTool的网络选项卡</h1><p id="cc2e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">打开DevTools后，我们可以从窗口顶部的下拉菜单中导航到<a class="ae jo" href="https://developer.chrome.com/docs/devtools/network/" rel="noopener ugc nofollow" target="_blank"> <em class="lq">网络选项卡</em> </a>。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/2f528af086f5fe7b6f4fb550ae9470f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DwKDN12tlkRLJtTn.jpg"/></div></div></figure><p id="221d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要填充报告，只需重新加载页面，您将能够看到浏览器正在运行的所有获取请求，以便在页面上呈现数据。滚动到底部后，浏览器向下面截图中的URL发送新的请求。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/e426692e2337f7fec9843581c4366b61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NXmhjfNeM90k5PJi.png"/></div></div></figure><p id="c2d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们在浏览器中试试这个新的URL，看看它会把我们带到哪里:</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/9ea58a4e1eb02f271c50027faa6a43b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Tjf3nJFBnTdbZUMU.png"/></div></div></figure><p id="fefb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">完美！这一页有我们想要的所有信息。一个额外的发现是，这个URL有一个结构，我们实际上可以很容易地操纵它。只需更改start参数中的值，我们就可以访问新数据。</p><p id="3edd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了测试这一点，让我们将值改为0 —这是有机URL的起始值:</p><p id="d180" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://www.linkedin.com/jobs/search?keywords=Product%20Management&amp;location=San%20Francisco%20Bay%20Area&amp;geoId=90000084&amp;trk=public_jobs_jobs-search-bar_search-submit&amp;position=1&amp;pageNum=0" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/jobs/search?keywords = Product % 20 management&amp;location = San % 20 Francisco % 20 bay % 20 area&amp;geoId = 90000084&amp;trk = public _ jobs _ jobs-search-bar _ search-submit&amp;position = 1&amp;pageNum = 0</a></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/b9cf6c1522a0d48c8ebdf537ad16f910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zM1EL2y9t7jkoH_E.jpg"/></div></div></figure><p id="97c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">是的，这的确奏效了。已确认，因为每页的第一个作业是相同的。</p><p id="02d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">实验对于网络抓取至关重要，因此在接受这个解决方案之前，我们尝试了以下几件事:</p><ul class=""><li id="4a6e" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn kx ky kz la bi translated">改变<code class="du lb lc ld le b">pageNum</code>参数不会改变页面上的任何东西。</li><li id="8c9b" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">对于每个新的URL，<code class="du lb lc ld le b">start</code>参数增加25。我们通过向下滚动页面并比较站点本身发送的获取请求发现了这一点。</li><li id="c2b3" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">将<code class="du lb lc ld le b">start</code>参数改变1(因此start=2，start=3，依此类推)将会通过隐藏之前的工作列表来改变结果页面——这不是我们想要的。</li><li id="c8ed" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">当前最后一页是start=975。打到1000就转到404页了。</li></ul><p id="ef53" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有了初始URL，我们可以进入下一步。</p><h1 id="148f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">4.使用请求和漂亮的汤解析LinkedIn</h1><p id="5ab3" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在Python中，发送请求并解析返回的响应非常简单。首先，让我们创建一个包含初始URL的变量，并将其传递给requests.get()方法。然后，我们将返回的HTML存储在一个名为“response”的变量中，以创建我们的Python对象。为了测试，让我们打印响应:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/1e5bdb49b39d9cfff7ac170703b159cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OxBSgLVeb8x4Faor.png"/></div></div></figure><p id="c9f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">太棒了。200状态代码表示HTTP请求成功。</p><p id="c39c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们开始提取任何数据之前，我们需要解析原始的HTML数据，以便更容易使用CSS选择器进行导航。为此，我们只需要创建一个新的漂亮的Soup对象，将response.content作为第一个参数，将我们的解析器方法作为第二个参数:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="44d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为测试应该是我们开发过程的一部分，所以让我们使用我们的新soup对象做一个简单的实验，选择并打印页面上的第一个职位，我们已经知道它包含在带有<code class="du lb lc ld le b">class base-search-card__title</code>的<h3>标记中。</h3></p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="82c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">soup.find会完全按照它所说的去做，它会在我们漂亮的soup对象中找到一个元素，这个元素与我们指定的参数相匹配。通过添加。方法，它将只返回元素内部的文本，而不返回围绕它的整个HTML。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/b76ab6e2e6c52a272c0ec19015255f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*swnT83WiX3clfjyP.png"/></div></div></figure><p id="79cc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要删除文本周围的所有空白，我们需要做的就是添加。strip()方法放在字符串的末尾。</p><h1 id="7eb3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">5.使用条件处理多个页面</h1><p id="b536" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">这是事情变得有点棘手的地方，但我们已经完成了最困难的部分:弄清楚如何浏览页面。简单地说，我们需要做的就是创建一些逻辑来改变URL中的start参数。</p><p id="36bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在之前的一篇文章中，我们谈到了<a class="ae jo" href="https://www.scraperapi.com/blog/how-to-deal-with-pagination-in-python-step-by-step-guide-full-code/" rel="noopener ugc nofollow" target="_blank">在Scrapy </a>中抓取分页页面，但是对于Beautiful Soup，我们将做一些不同的事情。</p><p id="4ee0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们将定义一个包含我们全部代码的新函数，并将网页和页码作为参数传递——将使用这两个参数来构建用于发送HTTP请求的URL。</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="7b8c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在变量next_page中，我们组合了两个参数——其中,<code class="du lb lc ld le b">webpage</code>是一个字符串，而,<code class="du lb lc ld le b">page_number</code>是一个数字，需要转换成一个字符串——然后将其传递给发送结果URL的请求。</p><p id="aaf2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了使下一步有意义，我们需要了解我们的铲运机将:</p><ul class=""><li id="4a7f" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn kx ky kz la bi translated">创建新的URL</li><li id="5a85" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">发送HTTP请求</li><li id="4368" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">解析响应</li><li id="9926" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">提取数据</li><li id="aa15" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">将其发送到CSV文件</li><li id="7f58" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">增加开始参数</li><li id="fad0" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">重复直到它断裂</li></ul><p id="933c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了增加循环中的开始参数，我们将创建一个If条件:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="a43e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们在这里说的是，只要page_number不高于25(所以如果它是26或更高，它将中断)，page_number将增加25，并将新的数字传递给我们的函数。</p><p id="a5cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为什么你问25？因为在全押之前，我们想确保我们的逻辑在一个简单的测试中有效。</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="94bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将打印响应状态代码和page_number，以验证我们正在访问这两个页面。要运行我们的代码，我们需要用相关的参数调用我们的函数。</p><p id="c8eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>注意，在参数中，我们将开始参数与其值分开。我们需要它的值变成一个数字，这样我们就可以在If语句中增加它。</p><p id="9cca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还为创建的新URL添加了一个print()语句，只是为了验证一切都正常工作。</p><h1 id="446e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">6.测试我们的选择器</h1><p id="9829" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我们已经找到了将用于解析器的元素和类。然而，在脚本之外测试它们以避免对服务器不必要的请求总是很重要的。</p><p id="05f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">就在DevTools的控制台内，我们可以使用<code class="du lb lc ld le b">document.querySelectorAll()</code>方法从浏览器测试每个CSS选择器。以下是一个职位名称的例子:</p><p id="45a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它返回一个25的节点列表，这与页面上的作业数量相匹配。我们可以对其他目标做同样的事情。以下是我们的目标:</p><ul class=""><li id="896c" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn kx ky kz la bi translated">职务:' h3 '，class _ = ' base-search-card _ _ title '</li><li id="9b22" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">公司:' h4 '，class _ = ' base-search-card _ _ subtitle '</li><li id="d428" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">位置:' span '，class _ = ' job-search-card _ _ location '</li><li id="859b" class="ks kt hi is b it lf ix lg jb lh jf li jj lj jn kx ky kz la bi translated">URL: 'a '，class_='base-card__full-link '</li></ul><p id="be62" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意我们改变了语法来匹配<a class="ae jo" href="https://www.w3schools.com/python/ref_string_find.asp" rel="noopener ugc nofollow" target="_blank">。</a>寻找()【方法】。如果您想继续使用相同的类似JQuery的选择器，可以使用<a class="ae jo" href="https://www.projectpro.io/recipes/select-function-beautiful-soup#:~:text=Beautiful%20Soup%20provides%20the%20.,the%20data%20from%20web%20pages." rel="noopener ugc nofollow" target="_blank">。选择()</a>功能代替。</p><h1 id="e3ec" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">7.提取LinkedIn工作数据</h1><p id="2938" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">提取数据非常简单，只需选择包装数据的所有父元素，然后遍历它们来提取我们想要的信息。</p><p id="75c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在每个<li>元素中，都有一个div，它包含一个我们可以作为目标的类:</li></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/e29371ac0634f493924b3efaa94f4661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DxIGPrIiLSPGWO9p.png"/></div></div></figure><p id="5672" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了访问里面的数据，让我们创建一个新变量来选择所有这些<div>。</div></p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="0e75" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们有了一个<div>列表，我们可以使用我们选择的CSS选择器创建循环的<a class="ae jo" href="https://www.w3schools.com/python/python_for_loops.asp" rel="noopener ugc nofollow" target="_blank">:</a></div></p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="3201" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果你觉得我们进展太快，我们推荐你阅读我们的<a class="ae jo" href="https://www.scraperapi.com/blog/web-scraping-python/" rel="noopener ugc nofollow" target="_blank"> python网页抓取初学者教程</a>。它更详细地介绍了这一过程。</p><h1 id="118c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">9.将提取的数据发送到CSV文件</h1><p id="6a12" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在我们的main函数之外，我们将打开一个新文件，创建一个新的writer，并告诉它使用。writerow()方法:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="9b5a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当打开一个你想要不断添加新行的文件时，你需要在<em class="lq">追加模式</em>下打开它，这就是为什么a作为open()函数的第二个参数。</p><p id="97e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们将使用从解析器提取的数据添加一个新行。我们只需要在for循环的末尾添加以下代码片段:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="9180" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在作业列表的每次迭代结束时，我们的scraper会将所有数据追加到一个新行中。</p><p id="9328" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>确保我们添加新数据的顺序与我们的标题顺序相同是很重要的。</p><p id="09de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了完成这一步，让我们添加一个else语句，以便在循环中断时关闭文件。</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><h1 id="0927" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">10.使用ScraperAPI避免阻塞</h1><p id="4a0a" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我们的最后一步是可选的，但从长远来看，可以节省您的工作时间。毕竟，我们并不打算只刮一两页纸。要扩展您的项目，您需要处理IP轮换，管理代理池，处理验证码，并发送正确的标题，以避免被阻止甚至终身禁止。</p><p id="1083" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，只需对我们现在使用的基本URL进行一些更改，ScraperAPI就可以处理这些和更多的挑战，并且<a class="ae jo" href="https://www.scraperapi.com/signup" rel="noopener ugc nofollow" target="_blank">创建一个新的免费ScraperAPI帐户</a>来访问我们的API密钥。</p><p id="5df9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从那以后，我们唯一要做的就是在URL的开头添加这个字符串:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="2bf8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导致以下函数调用:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="91a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这样，我们的HTTP请求将由ScraperAPI的服务器处理。它会在每次请求后轮换我们的IP，并根据多年的统计分析和机器学习选择正确的标题。</p><p id="813b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，ScraperAPI还特别选择了超高级代理来处理LinkedIn这样的硬网站。使用它们就像在我们的请求中添加参数<code class="du lb lc ld le b">ultra_premium=true</code>一样简单。</p><h1 id="3473" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">总结:完整代码</h1><p id="84e1" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">如果您已经了解了，那么您的代码库应该是这样的:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="lo lp l"/></div></figure><p id="e8e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们添加了一些<code class="du lb lc ld le b">print()</code>语句作为视觉反馈。运行代码后，脚本将使用抓取的数据创建一个CSV文件:</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/d94b85e43fb826e9b47db94fcd16c40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y6TP19Cqzq2wdlxs.jpg"/></div></div></figure><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/f469c2ad71792af3065bd920877f3fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bG5Ap4k1LSloAMo-.jpg"/></div></div></figure><p id="9464" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，您可以增加<code class="du lb lc ld le b">if</code>条件中的限制来抓取更多的页面，处理<code class="du lb lc ld le b">keywords</code>参数来遍历不同的查询，和/或更改<code class="du lb lc ld le b">location</code>参数来抓取不同城市和国家的同一职位。</p><p id="9b30" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请记住，如果项目不仅仅是几个页面，使用ScraperAPI的集成将帮助您避免路障，并使您的IP免受反抓取系统的攻击。</p><p id="bb75" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下次见，刮刮快乐！</p></div></div>    
</body>
</html>