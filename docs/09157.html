<html>
<head>
<title>Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策图表</h1>
<blockquote>原文：<a href="https://medium.com/codex/decision-tree-d77d4bf8b284?source=collection_archive---------18-----------------------#2022-09-28">https://medium.com/codex/decision-tree-d77d4bf8b284?source=collection_archive---------18-----------------------#2022-09-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ff43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是一种监督学习算法，可用于回归和分类。</p><p id="0b6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是一种监督预测模型，可以学习预测回答一组简单的问题。它回答了一系列的问题，这些问题把我们带到了树的某一条路线上。决策规则通常采用if-then-else语句的形式。树越深，规则就越复杂，模型就越合适。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/51144c3e2ebf8d3f54064bf7de0da5a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/0*SqZB4dXXZeDl-FVy"/></div></figure><p id="20ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策树是如何工作的？</strong></p><p id="237b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树使用不同的属性重复地将数据分成子集，并重复该过程，直到子集是纯的。纯意味着它们都具有相同的目标值。</p><p id="1fd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割是如何进行的，或者树是如何决定在哪个变量上进行分割的？</p><p id="843e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入变量可以是数字的或分类的，或者是数字和分类的混合，而不管目标变量是数字的(即回归问题)还是目标变量是分类的(即分类问题)。</p><p id="02cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在分类输入变量的情况下，我们必须找出一个变量来分割数据集。为了计算这个变量，我们需要使用不同的变量分割数据集，并选择最适合下面提到的指标的变量。</p><p id="3dbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于数值输入变量，我们必须找出一个变量和值的组合来分割数据集。为了计算这个变量和值的组合，我们需要使用不同的变量和这些不同变量的不同值来分割数据集，并选择最适合下面提到的指标的变量和值的组合。</p><p id="2ed0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回归</p><p id="f7b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-标准偏差的减少</p><p id="545e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分类</p><p id="8c7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-信息增益</p><p id="126a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-基尼杂质</p><p id="eb67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">标准差减少</strong></p><p id="b0d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标准偏差减少是一种当目标变量连续时用于分裂节点的方法。我们使用标准差来计算一个数字样本的同质性。如果数字样本是完全同质的，其标准偏差为零。</p><p id="7094" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分别计算一个属性和两个属性的标准差和变异系数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jl"><img src="../Images/e2f95b2a0113c1e9b92b0d2c8e73f8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jLt6DqCjX9dp72gF"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jq"><img src="../Images/bc0cc83ca0cc9d1bf625b11c43d1098e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/0*jBGvg_LiCqEOenYP"/></div></figure><p id="cc42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是如何工作的</p><p id="6ef0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤1:计算目标的标准偏差</p><p id="abe2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤2:计算每个拆分的标准偏差，作为子节点的加权平均标准偏差</p><p id="a4c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤3:选择要分割的分支，该分支给出标准偏差的最大减少</p><p id="b846" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第四步:在实践中，我们需要一些终止标准。检查分支的变化系数是否小于阈值和/或当分支中剩余的实例太少(n)时(例如3)，然后终止分裂，并且相关的叶节点获得目标变量的平均值。</p><p id="5e0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤5:重复上述步骤，直到所有数据都处理完毕。</p><p id="e2d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例子</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jr"><img src="../Images/1f1bd6238b6e26511eca94907a8fa192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7tjccrQovb15_EKi"/></div></div></figure><p id="c038" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一步:计算目标的标准偏差。</p><p id="e5c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标准偏差(比赛时间)= 9.32</p><p id="99ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第2步:然后数据集被分割成不同的属性。计算每个分支的标准偏差。从分割前的标准差中减去得到的标准差。结果是标准偏差减少</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/8ea2c9646529c2dfdf09e676832eb896.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/0*3A9TGJjV5kDF7A9k"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jt"><img src="../Images/5601c7315b6fd04669671d0dd3f5f82c.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*iR4ak5V4BxOBd9bc"/></div></figure><p id="f11d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤3:选择具有最大标准偏差减少的属性作为决策节点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ju"><img src="../Images/e1b7ce187dfa9b8a08d0944a9443353f.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/0*9Kx3KU_vUjJz36vE"/></div></figure><p id="939f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤4a:基于所选属性的值划分数据集。这个过程在非叶分支上递归运行，直到所有数据都被处理。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jv"><img src="../Images/149f49eb689cd9a655302c1ff1acea7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/0*SPqX-1ZOnfmuMal2"/></div></figure><p id="d8ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实际上，我们需要一些终止标准。例如，当分支的偏差系数(CV)变得小于某个阈值(例如10%)时和/或当分支中剩余的实例(n)太少时(例如3)。</p><p id="9a4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤4b:“阴”子集不需要任何进一步的分裂，因为其CV (8%)小于阈值(10%)。相关的叶节点获得“阴天”子集的平均值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jw"><img src="../Images/12bb336d3822b2bc97373a7731c08c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nDP2qqr-Zpd69S5X"/></div></div></figure><p id="e007" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤4c:然而,“阳光”分支的CV (28%)比阈值(10%)大，需要进一步分裂。我们选择“Windy”作为“Outlook”之后的最佳节点，因为它具有最大的SDR</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/1e56a61ea80085423ad15af6f212a2cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/0*P_CFew6c7YnS_Bbm"/></div></figure><p id="ed60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为两个分支(假和真)的数据点数等于或小于3，所以我们停止进一步分支，并将每个分支的平均值分配给相关的叶节点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jy"><img src="../Images/78e1d2998ff102a50c888674448cec09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*mls6PUoTMAMGs1l0"/></div></figure><p id="7f5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤4d:此外，“多雨”分支的CV (22%)大于阈值(10%)。这根树枝需要进一步劈开。我们选择“Windy”作为最佳节点，因为它具有最大的SDR。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/c0564399c3b3b7d35f9f17fc8cb0ebfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/0*7cF4hDh5TieVoq--"/></div></figure><p id="dc70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为所有三个分支(冷、热和温和)的数据点的数量等于或小于3，所以我们停止进一步分支，并将每个分支的平均值分配给相关的叶节点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ka"><img src="../Images/5b301dda14938b1fe27a3c7f816b6e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/0*X72AyyXveA9eBDZa"/></div></figure><p id="e8f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当一个叶节点上的实例数量多于一个时，我们计算平均值作为目标的最终值。</p><p id="2842" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">信息增益</strong></p><p id="ddb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集分割后熵减少的度量是信息增益。</p><p id="49c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息增益是一种统计属性，用于衡量给定属性根据目标分类将训练样本分开的程度。</p><p id="1f82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果使用X特征进行分割，则信息增益可以表示为</p><p id="8a9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息增益(T，X) =熵(T) —熵(T，X)</p><p id="f2b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或者</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kb"><img src="../Images/ee8115178b911b2d94a495f7daf8264f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mOjviEOOO8hTSXdW"/></div></div></figure><p id="e5d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中[加权平均值] *熵(子节点)=(左侧子节点中的示例数)/(父节点中的示例总数)*(左侧节点的熵)+(右侧子节点中的示例数)/(父节点中的示例总数)*(右侧节点的熵)</p><p id="778a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">熵是什么？</p><p id="f83e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">熵是一种测量杂质的方法。它是对数据集中随机性或不可预测性的一种度量。熵是无序或不确定性的度量，由于机器学习模型的目标通常是减少不确定性，熵越低，就越容易从数据中得出任何结论，相反，熵越高，就越难从该信息中得出任何结论。</p><p id="416c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">熵=∑pj log2pj</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kc"><img src="../Images/7f8fe29465b1657d33aad0919ea245af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2-VaqYJB6YscrHb9"/></div></div></figure><p id="6d48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-如果节点的所有样本属于同一类(样本是同类的)，则熵为0</p><p id="988a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">熵= 1 log 1 = 0</p><p id="c69d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-如果我们有一个均匀的类分布(样本被等分)，熵是最大的</p><p id="3d42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">熵= 0.5 log 0.5 0.5 log 0.5 = 1</p><p id="0a23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-分裂的熵越大，从分裂中获得的信息越少。分裂的熵越大，目标变量的无序/不确定性减少得越少</p><p id="feef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-分裂的熵越小，从分裂中获得的信息越多。分裂的熵越小，目标变量的无序/不确定性减少得越多</p><p id="e9db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-熵为0的分支是叶节点。</p><p id="6176" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-熵大于0的分支需要进一步分裂。</p><p id="5043" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步伐</p><p id="9472" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-计算目标的熵</p><p id="849a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-计算每个分裂的熵，作为子节点的加权平均熵</p><p id="ff95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-选择具有最低熵或最高信息增益的分裂</p><p id="b86c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-重复上述步骤，直到获得同类节点</p><p id="7421" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例子</p><p id="48b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑一个例子，我们正在构建一个决策树来预测给一个人的贷款是否会导致注销。我们的全部人口由30个实例组成。16个属于注销类，另外14个属于非注销类。我们有两个特性，即“Balance”可以取两个值—“&lt; 50K” or “&gt;50K”和“Residence”可以取三个值—“OWN”、“RENT”或“OTHER”。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kd"><img src="../Images/da5f2f0c5eb9b7e0066715e2ed8f519b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-VW7beNVYuOARwq2"/></div></div></figure><p id="a7fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">圆点是分类正确的数据点，星号是未注销的数据点。在属性平衡上分割父节点给我们2个子节点。左侧节点获得了总观察值中的13个，其中12/13 (0.92概率)观察值来自注销类，而只有1/13(0.08概率)观察值来自未写入类。右节点得到总观察值的17，其中13/17(0.76概率)观察值来自非注销类，4/17 (0.24概率)来自注销类。让我们计算父节点的熵，看看通过平衡分裂，树可以减少多少不确定性。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ke"><img src="../Images/69122657677f6fe59f865a5896649480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LpZV5cZiZzQTgWl9"/></div></div></figure><p id="5c89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割特征，“平衡”导致目标变量的信息增益为0.37。让我们做同样的事情为功能，“住宅”，看看它如何比较。在Residence上分割树给了我们3个子节点。左边的子节点得到总观察值中的8个，其中7/8 (0.88概率)观察值来自注销类，而只有1/8 (0.12概率)观察值来自非注销类。中间的子节点得到总观察值的10，其中注销类的观察值为4/10(概率为0.4)，非注销类的观察值为6/10(概率为0.6)。右边的子节点得到总观察值中的12个，其中5/12 (0.42概率)观察值来自注销类，7/12 (0.58)观察值来自非注销类。我们已经知道了父节点的熵。我们只需要计算分裂后的熵来计算“居住”的信息增益</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kf"><img src="../Images/9bd1e38ae36ed68921fe1e24b3330486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4dsRGrTXqYk0uCb5"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kg"><img src="../Images/83ce64b4e181b006bc3497b9e369e420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RQC2p1npepdFfBI9"/></div></div></figure><p id="cacd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来自特征平衡的信息增益几乎是来自居住地的信息增益的3倍。Balance比Residence提供了更多关于我们的目标变量的信息。它减少了我们的目标变量中更多的无序。决策树算法将使用此结果，使用Balance对我们的数据进行第一次拆分。</p><p id="86dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基尼指数或基尼不纯度</strong></p><p id="5cd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是最小化错误分类概率的标准</p><p id="5fed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以将其视为用于评估数据集中分割的成本函数。</p><p id="704a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个衡量随机选择的元素被错误识别的频率的指标。</p><p id="3385" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它的计算方法是从1中减去每类概率的平方和。基尼指数与分类目标变量“成功”或“失败”一起工作。它只执行二进制分割。</p><p id="ab27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼= 1∑p j</p><p id="5f66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-如果类别完全混合(例如二元类别)，基尼指数最大</p><p id="e8ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼= 1(P1+p2)= 1((0.5)+(0.5))= 0.5</p><p id="aac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与基尼系数较高的属性相比，基尼系数较低的属性被错误分类的可能性较小。因此，在构建决策树时，我们倾向于选择基尼系数最小的属性/特征进行拆分。</p><p id="ed9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-基尼优于熵，因为它的计算速度更快</p><p id="5ef9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步伐</p><p id="af38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-计算每个分裂的基尼系数，作为子节点的加权平均基尼系数</p><p id="855a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-选择基尼系数最低的分割点</p><p id="3ba1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-重复上述步骤，直到获得同类节点</p><p id="2c97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例子</p><p id="2086" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用与信息增益示例相同的数据样本。让我们试着用基尼指数作为判断标准。这里，我们有5列，其中4列有连续数据，第5列由类标签组成。</p><p id="df1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a、B、C、D属性可被视为预测值，E列类标签可被视为目标变量。为了从这些数据中构建决策树，我们必须将连续数据转换成分类数据。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kh"><img src="../Images/49113fdd6c49fb5b00f6cb490b5d2c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/0*eWb7CwlqhLB0Nboq"/></div></figure><p id="797d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们选择了一些随机值来对每个属性进行分类:</p><p id="728d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">A</p><p id="55db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">B</p><p id="e3b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">C</p><p id="a795" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">D</p><p id="6d07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">&gt;= 5</p><p id="2fe7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">&gt;= 3.0</p><p id="193b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">&gt;=4.2</p><p id="6a4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">&gt;= 1.4</p><p id="a75d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">&lt; 5</p><p id="349a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">&lt; 3.0</p><p id="23ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">&lt; 4.2</p><p id="63f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">&lt; 1.4</p><p id="4067" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Var A的基尼指数</p><p id="027c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">16条记录中有12条记录的Var A值&gt; =5，4条记录的值&lt;5 value.</p><p id="4304" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For Var A &gt; = 5 &amp; class ==正:5/12</p><p id="40d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于变量A &gt;= 5 &amp; class ==负:7/12</p><p id="df15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼(5，7) = 1- ( (5/12)2 + (7/12)2 ) = 0.4860</p><p id="bc64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于16条记录中的12条记录的Var A &lt;5 &amp; class == positive: 3/4</p><p id="f7a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For Var A &lt;5 &amp; class == negative: 1/4</p><p id="33c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">gini(3,1) = 1- ( (3/4)2 + (1/4)2 ) = 0.375</p><p id="ccf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">By adding weight and sum each of the gini indices:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ki"><img src="../Images/ed4956dadd092ff79cf7ed6c2e6ab544.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/0*0sX8AUgJQxq41puG"/></div></figure><p id="dba1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Gini Index for Var B</p><p id="daad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Var B has value &gt; =3，以及值为&lt;5 value.</p><p id="1918" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For Var B &gt; = 3 &amp; class ==正数的4条记录:8/12</p><p id="83f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Var B &gt;= 3 &amp; class ==负:4/12</p><p id="32a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼(8，4) = 1- ( (8/12)2 + (4/12)2 ) = 0.446</p><p id="c164" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Var B ❤ &amp; class ==正数:0/4</p><p id="c2a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Var B ❤ &amp; class ==负:4/4</p><p id="75b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">gin(0，4) = 1- ( (0/4)2 + (4/4)2 ) = 0</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kj"><img src="../Images/4ae96712ab3279640cce31a83be8faed.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/0*rfIs3AU4UxwZ68hy"/></div></figure><p id="3c20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Var C的基尼指数</p><p id="7280" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">16个记录中有6个记录的Var C值&gt; =4.2，10个记录的值&lt;4.2 value.</p><p id="51b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For Var C &gt; = 4.2 &amp; class ==正数:0/6</p><p id="2cb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Var C &gt;= 4.2 &amp; class ==负:6/6</p><p id="cb1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼(0，6) = 1- ( (0/8)2 + (6/6)2 ) = 0</p><p id="e066" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于16个记录中的5个记录，Var C &lt; 4.2&amp; class == positive: 8/10</p><p id="60db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For Var C &lt; 4.2 &amp; class == negative: 2/10</p><p id="7e94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">gin(8,2) = 1- ( (8/10)2 + (2/10)2 ) = 0.32</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kk"><img src="../Images/a806b6631cc71a8d9ccd160dcc7dd1dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/0*vri3UDJqpukqbhah"/></div></figure><p id="e4b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Gini Index for Var D</p><p id="e528" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Var D has value &gt; =1.4，并且11个记录的值&lt;1.4 value.</p><p id="194c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For Var D &gt; = 1.4 &amp; class ==正:0/5</p><p id="ee98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于变量D &gt;= 1.4 &amp; class ==负:5/5</p><p id="98ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼(0，5) = 1- ( (0/5)2 + (5/5)2 ) = 0</p><p id="6d5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Var D &lt; 1.4 &amp; class == positive: 8/11</p><p id="cb51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For Var D &lt; 1.4 &amp; class == negative: 3/11</p><p id="b5ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">gini(8,3) = 1- ( (8/11)2 + (3/11)2 ) = 0.397</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kl"><img src="../Images/c703f20bde379defd8eb5ff3fd0cfd77.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*K_iVjRFm5y1AJrt3"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es km"><img src="../Images/6414a268a0a5ac5fba829bc60c029362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/0*mGl-zkNbqcDisD6k"/></div></figure><p id="792f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问题用决策树</strong></p><p id="6e4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们构建决策树时，它会考虑数据集中的所有列，并逐个分支地选择最佳列进行拆分。因此，在根节点进行分割的列被认为比在其他分支的列更重要，因此所有预测将对顶部的列产生巨大影响，并且当在实时场景中，如果出现第一列不是进行预测的最重要特征的观察时，模型将不能很好地预测。</p><p id="dd42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个问题会导致高方差，即当我们更改数据集进行预测时，准确性会降低。解决方案是使用随机森林模型，该模型通过在随机样本子集上使用随机特征子集来训练模型，从而减少方差。因此，输出不是来自一个决策树，其中具有固定重要性级别的列用于所有样本的预测，而是来自许多决策树的输出，其中不同的特征被认为具有不同的重要性。</p><p id="0f44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">修剪</strong></p><p id="4fc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在建立决策树模型时，过拟合是一个实际问题。我们可以使用随机森林算法或剪枝技术来克服过度拟合的问题。</p><p id="c195" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">修剪是删除树叶和树枝以提高决策树性能的过程。修剪是分裂的相反过程。</p><p id="c5de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">修剪包括移除利用低重要性特征的分支。这样，我们降低了树的复杂性，从而通过减少过拟合来提高其预测能力。</p><p id="0bb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">预修剪</strong></p><p id="2ff5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在预修剪中，它提前停止了树的构建。这种方法阻止了非重要分支的生成。它根据给定的条件终止新分支的生成。如果节点的品质度量低于阈值，则优选不分割节点。</p><p id="dc13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">后期修剪</strong></p><p id="09e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生成完整的树，然后修剪/移除不重要的分支。在每一步都要进行交叉验证，以检查新分支的添加是否会提高准确性。否则，该分支将被转换为叶节点。</p><p id="1bd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">执行修剪的一些重要参数是减少叶节点数量的max_leaf_nodes、限制样本叶大小的min_samples_leaf、减少树的深度以构建一般化树的max_depth。</p><p id="07bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">#数据科学#机器学习算法#深度学习算法#数据#算法#德勤#德勤大学#英特尔#target #pwcindia</p></div></div>    
</body>
</html>