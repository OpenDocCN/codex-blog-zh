<html>
<head>
<title>Principle Component Analysis (PCA) simplified with implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过实施简化主成分分析(PCA)</h1>
<blockquote>原文：<a href="https://medium.com/codex/principle-component-analysis-pca-simplified-with-implementation-a05eb86084f8?source=collection_archive---------8-----------------------#2022-04-01">https://medium.com/codex/principle-component-analysis-pca-simplified-with-implementation-a05eb86084f8?source=collection_archive---------8-----------------------#2022-04-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="5653" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我希望有人教过我的方式</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/692a8d93c1ea132c6b6c0f7ed0224264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8Iwg-wftbeMhwRNc0noUQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">照片由Pietro Jeng在Unsplash上拍摄</figcaption></figure><p id="457d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">主成分分析(PCA)是一种众所周知的降维技术。PCA属于机器学习的无监督分支，它使用基于协方差矩阵的特征向量和特征值分解的“正交线性变换”，将数据集的属性投影到新的坐标系上。其目的是在减少特征数量的同时保留大部分原始信息。<strong class="jv hj"><em class="kr">PCA的核心思想</em> </strong>是计算主成分，并用它们对数据进行基的变换，有时只使用前几个主成分，而忽略其余的。它通过生成新的不相关变量来实现这一点，这些变量以连续的方式最大化方差。完美的描述，但对想要理解它的人来说不是很有用。</p><p id="169b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj"> <em class="kr">那么，我们再来试试</em> </strong>，这次用一个例子。想象你现在手里或面前有一个物体，可能是笔、电脑、电话或其他什么东西。对我来说，这是我的手表，现在仔细观察它的宽度、高度和深度(一般尺寸)，如果我让你在纸上画出来，你会怎么画？或者，如果我让你在它上面投射一个手电筒或灯光来创建原始对象的二维(2D)阴影，你会如何旋转它，以便你甚至可以在没有看到原始3D对象的情况下识别它？你要么根据手表的特征在纸上画一只手表，要么投射光线，使阴影与原手表的高度、宽度或曲线相似。</p><p id="47df" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在的问题是，你画的手表和原来的3D手表是一样的，还是手表的阴影保留了它的颜色或材料？绝对不行。但即使在所有的瑕疵之后，如果你问任何人你的画或影子，他们也会简单地说“这是一块手表”。<strong class="jv hj">这就是PCA所做的，</strong>它将具有大量特征(我们的3D手表)的数据集投影到少量变换的特征(2D图或手表的阴影)中，这些特征保留了原始对象(一般数据集)的大部分信息。</p><p id="e229" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在你会认为这很容易，但是有一个陷阱。你如何决定你的手表相对于光源的方向或角度，以获得完美的阴影？你的答案可能是击中和跟踪，但如果对象是非常复杂的形状，而不是手表，这将需要永恒。这就是我要介绍特征向量和特征值的地方，我们先来了解一下它们是什么。</p><h2 id="e65f" class="ks ig hi bd ih kt ku kv il kw kx ky ip ke kz la it ki lb lc ix km ld le jb lf bi translated">特征向量和特征值:</h2><p id="71a6" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">特征向量是实非零向量，当对其应用线性变换时，其方向保持不变。特征值是一个数字，表示数据在该方向上的变化量。简单地说，你正在寻找的方向或角度，以确定获得最佳投影(阴影)的手表是一个特征向量。由于我们的手表是一个3D产品，它可能只有很少的特征向量，可以很容易地识别，但现在考虑一个具有50或100维(特征)的数据集，是否有可能通过hit&amp; trail确定其特征向量的数量？或者，即使你设法识别了所有的特征向量，你如何分类哪个是主向量？答案是特征值，具有最高特征值的特征向量是主特征向量(PCA中的第一主分量)。前进之前的另一个重要概念是<strong class="jv hj">协方差矩阵。</strong></p><h2 id="92ff" class="ks ig hi bd ih kt ku kv il kw kx ky ip ke kz la it ki lb lc ix km ld le jb lf bi translated">协方差矩阵:</h2><p id="9113" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">协方差矩阵描述了数据集的两个要素中的任何一个如何一起变化。对于我们的情况，手表的高度和宽度或宽度和深度之间的关系。要看的是，他们之间有什么联系吗？宽度随着深度的变化而变化吗？这就是协方差矩阵所保持的，它反过来帮助我们识别投射手表的正确方向(特征向量和值)。</p></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><p id="3a2b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在你对PCA有了一个大概的了解，是时候实现它来巩固你的概念了。</p><p id="9ba9" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们将按照给定的顺序执行以下步骤，以达到最终目标:</p><p id="ab0c" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">1️⃣读取数据帧中的数据</p><p id="a046" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">2️⃣缩放数据</p><p id="22ca" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">3️⃣创建协方差矩阵</p><p id="bfe4" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">4️⃣选择主成分</p><p id="e8ba" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">5️⃣验证保存的信息</p><p id="e2f9" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">6️⃣可视化</p><p id="0e93" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj">数据读取:</strong></p><p id="f075" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我已经使用<a class="ae ls" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html" rel="noopener ugc nofollow" target="_blank"> smote </a>创建了几个国家的蛋白质消费的合成数据集，我们将使用它来实现PCA。以下是数据集的导入和加载:</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="f089" class="ks ig hi lu b fi ly lz l ma mb">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="a134" class="ks ig hi lu b fi mc lz l ma mb">df = pd.read_csv('/content/proteindata.csv')<br/>df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es md"><img src="../Images/4d7d084f267a64edb793addb06fde399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QbLkKSaYnilqcPW3J_ubmw.jpeg"/></div></div></figure><p id="87f1" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj">缩放:</strong></p><p id="edce" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在，我们将我们的特征与标签(国家)分开，如下所示:</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="7f32" class="ks ig hi lu b fi ly lz l ma mb">features = df.iloc[:, 1:]<br/>Label = df.iloc[:, 0:1]</span></pre><p id="488f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们将数据分成X，Y，因为我们需要缩放我们的特征。缩放数据集的主要原因是我们不希望某些特征由于比例差异而在算法上有偏差。我们有两种选择来缩放数据帧，一种是通过scikit-learn的标准缩放器:</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="2aaa" class="ks ig hi lu b fi ly lz l ma mb">from sklearn.preprocessing import StandardScaler<br/>features_norm = StandardScaler().fit_transform(features)<br/>features_norm</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/080e0ac3dd39c04b76803e1701578ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MWP4GgVdmwwpG_Dv0E-4Zw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">标准缩放器输出</figcaption></figure><p id="c2bb" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">另一种方法是从数据集中减去每个变量的平均值，使数据集以原点为中心:</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="0cc6" class="ks ig hi lu b fi ly lz l ma mb">feature_mean = features - np.mean(feautres , axis = 0)<br/>feature_mean</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mf"><img src="../Images/98afbdece3bf2f61da0ba61224b49067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2zv5yccuw5bM7BxyenJOpA.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">以原点为中心</figcaption></figure><p id="0bfe" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj">创建协方差矩阵:</strong></p><p id="073b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">因为你已经对协方差矩阵有了一个直觉，这就是我们在简单地使用<strong class="jv hj"> <em class="kr"> numpy.cov( ) </em> </strong>方法缩放我们的数据集之后如何得到它。</p><p id="6313" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">很高兴知道在协方差矩阵的对角线上有方差，其他元素是协方差。</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="3381" class="ks ig hi lu b fi ly lz l ma mb">transpose_feat = features_norm.T<br/>covar_matrix = np.cov(transpose_feat)<br/>covar_matrix</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mg"><img src="../Images/8a893c17a12837bdb6b3e6898c0b3be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VtXVu_NkUNVrQ6GcVbpMGw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">对角线有方差</figcaption></figure><p id="d4c0" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj">选择主成分:</strong></p><p id="f30e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">从协方差矩阵，我们现在可以计算特征向量和相应的特征值。但是在这样做之前，让我们理解为什么这样做很重要。由于我们的协方差矩阵是对称的，解释大部分方差的第一特征向量(主分量)将与在第一主分量之后保留大部分方差的第二特征向量正交，依此类推。由于<a class="ae ls" href="https://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">对称矩阵的特征向量是正交的</strong> </a>，这将继续我们数据集中的特征总数。使用numpy的<strong class="jv hj"> <em class="kr"> linalg.eig() </em> </strong>实现如下:</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="d02a" class="ks ig hi lu b fi ly lz l ma mb">eig_values, eig_vectors = np.linalg.eig(covar_matrix)<br/>eig_values</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mh"><img src="../Images/15b04b822a3a1989b0b439f914b60039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*nWsrBeO_X3kysivccFPxzA.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">协方差矩阵的特征值</figcaption></figure><p id="4efc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">上面是特征值，下面是特征向量</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="5a11" class="ks ig hi lu b fi ly lz l ma mb">eig_vectors</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/47ea5089fee2e1a3639352d144b8f49d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ww8jXIuS6-qTBMM490pBMg.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">协方差矩阵的特征向量</figcaption></figure><p id="6738" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj">验证保存的信息:</strong></p><p id="128a" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在我们将计算每个特征向量保留了多少信息(方差)。就像把光投射到我们的物体上来看它的影子一样，但是在这里，前几个特征值的累积和将有助于我们识别物体</p><p id="8cf9" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">下面是计算每个特征向量保留的信息的代码:</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="43e8" class="ks ig hi lu b fi ly lz l ma mb">information_preserved = []<br/>for i in range(len(eig_values)):<br/>    information_preserved.append(eig_values[i] / np.sum(eig_values))</span><span id="408d" class="ks ig hi lu b fi mc lz l ma mb">print(sum(information_preserved))</span></pre><p id="37c5" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj"> <em class="kr">信息_保存的</em> </strong>或<strong class="jv hj"> <em class="kr"> </em> </strong>方差应始终等于1。</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="1614" class="ks ig hi lu b fi ly lz l ma mb">information_preserved</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/25ce7cb2070b693649f962b828e783d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*Ko4YahZJpJNJUF1bwaZLkQ.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">每个向量保存的信息</figcaption></figure><p id="2c8f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在我们的例子中，前4个PC(主成分)解释了几乎96%的方差(保留了信息)，这是惊人的，因为我们现在可以使用这4个用于模型训练、评估，前2个也用于可视化。这是PCA背后的主要思想，我们可以在低维中投影高维数据，而不会留下大部分信息内容。由于PCA将大量信息保留在较少的特征(维度)中，因此它也有助于减少机器学习模型的训练时间。</p><p id="a01f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">让我们想象一下我们所有精选的电脑:</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="d05d" class="ks ig hi lu b fi ly lz l ma mb">total_info = np.cumsum(information_preserved)<br/>plt.figure(figsize=(8, 6))<br/>plt.bar(range(1,10), information_preserved, alpha=0.5, align='center')</span><span id="b1f0" class="ks ig hi lu b fi mc lz l ma mb">plt.step(range(1,10), total_info, where='mid', color='red')<br/>plt.ylabel('variation/information preserved by each PC')<br/>plt.xlabel('Principal components 1-9')<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/3740f842a29a8a7ca8845ca0ef0ca3cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*r-0LBQlqNuz06YFjceuEyg.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">每个主成分保存信息</figcaption></figure><p id="f308" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj">可视化:</strong></p><p id="58fe" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在，为了直观显示基于第一个2 PC的输出，我们开始为</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="32de" class="ks ig hi lu b fi ly lz l ma mb">pca_1 = features_norm.dot(eig_vectors.T[0])<br/>pca_2 = features_norm.dot(eig_vectors.T[1])<br/>PCA_df = pd.DataFrame(pca_1, columns=['PCA_1'])<br/>PCA_df['PCA_2'] = pca_2<br/>PCA_df['Countries'] = Label<br/>PCA_df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/eda8faeb4a89f9f15e29e83624fec81a.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*cHATkOEVhKTUDykRcRcJww.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">前2张标签</figcaption></figure><p id="2dea" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们用缩放后的数据进行前2个点的点积，现在我们可以绘制散点图，如下所示:</p><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="bbef" class="ks ig hi lu b fi ly lz l ma mb">y1 = Label.values.ravel()</span><span id="f0e9" class="ks ig hi lu b fi mc lz l ma mb">plt.figure(figsize=(10, 8))<br/>sns.scatterplot(x='PCA_1', y='PCA_2', hue='Countries', data=PCA_df, legend=False, s=22)<br/>plt.legend(markerscale=4)</span><span id="b6c1" class="ks ig hi lu b fi mc lz l ma mb">for i, txt in enumerate(y1):<br/>     plt.annotate(txt, (pca_1[i], pca_2[i]), fontsize=12)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mm"><img src="../Images/105b231bf7f073be3322a86aa996af4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h-hir_5QqGfZIfVM8j4bJQ.jpeg"/></div></div></figure><p id="6550" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">今天到此为止。除了所有这些实现，scikit-learn还提供了一个<a class="ae ls" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> PCA模块</a>。希望你喜欢读这篇文章，我将发表另一篇关于处理稀疏矩阵的截断&amp;稀疏PCA的文章，如果你不想错过，请跟我来。另外，如果您对具体的实施有任何要求，请<a class="ae ls" href="https://www.linkedin.com/in/muhammad-saad-31740060/" rel="noopener ugc nofollow" target="_blank">给我</a>写信。</p></div></div>    
</body>
</html>