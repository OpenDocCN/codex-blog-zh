<html>
<head>
<title>How to compute gradients in Tensorflow and Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Tensorflow和Pytorch中计算梯度</h1>
<blockquote>原文：<a href="https://medium.com/codex/how-to-compute-gradients-in-tensorflow-and-pytorch-59a585752fb2?source=collection_archive---------4-----------------------#2021-04-09">https://medium.com/codex/how-to-compute-gradients-in-tensorflow-and-pytorch-59a585752fb2?source=collection_archive---------4-----------------------#2021-04-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3fa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算梯度是许多机器学习算法的核心部分之一。幸运的是，我们有深度学习框架为我们处理。这篇文章将通过一个例子解释Tensorflow和Pytorch如何帮助我们计算梯度。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/dfb6557b83b0a40f825d681ccfc106d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RnSiTkDwg5SrvDIy"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">照片由<a class="ae jt" href="https://unsplash.com/@dusk_cicada?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">巴卫·斯泰内克</a>在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="f7e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们许多人都熟悉使用TensorFlow和PyTorch来训练神经网络。我们已经知道如何计算梯度，并使用优化器通过几行代码更新权重参数。这篇文章将把这些库中计算渐变的部分分离出来，看看代码背后发生了什么。</p><h1 id="f074" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">1.导数和梯度</h1><p id="219e" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在一维中，函数的导数定义如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kx"><img src="../Images/8a0cbdd0de3783c63625a8786998c93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*26cYy9IgoaRmRq1bk2exrQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">等式1。导数的极限定义</figcaption></figure><p id="2ba8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常，在多维中，梯度是沿着每个维度的偏导数的向量。因此，梯度和x有相同的形状，梯度的每个元素会告诉我们，如果我们在坐标方向上移动，函数f的斜率是多少。</p><p id="ba8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个梯度有很好的特性。它指向函数最大增长的方向。相应地，梯度的负值给出了函数最大下降的方向。</p><h1 id="112f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">2.如何评估渐变</h1><p id="9b4c" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在计算机中评估梯度的一种简单方法是使用有限差分法，该方法使用梯度的极限定义(等式1)。具体地说，我们对x的每个维度用该维度的小h值迭代地评估等式1。当x的大小很大时，它会非常慢。</p><p id="dd05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但谢天谢地，我们不必这样做。我们可以用微积分来计算解析梯度，也就是写下梯度应该是多少的表达式。</p><p id="f510" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，有两种方法可以计算梯度。</p><ul class=""><li id="ad9c" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated"><strong class="ih hj">数字渐变</strong>:近似，缓慢，易写。</li><li id="937e" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated"><strong class="ih hj">解析梯度</strong>:精确、快速、易错。</li></ul><p id="8abc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实践中，我们应该总是使用解析梯度，但是用数值梯度来检查实现。这被称为<strong class="ih hj">梯度检查</strong>。</p><h1 id="3c96" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">3.举例说明</h1><p id="c060" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">现在，让我们跳到一个例子(来自参考资料中的Coursera课程)。在这个例子中，我们将有一些计算，并使用链规则来计算梯度自己。然后我们看PyTorch和Tensorflow如何为我们计算梯度。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/d8e37d279105b4dea215b472deeca1cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*eY2RfoahxabXSuiaL-2Utw.png"/></div></figure><h1 id="b491" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">4.PyTorch代码</h1><p id="314c" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在PyTorch中实现代码将给出我们对上面例子的预期。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/95d36211f92f652e926bb14ac9c01df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*1UnNI6qaCdmzzaMO0fuEnw.png"/></div></figure><p id="d9fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是PyTorch的自动微分引擎，帮助我们计算梯度。</p><p id="ee32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们首先用<code class="du lo lp lq lr b">requires_grad=True</code>创建一个张量<code class="du lo lp lq lr b">x</code>。这向<code class="du lo lp lq lr b">autograd</code>发出信号，它上面的每一个操作都应该被跟踪。当我们在<code class="du lo lp lq lr b">z</code>上调用<code class="du lo lp lq lr b">.backward()</code>时，<code class="du lo lp lq lr b">autograd</code>计算这些梯度并将它们存储在张量的<code class="du lo lp lq lr b">.grad</code>属性中。因此，我们可以在<code class="du lo lp lq lr b">x.grad</code>中看到渐变。</p><h1 id="de9b" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">5.张量流代码</h1><p id="10a7" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在TensorFlow中，优化器是使用TensorFlow自动微分API调用<strong class="ih hj">梯度带</strong>实现的。这个API让我们计算和跟踪每个可微分张量流操作的梯度。</p><p id="859a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果至少一个变量被监视，梯度带范围内的操作将被记录。如果我们观察变量<code class="du lo lp lq lr b">x</code>，录像带会观察你在下面看到的其余操作。当我们调用<code class="du lo lp lq lr b">tape.gradient</code>来计算<code class="du lo lp lq lr b">z</code>相对于<code class="du lo lp lq lr b">x</code>的梯度时，我们会得到与之前相同的结果。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/7daf01f3282f1cd330aec5b5b741a05b.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*y3tEZoR0GeYxUkbxGg2czQ.png"/></div></figure><h1 id="5d94" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结论</h1><p id="6a55" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">这篇文章提供了一个关于如何使用PyTorch的<strong class="ih hj">自动签名</strong>和TensorFlow的<strong class="ih hj">渐变胶带</strong>计算渐变的简单例子。<strong class="ih hj"> </strong>我们实际上用它们来做更复杂的功能，用来训练深度神经网络。</p><p id="f2af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">微分机制的秘密(据我所知)来自图计算。然而，图形计算、反向传播或这些框架的显式实现超出了本文的范围。您可以在参考资料或它们的源代码中寻找更多的解释。</p><h1 id="b52f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">参考</h1><ol class=""><li id="5f5c" class="ky kz hi ih b ii ks im kt iq lt iu lu iy lv jc lw le lf lg bi translated">斯坦福CS231n第三讲损失函数与优化:<a class="ae jt" href="https://youtu.be/h7iBpEHGVNc" rel="noopener ugc nofollow" target="_blank">https://youtu.be/h7iBpEHGVNc</a></li><li id="5731" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc lw le lf lg bi translated">斯坦福CS231n第四讲神经网络简介:<a class="ae jt" href="https://youtu.be/d14TUNcbn1k" rel="noopener ugc nofollow" target="_blank">https://youtu.be/d14TUNcbn1k</a></li><li id="b67c" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc lw le lf lg bi translated">PyTorch亲笔签名教程:<a class="ae jt" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/blitz/亲笔签名_tutorial.html </a></li><li id="aee3" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc lw le lf lg bi translated">coursera Custom-Distributed Training with tensor flow:<a class="ae jt" href="https://www.coursera.org/learn/custom-distributed-training-with-tensorflow?specialization=tensorflow-advanced-techniques" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/Custom-Distributed-Training-with-tensor flow？专业化=张量流-高级-技术</a></li></ol></div></div>    
</body>
</html>