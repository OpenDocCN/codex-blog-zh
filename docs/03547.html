<html>
<head>
<title>Stacking Ensemble Modelling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">堆积系综建模</h1>
<blockquote>原文：<a href="https://medium.com/codex/stacking-ensemble-modelling-5c1362ab8214?source=collection_archive---------10-----------------------#2021-09-08">https://medium.com/codex/stacking-ensemble-modelling-5c1362ab8214?source=collection_archive---------10-----------------------#2021-09-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/eb12636661d48413cd0ed6dce0024b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IXQOxSfb5Am80fZC"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">信用:<a class="ae iu" href="https://unsplash.com/@priscilladupreez" rel="noopener ugc nofollow" target="_blank">普里西拉杜普里兹</a>(unsplash.com)</figcaption></figure><p id="20b6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">堆叠(也称为堆叠概化)是一种集成建模技术，涉及多个模型预测数据的组合，用作生成新模型和进行预测的要素。被组合的模型被称为基础模型，并且它们的被用作训练最终模型的附加特征的预测被称为元特征。</p><p id="694b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">堆叠广泛用于现实应用中，以获得最佳预测精度。这是一个在流行的网飞竞赛中获胜的模型，其中元特征来自单个模型，如奇异值分解(SVD)、受限玻尔兹曼机器(RBMs)和K-最近邻(KNN)。我将建立在我的Kaggle <a class="ae iu" href="https://usmanbiu.medium.com/titanic-project-breakdown-kaggle-dataset-52cf812face5" rel="noopener">泰坦尼克号</a>解决方案的基础上，在那里我已经做了数据清理和模型开发。我只会结合随机森林的元特征，极端梯度推进和逻辑回归模型。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="f2fa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">导入必要的库</strong></p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="316d" class="kj kk hi kf b fi kl km l kn ko">from sklearn.linear_model import LogisticRegression<br/>from sklearn.ensemble import RandomForestClassifier<br/>from xgboost import XGBClassifier<br/>from sklearn.model_selection import KFold</span><span id="88ee" class="kj kk hi kf b fi kp km l kn ko">xgb = XGBClassifier(random_state =1)<br/>rf = RandomForestClassifier(random_state = 1)<br/>lr= LogisticRegression(max_iter = 2000)</span></pre><p id="1392" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我将使用随机森林分类器和极端梯度推进模型作为我的基础模型，而物流回归模型将是我的堆叠模型。<br/>‘k fold’库将训练数据分成训练和验证文件夹，以防止在对相同数据进行训练和验证模型时发生数据泄漏，从而导致误导模型性能。</p><p id="3712" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看一下“X”</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="45d7" class="kj kk hi kf b fi kl km l kn ko">X.head</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/9bffedb848df47b08a3c7b93c4c51f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VGXV1E-e31_V4EZ2HZzqPA.png"/></div></div></figure></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="7840" class="kr kk hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">我们基本型号的个性表现</h1><p id="ae55" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">物流回归</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="4f39" class="kj kk hi kf b fi kl km l kn ko">lr = LogisticRegression(max_iter = 2000)<br/>cv = cross_val_score(lr,X,y,cv=5)<br/>print(cv)<br/>print(cv.mean())</span><span id="9bac" class="kj kk hi kf b fi kp km l kn ko">#output<br/>[0.82681564 0.8258427  0.79775281 0.80898876 0.85393258]<br/>0.822666499278137</span></pre><p id="36e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随机森林- rf</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="26a4" class="kj kk hi kf b fi kl km l kn ko">rf = RandomForestClassifier(random_state = 1)<br/>cv = cross_val_score(rf,X,y,cv=5)<br/>print(cv)<br/>print(cv.mean())</span><span id="b8bd" class="kj kk hi kf b fi kp km l kn ko">#output<br/>[0.83240223 0.80898876 0.84831461 0.75280899 0.8258427 ]<br/>0.8136714581633294</span></pre><p id="db9f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">极端梯度提升— xgb</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="394f" class="kj kk hi kf b fi kl km l kn ko">from xgboost import XGBClassifier<br/>xgb = XGBClassifier(random_state =1)<br/>cv = cross_val_score(xgb,X,y,cv=5)<br/>print(cv)<br/>print(cv.mean())</span><span id="a530" class="kj kk hi kf b fi kp km l kn ko">#output<br/>[0.81005587 0.82022472 0.85955056 0.79213483 0.84269663]<br/>0.8249325214989642</span></pre></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="a972" class="kr kk hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">集合模型</h1><p id="74e4" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">是时候将我们的数据分成训练集和测试集了。我将使用清理后的数据集“X”及其相应的输出数据“y”来训练模型。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="b033" class="kj kk hi kf b fi kl km l kn ko">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre><p id="0faa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“X_train”是使用“KFold”函数拆分成训练集和验证集的训练数据，而“X_test”是测试集。随机森林分类器和极端梯度推进模型将是基础模型，而逻辑回归模型将是叠加模型。</p><p id="ef2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> Numpy数组</strong></p><p id="0e85" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">需要两个额外的列来存放来自两个基本分类器预测的元数据，因此，我将使用数据“X_train”和元数据创建并填充一个Numpy数组。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="adb9" class="kj kk hi kf b fi kl km l kn ko">x_train_with_metapreds = np.zeros((X_train.shape[0], X_train.shape[1]+2)) #create a numpy array with the shape of X_train and additional 2 columns<br/>x_train_with_metapreds[:, :-2] = X_train  #fill the numpy array with X_train<br/>x_train_with_metapreds[:, -2:] = -1   #fill the 2 new columns with -1<br/>print(x_train_with_metapreds)</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/7d50fc23b4f67c8f118a7c7ecca798c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*mVYYXupy8xoB9_6BzMyGkw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">带有两个附加列的X_train的numpy数组</figcaption></figure><p id="75a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">训练基本模型的时间</strong></p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="f74e" class="kj kk hi kf b fi kl km l kn ko">from sklearn.model_selection import KFold<br/>kf = KFold(n_splits=5, random_state=11, shuffle=True) # splitting X_train into 5 folds (4 for training and one for validation)<br/>for train_indices, val_indices in kf.split(X_train):<br/>    kfold_x_train, kfold_x_val = X_train.iloc[train_indices], X_train.iloc[val_indices] #extracting the training and validation folds using the row numbers/indices<br/>    kfold_y_train, kfold_y_val = y_train.iloc[train_indices], y_train.iloc[val_indices]<br/>    rf.fit(kfold_x_train, kfold_y_train) # fiting and training the base model<br/>    rf_pred = rf.predict(kfold_x_val) # making and storing the predictions<br/>    xgb.fit(kfold_x_train, kfold_y_train)# fiting and training the base model<br/>    xgb_pred = xgb.predict(kfold_x_val)  # making and storing the predictions<br/>    x_train_with_metapreds[val_indices, -2] = rf_pred  #filling the predictions into the numpy array<br/>    x_train_with_metapreds[val_indices, -1] = xgb_pred</span></pre><p id="bc01" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">拟合堆栈模型并进行预测</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="b4a3" class="kj kk hi kf b fi kl km l kn ko">lr.fit(x_train_with_metapreds, y_train)<br/>lr_preds_train = lr.predict(x_train_with_metapreds)</span></pre><p id="56e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">栈模型已经拟合好了，下一步是在我们的测试集‘X _ test’上重复上面最后两个单元格中的步骤；生成元特征并对测试集进行预测。</p><p id="da09" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">创建Numpy数组</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="d08e" class="kj kk hi kf b fi kl km l kn ko">x_test_with_metapreds = np.zeros((X_test.shape[0], X_test.shape[1]+2)) #create a numpy array with the shape of X_train and additional 2 columns<br/>x_test_with_metapreds[:, :-2] = X_test #fill the numpy array with X_test<br/>x_test_with_metapreds[:, -2:] = -1 #fill the 2 new columns with -1<br/>print(x_test_with_metapreds)</span></pre><p id="df88" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">拟合基本模型并为测试集生成元特征</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="9483" class="kj kk hi kf b fi kl km l kn ko">rf.fit(X_train, y_train)<br/>xgb.fit(X_train, y_train)<br/>rf_pred = rf.predict(X_test)<br/>xgb_pred = xgb.predict(X_test)<br/>x_test_with_metapreds[:, -2] = rf_pred<br/>x_test_with_metapreds[:, -1] = xgb_pred</span></pre><p id="85b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">是时候做预测了</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="d4c5" class="kj kk hi kf b fi kl km l kn ko">lr_preds_test = lr.predict(x_val_with_metapreds)</span><span id="207c" class="kj kk hi kf b fi kp km l kn ko">from sklearn.metrics import accuracy_score<br/>print('Stacked Classifier:\n Accuracy on validation data = {:.4f}'.format(accuracy_score(y_true=y_test, y_pred=lr_preds_test)))</span><span id="e33b" class="kj kk hi kf b fi kp km l kn ko">#output<br/>Stacked Classifier:<br/>&gt; Accuracy on test data = 0.8436</span></pre><p id="5087" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从堆叠模型的精度来看，它优于单个模型，其精度为rf=0.8137、xgb = 0.8249和lr =0.8226。</p><p id="0d14" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结束了。</p></div></div>    
</body>
</html>