<html>
<head>
<title>Cross-Entropy but not without Entropy and KL-Divergence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉熵但不是没有熵和KL散度</h1>
<blockquote>原文：<a href="https://medium.com/codex/cross-entropy-but-not-without-entropy-and-kl-divergence-a8782b41eebe?source=collection_archive---------7-----------------------#2022-04-03">https://medium.com/codex/cross-entropy-but-not-without-entropy-and-kl-divergence-a8782b41eebe?source=collection_archive---------7-----------------------#2022-04-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f53e3f4c4eeaeb1545f6a15cc39a1e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*su7U9ApanIE1mH_1"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae iu" href="https://unsplash.com/@thisisengineering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> ThisisEngineering RAEng </a></figcaption></figure><p id="d3f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在处理机器/深度学习问题时，损失/成本函数用于确保模型在训练时变得更好。目标是使损失函数尽可能小。如果损失越小，模型越好。当试图对图像/文本/音频等进行分类时。交叉熵损失是最流行的代价函数。它用于使分类模型更好。在这里，我们将讨论熵、交叉熵、KL散度，以及它在本文中是如何工作的。</p><p id="4060" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TLDR:交叉熵是特定随机变量或事件集合的两个概率分布之间的差异。</p><p id="ac8c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">所需术语:</strong></p><ol class=""><li id="e53c" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">概率分布</li></ol><blockquote class="kc kd ke"><p id="c8f4" class="iv iw kf ix b iy iz ja jb jc jd je jf kg jh ji jj kh jl jm jn ki jp jq jr js hb bi translated">概率分布是概率论和统计学中的一个数学函数，它表示实验中几种可能结果的发生概率。</p></blockquote><p id="fba2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.随机变量</p><blockquote class="kc kd ke"><p id="2fac" class="iv iw kf ix b iy iz ja jb jc jd je jf kg jh ji jj kh jl jm jn ki jp jq jr js hb bi translated">非正式地说，随机变量的值依赖于随机现象的结果。</p></blockquote><p id="2b92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.熵</p><blockquote class="kc kd ke"><p id="d65b" class="iv iw kf ix b iy iz ja jb jc jd je jf kg jh ji jj kh jl jm jn ki jp jq jr js hb bi translated">熵是万物变得越来越随机的过程。是无敌的。</p></blockquote><p id="dca5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在转向交叉熵之前，我们必须知道<strong class="ix hj">熵</strong>的意思:</p><p id="85c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi kj translated">熵，最简单的形式，是从特定概率分布中选择的单个样本中获得的平均信息量的度量。</p><p id="296e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们考虑一下天气预报系统；如果我们测量沙漠中的天气熵，它将总是接近于0，因为那里总是阳光明媚。如果没有变化，那么熵将接近于0，反之亦然。</p><p id="e530" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要计算熵，我们需要一些度量来计算信息量。信息论由<a class="ae iu" href="https://en.wikipedia.org/wiki/Claude_Shannon" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">克劳德·沙农</strong> </a>解决了这个问题。</p><p id="c640" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="kf">信息内容</em> </strong>是一个随机变量发生特定事件的可能性有多大，基于熵的定义，我们需要一个当概率高时(因为没有变化)接近0，同时当概率低时(因为变化大)接近1的函数。我们可以将用于计算从任何事件E获得的信息的对数函数视为:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/768c6c440a4907216d7e14b14d7d718e.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*ipVTLF8AKrzigVPONPkFFA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Information_content" rel="noopener ugc nofollow" target="_blank">信息内容</a></figcaption></figure><p id="e3b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这相当于:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/74a7f257fc34a6cc44ef91a3030eb1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*W4eEk7AxxtOu6ULdJbc9IQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Information_content" rel="noopener ugc nofollow" target="_blank">信息内容</a></figcaption></figure><p id="fa82" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回到熵的定义，它是从分布中选择的单个样本获得的平均信息量。上述信息内容公式变为:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/6de1b91cd12b5307489ef9d0e6e47839.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*KQXeu6Smu-LMlyL9NKNUxQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank">熵</a></figcaption></figure><p id="7d8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在熵的情况下，我们总是考虑相同的概率分布(即大部分是真实分布)。但是交叉熵处理的是我们既有预测分布又有真实分布的情况。</p><p id="e397" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kf">(可以从任何函数或模型等得到预测分布。)</em></p><p id="5825" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们把真实的概率分布看作<strong class="ix hj"> <em class="kf"> p </em> </strong>，把预测的概率分布看作<strong class="ix hj"> <em class="kf"> q </em> </strong> <em class="kf">。我们的公式变成交叉熵公式为:</em></p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/56cf13073741751967b1f6186cc48afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*9soM0NXEqYVTIIOx.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://stackoverflow.com/questions/41990250/what-is-cross-entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a></figcaption></figure><p id="1f8e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用这里的基本熵定义，交叉熵公式给出了从关于真实分布<strong class="ix hj"> p. </strong>的预测分布<strong class="ix hj"> q </strong>获得的平均信息</p><p id="379c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当预测的和真实的分布接近时，这意味着交叉熵接近于0。(这就是我们想预测某个东西时想要的，所以它必须接近真值或分布)，否则交叉熵会很高。因此，我们总是试图减少交叉熵，因为它会使真实分布和预测分布接近。</p><blockquote class="kc kd ke"><p id="1072" class="iv iw kf ix b iy iz ja jb jc jd je jf kg jh ji jj kh jl jm jn ki jp jq jr js hb bi translated">如果我们考虑理想情况，其中我们的真实预测分布p与真实分布q相同，那么交叉熵等于熵。</p></blockquote><p id="de87" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通常，在现实世界的场景中，分布是不同的，因为真实分布不可能等于预测分布；在这种情况下，交叉熵总是比熵大一些值。交叉熵大于熵的量称为相对熵，或者用数学术语来说，称为kull back-lei bler散度(KL散度)。</p><p id="597c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以简单来说:</p><blockquote class="la"><p id="95f7" class="lb lc hi bd ld le lf lg lh li lj js dx translated"><strong class="ak"> <em class="lk">交叉熵=熵+ KL散度</em> </strong></p></blockquote><p id="42f5" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">或者说<strong class="ix hj"> KL散度</strong>由下式给出:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/d05a8ac25f44d00a334d17ec91a2110b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YZOx050epRCQUUK7.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://towardsdatascience.com/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e" rel="noopener" target="_blank"> KL发散</a></figcaption></figure><blockquote class="kc kd ke"><p id="e344" class="iv iw kf ix b iy iz ja jb jc jd je jf kg jh ji jj kh jl jm jn ki jp jq jr js hb bi translated">基本上，KL散度是从真实分布到预测分布的自然距离。</p></blockquote><p id="eb24" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">KL散度衡量一个概率分布与另一个概率分布的不同程度。它显示了它们之间的距离。更具体地说，它是我们需要从一个分布更新的信息量，以便我们可以从一个分布转移到另一个分布(贝叶斯法则)。</p><p id="982d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们已经预测概率为<strong class="ix hj"> p </strong>和真实概率为<strong class="ix hj"> q </strong>，那么随着时间的推移，我们试图减少KL散度，这样它们可能是相似的，</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/1a9514109ac6d37d357bfbde23e113e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*M9-q6KdP3XcGsRRFlekUVA.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">随着时间的推移最小化KLD</figcaption></figure></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><p id="475d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，在分类问题中，使用交叉熵总训练样本的和进行优化等价于使用KL散度总训练样本的和进行优化；这就是为什么交叉熵是最佳和最常用的成本/损失函数，因为它有助于使预测分布和真实分布更接近。</p><p id="93a3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用于总结:</p><ol class=""><li id="20a6" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">如果我们有一个概率为1(近似值)的事件，那么它不会给出任何信息，所以熵将接近于0。</li><li id="993a" class="jt ju hi ix b iy lz jc ma jg mb jk mc jo md js jy jz ka kb bi translated">相反，如果我们有一个概率接近0(近似值)的事件，任何事情都可能发生，给出大量信息，因此熵相对于1。</li><li id="526b" class="jt ju hi ix b iy lz jc ma jg mb jk mc jo md js jy jz ka kb bi translated">扩展熵概念如果我们想到监督学习(或类似学习)当我们知道真实分布并且我们的模型/函数给出预测分布时，我们可以使用交叉熵来确定我们的预测分布给出了多少关于真实分布的信息。</li><li id="eeb3" class="jt ju hi ix b iy lz jc ma jg mb jk mc jo md js jy jz ka kb bi translated">当预测分布与真实分布具有高相似性时，交叉熵应该接近0；这就是我们想要的；否则，交叉熵会很高。</li><li id="c13b" class="jt ju hi ix b iy lz jc ma jg mb jk mc jo md js jy jz ka kb bi translated">我们总是试图减少交叉熵，因为它会使真实的和预测的分布接近，但这之间总是有一个微小的差距。</li><li id="2f37" class="jt ju hi ix b iy lz jc ma jg mb jk mc jo md js jy jz ka kb bi translated">理想情况下，当预测分布和实际分布相等时，交叉熵和熵将是相同的。</li><li id="ea4b" class="jt ju hi ix b iy lz jc ma jg mb jk mc jo md js jy jz ka kb bi translated">在现实世界中，不可能有相同的分布，交叉熵会高一些；这个量叫做KL散度。</li></ol></div></div>    
</body>
</html>