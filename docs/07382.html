<html>
<head>
<title>NATO symbol recognition with neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于神经网络的北约符号识别</h1>
<blockquote>原文：<a href="https://medium.com/codex/nato-symbol-recognition-with-neural-networks-5a4dfc32436d?source=collection_archive---------18-----------------------#2022-06-13">https://medium.com/codex/nato-symbol-recognition-with-neural-networks-5a4dfc32436d?source=collection_archive---------18-----------------------#2022-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d30d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">埃里克·科伊弗、米赫克尔·莱普森、蜜桃红·苏拉格·里斯、卡尔-克里斯扬·科韦里克</p><p id="5757" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">主管:阿迪·坦普</em></p><p id="fa93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Github代码笔记本链接:【https://github.com/MihkelLepson/NATO_symbols T4】</p><h1 id="eb7d" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">介绍</h1><p id="adb2" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">一切都变得更快了。几十年前，随着数字时代和互联网的到来，越来越少的人通过在一杯咖啡后阅读报纸来获取每日新闻，或者通过在剧院看四个小时的戏剧来满足他们的娱乐需求。这种趋势并没有随着岁月的流逝而减缓；如今，随着科技世界的词汇逐渐融入口语，我们消费着零碎的信息。显而易见，我们希望越来越快地传递信息，然而我们的方法却跟不上。人类的语音以第一代调制解调器所显示的速率传递信息。事实上，无论语言说得多快或多慢，它们传递信息的速度都差不多:每秒39比特。随着普通数据速率达到千兆位范围，一个问题出现了:我们如何弥合这一差距？这个问题是由平民世界提出的，在军方也是一样，这个项目希望在军方迈出回答这个问题的第一步。</p><p id="a3e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该项目是由爱沙尼亚国防学院间接委托的。这是基于北约联盟的军事和国防部队对在盟国部队之间和内部实现更快和更清晰的通信的永久需求。为此，已经建立了一套用于绘制战术计划的符号[2]。这些符号用于从一名军官向另一名军官准确传达有关部队将要进行的行动的信息，对于行动的成功和最大限度地减少伤亡至关重要。</p><p id="7dbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目前，这些计划是用这组符号绘制在覆盖在地图上的塑料封面上的。传达这些计划通常发生在军官会议上，在会上，每个人都要靠在桌子上，看着一张通常光线昏暗的地图，以加快速度。然而，爱沙尼亚国防学院希望在这个机器智能时代改善这种情况。</p><p id="6d21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们的工作开始的地方:我们要提供一个部分的机器“翻译”,用图像把这些计划的符号翻译成文字。最初的任务是能够识别每个图像中的一个符号，但随着项目的持续，也演变成在任何带有白色背景的透明塑料覆盖物上定位几个符号。可以使用这种方法，因为如果数据被截获，它不会危及任何计划，因为塑料覆盖层本身被编码为不容易与任何地图连接，但将大大有助于理解、数字化和清楚地传达有问题的计划。</p><h1 id="ba5e" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><span class="l ki kj kk bm kl km kn ko kp di"> T </span>何数据</h1><p id="537c" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">每个机器学习项目都需要一个好的数据集来提供一个可接受的结果，在许多方面，实现一个好的数据集是机器学习最困难的部分之一，因为一个好的数据集将为所用的神经网络提供最佳的学习材料，就像为学生提供一套好的讲座材料来准备考试一样。</p><p id="250b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的例子中，这项任务涉及15种类型的符号，这些符号是从更广泛的北约符号阵列中为我们选择的，给了我们15个检测类别。在每堂课中，我们都拿到了10个来自ENDC的学员画的例子。由于我们最初的目标是对每个符号在其各自的图像中进行单独分类，因此创建了初始的10*15=150个符号数据集，每个符号作为一个单独的图像提供，以类作为标题。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kq"><img src="../Images/2e0c1c2617124d23d9d207c89de392a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*xzH7mZmabn_fbk2dTGWJDA.jpeg"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图一。学员绘制的符号示例</figcaption></figure><p id="08be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你，读者，可能已经注意到了我们的第一个障碍:我们每节课只有10个例子可以学习。相比之下，MNIST手写数字数据库(包含的数据本质上与我们的非常相似)有一个60 000个样本的训练集和一个10 000个样本的测试集[3]，数量级高于我们可用的数量级。</p><p id="be2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于ENDC不能为我们提供额外的数据，我们的第一步是继续下去，在每一类中再画出20个例子，达到大约450个独特符号的数据集；不伟大，也不可怕。这是用黑色记号笔在白色打印纸上完成的，复制了由提供的数据集呈现给我们的条件。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lg"><img src="../Images/bd47c0a34d65d901b6a540c707cc490a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNR3qvrkfvCLwrvSrFqI1Q.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图二。每个类别中的一个示例符号；稍后添加的类名。</figcaption></figure><p id="b027" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将这组符号放在一起看，我们注意到另一个可能的问题，即多个符号，即<em class="jd">盖、</em>屏、<em class="jd">卫</em>，仅通过一个字符相互区分，因此对于网络来说没有太多的借鉴点，可能导致三者经常混淆。</p><p id="b4d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在进一步检查其他可能的负面兴趣点的数据后，我们确定图像亮度、符号旋转和因相机角度导致的失真是我们认为对结果影响最大的因素。</p><h1 id="3221" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">方法学</h1><p id="b8eb" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">在创建了大约450个符号库数据集之后，我们的工作从明确问题开始，然后一个接一个地(或者事实证明，有时也是并行地)解决它们:</p><ol class=""><li id="9eff" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">我们如何进一步改善我们的数据集？</li><li id="7492" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">我们能使用像KNN这样简单的数据模型来解决分类问题吗？(因为我们的数据看起来与手写数字数据集非常相似。)</li><li id="d6f6" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">如果答案为2。是‘否’，那么我们应该使用来自什么库/框架的什么神经网络？</li><li id="5fe9" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">在这个项目范围内，我们的最终目标是什么？</li></ol><h2 id="a166" class="lv jg hi bd jh lw lx ly jl lz ma mb jp iq mc md jt iu me mf jx iy mg mh kb mi bi translated">数据集扩充</h2><p id="e4c5" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">为了从我们有限的数据集中挤出更多的数据，并根据前面提到的参数(如旋转和扭曲)对其进行多样化/统一，创建了一个协作增强工具，对输入图像应用随机有限的大小调整、旋转、线条粗细变化和图像翻转。这使我们能够创建一个比简单地绘制更多符号更多样化、更大的数据集。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mj"><img src="../Images/a2b3f413bdcfa8d73fdceaf7f86c03ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4EbfeLao7hVT8SNOGhb1WA.png"/></div></div></figure><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mk"><img src="../Images/3140c9482a9683855474008a37b55c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v79_OAoAI72zJqyEzAgHNw.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图3。旋转符号和向符号添加线条粗细的示例</figcaption></figure><h2 id="544b" class="lv jg hi bd jh lw lx ly jl lz ma mb jp iq mc md jt iu me mf jx iy mg mh kb mi bi translated">k-最近邻</h2><p id="56b5" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">我们的第一组结果来自KNN算法的多次实现。这里，每个训练图像都被看作是空间中的一个点，空间的维数与像素的数目一样多，然后计算被分类的图像和训练图像之间的距离。因此，举例来说，一个全白的图像会“接近”一个非常明亮的图像，但“远离”一个全黑的图像。然后，检查该空间中每个最近邻的类，具有最接近实例的类就是预测类。</p><p id="e98b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法的最大优点是简单。达到的精确度很低，但比机会好。使用大量生成的训练样本大大提高了准确性，但鉴于我们拥有的真实数据样本如此之少，这可能是由于过度拟合。此外，应当注意，当生成数据和应用模型时，生成超过100，00 0个符号需要相当多的计算时间。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es ml"><img src="../Images/6a9eeec40acca78816393ddbb44559b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*rP4syeRM3KLSKNruUjkJ1w.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图4。基于生成符号数量的KNN精度。</figcaption></figure><h2 id="fa56" class="lv jg hi bd jh lw lx ly jl lz ma mb jp iq mc md jt iu me mf jx iy mg mh kb mi bi translated"><span class="l ki kj kk bm kl km kn ko kp di"> C </span>关于选择性神经网络</h2><p id="a01b" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">从之前的结果来看，简单的KNN并不能解决我们所有的问题(😞)，最初采取的另一种方法是创建我们自己的分类CNN。自Alex net[4]于2012年首次亮相以来，它一直是用于图像识别和检测任务的最受欢迎的网络类型之一。简而言之，CNN背后的主要思想是一大组“过滤器”，每个过滤器捕捉一种特定的图像模式，然后将它们组合在一起，以决定整个图像显示的是什么。</p><p id="9559" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了训练CNN模型，从现有符号生成训练数据集。对于所有的训练符号，应用旋转、翻转、倾斜以及改变符号的大胆度。此外，对于一些符号，随机像素被改变为相反的颜色。通过系统地和随机地使用这些扩充，我们生成了具有361 640个符号的数据集。为了训练模型，5%的数据用于验证。</p><p id="89c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在构建CNN架构之前，人们观察到符号主要由原始形状组成，如直线、曲线、圆和箭头。此外，符号将在一个清晰的背景，这将消除很多噪音。由于对象的简单性，我们决定研究具有三个隐藏层的架构。前两个是卷积层，最后一个隐藏层是全连接层。对于两个卷积层，滤波器的数量被选择为50。给定的尺寸似乎工作得很好，并且在训练期间没有改变。层中的过滤器尺寸是9x9像素。与较小尺寸相比，9x9尺寸的过滤器尺寸给出了更好的结果。使用3×3滤波器时，模型不如使用9×9滤波器精确，并且模型过度拟合。使用3×3过滤器的训练和验证准确度之间的差距为3%。使用9x9过滤器时，没有间隙。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es mm"><img src="../Images/58a2e5990f2c76e86dbaf5358f87245b.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*-O5n5QFVqXhJfSSq_NBfyA.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图5。我们CNN模型的完整架构，从100x100灰度图像到分类。</figcaption></figure><p id="9ddf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于训练的超参数有:</p><pre class="kr ks kt ku fd mn mo mp mq aw mr bi"><span id="6062" class="lv jg hi mo b fi ms mt l mu mv">learning rate = 0.0005, batch size = 64, epochs = 10</span></pre><p id="7047" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从图5中模型的训练历史中，我们可以看到该模型相当快地达到了良好的准确性，然后逐渐保持改进。值得记住的是，训练和验证数据都来自相同的生成过程，因此模型的实际准确性可能会稍低一些。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mw"><img src="../Images/890ef51b058bf1b080d8e59830d96a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WXGt92b8RalEW0DJdspf3A.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图6。CNN训练史。</figcaption></figure><p id="d1d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，由于生成了非常大量的符号，过拟合也可能是非常高的精度的一个可能原因，但这没有经过测试。然而，为了了解CNN在新数据上的表现，它在300个以前从未见过的符号上进行了测试。同样在这种情况下，应该记住用于测试的300个符号与训练符号相似。在测试集的300个符号中，该模型能够对其中的282个进行分类，这给了我们94%的准确率。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mx"><img src="../Images/6a8ee1cf041edb28a9e8fa0c9980b556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P4j7lR7QRHwdHdTja0oNfQ.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图7。CNN混乱矩阵。</figcaption></figure><p id="0e90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">混淆矩阵如图6所示。我们看到18个错误中有11个混淆了封面和屏幕。这些符号非常相似。唯一的区别是一个有字母C，另一个有字母S。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es my"><img src="../Images/b57a9d8f8f9465d1c4c6516dd3712e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4EIu-3rNC1JUz_YvBM1WHQ.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图8。一些过滤器的例子摘自我们自己的CNN。直观地说，你可以认为每个过滤器都在“寻找”它所学习的模式，当在图像中找到该模式时就激活。</figcaption></figure><h2 id="0429" class="lv jg hi bd jh lw lx ly jl lz ma mb jp iq mc md jt iu me mf jx iy mg mh kb mi bi translated">下一站:终点站</h2><p id="c3e3" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">到目前为止，我们已经花费了大部分的时间在这个项目上，并且已经确定，尽管我们的数据集太小(或者有一些其他未知的问题)而不能用作像KNN这样的东西的基础，但是我们可以可靠地构建一个CNN来对符号进行分类，因为它们是作为只有一个符号的单个图像呈现在网络上的。这与现实生活中的网络应用相差甚远，但仍能满足我们认为的项目最低可行结果。</p><p id="8e2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，在与我们的主管开会期间，我们讨论了使用YOLOv5 [5]对包含多个符号的图像上的符号进行定位和分类的可能性，这使我们更加接近现实生活中的用例。因此，在同一次会议上，我们决定这将是我们在项目的剩余时间里要做的事情，并设计了一个为YOLO创建数据集和训练网络的粗略计划。</p><h2 id="bc4a" class="lv jg hi bd jh lw lx ly jl lz ma mb jp iq mc md jt iu me mf jx iy mg mh kb mi bi translated">创建数据集</h2><p id="0a64" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">创建数据集的另一种方法是必要的，因为YOLO不是像我们迄今为止那样使用单符号图像，而是使用具有多个符号的图像作为训练基础。对于这种数据集，一般的方法是获取一组包含检测类的图像，然后手动标记它们(非常麻烦)，或者使用Roboflow [6]之类的工具(稍微简单一点)。</p><p id="543f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们已经有了一个450个符号的数据集，我们希望利用它，这意味着以某种方式从这450个独特的图像组合中生成一个YOLO数据集。对于这一点，我们是幸运的！这些图像都有一个白色或灰色的背景，一个黑色的符号，让我们可以将它们拼接在一起，同时准确地知道这些符号将被放置在每张图像的哪个位置。</p><p id="ed07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这正是<em class="jd"> YOLOtrainer </em>笔记本的<em class="jd">生成YOLOv5数据集</em>部分要完成的工作:</p><ol class=""><li id="1c6d" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">每个单个符号图像及其标签都是从指定的源文件夹加载的，该文件夹可以有多个子文件夹</li><li id="2a3d" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">自适应阈值被应用于每幅图像，试图均衡亮度</li><li id="a5e4" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">加载的图像被传递到生成器函数，该函数可配置最终图像的大小、每个图像上的符号数量等。</li><li id="2279" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">在生成过程中，每个符号都添加了前面提到的增强功能:旋转、翻转、调整大小、加粗、扭曲。</li></ol><p id="fcde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.返回最终镶嵌图和其上符号的相关数据。</p><p id="1c02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.图像和符号数据以YOLO格式保存。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mz"><img src="../Images/b0642f2e93571e722e26a3c9b38363ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ia-hpBh_gbCQ6nzP__yo2w.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图9。生成的多符号图像的早期迭代示例</figcaption></figure><p id="6440" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过图10，我们可以说明符号镶嵌生成系统面临的主要挑战:阈值处理有些过于苛刻，难以获得精确的结果，因此它可以正确处理具有各种亮度的图像。最终我们得到了大部分正确的结果，如图11所示。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es na"><img src="../Images/3c6074c6915a8b7ba1236773e56a8c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xn_Ju2hJzILFotpBGUdQgw.jpeg"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图10。最终生成的数据集示例</figcaption></figure><h2 id="f6c2" class="lv jg hi bd jh lw lx ly jl lz ma mb jp iq mc md jt iu me mf jx iy mg mh kb mi bi translated">什么是YOLO？</h2><p id="2d6d" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">YOLO是首字母缩写，意思是你只看一次。它目前正处于积极开发的第五代，通常以其速度和准确性而闻名。可用的预训练权重在COCO数据集上进行训练，并具有很好的性能。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es nb"><img src="../Images/b65aca287f2df0f52eb5150f3a733276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*30DDNAgPcn179aldRdxU9A.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图11。YOLOv5在COCO数据集上的检测结果[5]</figcaption></figure><p id="e55c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在COCO数据集上进行预训练是我们使用YOLO作为项目问题解决方案的另一个绿色信号，因为COCO数据集包括交通标志的类，就形状而言，这些标志有点类似于我们希望检测的符号。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es nc"><img src="../Images/9a365086083d946e13206119ed5b487b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*_nzpRagS5QbHjJnDsGkAew.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图12。YOLO物体探测过程[7]</figcaption></figure><p id="4fe1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLO网络架构由三个主要部分组成:</p><ol class=""><li id="deea" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">主干网-以不同粒度(不同细节层次)形成图像特征的卷积网络。</li><li id="257a" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">颈部——混合和组合这些特征，将它们传递到头部。</li><li id="0135" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">Head基于获取的要素处理预测。</li></ol><p id="76ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于定制训练的YOLO模型，有多个指标来帮助评估它:</p><ul class=""><li id="25c8" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc nd ln lo lp bi translated">对象损失-显示运行检测的每个分段像元的损失</li><li id="a625" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated">分类损失-如果检测到对象，则根据每个类别的概率计算该损失</li><li id="7490" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated">框丢失-显示基础真实值和检测到的边界框之间的误差</li></ul><p id="c8ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练评估期间，这些都可以被不同地赋值:当您想要非常精确地检测边界框时，低框丢失是重要的，但是当主要焦点是检测特定类时，低框丢失就不那么重要了，在这种情况下，分类丢失是最重要的。物体损失是一个很难分类的度量，但是涉及到在每个检测单元中物体存在的检测。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es ne"><img src="../Images/35fb70b3baf0a8139fa7fb6b4633cdcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*9Bav4Yi_OEhOzrjPkPYnTA.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图13。精确度和召回率的计算</figcaption></figure><p id="10eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，precision和recall也可用于评估训练好的模型，precision显示真实结果与实际结果的比率，recall显示真实结果与预测结果的比率。</p><p id="b9f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLO数据集由每个影像的两个项目组成:影像本身，以及一个详细描述影像中对象位置、其边界框及其类别的文本文件。在运行时，YOLO也可视化这些输入边界框，如图15所示。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es nf"><img src="../Images/9c4fa45a78cc303f151f1dd7bd1eec2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJOncJbWtDz0LYMsattcqg.jpeg"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图14。YOLO包围盒地面真相调试图像</figcaption></figure><p id="31e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了这两个文件，还有一个<em class="jd">。训练脚本检查yaml </em>配置文件以获取数据集的位置、类的名称和数量。</p><h2 id="a9dc" class="lv jg hi bd jh lw lx ly jl lz ma mb jp iq mc md jt iu me mf jx iy mg mh kb mi bi translated">最终方法</h2><p id="b38e" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">运行了多个训练迭代。更成功的运行日志是由一个名为<a class="ae je" href="http://wandb.ai" rel="noopener ugc nofollow" target="_blank"> wandb.ai </a>的内置运行监控应用程序记录的。所有这些成功的运行可以进一步探讨<a class="ae je" href="https://wandb.ai/erkoiv/NATO-Symbols-Log?workspace=user-erkoiv" rel="noopener ugc nofollow" target="_blank">在这里。</a>为了运行，使用了各种硬件:运行RTX 2070、谷歌Colab GPU和塔尔图大学HPC的本地笔记本电脑。</p><p id="b212" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用的一般训练命令如下:</p><pre class="kr ks kt ku fd mn mo mp mq aw mr bi"><span id="d2ce" class="lv jg hi mo b fi ms mt l mu mv">python3 {train_dir} --img 640 --batch 24 --epochs 300 --data NATO-Symbols.yaml --weights yolov5m.pt --project NATO-Symbols-Log --name {run_id} --cache</span></pre><p id="234c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，您可以看到运行时调用的多个变量和参数集。</p><ul class=""><li id="2a6e" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc nd ln lo lp bi translated"><em class="jd"> train_dir </em>是一个指向YOLOv5训练脚本的变量</li><li id="5fcd" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated"><em class="jd"> img 640 </em>设置每个训练图像的尺寸，允许用户在数据集中拥有任何尺寸的图像</li><li id="fc8a" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated"><em class="jd">批次24 </em>给出批次大小</li><li id="c483" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated"><em class="jd">时期</em>决定运行训练的迭代次数</li><li id="48aa" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated"><em class="jd">数据</em>将训练脚本指向自定义<em class="jd">。之前描述的yaml </em>文件</li><li id="17e7" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated"><em class="jd">重量</em>决定使用哪个起始重量，这可以是默认的YOLO预训练重量，也可以是自定义训练重量</li><li id="3ca3" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated"><em class="jd">项目</em>决定保存运行日志的文件夹名称</li><li id="82dd" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated"><em class="jd">名称</em>决定了运行名称</li><li id="bb8f" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nd ln lo lp bi translated"><em class="jd">缓存</em>决定数据集是否缓存在RAM中，以便在训练期间更快地访问</li></ul><p id="29a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLO的另一个特点是，训练脚本包含一个标志，用于为YOLO框架的约25个超参数运行遗传算法(GA)超参数进化。这是一个时间和资源密集型过程，使用以下命令运行了100代10个时期:</p><pre class="kr ks kt ku fd mn mo mp mq aw mr bi"><span id="7d5a" class="lv jg hi mo b fi ms mt l mu mv">python3 {train_dir} --epochs 10 --data NATO-Symbols.yaml --weights yolov5m.pt --project NATO-Symbols-Log --name {run_id} --cache --evolve</span></pre><p id="dbbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在跑步监测站点有更多的跑步可供探索，但是这里将进一步比较和分析使用不同硬件、不同基重尺寸和不同超参数的三次跑步。</p><p id="9840" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这三次运行被称为“5m-300-colabGPU”、“5l-300-hpc”和“5m-300-local”。就项目生命周期而言，“colabGPU”运行最晚，“local”运行最早。从运行名称后缀可以明显看出所用的硬件。前缀“5m”或“5l”表示所用基重的尺寸，中型为<em class="jd"> m </em>，大型为<em class="jd"> l </em>。没有使用小的权重，因为它们意味着用于移动应用的训练网络。在HPC上运行时使用了更大的砝码，因为我们可以使用超过15GB的VRAM，这是使用砝码以合理的批量(&gt; 10)运行训练所必需的，因为需要的VRAM数量会随着砝码设置的增加而大幅增加。对于“5m-300-colabGPU ”,使用定制的进化超参数，其他运行使用默认超参数。所有运行都运行到完成300个周期。</p><div class="kr ks kt ku fd ab cb"><figure class="ng kv nh ni nj nk nl paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/c262ce6d7ed279aca49049bb26f1cb62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*5Kne0Ji3lg7xhRideRNcwg.png"/></div></figure><figure class="ng kv nh ni nj nk nl paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/df5b64425a2b285698dadb80038283be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*I9z4UNaMohAxIuMoH0KH5w.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx nm di nn no translated">图15。“5m-300-colabGPU”、“5l-300-hpc”和“5m-300-local”运行的箱型和类损失图</figcaption></figure></div><p id="7e81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先让我们看一下运行的类和盒损失图。在训练期间，监测的是损失是否随着时间的推移开始增加，这可能表明过度拟合。这种情况并没有发生，此外，我们可以看到，与其他运行相比，演进的超参数运行显示了更快的损耗降低和更低的盒损耗，尽管HPC运行是在更大的权重集上运行的。</p><div class="kr ks kt ku fd ab cb"><figure class="ng kv nh ni nj nk nl paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/eee4fb8b636313dfca9801f31609117c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*FLXPrN3Acf1RfhJ5hFvL3g.png"/></div></figure><figure class="ng kv nh ni nj nk nl paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/0e5a4153e2b454eaa62a1b896a4d378b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Dm9cg-Bj56Gsub2ZY4n17w.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx nm di nn no translated">图16。“5m-300-colabGPU”、“5l-300-hpc”和“5m-300-local”运行的精度和召回图</figcaption></figure></div><p id="7611" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其次，训练过程中的精度和召回指标表明，虽然模型最终在这两个方面都达到了相同的水平，但超参数进化加快了训练过程，以便在更强大的硬件上与更大尺寸的权重相媲美。这凸显了超参数进化的重要性。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es np"><img src="../Images/6d4a1ba24a9e5c0ac2a5bbf4caf17692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYgUkDp5nBq1Zpv9GWWYtA.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图17。“5m-300-colabGPU”、“5l-300-hpc”和“5m-300-local”运行的平均精度指标</figcaption></figure><p id="c9bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，添加另一个度量mAP_0.5:0.95，它综合了所有类的精度和召回率，并总体显示了模型的整体性能，我们可以再次看到超参数进化的“colabGPU”运行比其他模型训练得更快，并达到了明显更高的结果。</p><p id="b603" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对生成的图像运行时，在理想条件下的性能如图19所示。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es na"><img src="../Images/09fa6eefa32d73a159bedcf879683542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3J3VLTl82ASM5JuNeTBeXA.jpeg"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图18。理想测试案例</figcaption></figure><p id="cc7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如所担心的，“屏幕”符号被误标为“封面”，但除此之外，一切都是正确和自信的。</p><p id="e8ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们看看这个模型是如何经受住一个真实的、非理想的、带有符号的图像的挑战的，而这些符号还没有被教给它。</p><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es nq"><img src="../Images/db85e50018424f7005daeaf393d28bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Q8-jxqGGrXk3dBK_2eRLw.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">图19。更现实的测试案例</figcaption></figure><p id="4e64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以清楚地看到，虽然已经正确地识别了几个符号，但是还存在几个没有检测到教授类别的符号的情况，以及几个更多的对非教授符号的错误检测。</p><p id="e814" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在真实图像上的这种低性能很可能是由于训练数据集没有正确地表示真实情况而导致的。无论这是由于缺乏数据，还是数据质量，或者数据生成方法，都必须在未来进行检验。</p><p id="fd47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在未来，可能的解决方案将是手动标记数百个真实图像上的数百个符号，并使用非生成的数据集，并在训练数据中包括非符号，以便模型也知道要检测什么<strong class="ih hj">而不是</strong>。一种完全不同的方法是将YOLO仅用作定位工具，教导一个模型检测任何符号，然后将裁剪后的符号传递给不同的网络进行检测。</p><h1 id="c5fa" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><span class="l ki kj kk bm kl km kn ko kp di"> C </span>结束语</h1><p id="b004" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">这个项目试图通过帮助人类更快地相互传达用符号表示的作战计划来改善军事通信。</p><p id="2b55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过创建脚本来扩充现有数据并人工生成更多数据，解决了数据匮乏的问题。</p><p id="fc19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一开始，我们测试了许多不同的解决方案，作为一种初步的“尝试”。发现由于缺乏数据和可能的过拟合，简单的KNN不能解决这个问题。接下来测试了一个定制的CNN，当应用于单符号图像时显示出了希望，尽管过度拟合可能是成功的原因。</p><p id="1091" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了在多符号图像上实现定位和分类，选择YOLOv5作为合适的框架，并在多个硬件和超参数设置上训练模型。通过度量，最佳性能模型的应用结果在理想情况下被证明是好的，但是在真实情况下严重缺乏。</p><p id="2bed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于未来的改进，我们建议创建一个更好的数据集，用于更准确地训练真实世界，甚至可以将YOLOv5与不同的模型结合使用，以利用不同网络的不同优势。</p><h1 id="ea1e" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">来源</h1><p id="3654" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">[1] <a class="ae je" href="https://www.science.org/content/article/human-speech-may-have-universal-transmission-rate-39-bits-second#:~:text=Indeed%2C%20no%20matter%20how%20fast,the%20speed%20of%20Morse%20code." rel="noopener ugc nofollow" target="_blank">人类语音可能有一个通用的传输速率:每秒39比特</a></p><p id="9763" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2] <a class="ae je" href="https://www.cimic-coe.org/resources/external-publications/app-6-c.pdf" rel="noopener ugc nofollow" target="_blank">北约联合军事符号</a></p><p id="6c2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]<a class="ae je" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">MNIST数据库</a></p><p id="6e27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4] <a class="ae je" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> AlexNet论文</a></p><p id="3842" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【5】<a class="ae je" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank">yolov 5 GitHub</a></p><p id="36d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6] <a class="ae je" href="http://roboflow.com" rel="noopener ugc nofollow" target="_blank"> Roboflow </a></p><p id="eedb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7] <a class="ae je" href="https://arxiv.org/pdf/2004.10934.pdf" rel="noopener ugc nofollow" target="_blank"> YOLOv4纸</a></p></div></div>    
</body>
</html>