<html>
<head>
<title>Deploy and Serve AI Models (Part-1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">部署和服务人工智能模型(第1部分)</h1>
<blockquote>原文：<a href="https://medium.com/codex/deploy-and-server-ai-models-part-1-bf309dc41f4b?source=collection_archive---------7-----------------------#2021-07-28">https://medium.com/codex/deploy-and-server-ai-models-part-1-bf309dc41f4b?source=collection_archive---------7-----------------------#2021-07-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/35a7beb34e941d3bb1d11dbb2ae08e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/0*DJpXJbVYrhNQjrCk.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">来源Nvidia</figcaption></figure><p id="8efb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">模型创建只是创建真实世界AI解决方案的一步。此外，AI模型需要部署、托管和服务，以便在现实世界场景中运行输入的预测、检测和分类(通常运行模型推理)。大多数人工智能应用程序都有以下工作流程</p><ul class=""><li id="424c" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">基于模型捕获输入数据。例如用于对象检测的来自视频源的帧。</li><li id="4bd9" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">对输入数据进行预处理。大多数模型期望输入数据具有特定的格式或维度。示例将图像裁剪成特定的大小，应用标准化或改变数据的维度等。</li><li id="ada4" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">使用对象检测、预测、识别或分类的输入数据运行模型推理</li><li id="83d6" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">对推理结果进行后处理。有时需要根据阈值从输出中丢弃数据，或者需要为对象检测结果选择最合适的边界框(非最大抑制)。</li></ul><h2 id="ced4" class="kc kd hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">人工智能模型的推理</h2><p id="d7a5" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">深度学习推理是运行经过训练的DNN模型的过程，以针对以前从未见过的数据进行检测、预测或分类。因此，在人工智能推理应用中，将经过训练的DNN模型部署到适当的平台是非常重要的，因为它需要更多的计算和存储能力。AI应用程序的性能和延迟(从向DNN输入数据到收到结果的响应时间)取决于所部署系统的计算能力。所以大多数时候人工智能训练的模型应该被优化(剪枝和量化)以降低计算能力和延迟。</p><p id="93c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">模型推理可以在内部完成，也可以在云中使用CPU或GPU完成。以便可以从桌面、移动或web应用程序或云服务访问这些内容。</p><p id="a670" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，我们将关注模型部署/推理。部署策略应取决于应用类型(桌面、web应用等)、模式(离线或在线)或实时和延迟。</p><h2 id="888d" class="kc kd hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">人工智能模型的离线部署</h2><p id="95b2" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">如果您的应用程序对internet的访问有限，或者网络通信时间在您的应用程序决策过程中至关重要，那么这种方法会更好。例如，火灾探测系统应该从摄像机实时产生低延迟的警报。在这种情况下，人工智能模型需要与应用程序一起提供，并在内部运行推理。</p><p id="93d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">可以在用户PC或单板计算机(如nano jetson或raspberry pi)上运行的独立应用程序更适合这种类型。这种方法的主要缺点是，主机应该像GPU一样具有高计算能力，以便通过并行化深度学习执行来加速。</p><p id="90be" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您计划跨平台开发，QT或Python是构建这类应用程序的合适框架。QT是一个强大的框架，可以在c++中构建跨平台的独立应用程序，还提供python绑定来在python环境中构建应用程序。如果您的应用程序是性能关键型的，QT c++是一个合适的选择。大多数ML框架如TensorFlow、PyTorch都提供python和c++环境下的API。</p><h2 id="23dd" class="kc kd hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">人工智能模型的在线部署</h2><p id="ea58" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">经过训练的AI模型可以部署在云中或本地服务器上，并且可以通过网络连接从客户端应用程序访问，以进行模型推断。这种方法适用于所有类型的应用程序，但是与离线应用程序相比，您需要考虑网络延迟。这通过使用HTTP/REST和GRPC协议为客户端应用程序提供了远程推理能力。使得用户不需要像GPU那样配备很大的计算能力，降低了成本。有很多服务提供商像AWS、Azure、GCP等提供云设施来部署和托管你的ML模型。但是我更关注推理服务器。</p><p id="9942" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae lc" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> Tensorflow serving </a>和<a class="ae lc" href="https://github.com/triton-inference-server/server" rel="noopener ugc nofollow" target="_blank"> Nvidia Triton推理服务器</a>是最受欢迎的推理服务器，允许托管您的模型。然而，Triton服务器支持多种框架，如TensorFlow、TensorRT、PyTorch、ONNX运行时，甚至自定义框架后端。此外，它支持为推理服务创建定制的python后端。它提供了选择项目框架的灵活性。Triton高性能推理允许在GPU上并发运行模型以最大化利用率，支持基于CPU的推理，并提供模型集成和流推理等高级功能。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/75fd41edcc62e230d09c00210b9f0939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/0*OaybJ-ooiFmIs-68.jpg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">来源<a class="ae lc" href="https://developer.nvidia.com/sites/default/files/akamai/ai-for-enterprise-print-update-to-triton-diagram-1339418-final-r3.jpg" rel="noopener ugc nofollow" target="_blank">英伟达</a></figcaption></figure><p id="292f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Triton服务器优势</strong></p><ol class=""><li id="3f36" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn li ju jv jw bi translated">从本地存储或云平台加载模型</li><li id="27fd" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn li ju jv jw bi translated">无需重启服务器即可轻松更新模型</li><li id="dce2" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn li ju jv jw bi translated">从相同或不同的框架中运行多个模型</li><li id="77ac" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn li ju jv jw bi translated">支持实时推理和批量推理</li><li id="a889" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn li ju jv jw bi translated">支持模型集成</li></ol><h2 id="8e01" class="kc kd hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">结论</h2><p id="ce44" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">这个主题只是讨论了ML模型的基本部署策略，以便为最终用户构建应用程序。在接下来的系列中，我将更多地关注如何使用推理服务器(如服务于T1和T2的Tensorflow)来服务ML模型，以及如何在没有单独服务器的情况下适应后置和预处理逻辑(整体模型)。</p></div></div>    
</body>
</html>