<html>
<head>
<title>Four pillars of Machine Learning #2 — Linear algebra and calculus</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的四大支柱# 2——线性代数和微积分</h1>
<blockquote>原文：<a href="https://medium.com/codex/foundation-of-machine-learning-2-linear-algebra-and-calculus-9783d4592b98?source=collection_archive---------2-----------------------#2022-01-15">https://medium.com/codex/foundation-of-machine-learning-2-linear-algebra-and-calculus-9783d4592b98?source=collection_archive---------2-----------------------#2022-01-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f270a21617f63a460d41b6a78cd84b36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*R_MZlGnN_FqMK4To"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@pyssling240?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">托马斯T </a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="0eb8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在早期的帖子中，我们已经讨论了机器学习的两个支柱“统计”和“概率”，这是清晰而轻松地掌握ML的所有概念所必需的。这样，我们也可以很好地完成<a class="ae iu" rel="noopener" href="/@harshit_yadav/getting-familiar-to-the-world-of-machine-learning-ea31974ed0c4">熟悉机器学习世界</a>中讨论的“数据预处理”和“数据分析”。我们将学习模型训练和评估所需的代数和微积分。</p><h1 id="02a4" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">3.代数学</h1><p id="456e" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">机器学习是关于多变量方程和求解它们的。猜猜线性代数是什么？是的，线性代数是数学的一个分支，它处理像- <code class="du kw kx ky kz b">y = mx + c</code>这样的线性方程及其在向量空间中的表示(向量空间是指当一组向量连同标量被用来描述感兴趣的点时)。线性代数是一个广阔的领域，可以有多种解释。尽管如此，我们将只研究那些本质的东西，跳过任何不必要的解释。</p><h2 id="dc6d" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">3.1标量</h2><p id="f0d2" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">即使任何人是机器学习的新手或者没有听说过线性代数或标量。但应该已经学过基础数学，像加法、乘法等运算。每个操作都包括一个使用这些操作进行操作的数字；这些数字被称为标量。像什么东西的成本或者温度或者像<code class="du kw kx ky kz b">y = 2*x + 3</code>这样的方程都是用标量来表示的，在上面的方程中，<code class="du kw kx ky kz b">y</code>，<code class="du kw kx ky kz b">x</code>，2，3都是标量。</p><blockquote class="lo lp lq"><p id="6849" class="iv iw lr ix b iy iz ja jb jc jd je jf ls jh ji jj lt jl jm jn lu jp jq jr js hb bi translated"><strong class="ix hj">注意</strong>:在整个系列中，我们将使用不大写的小写字母来表示标量</p></blockquote><h2 id="f9f9" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">3.2矢量和矩阵</h2><h2 id="d12d" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">3.2.1矢量</h2><p id="157c" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">许多人可能认为，如果我们可以用标量的形式来表示一切，为什么我们需要向量或矩阵。想想我们将如何表示整个标量集合。设想一个包含有150个或更多房间数量、大小等特征的房屋的数据。现在，如果我们想讨论一个房子的单个实例，编写所有150个标量来表示一个房子将是非常乏味的；在这里，矢量来拯救。把向量想象成标量的集合；它可以写成</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/fb7128443b694058f9d13b80acbcabb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*0_8UOHj6UlIA839xUrTrFA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">向量。图片来源:自制</figcaption></figure><p id="af4b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里x₁、x₂、… xₙ都是标量，而<strong class="ix hj"> x </strong>是代表它们集合的向量。上图中的向量称为列向量，在大多数文献中，都使用列向量</p><blockquote class="lo lp lq"><p id="87c2" class="iv iw lr ix b iy iz ja jb jc jd je jf ls jh ji jj lt jl jm jn lu jp jq jr js hb bi translated"><strong class="ix hj">注</strong>:在这整个系列中，向量将用粗体的未大写字母表示，如<strong class="ix hj"> x </strong></p></blockquote><h2 id="e102" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">矩阵</h2><p id="a7af" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">使用类似的思维过程，我们将如何表示向量的集合，整个数据集，或几个方程的集合。这就要求我们使用二维或多维的东西；关于维数，我们指的是一个矩阵的维数，就像上面的例子，维数向量是(n×1)。</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/74369545f9a665cd6eed7c8c625d980e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxojIKeTBW0xQ8Mqua4QFg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">矩阵表示。图片来源:自制</figcaption></figure><blockquote class="lo lp lq"><p id="8763" class="iv iw lr ix b iy iz ja jb jc jd je jf ls jh ji jj lt jl jm jn lu jp jq jr js hb bi translated"><strong class="ix hj">注</strong>:在整个系列中，矩阵将用粗体大写字母表示，如<strong class="ix hj"> A. </strong></p><p id="b45d" class="iv iw lr ix b iy iz ja jb jc jd je jf ls jh ji jj lt jl jm jn lu jp jq jr js hb bi translated"><strong class="ix hj">注</strong>:矩阵可以有更高的维数，如(m x n x k)。但是在这个系列和ML中，我们将仅限于二维矩阵。</p></blockquote><h2 id="630d" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">3.3向量和矩阵运算</h2><h2 id="ce6d" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">向量</h2><p id="7902" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><strong class="ix hj"> a)范数:</strong>用外行人的话来说，向量的范数告诉我们向量的长度。之前，我们讨论过矢量的大小。这里的大小是指维度。这与向量的长度不同。向量<strong class="ix hj"> a </strong>的范数或长度|| <strong class="ix hj"> a </strong> ||由下式给出:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/5c182dbfb6de0f1d198623445eb39b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*HUm9YZ3OkHJFWkOYWj34mw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">向量的范数。图片来源:自制</figcaption></figure><p id="c182" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> b)向量加减:</strong>只能对两个等长的向量进行加减运算。通过在给定索引<strong class="ix hj">处增加或减少单个元素来计算新向量。</strong>对于两个给定的向量，<strong class="ix hj"> x，</strong>和<strong class="ix hj"> y </strong></p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/f70f3dbc405a3ab649115d8dd647092f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*Cid_UDLT0DLYN2zDmXtdhQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">向量加法。图片来源:自制</figcaption></figure><p id="6426" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> d)点积:</strong>简单来说两个向量的点积就是向量中每个元素的乘积之和，即第一个元素乘以第一个元素，第二个元素乘以第二个元素，以此类推。<strong class="ix hj"> </strong>对于两个给定的矢量，<strong class="ix hj"> x </strong>和<strong class="ix hj"> y </strong>的点积由下式给出:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es md"><img src="../Images/a466c21d72e22f316c6791496465f7bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*Px79VeadAXLWeUmhZm2zLw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">两个向量的点积。图片来源:自制</figcaption></figure><h2 id="d13d" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">3.3.2 <strong class="ak">矩阵</strong></h2><p id="e794" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><strong class="ix hj"> a)转置:</strong>一个矩阵的转置是指一个新的矩阵，其中的行和列被翻转。上标“t”即Aᵀ表示它。</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es me"><img src="../Images/826bc1728acc34b8dfd2a5d756aac062.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*FQJdDek1KK3MiBFzN1Sang.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">矩阵的转置。图片来源:自制</figcaption></figure><p id="6a0b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> b)求逆:</strong>一个矩阵的逆是一个矩阵，当与该矩阵相乘时，给出单位矩阵(单位矩阵是指所有对角线为1，其余元素为0的矩阵，用<strong class="ix hj"> I </strong>表示)。求逆的过程叫求逆。</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/5e279078b84f4dbc7a6e94ebbecd924e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_9iSvX0geXYYHQqNfgoQjA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">矩阵求逆。图片来源:自制</figcaption></figure><p id="fac2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> c)矩阵-矩阵乘法:</strong>矩阵乘法是解决或解释任何机器学习模型时最重要和最常用的主题之一。我们需要为乘法矩阵做行和列的“点积”。下图清楚地显示了这一过程:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/47d8e54c3bc926394dc931e153e63a64.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*NrZ069XMuR5kzLR5WGPSUw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">两个矩阵相乘。图片来源:自制</figcaption></figure><p id="f78f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> d)行列式:</strong>行列式只能计算方阵(方阵是指行列数相同矩阵)。所以，用det( <strong class="ix hj"> A </strong>)或| <strong class="ix hj"> A </strong> |表示的方阵的行列式，就是边长由A的行给定的盒子的体积，它也告诉我们方阵是否可以求逆，即如果det( <strong class="ix hj"> A </strong> ) = 0，那么这个矩阵就不能求逆。你可以在这里阅读更多关于决定因素的内容。</p><p id="507d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> e)秩:</strong>矩阵的秩是矩阵的独立行向量的个数。使用independent，意味着不能使用其他剩余行向量的向量加法或减法来表示向量。这是机器学习中会用到的，但是如果有人有兴趣更深入的了解，可以参考<a class="ae iu" href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/null-column-space/v/dimension-of-the-column-space-or-rank" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><h2 id="0d43" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">3.4特征向量和特征值</h2><p id="7865" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">特征值和特征向量被认为是线性代数中最重要的课题。寻找特征值和特征向量放弃了对矩阵性质的深入了解。它使各种操作，如寻找电源和许多更容易。</p><p id="9cc8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数学上，对于矩阵，<strong class="ix hj"/>特征值由标量λ表示，特征向量如其名称所示是由以下等式给出的向量<strong class="ix hj"> v </strong>:</p><pre class="lw lx ly lz fd mh kz mi mj aw mk bi"><span id="9544" class="la ju hi kz b fi ml mm l mn mo">                               <strong class="kz hj">Av</strong> = λ<strong class="kz hj">v</strong></span></pre><p id="d78b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">机器学习中的特征向量可以减少特征或维数，这是数据预处理的主要步骤之一，可以使模型训练更快更好。这是因为特征向量可以给出数据的重要特征。要获得更深入的见解，请参考本。</p></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><h1 id="49c8" class="jt ju hi bd jv jw mw jy jz ka mx kc kd ke my kg kh ki mz kk kl km na ko kp kq bi translated">4.结石</h1><p id="ce8d" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">你有没有想过如何证明正方形、长方形等各种图形的面积公式？和我一样，我相信肯定有很多人认为这些公式是微不足道的，但是没有，这些都是可以借助微积分证明的。微积分是一个几乎无处不在的数学领域，从计算斜率、求任何形状的面积，甚至在物理、机器和深度学习中，我们将在后面讨论。并且，那些想要深入研究与模型相关的数学以及模型训练如何工作的人必须至少知道微积分的基础知识，这一点我们将在这里讨论。</p><blockquote class="lo lp lq"><p id="6007" class="iv iw lr ix b iy iz ja jb jc jd je jf ls jh ji jj lt jl jm jn lu jp jq jr js hb bi translated"><strong class="ix hj">注</strong>:那些想知道我们在哪里可以更精确地使用微积分的人。然后，只要知道这一点，我们必须通过一个称为优化的缓慢过程来训练我们的模型；这种优化是在微分的帮助下完成的。</p></blockquote><h2 id="6f43" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">4.1微分和导数</h2><p id="c49d" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">用外行人的话来说，微分就是求任意因变量<code class="du kw kx ky kz b">y</code>相对于任意自变量<code class="du kw kx ky kz b">x</code>的变化率。从属是指<code class="du kw kx ky kz b">y</code>可以用<code class="du kw kx ky kz b">x</code>例子<code class="du kw kx ky kz b">y = 2x</code>或其他任何形式来写。简单地说，我们把速度计算为距离相对于时间的变化率。操作员表示它</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es nb"><img src="../Images/aacb7fd420a275a7eae2445735fad0e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:124/format:webp/1*7WvCT0HU0xgSAvLykfTYTg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">y wrt x .图片来源:自制</figcaption></figure><p id="0724" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它也用一个像f(x)'这样的函数上面的小破折号来表示。导数意味着任何特定函数的微分。下面列出了您需要的一些基本衍生产品:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/38040dc9ff2ebe3e8b150229bac7b540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ky51-RVirl3VLyYJy7_8pg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">基本导数。图片来源:自制</figcaption></figure><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/948cccfd3d91490050dccd72c4e39416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*HP7VvX-bnY3hiTLbdYnofA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">指数导数。图片来源:自制</figcaption></figure><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/5188585320d162c3cfe79ed1eec5b1ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FRZnIczXGwsDRREWGHXr3g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">产品规则。图片来源:自制</figcaption></figure><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/0256961ece110ccade28bfb6c911ff24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*9kpqRIzX91umxoC7RWz2xA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">商数法则。图片来源:自制</figcaption></figure><blockquote class="lo lp lq"><p id="8dd9" class="iv iw lr ix b iy iz ja jb jc jd je jf ls jh ji jj lt jl jm jn lu jp jq jr js hb bi translated"><strong class="ix hj">注意</strong>:如果你不能处理这么多公式，看看它们，并把这篇文章加入书签以备后用。其他想要更深刻理解的人可以参考这个。</p></blockquote><h2 id="3f98" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">4.2偏导数和梯度</h2><h2 id="075f" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">偏导数</h2><p id="30a2" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">导数告诉我们，如果我们将自变量<code class="du kw kx ky kz b">x</code>改变一个单位，因变量<code class="du kw kx ky kz b">y</code>会发生什么变化。如果<code class="du kw kx ky kz b">y</code>只依赖于一个独立变量<code class="du kw kx ky kz b">x</code>，这是正确的，但是如果有两个或更多的独立变量，并且我们只想计算相对于单个变量的变化。例如，假设我们想计算我们的知识相对于时间的变化；这似乎是一个需要通过计算导数来解决的问题。然而，除了时间之外，其他因素也在影响我们的知识，比如我们的专注程度、我们的来源等等。所以，这里我们需要计算知识对时间的偏导数。符号表示它:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/be804890f6d994fb7c022200fbf48c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:214/format:webp/1*S8G7ae4pIz_fEBhKqaF60w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">y wrt x的偏导数图片来源:自制</figcaption></figure><p id="5b37" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们假设一个因变量<code class="du kw kx ky kz b">y</code>依赖于两个自变量<code class="du kw kx ky kz b">x</code>和<code class="du kw kx ky kz b">z</code>，并让关系为<code class="du kw kx ky kz b">y = 2x^2 + z</code>，现在我们要计算<code class="du kw kx ky kz b">y</code>相对于<code class="du kw kx ky kz b">x</code>的偏导数。此外，需要注意的重要一点是，当计算变量的偏导数时，所有其他变量都被认为是常数。从下图中可以看出:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/558bc14321976996f9dd591ffd61ba02.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*oNbdFfDj_NPRnw0kqPg0-g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">偏导数的例子。图片来源:自制</figcaption></figure><h2 id="31a9" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">梯度</h2><p id="0063" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">梯度只是对所有独立变量的偏导数的集合。它以向量的形式表示。给出者:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/f9801b5a7fa1f2d4ca393b0bcbfbb0e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4e_TFXj4bYJpTttTYWb49A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">y的梯度代表所有独立变量。图片来源:自制</figcaption></figure><p id="6660" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如<code class="du kw kx ky kz b">y = 2x^2 + z</code>在偏导数中，梯度是:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/f0242dcbdc8e6bdc716c54be7fd8394e.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*IQvrKUFL69CtXcTfYGY86g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">渐变的例子。图片来源:自制</figcaption></figure><h2 id="dcd1" class="la ju hi bd jv lb lc ld jz le lf lg kd jg lh li kh jk lj lk kl jo ll lm kp ln bi translated">4.3链式法则</h2><p id="35e4" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">现在，考虑一种情况，其中变量<code class="du kw kx ky kz b">y</code>依赖于变量<code class="du kw kx ky kz b">x</code>，变量<code class="du kw kx ky kz b">x</code>也依赖于变量<code class="du kw kx ky kz b">z</code>。现在，如果我们想首先计算<code class="du kw kx ky kz b">y</code> w.r.t对<code class="du kw kx ky kz b">z</code>的导数，我们必须将<code class="du kw kx ky kz b">y</code>的方程以<code class="du kw kx ky kz b">x</code>的形式转换为变量<code class="du kw kx ky kz b">z</code>，在大型方程的情况下，这将成为一个笨拙的过程。在这里，连锁规则来拯救我们。根据链式法则:</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nk"><img src="../Images/b6b0db3798771dcec98fa77960bed1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpfHsRKLEXuSFF1ZaTmDiw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">链式法则。图片来源:自制</figcaption></figure><h1 id="84e3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="8a09" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在今天的博客中，我们讨论了机器学习的两大支柱，它们负责机器学习管道中的所有训练和评估部分。我们首先学习了向量和矩阵的基础知识以及为什么需要它们，然后我们开始了解它们的基本运算。在这一节的最后，我们简要地讨论了矩阵的秩、特征值和特征向量。</p><p id="86b5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这一节中，我们以最简单的方式讨论了导数，并介绍了一些最重要和必需的导数。然后，我们学习了如何计算一个有多个自变量的变量的导数。之后，我们讨论了一种将所有偏导数表示为向量的方法，即梯度。最后，我们讨论了最重要的链式法则。</p><p id="1ed5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">至此，我们完成了机器学习的四大支柱，在下一篇博客中，我们将深入了解监督学习。在这里，我们不仅将学习线性回归、lasso和ridge回归、逻辑回归等基本算法背后的理论和直觉，还将以最简单的方式学习它们背后的数学。</p><p id="e0c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢我们的帖子，请关注我Sarthak Malik和我的同事Harshit Yadav on medium，并订阅我们的邮件列表以获得定期更新，并与我们一起踏上这一旅程。</p><p id="6029" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">谢谢你，</p><p id="df70" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">本系列之前的博客:</strong> <a class="ae iu" rel="noopener" href="/@malikSarthak/foundation-of-machine-learning-1-b21f7b3e5850">机器学习的四大支柱# 1——统计学和概率</a></p><p id="5d74" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">本系列的下一篇博客:</strong> <a class="ae iu" rel="noopener" href="/codex/your-guide-to-supervised-machine-learning-regression-d6b822563e44">你的监督机器学习指南——回归</a></p><div class="nl nm ez fb nn no"><a rel="noopener follow" target="_blank" href="/@malikSarthak"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">萨尔萨克·马利克-中等</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">免责声明:如果你是ML的新手，还没有看过早期的帖子，请访问它们以保持一致。其他人在看…</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">medium.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc io no"/></div></div></a></div><div class="nl nm ez fb nn no"><a rel="noopener follow" target="_blank" href="/@harshit_yadav"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">Harshit Yadav -中等</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">机器学习的鸟瞰图注:这是“完整的机器学习和深度学习…</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">medium.com</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc io no"/></div></div></a></div></div></div>    
</body>
</html>