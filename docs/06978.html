<html>
<head>
<title>How to Increase Computational Efficiency for PReLU in CUDA — OneFlow Performance Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何提高CUDA中PReLU的计算效率——one flow性能优化</h1>
<blockquote>原文：<a href="https://medium.com/codex/how-to-increase-computational-efficiency-for-prelu-in-cuda-oneflow-performance-optimization-20e6e336b8b8?source=collection_archive---------7-----------------------#2022-05-23">https://medium.com/codex/how-to-increase-computational-efficiency-for-prelu-in-cuda-oneflow-performance-optimization-20e6e336b8b8?source=collection_archive---------7-----------------------#2022-05-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/cdb099bf17eddca5db9951cf3b9c9dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LTriBhcZ-QBe9MtmwTsZnQ.jpeg"/></div></div></figure></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="4da4" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj"> <em class="jv">出自郑；胡译，沈佳丽译</em> </strong></p><blockquote class="jw jx jy"><p id="7fb6" class="ix iy jv iz b ja jb jc jd je jf jg jh jz jj jk jl ka jn jo jp kb jr js jt ju hb bi translated">PReLU是InsightFace中经常使用的激活功能。它有两种操作模式:PReLU(1)和PReLU(通道)。对于后者，PReLU相当于二进制广播操作。在本文中，我们将讨论如何优化CUDA中的广播操作。</p></blockquote><p id="5308" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">PReLU是InsightFace中经常使用的激活功能。它有两种操作模式:</p><ol class=""><li id="c3a0" class="kc kd hi iz b ja jb je jf ji ke jm kf jq kg ju kh ki kj kk bi translated">PReLU(1)。权重α的形状为(1，)。在这种情况下，PReLU相当于elementwise操作。</li><li id="69f5" class="kc kd hi iz b ja kl je km ji kn jm ko jq kp ju kh ki kj kk bi translated">预路(通道)。权重α的形状为(channels，)，其大小与输入张量的(N，C，H，W)中的“C”的大小相同。在这种情况下，PReLU相当于二进制广播操作。</li></ol><p id="0944" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">InsightFace采用PReLU的第二种模式。我们之前写过<a class="ae kq" href="https://oneflow2020.medium.com/oneflows-optimization-of-cuda-elementwise-template-library-practical-efficient-and-extensible-b375c3bd15c6" rel="noopener">如何优化CUDA元素式操作</a>。今天，我们将讨论如何优化CUDA中的广播操作。</p><h1 id="ed01" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">1个简单的实现</h1><p id="71c5" class="pw-post-body-paragraph ix iy hi iz b ja lp jc jd je lq jg jh ji lr jk jl jm ls jo jp jq lt js jt ju hb bi translated">这是一个简单的实现。首先，获取循环中当前元素“x”的索引。其次，推断出对应阿尔法权重的指数。第三，通过查看“x”&gt; 0返回结果:如果“x”&gt; 0，则返回“x”；如果在CUDA中“x”&lt;0, return “alpha*x”. The code is as follows:</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="8b55" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">Note:</p><ul class=""><li id="2833" class="kc kd hi iz b ja jb je jf ji ke jm kf jq kg ju ma ki kj kk bi translated"> 【T0】  refers to the product of the values of the dimensions following the channel dimension. Take the  【T1】  format as an example:  【T2】 .</li><li id="3c15" class="kc kd hi iz b ja kl je km ji kn jm ko jq kp ju ma ki kj kk bi translated"> 【T3】  refers to the size of the channel dimension.</li></ul><p id="2ff5" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><a class="ae kq" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#maximize-instruction-throughput" rel="noopener ugc nofollow" target="_blank">整数除法伴随着很高的计算成本。</a>您可以在CUDA工具包文档中找到(第5.4.1章算术指令):</p><blockquote class="jw jx jy"><p id="c26b" class="ix iy jv iz b ja jb jc jd je jf jg jh jz jj jk jl ka jn jo jp kb jr js jt ju hb bi translated"><em class="hi">整数除法和模运算的成本很高，因为它们要编译多达20条指令。</em></p></blockquote><p id="3f77" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">计算α的指数涉及一个整数除法和一个模运算。这些计算代表了内核工作负载的一半。因此，在接下来的内容中，我们将告诉您如何减少整数除法和模运算，同时使用矢量化方法增加读/写带宽。</p><h1 id="2f2d" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">2通过打包类型矢量化进行优化</h1><p id="3ab2" class="pw-post-body-paragraph ix iy hi iz b ja lp jc jd je lq jg jh ji lr jk jl jm ls jo jp jq lt js jt ju hb bi translated">这里有一个简单的情况:如果输入的张量形状是(1，2，4，4)，那么它的操作模式将是PReLU(2)。</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/051ee5feab55a79ae624444880c77462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KGyvV-JvTDU-1fbW"/></div></div></figure><p id="00b2" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">显而易见的是，H和W维度上的输入是连续的。在这种情况下，如果<code class="du mb mc md me b">inner_size</code>可被包装尺寸整除，则包装中的元素将被应用于相同的alpha权重，如下图所示:</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d5972d9503ee0a55466d960c37100a06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w6CG89ojpfOwd-g6"/></div></div></figure><p id="7939" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">这样，矢量化操作有助于提高读/写带宽利用率。并且每个包中的元素只需要计算一次，这减少了大量的计算，因为我们不再需要一个接一个地计算元素。代码如下:</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="1021" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">让我们比较一下Nsight Compute提供的优化操作前后的结果。在A100–40GB GPU上测试数据(96、64、112、112)后，我们得到了下图所示的两个内核的性能结果:蓝色的内核通过向量化进行了优化，而绿色的内核则被天真地实现了。很明显，优化操作有效地减少了20%-30%的计算，并将吞吐量提高了30%。此外，优化后的内核带宽高达1350GB/s，非常接近A100的理论极限。</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/769f88994bd1292a475feae3bdd1de0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GkiduqQRtsElB-l2"/></div></div></figure><p id="5495" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">但是，并非所有张量形状都支持矢量化操作。如果<code class="du mb mc md me b">inner_size</code>不能被它对应的<code class="du mb mc md me b">pack_size</code>整除，这个形状只能满足于一个简单的实现。</p><h1 id="71e4" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">3基准</h1><p id="2d35" class="pw-post-body-paragraph ix iy hi iz b ja lp jc jd je lq jg jh ji lr jk jl jm ls jo jp jq lt js jt ju hb bi translated">在NVIDIA A100–40GB GPU上进行基准测试时，我们比较了OneFlow和PyTorch在InsightFace库中处理不同张量形状时的性能。测试结果如下:</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c0f2948fb43bf9084f94f3f47e786d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BwBsXhu0iVfT6p0_"/></div></div></figure><p id="6a2d" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们可以看到，<a class="ae kq" href="https://github.com/Oneflow-Inc/oneflow" rel="noopener ugc nofollow" target="_blank"> OneFlow </a>，由优化的激活函数-PReLU授权，在大多数情况下比PyTorch提供了近200%的性能。至于最后一个测试示例，张量形状非常特殊，以至于矢量化优化不起作用，因此OneFlow的性能与PyTorch相当。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="b495" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj"> <em class="jv">相关文章:</em> </strong></p><ol class=""><li id="7452" class="kc kd hi iz b ja jb je jf ji ke jm kf jq kg ju kh ki kj kk bi translated"><a class="ae kq" href="https://oneflow2020.medium.com/oneflow-v0-7-0-came-out-4ac0653339d5" rel="noopener"> <strong class="iz hj"> <em class="jv"> OneFlow v0.7.0出来了！</em> </strong> </a></li><li id="ce82" class="kc kd hi iz b ja kl je km ji kn jm ko jq kp ju kh ki kj kk bi translated"><a class="ae kq" href="https://oneflow2020.medium.com/the-execution-process-of-a-tensor-in-a-deep-learning-framework-a4d853645d5b" rel="noopener"> <strong class="iz hj"> <em class="jv">深度学习框架中张量的执行过程</em> </strong> </a></li></ol><p id="db4c" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><em class="jv">欢迎访问one flow on</em><strong class="iz hj"><em class="jv"/></strong><a class="ae kq" href="https://github.com/Oneflow-Inc/oneflow" rel="noopener ugc nofollow" target="_blank"><strong class="iz hj"><em class="jv">GitHub</em></strong></a><strong class="iz hj"><em class="jv"/></strong><em class="jv">并关注我们on</em><strong class="iz hj"><em class="jv"/></strong><a class="ae kq" href="https://twitter.com/home" rel="noopener ugc nofollow" target="_blank"><strong class="iz hj"><em class="jv">Twitter</em></strong></a><strong class="iz hj"><em class="jv"/></strong><em class="jv">和</em> <strong class="iz hj"> <em class="jv"> </em> </strong>T47】</p><p id="ed66" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">还有，欢迎加入我们的<a class="ae kq" href="https://discord.gg/4kpjGA5bZY" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj"> <em class="jv">不和谐群</em></strong></a><strong class="iz hj"><em class="jv"/></strong>讨论和提问OneFlow相关问题，与OneFlow的贡献者和全世界的用户联系。</p></div></div>    
</body>
</html>