# Reddit 文本挖掘与 AWS 人工智能服务和 R

> 原文：<https://medium.com/codex/reddit-text-mining-with-aws-ai-services-and-r-6b055c10725d?source=collection_archive---------5----------------------->

![](img/cc60a40d8697ed7312518397850c45f9.png)

## 了解 r/匈牙利

# 介绍

我一直被 Reddit 社区所吸引，我经常发现自己在大量的渠道中寻找解决方案和项目想法，Redditors 称之为 subreddits。

我意识到这篇博客的标题有点偏离我过去的条目，但它有一个目的。在最近发现 AWS 作为我大学课程的一部分提供的[人工智能服务](https://aws.amazon.com/machine-learning/ai-services/)后，我被指派提出一个用例，这些工具可以足够方便地使用。

因此，结合我对主要基于文本的社交媒体平台的喜爱和我的大学职责，我最终决定尝试一下，对[**r/Hungary**](https://www.reddit.com/r/hungary/)**subreddit**做一次**入门级文本分析。**

*(请注意，我在 NLP 方面没有经验，这只是我的一个小工作。)*

但是你可能会想，为什么是匈牙利呢？为什么不是 r/wallstreetbets 或任何其他国际知名的 subs？因为我住在布达佩斯，这个社区不仅几乎取代了我所有的匈牙利主流媒体，也是我在漫长的工作日之后的必去之地。

在本教程/分析中，我将向您展示如何从您选择的任何子主题中获得快速分析和见解。此外，我将测试 AWS 如何翻译和理解俚语文本。

系好安全带！

> 所有的工件，包括这篇博文背后的 R 脚本都可以在我的 [Github repo](https://github.com/nszoni/reddit-sentiments-aws) 上找到

# 要求

## 稀有

要重现这个工作流，您需要将以下包安装到您的 R 环境中。

*   **数据操作与可视化:**[](https://www.tidyverse.org)**[*stringr、*](https://www.rdocumentation.org/packages/stringr/versions/1.4.0)*[*Data . table*](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html)***
*   ****文本挖掘:** [*tm*](https://cran.r-project.org/web/packages/tm/tm.pdf) *，*[*tidy Text*](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)*，* [*雪球 C*](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf)**
*   ****AWS AI from**[**cloud yr**](https://github.com/orgs/cloudyr/repositories)**:***AWS . translate，AWS . understand***
*   ****访问 Reddit:**[*Reddit extractor*](https://github.com/ivan-rivera/RedditExtractor)**

> ***提示:你可以安装和加载这些库，重复* install.packages() *和* library() *命令，或者让* [pacman](https://www.rdocumentation.org/packages/pacman/versions/0.5.1) *以整齐的方式处理项目依赖关系。***

## **自动警报系统**

**您还需要设置我们的 AWS 凭据，以便能够访问我们的云资源。**

> **N 注意，下面的步骤是在假设你已经有一个 IAM 用户的情况下进行的，如果没有，这里[你可以找到如何做。](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html)**

1.  **在[https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/)打开 IAM 控制台。**
2.  **在导航菜单上，选择**用户**。**
3.  **选择您的 IAM 用户名。**
4.  **打开**安全凭证**选项卡，然后选择**创建访问密钥**。**
5.  **要查看新的访问键，选择**显示**。您的凭据应该类似于以下示例:**

*   **访问密钥 ID: `AKIAIOSFODNN7EXAMPLE`**
*   **秘密访问密钥:`wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY`**

**要下载密钥对，选择**下载。csv 文件**。存储。csv 文件，其密钥与您的 R 脚本位于同一目录。**

**最后，将您的凭证加载到 R 会话中(确保您的`*accessKeys.csv`在您的工作目录中):**

**设置 AWS 凭据**

# **子编辑处理**

## **Reddit API**

**下一步是从公共 Reddit API 中抓取数据。作为一种捷径，我使用了 [**RedditExtractoR**](https://github.com/ivan-rivera/RedditExtractor) ，这是一个在 r 中进行 Reddit API 调用的极简包装器，简而言之，该包可以提取、用户数据、subreddit 帖子，并根据关键字找到线程的 URL。**

```
**#monthly top entries
hungary1 <- find_thread_urls(subreddit="hungary", sort_by="top", period = "month")**
```

**查看响应数据帧，您应该看到与此类似的结构:**

**![](img/5b73e7649949d42e2682fe4e3201be85.png)**

**原始子编辑表**

**在开始处理数据之前，我已经可以对数据做一些初步的评论了。**

1.  **由于**匈牙利语“ő"s”未被正确解析**，编码有点偏离。这是我们的 API 包装器的一个局限，没有明确的方法来补救。**
2.  **合理的做法是**将标题和它们的文本**粘在一起，以最小化将文本检测器算法映射到一列字符串时的信息损失。**
3.  **根据我的分析，Subreddit 和 URL 列是多余的，所以我们只排除它们。**
4.  ****没有正文的帖子应该被删除**，因为有歧义，而且标题通常不够长，无法满足我们的分析要求。**
5.  ****标题/正文中包含 URL 链接**的帖子在处理时可能会导致问题。**
6.  ****过长或过短的文本会使 AWS AI 服务因其字符处理限制(每个请求最多 5000 个字符)而退出**。**

**记住这一点，下面的代码片段应该处理除第一个问题之外的所有问题:**

**数据的初步清理**

****我从 1，000 个观察值开始，在缩小数据集以满足我们的需求后，最终得到了 172 个**。这应该足以呈现我的用例，并尊重我的大学的预算。**

## **超级加工功能**

**现在，我可以开始检测这个子编辑的关键情绪和短语。**

**为此，我创建了一个**超级函数，逐篇遍历我的数据帧**的行，并遵循以下步骤:**

1.  ****检测其语言并通过 *AWS 翻译*将**外国帖子翻译成英语**
2.  ****通过 *tm* 包预处理文本**(即删除数字、标点、空格、大写字母、停用词等。).**
3.  ****检测被处理文本的情感**，**通过 *AWS 理解*提取置信度最高的关键短语****

**处理任何子数据的超级函数**

**我还为该函数配备了一些用于调试目的的[日志](https://github.com/daroczig/logger)，然后将我过滤后的数据输入超级函数:**

```
**> hungary_processed_test <- subreddit_processer(hungary5)
INFO [2021-11-21 12:20:22] Preprocessing post No. 1
INFO [2021-11-21 12:20:22] Translating...
INFO [2021-11-21 12:20:22] Detect sentiments...
INFO [2021-11-21 12:20:23] Detect keyphrases...
INFO [2021-11-21 12:20:23] Post No. 1 processed!
INFO [2021-11-21 12:20:23] Preprocessing post No. 2**
```

**整个过程**花了大约 2.2 分钟完成**，现在我有了更容易理解的东西来做我的分析。**

**![](img/4f3dc7929678c313baf9c847de5cc16e.png)**

**带有情感和关键短语的已处理表格**

**正如你所看到的，超级函数**在消除停用词和各种语法对象**方面做得很好，这些会给我们进一步的数据管理带来问题。然而，我注意到**在将大量非正式的匈牙利文本**翻译成英语时的不一致性，因为在某些情况下，翻译与原文不可比或者根本没有意义。这也部分是由于前面提到的编码问题。**

## **关键短语**

**为了提高我们对子编辑情感的洞察力，我从 AWS 识别的关键短语中构建了一个**单词语料库。之所以对关键短语而不是经过处理的文本进行过滤，是因为我想加倍过滤不属于现成停用词词典的常用词(例如，like、think 等)。).****

**为了创建语料库，我对关键短语列**进行了标记，按频率统计了每个单词。之后，我**对单词列表**进行词干处理，以避免具有不同后缀和前缀的单词的重复，最后将按单词分组的频率加起来**以消除单词条目**的重复。****

**关键短语的后处理**

## **实体**

**[实体检测](https://gist.github.com/nszoni/1d7c7efc78ae6f259f8d920002ed76fc)也是 AWS 理解的一部分，因此，我可以利用它来查看社区的**前 10 个实体。就像关键短语一样，我必须为处理过的文本中出现的每个实体创建一个频率表。****

**实体识别片段**

# **DataViz**

**确保一切正常后，我使用 [ggplot](https://ggplot2.tidyverse.org/index.html) 从我在前面步骤中创建的视图中提取一些视觉洞察。**

## **月度热门帖子的字符分布**

**每篇文章的字符数分布遵循右偏的 T21 分布，平均长度约为 746 个字符。这样的统计数据应该让我对文本分析的可靠性更有信心。**

**![](img/bd215ba989ac774e5e1f06508d4e6520.png)**

**职位特征分布**

## **子编辑的关键字**

**为了说明 r/hungary 子编辑中最常用的短语，我使用了词干化和标记化的关键语料库。以我的经验来看，这个子话题在某种程度上是由政治推动的，这反映了为什么像疫苗接种、政党(例如，青年民主党(Fidesz))**这样的话题会成为最常见的讨论话题。除了有争议的主题，社区成员通常还会就家庭、学校、社会生活和职场相关的观点展开辩论。****

**![](img/9a8adbffc71a9e9db96e15ad82a78a4c.png)**

**常见关键短语的词云**

## **顶级实体**

**让我们看看最热门的月度帖子中排名靠前的是哪些实体。**

**![](img/c96a5447a040507f4bb3b4cd623f03ab.png)**

**社区提及的顶级实体**

**作为一个政党，Fidesz 在与分区国籍直接相关的一般实体中也名列前茅。由于 Reddit 的性质和不正确的编码，我已经可以检测到词干和处理语法不正确的非结构化消息中的异常。**

## **上个月的每周活动排名**

**当我查看按“上月热门”排序的文章的每周活动分布时，我可以观察到**第 42 周在热门帖子**下的评论方面相对来说是一个比较安静的时期。与此同时，上个月剩余几周的活动一直徘徊在五千条评论左右。**

**此外，**上个月热门内容的平均每周评论数为 4，489 条**，考虑到子主题的规模，这一数字令人印象深刻。**

**![](img/b9ba8d24fa1ed388a30cb6a592d15589.png)**

**每周活动指标**

**不幸的是，不可能从 Reddit API 包装器中检索每个帖子的投票数，这将对活动的定义和标准化产生影响。**

## **每周感悟**

**现在我已经看到了关于社区的描述性统计数据，让我们讨论一下在过去的一个月里是什么情绪驱使人们进行互动。下面的比例堆积条形图表明**正极接线柱很少能达到顶部螺纹。**同样，**负面帖子最多的是第 43 周和第 44 周**、**，最少的是第 42 周**。上述时期的消极情绪可以归因于 10 月 26 日游行和一部电影首映后的巨大政治动荡，这部电影分裂了两个政治派别的支持者。**

**![](img/10eb74b3afb8d04a3b3752a044e2762b.png)**

**每周感悟**

> **在美学上有多种颜色的图表中，我使用了 ggplot 的[绿色](https://ggplot2.tidyverse.org/reference/scale_viridis.html)标度，这是一种色盲友好的调色板。**

## **负面帖子最多**

**在最后两个小节中，我将详细讨论上个月最活跃的负面和正面帖子。**

**以下是评论最多、负面情绪最多的前 5 篇帖子:**

**![](img/acd9ada3b589a94a3b9266360d08425d.png)**

**按关键词排列的负面帖子**

**活跃度最高的负面帖子(*【人物】*为关键词)是这样的:**

> **我如何发现我的同事对宗教和上帝相关的事情不感兴趣？…你没有尊重我的不同意见。有没有可能让这样的人不做承诺？…**

**不寻常的措辞，但你得到的想法。这也反映了 AWS 翻译在处理充满城市术语的文本时的**弱点**。此外，为什么宗教不是这篇文章最突出的关键词，这是有争议的。**

## **热门正面帖子**

**正如我之前强调的那样，除了要求文献推荐和一名成员分享他作为外国人学习匈牙利语六年的经验之外，最近几个月的热门帖子中没有太多积极的内容。**

**![](img/6349c4f8a072824cb31ef9a99c8b1384.png)**

**按关键词排列的正面帖子**

**同样，这里有一小段关于文献推荐的正面帖子:**

> **你能推荐一本作者清楚地理解科学、科学哲学，并且贯穿全书的文学作品吗？…我甚至不是说，详细的植物学事实出现了，但是…**

**再说一次，*“植物学事实”*是一个非常具体的关键词，但我不会说它完全代表了帖子本身。**

# **外卖食品**

**总的来说，我很喜欢写这篇博文，不仅因为 Reddit，还因为我能够展示 AWS 的一些独特服务的用例。**

**回想起来，我用来分析 r/hungary 的所有组件都很简单，足以让我快速了解 Reddit 社区。不可否认的是，无论是在来源层面还是处理层面，都还有改进的空间，比如缺乏对帖子的 upvote 数据，进一步的预处理，以及对数据中关键短语的提取进行微调。**

**同样值得研究的是 AWS 的情感检测如何处理由表情符号组成的文本，它与其他云服务的产品相比如何，以及进一步分析结构化正式文本和我在这篇博文中向你展示的文本之间的性能差距。**

> **我希望您喜欢我这次短暂的旅程，并请随时留下您对这些服务的体验！**