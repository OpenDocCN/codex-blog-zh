# 当人们呼唤人工智能的时候

> 原文：<https://medium.com/codex/when-people-call-out-the-ai-5a370bfd6450?source=collection_archive---------12----------------------->

## 第一次算法偏差奖金挑战

许多人认为我们仍然在等待一个终结者时刻，那时我们应该开始担心 AI 的有害“意图”。

![](img/c3e057c30036f5e288baab4487b44877.png)![](img/cc558e0991a79aaef0d838a40df93b04.png)

插图借用了电影《终结者》(左)和《社会困境》(右)

事实上，我们的社会已经在以一种更难以察觉的方式与人工智能融合。目前，人类与人工智能的关系令人担忧地不平衡，这表现在推荐算法、搜索引擎、路线规划者、医疗结果预测者等方面。，归脸书、谷歌、Twitter、Spotify、苹果、亚马逊、微软和大量人工智能初创公司所有。我们向这些算法输入我们的行为数据——点击量、观看次数、观看帖子/视频的时间、喜欢程度、滚动速度等。——反过来，它们形成了我们的品味。这是一个我们突然发现自己处于其中的秘密反馈循环，很少有人意识到这一点。在之前的[帖子](https://anilill.medium.com/how-i-changed-my-gender-bd466e9c555e)中，我提到了我们作为用户可以单独做些什么。现在，我们来看看那些对这些算法拥有一些权力的人的责任。

一些学术研究已经解决了[人工智能公平问题](https://www.sciencedirect.com/science/article/pii/S2666389921000611#bib1)，例如[面部识别中的性别和种族偏见](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)、[谷歌图片搜索](https://www.washington.edu/news/2015/04/09/whos-a-ceo-google-image-results-can-shift-gender-biases/)和 l [语言建模](https://www.pnas.org/content/115/16/E3635)。然而，将这视为纯粹的学术问题是在拐弯抹角，因为每天与数十亿人互动的人工智能算法是由科技公司开发和运营的。

公司市场激励以保持我们对其平台的关注的形式，被构建到这些算法的目标中。这可能会有各种各样的副作用，甚至超过明显的上瘾，例如有迹象表明 YouTube 的推荐系统[会驱使人们转向越来越激进的内容](https://assets.mofoprod.net/network/documents/Mozilla_YouTube_Regrets_Report.pdf)。已经有几家初创公司使用人工智能来[预测面部照片的美丽分数](http://web.archive.org/web/20210307074108/https://www.technologyreview.com/2021/03/05/1020133/ai-algorithm-rate-beauty-score-attractive-face/)，这些照片基于过度代表白人或中国人面部的数据。然后，他们推荐美容产品和整形手术来修复“缺陷”。一项[研究](http://web.archive.org/web/20210310173019/https://aisel.aisnet.org/icis2019/future_of_work/future_work/15/)显示，人们在听到人工智能预测的分数后，给出的分数比以前更接近算法生成的结果。推荐算法正在改变我们的偏好，这种偏好可以通过这种方式不断缩小。
(这里我想指出，你也是在一个商业社交媒体平台上读到这篇文章的，这个平台叫做 [Medium](https://en.wikipedia.org/wiki/Medium_(website)) ，有自己的[商业模式](https://web.archive.org/web/20210324005623/https://www.nytimes.com/2021/03/23/business/media/medium-editorial-buyout.html)。)

在过去的几年里，所有提到的大公司都出现了负责任的人工智能团队。我认为，其中一个原因是监管机构也表现出了对人工智能风险和公平性问题的意识。虽然它仍处于起步阶段，但这可能会成为一种制衡的形式，激励这些公司应对他们的技术带来的潜在危害，如果不是没有颠簸的话。

# Twitter 的第一次算法偏见奖金挑战

去年，用户在 Twitter 的缩略图生成图像裁剪算法上发现了[种族偏见。该公司](https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm)[反应迅速](https://blog.twitter.com/en_us/topics/product/2020/transparency-image-cropping)，并宣布了一种新的缩略图裁剪方法，让用户更容易看到和控制他们的图像看起来会是什么样子。

今年春天，Twitter 成立了一个致力于负责任的人工智能的有前途的团队。他们被称为 [META](https://blog.twitter.com/en_us/topics/company/2021/introducing-responsible-machine-learning-initiative) (代表 ML 道德，透明度和问责制)。

7 月 30 日，他们宣布了[第一次算法偏见奖金挑战](https://hackerone.com/twitter-algorithmic-bias?type=team)，任务是展示这种图像裁剪算法可能带来的潜在危害。关于提到的惨败，他们分享了自己的[代码](https://github.com/twitter-research/image-crop-analysis)和[论文](https://arxiv.org/pdf/2105.08667.pdf)。来自世界各地的人们可以用各种可能揭示更多有害副作用的反面例子来尝试和测试 Twitter 的模式。

我也参加了挑战，非常好玩。虽然我没有获奖，但我从别人的作品中学到了很多。我决定分享我在 DEFCON AIVilage 上提交的获奖作品的摘要。

每一个都强调了不同类型的伤害:

## 如何变得更加突出？

第一名由一个项目获得，该项目成功地利用了社交媒体和人类心理的反馈循环特性，这也是评委和我都非常兴奋的事情。他们生成人脸并对其进行修改，以增加裁剪算法的内部得分。他们发现，让一张脸看起来更苗条、更年轻、肤色更浅或更暖会让它得分更高。这是一个美丽的展示，展示了人工智能如何加剧美丽标准、年龄歧视和种族主义的问题。

![](img/543525f5409299fa4e44b593b5e21c52.png)![](img/1c37a505473b06079715206ca820dd76.png)![](img/9b0abe733b18d428428810c46e5a6f4a.png)

## 集体照中的年龄歧视、能力歧视和种族歧视

二等奖的获得者[分析了集体照片，发现对头发灰白、皮肤黝黑和有残疾的个人有偏见。他们还使用了修改过的照片，将一些人的头发染成白色以显示效果。](https://github.com/erickmu1/Twitter-Algorithmic-Bias)

![](img/b8024b50f5b9b5739f37fe05b9d220fd.png)

原始图像上的焦点

![](img/a1b8255cca47eed7d8b0c6efc6dbaf97.png)

把中间女孩的头发变白后的对焦点。

## 双语模因的“母语凝视”

第三届[获奖者项目](https://github.com/royapakzad/image-crop-analysis)使用双语模因展示了语言偏见。他们发现，该算法更喜欢拉丁文字，而不是阿拉伯文字，这可以表明语言多样性和在线表示方面的危害。

![](img/23741c9985a4daf123a67438acab4e7c.png)

## 老兵、宗教、剧透

另外三个项目获得了荣誉奖。

第一个显示了对迷彩服的偏见。这可能被认为是按预期工作，但是作者想要强调的是，“按预期工作”并不总是意味着对人们来说很好——强调显著性并不总是相关性的良好代理。

第二个项目表明，图像裁剪对与某些宗教团体相关的某些头饰有着压倒性的偏见。

最后一个表明，该算法违反了不共享剧透的社会契约，因为它始终强调漫画的最后一帧。

## 表情符号和黑客边界

**最具创新**奖由一件作品获得，该作品显示出对浅肤色表情符号的偏爱，而不是深肤色表情符号。

最一般化的提交的创作者在图像上插入了一个小边框，人眼看不到，但足以让机器裁剪盒子里的东西。这是一种与之前完全不同的方法，因为它显示了算法的一种偏差，而不是源于数据中的一些人为偏差。相反，它指出了人类和机器的视觉和思维之间的显著差异。这提醒开发人员也要考虑难以捉摸的机器特有的弱点。

## 富人、穷人和有魅力的人

最后，让我分享一下[我对全球阶级偏见的发现](https://www.cl.cam.ac.uk/~alv34/Twitter_Algorithmic_Bias___Report.pdf)，基于收入( [Github 代码](https://github.com/anitavero/twitter_thumbnail))。

我在 3 天内了解了为期一周的挑战，但很幸运地获得了一些由美元街的创造者提供的非常有趣的数据。
我用美元街的收入标签作为代理，测试了便宜的和昂贵的物品的图像算法。我发现算法偏向于*便宜的*房间和空间，偏向于*贵的*对象。这有可能强化关于人们如何生活在不同社会阶层的刻板印象，并可能夸大高收入阶层的说法。

![](img/30720e1f88706f932b940758313a5131.png)

左:*便宜的*，右:贵的洗洁精。

![](img/65d3f78f9546e8150a09861ec369cb82.png)

左:*便宜的*，右:贵的香料。

![](img/eca192e50083f0cf16104b8ae460964b.png)

左:*便宜的*，右:贵的“墙内”。

# 从这里去哪里？

这种奖金挑战的形式为日常人工智能用户开辟了一种新的方式，让他们可以与这种人工智能技术互动，并根据它的偏见召唤它。

在[会议](https://www.twitch.tv/videos/1112111593)期间，Twitter 元评委强调了想法多样性的关键方面。他们报告了通过了解来自五大洲的人们的想法来面对自己的偏见，这些想法是由团体和个人提交的。我认为这可能是这次活动最重要的信息之一。

到今天为止，我认为人类和人工智能之间的关系应该是这样的:

1.  可能需要监管来平衡科技公司的市场激励。
2.  因此，公司有望进一步投资于他们负责任的人工智能研究团队。
3.  人工智能模型不仅应该由公司开发人员和测试人员使用标准软件测试来测试。
    应该有独立责任和透明度测试的要求(可能以法规的形式)。
4.  责任和透明度测试也应该由独立机构进行，这也会激励公司内部的测试。
5.  测试机构的多样性是重中之重。
6.  应该在社会中鼓励关于伦理、社会问题和技术的教育和交流，以促进个人用户对他们每天与人工智能算法互动的认识。
7.  当涉及到法律和政策制定时，伦理一直是哲学的一部分，也是一个实用的话题。即使在后一种情况下，变化也需要几十年或几代人的时间。在 AI 时代，很多问题，比如“美是什么？”，“真理是什么？”，“谁的叙述应该占上风？”突然变成了刻不容缓的现实问题。因此，我认为，今天，关于基本伦理问题的社会对话具有前所未有的相关性。

在我看来，这里讨论的挑战是一个良好的开端。我很高兴看到其他公司也有更多这样的产品。但是我也认为这仅仅是从零开始建立结构和机构的开始，这些结构和机构将处理我们与人工智能融合的快速发展。