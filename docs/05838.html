<html>
<head>
<title>Explainable AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的人工智能</h1>
<blockquote>原文：<a href="https://medium.com/codex/explainable-ai-1d4f6bd1b186?source=collection_archive---------24-----------------------#2022-03-30">https://medium.com/codex/explainable-ai-1d4f6bd1b186?source=collection_archive---------24-----------------------#2022-03-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b03d070b97a0a8fd33226b155a1898ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZbxnNNCc1BcKcLb6.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:软网解决方案</figcaption></figure><blockquote class="iu"><p id="f703" class="iv iw hi bd ix iy iz ja jb jc jd je dx translated">根据普华永道的一项年度调查，绝大多数(82%)的首席执行官同意，基于人工智能的决策要想得到信任，它们必须是可解释的。</p></blockquote><h1 id="d4ca" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">什么是可解释的人工智能？</h1><p id="92f9" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la je hb bi translated">可解释人工智能(XAI)是一种人工智能技术，它使整个决策过程透明、可理解和高效。换句话说，XAI消除了感知的黑箱，并通过详细展示选择是如何产生的来解释计算、发现或预测。</p><p id="92d0" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">可解释的人工智能将成为智能应用的基础，允许制造、金融、医疗、保险、供应链管理等行业的组织做出万无一失的判断。然而，当涉及到可解释的医学人工智能时，如果人工智能模型缺乏清晰的思维，它的决定或预测将受到挑战，被忽略，甚至被推翻。因此，预测背后的背景和推理，如临床状况、潜在的治疗方法和自动处方，可能是可疑的、灾难性的和错误的。</p><h1 id="df3b" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr lg jt ju jv lh jx jy jz li kb kc kd bi translated">准确性可解释性权衡:</h1><p id="15d4" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la je hb bi translated">在市场中，经常听到关键客户喜欢更易解释的模型，如线性模型和树，因为它们简单、易于验证和解释。</p><p id="8c43" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">这些类型的模型(集成、神经网络等)被称为黑盒模型。随着模型的发展，描述其功能变得越来越困难。</p><p id="74f6" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">如果我们向利益相关者推荐我们的方法，他们会绝对信任并立即开始使用吗？不。他们会提出这样的疑问，“我为什么要相信你的模型？”为什么模型实现了它做出的决定？哪些因素会影响模型预测？</p><p id="5c05" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">我们应该考虑提高模型的准确性，同时避免在解释中漫无目的地徘徊。必须有一个合理的平衡。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/371fac0f20c88ca6f3527164f0468b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tT2lMD5msO2_zBUQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:D-Phi高级ML训练营</figcaption></figure><p id="77cd" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">探索性数据分析、可视化和模型评估指标是传统方法的例子。在他们的帮助下，我们可以了解模型的方法。然而，它们也有一定的局限性。</p><p id="9a89" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">帮助我们克服这些限制的一些库如下:</p><ul class=""><li id="c11a" class="lo lp hi kg b kh lb kl lc kp lq kt lr kx ls je lt lu lv lw bi translated"><strong class="kg hj">石灰</strong>(本地可解释的模型不可知的解释)</li><li id="eff3" class="lo lp hi kg b kh lx kl ly kp lz kt ma kx mb je lt lu lv lw bi translated"><strong class="kg hj"> SHAP </strong>(沙普利补充说明)</li><li id="ec03" class="lo lp hi kg b kh lx kl ly kp lz kt ma kx mb je lt lu lv lw bi translated"><strong class="kg hj"> ELI5 </strong>(像我5岁一样解释)</li><li id="d52f" class="lo lp hi kg b kh lx kl ly kp lz kt ma kx mb je lt lu lv lw bi translated"><strong class="kg hj">溜冰者</strong></li></ul><h1 id="9ef7" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr lg jt ju jv lh jx jy jz li kb kc kd bi translated">举例说明人工智能:</h1><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/149738fec5c6d0ef9834522ff08f7ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ErEy-6ucNlqgGb6e.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:走向数据科学</figcaption></figure><p id="08a8" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">我们通过应用一些特定的机器学习算法，使用房屋数据训练了一个模型。</p><p id="93c3" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">在他们学会这种算法后，我们可以为其提供新的数据，在我们的例子中是额外的住宅，它将提供关于输入房屋估价的预测。最后，一个消费者，在这个例子中是一家房地产公司的所有者，检查信息并做出判断或启动特定的操作。</p><p id="f7ab" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">从用户的保留意见可以看出，这里的困难在于预测没有任何依据。我们的模型可以有很低的误差，仍然有一定的偏差，或者时不时做出耐人寻味的预测。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/cf064704eea0068ef4b9266e36b058f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4RBDCSpTsCfxwACC.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:走向数据科学</figcaption></figure><p id="3b99" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la je hb bi translated">这表明，这一程序不仅能够作出预测，而且能够证明为什么会产生这种预测。新的解释性界面提供了更多的知识，可能有助于我们的用户理解为什么这样的预测。</p><blockquote class="md me mf"><p id="71bb" class="ke kf mg kg b kh lb kj kk kl lc kn ko mh ld kr ks mi le kv kw mj lf kz la je hb bi translated">我们不仅需要拥有高性能模型的能力，还需要理解我们何时不能信任这些模型。—亚历山大·阿米尼</p></blockquote></div></div>    
</body>
</html>