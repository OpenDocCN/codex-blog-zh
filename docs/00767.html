<html>
<head>
<title>How Artificial Intelligence Works: Open the Black Box!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能如何工作:打开黑盒！</h1>
<blockquote>原文：<a href="https://medium.com/codex/how-artificial-intelligence-works-open-the-black-box-12eb97704abe?source=collection_archive---------10-----------------------#2021-03-17">https://medium.com/codex/how-artificial-intelligence-works-open-the-black-box-12eb97704abe?source=collection_archive---------10-----------------------#2021-03-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="c81c" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph"><a class="ae ge" href="http://medium.com/codex" rel="noopener">法典</a></h2><div class=""/><p id="74a7" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">人们常说，大约99%的数据科学(DS)、机器学习(ML)和深度学习(DL)从业者仍然将人工神经网络(ANN)模型视为黑盒模型。</p><p id="d658" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">如果你属于这99%,而任何人对此都无能为力，你可能会在下面的图片中找到娱乐价值。如果你是那99%中的一员，并且对这种情况感到不舒服，这篇文章打算打开那个黑匣子。但是如果你是那1%的人并且仍然好奇，这篇文章展示了你可能熟悉的深度人工神经网络反投影方程可以从这里展示的3个方程中恢复出来(面板3、5和6)。</p><p id="980c" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">考虑下面图1所示的浅层人工神经网络，我们可以很容易地归纳出面板1所示的正向传播计算公式，这也适用于深层人工神经网络。对于每个单元(神经元)，线性函数将自变量(输入参数)映射到预激活值，然后由激活函数将其映射到后激活值。网络中每层的总激活值由该层中所有单元的激活值之和给出。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/e0de0c28cb375e824ebf5adf26935829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LboEZKsVCbRu6mGiNE_8hQ.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图1:浅层神经网络(图片由作者提供)。</figcaption></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kc"><img src="../Images/a219b3c66f3dac5c244dc25d20b31bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*o6hN6y0rxJc-wsN8Dn9Y9A.png"/></div></figure><p id="1d41" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">让我们假设交叉熵损失用于计算学习目标的成本函数(J)。面板2显示了成本函数的表达式。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kd"><img src="../Images/953ec4645c77741cece91f24bfe54f33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PJDe2Drq9zfxvbSuEknOdw.png"/></div></div></figure><p id="3d7e" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">量化成本相对于图2中红色边缘参数的梯度涉及链式法则的应用，如面板3所示。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ke"><img src="../Images/16162a7a51cda3fcf775f303ab1f3967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6y5WD9IjeZ0QvMxyiqVfrg.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图2:第二层网络中红色边的参数(权重)(图片由作者提供)。</figcaption></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kd"><img src="../Images/91c6bdd266a945465b1ae4dd989dd966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjMIRGAYhoM1qWq9gcBCuw.png"/></div></div></figure><p id="1cc4" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">然而，如果我们根据激活函数来表达成本函数，则可以通过避免后激活项来截断链式规则，如面板4所示。但是为了保持一致，让我们保留链式法则的长形式。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kf"><img src="../Images/a1cb7529f12e38738fe643b41ffb3965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DppSGEzYxQNveFPHLIbKrA.png"/></div></div></figure><p id="5a96" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">此外，在浅网络的下一层上，面板5示出了图3中彩色边缘上的梯度的计算。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/956407b24931eb429082f92a8b82119f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y_U9WhmJ4SGqMwym7JN8pg.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图3:网络第一层红色边的参数(权重)。</figcaption></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kg"><img src="../Images/acaf5b7ef7671ebaf0adc385e7b9f808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNVGrftgDg5phWEPhoVHRw.png"/></div></div></figure><p id="0024" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">到目前为止，我们只看了一个2层网络(不包括输入层，这在实践中很常见)。有趣的是，我们可以将直觉从浅层网络扩展到图4所示的L层深层网络。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/31e58f920812819e770e28994a8b4cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGNEDAu8o6uCX5NLGMkIZA.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图4:深度神经网络(图片由作者提供)。</figcaption></figure><p id="0c1f" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">类似地，对于深层网络，我们可以确定相对于隐藏层中任何参数的成本梯度。图6显示了相对于图5中边缘参数的梯度。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/bdca746e862558461a35facb58e6a60a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rqSWfN0gkpy5YVACsyxubw.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图片5:网络隐藏层中红色边的参数(权重)。</figcaption></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kh"><img src="../Images/5dc90cc02c50b779b45d29f8d89453d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5gYZQucssrWFWmvf96LgUA.png"/></div></div></figure><p id="81f7" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">如果你熟悉吴恩达课程中的方程，你可能会注意到，当按顺序考虑时，面板7中的表达式等同于前面给出的表达式(面板3、5和6)；对于每个渐变，迭代一层又一层。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ki"><img src="../Images/347bfc348fb927ee68a0f49a6057a805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ji8rmLedqxosDBfg5CCuuA.png"/></div></div></figure><p id="519a" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">请注意，在吴恩达公式中，成本函数是以激活函数的形式表示的(如面板4所示)。</p></div></div>    
</body>
</html>