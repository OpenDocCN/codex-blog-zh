<html>
<head>
<title>Executing Spark jobs with Apache Airflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Airflow执行Spark作业</h1>
<blockquote>原文：<a href="https://medium.com/codex/executing-spark-jobs-with-apache-airflow-3596717bbbe3?source=collection_archive---------1-----------------------#2021-08-07">https://medium.com/codex/executing-spark-jobs-with-apache-airflow-3596717bbbe3?source=collection_archive---------1-----------------------#2021-08-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5ad0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Spark是一个对分布式数据处理很有帮助的解决方案。为了自动化这项任务，一个很好的解决方案是在<a class="ae jd" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Airflow </a>中调度这些任务。在本教程中，我将与您分享在Apache Airflow中创建能够运行Apache Spark作业的DAG的方法。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/cca80e0b86971bc0539fab6e56a915ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kqtDYbzk-mYeqmH7P4Vg2w.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">照片由<a class="ae jd" href="https://unsplash.com/@wilstewart3?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">威尔·斯图尔特</a>在<a class="ae jd" href="https://unsplash.com/@wilstewart3?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="47f3" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">在气流机上准备环境</h2><p id="65df" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">我测试本教程的机器运行的是Debian 9。要使用<strong class="ih hj"> PythonOperator </strong>和<strong class="ih hj"> BashOperator </strong>运行Spark on Airflow，必须配置<strong class="ih hj"> JAVA_HOME </strong>环境<strong class="ih hj"> </strong>。如果您没有安装java，请使用以下命令安装它:</p><pre class="jf jg jh ji fd lb lc ld le aw lf bi"><span id="bd36" class="kb kc hi lc b fi lg lh l li lj"><em class="lk">sudo apt update<br/>sudo apt install default-jdk</em></span></pre><p id="30f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">安装java后，必须通过映射JAVA安装的位置来配置操作系统中的JAVA_HOME。例如，在Debian上，在。bashrc文件，在根目录中，您将通知下面几行:</p><pre class="jf jg jh ji fd lb lc ld le aw lf bi"><span id="ff67" class="kb kc hi lc b fi lg lh l li lj">export JAVA_HOME='<!-- -->/usr/lib/jvm/java-8-openjdk-amd64'<br/>export PATH=$PATH:$JAVA_HOME/bin</span></pre><p id="396e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您在linux上，在编辑文件后，请记住运行以下命令:</p><pre class="jf jg jh ji fd lb lc ld le aw lf bi"><span id="1f1e" class="kb kc hi lc b fi lg lh l li lj">source ~/.bashrc</span></pre><p id="915b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要使用Airfow操作符<strong class="ih hj"> SparkSubmitOperator </strong>运行脚本，除了JAVA_HOME之外，还必须添加和映射Spark二进制文件。在<a class="ae jd" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"> Spark页面</a>上，您可以下载tgz文件，并将其解压缩到托管Airflow的机器上。放入文件中。bashrc SPARK _ HOME并将其添加到系统路径中。</p><pre class="jf jg jh ji fd lb lc ld le aw lf bi"><span id="e98c" class="kb kc hi lc b fi lg lh l li lj">export SPARK_HOME='/opt/spark'<br/>export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin</span></pre><p id="4562" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，你必须将<strong class="ih hj"> pyspark </strong>包添加到气流流动的环境中。</p><pre class="jf jg jh ji fd lb lc ld le aw lf bi"><span id="83c8" class="kb kc hi lc b fi lg lh l li lj">pip install pyspark</span></pre><p id="fcea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的例子中，我们有一个从SQL数据库ETL到mongo数据库的Spark任务的例子。使用的Spark版本是与mongo连接器包org . MongoDB . Spark:mongo-Spark-connector _ 2.12:3 . 0 . 0兼容的<a class="ae jd" href="https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank"> 3.0.1 </a></p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="12ed" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">python运算符</h2><p id="67dc" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">使用<a class="ae jd" href="https://airflow.apache.org/docs/stable/_api/airflow/operators/python_operator/index.html?highlight=pythonoper#module-airflow.operators.python_operator" rel="noopener ugc nofollow" target="_blank"> PythonOperator </a>，只需创建python方法，运行Spark作业，从Airflow发送它。这种方法的问题是您没有Spark作业执行的日志细节。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ll lm l"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">脚本dag气流com pythonoperator</figcaption></figure><h2 id="d250" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">bash运算符</h2><p id="cf5d" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">要使用该操作符，可以创建一个包含Spark代码的python文件和另一个包含DAG代码的python文件用于气流。在<a class="ae jd" href="https://airflow.apache.org/docs/stable/_api/airflow/operators/bash_operator/index.html?highlight=bashoperator#module-airflow.operators.bash_operator" rel="noopener ugc nofollow" target="_blank"> BashOperator </a>中，bash_command参数接收将在操作系统的bash中执行的命令。例如，在该参数中，可以执行命令<em class="lk"> python jobspark.py </em>。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ll lm l"/></div></figure><p id="02e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个操作符中，您将从Spark作业中获得更多的日志细节。这些日志记录了作业的阶段和完成百分比，就像在终端中执行一样。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/77c8ae1a6f298e53fc7e3575f70bda5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nG7RfRCgi8dMsxF9IfB9Aw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">BashOperator日志详细信息</figcaption></figure><h2 id="e5cf" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">SparkSubmitOperator</h2><p id="ea4d" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">要使用该操作符，在Airflow机器上映射JAVA_HOME和Spark二进制文件后，必须在Airflow管理面板中注册主Spark连接。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lo"><img src="../Images/19748771eb260821d120c3d784db2d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kcph5ahPz7ztCzvPTF6jbg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">火花主连接</figcaption></figure><p id="a4ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jd" href="https://airflow.apache.org/docs/stable/_api/airflow/contrib/hooks/spark_submit_hook/index.html?highlight=sparksubmit#airflow.contrib.hooks.spark_submit_hook.SparkSubmitHook" rel="noopener ugc nofollow" target="_blank"> SparkSubmitOperator </a>中，conn_id参数将由通过管理面板注册的Conn Id填充。我认为这个操作符的主要优点之一是能够配置和通知所有的Spark作业属性。从而使Spark脚本更加精简，实际上只有逻辑要在集群中发送和执行。在幕后，这个操作符使用bash <em class="lk"> spark-submit </em>命令，该命令使用操作符中给出的设置。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ll lm l"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">SparkSubmitOperator示例</figcaption></figure><p id="e671" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个操作符中，任务日志要详细得多，包含关于每个任务开始和结束的TaskSetManager信息</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="6de7" class="lp kc hi bd kd lq lr ls kh lt lu lv kl lw lx ly ko lz ma mb kr mc md me ku mf bi translated">解决纷争</h1><p id="9589" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">在DAG的创建过程中，我遇到了一些问题，在这一部分，我想分享一下如何解决这些问题。</p><h2 id="51c8" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">错误—[错误2]没有这样的文件或目录:“bash”</h2><p id="9b03" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">这种情况发生在执行bash命令时的airflow中。出于某种原因，气流可能无法识别操作系统的bash路径。要解决这个问题，在BashOperator中添加env属性，通知包含bash的路径。在SparkSumbitOperator中，必须在env_vars属性中指定路径。这里有一个例子:</p><pre class="jf jg jh ji fd lb lc ld le aw lf bi"><span id="d8af" class="kb kc hi lc b fi lg lh l li lj">print_path_env_task = BashOperator(<br/>    task_id='elt_documento_pagar_spark',<br/>    bash_command="python ./dags/spark-jdbc-sql-test.py",<br/>    dag=dag,<br/>    env={'PATH': '/bin:/usr/bin:/usr/local/bin'}<br/>)</span><span id="f0fc" class="kb kc hi lc b fi mg lh l li lj">task = SparkSubmitOperator(<br/>    task_id='elt_documento_pagar_spark',<br/>    conn_id='spark',<br/>    application="./dags/spark-jdbc-sql-test.py",<br/>    env_vars={'PATH': '/bin:/usr/bin:/usr/local/bin'},<br/>    packages="org.mongodb.spark:mongo-spark-connector_2.12:3.0.0,com.microsoft.sqlserver:mssql-jdbc:8.4.1.jre8"<br/>)</span></pre><h2 id="dce7" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">语法错误:非ASCII字符</h2><p id="0c50" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">处理包含特殊字符的Spark代码时会发生这种情况。要解决这个问题，只需在python文件的顶部写下下面的注释。</p><pre class="jf jg jh ji fd lb lc ld le aw lf bi"><span id="c606" class="kb kc hi lc b fi lg lh l li lj"># -*- coding: utf-8 -*-</span></pre></div></div>    
</body>
</html>