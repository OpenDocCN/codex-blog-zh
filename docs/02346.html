<html>
<head>
<title>Running a Multi-Node Hadoop Cluster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">运行多节点Hadoop集群</h1>
<blockquote>原文：<a href="https://medium.com/codex/running-a-multi-node-hadoop-cluster-257068e5f276?source=collection_archive---------5-----------------------#2021-07-14">https://medium.com/codex/running-a-multi-node-hadoop-cluster-257068e5f276?source=collection_archive---------5-----------------------#2021-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c6e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Hadoop软件库是一个框架，允许跨计算机集群分布式处理大型数据集。建立一个Hadoop集群并不难，但是没有一个统一的文档，所以我决定写一篇关于它的文章。让我们开始吧…</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/3de99052c89f6f17436551a4b652db97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*McESTYuIIxkef2HhbyqQCg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">阿帕奇Hadoop[<a class="ae jt" href="https://hadoop.apache.org/" rel="noopener ugc nofollow" target="_blank">https://hadoop.apache.org/</a></figcaption></figure><p id="667a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有三台Linux服务器，一台是主服务器，其他的是worker1和worker2。我可能会使用一些CentOS (RHEL)命令。我们将通过以下步骤来运行集群。</p><ol class=""><li id="9d65" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">准备服务器</li><li id="e83e" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">安装JDK</li><li id="30fe" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">配置HDFS和纱线</li><li id="5c23" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">启动HDFS和纱线守护进程</li></ol></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><h1 id="465c" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">准备服务器</h1><p id="396a" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">由于</span> HDFS和YARN使用主机名进行通信，我们应该为所有服务器设置主机名，并将它们添加到<code class="du mb mc md me b">/etc/hosts</code>文件中。让我们假设服务器的IP地址是<code class="du mb mc md me b">192.168.0.1</code>、<code class="du mb mc md me b">192.168.0.2</code>和<code class="du mb mc md me b">192.168.0.3</code>。<code class="du mb mc md me b">SSH</code>每台服务器，并将这些行复制到<code class="du mb mc md me b">/etc/hosts</code>文件中。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="041d" class="mj kq hi me b fi mk ml l mm mn">[root@192.168.0.x ~]# vi /etc/hosts</span><span id="0f2e" class="mj kq hi me b fi mo ml l mm mn"># Add these lines:<br/>192.168.0.1   master<br/>192.168.0.2   worker1<br/>192.168.0.3   worker2</span></pre><p id="02d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用这个符号<code class="du mb mc md me b">[{user}@{server} {working directory}]</code>来指定应该运行命令的<em class="mp">用户</em>、<em class="mp">服务器</em>和<em class="mp">工作目录</em>，所以在运行脚本和命令之前检查它们。现在，是时候设置<code class="du mb mc md me b">hostname</code>了。<code class="du mb mc md me b">SSH</code>每台服务器，使用以下命令设置相应的主机名，并重新启动它们:</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="4dcb" class="mj kq hi me b fi mk ml l mm mn">[root@192.168.0.x ~]# hostnamectl set-hostname {name} # name: master, worker1, worker2<br/>[root@192.168.0.x ~]# hostnamectl # Should print {name}<br/>[root@192.168.0.x ~]# reboot</span></pre><p id="9f58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用以下命令，在<code class="du mb mc md me b">hadoop</code>组中创建一个名为<code class="du mb mc md me b">hduser</code>的用户，使其成为<code class="du mb mc md me b">sudoer</code>，并为其设置密码:</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="f1e0" class="mj kq hi me b fi mk ml l mm mn">[root@{hostname} ~]# groupadd hadoop<br/>[root@{hostname} ~]# useradd -m hduser<br/>[root@{hostname} ~]# usermod -aG hadoop hduser<br/>[root@{hostname} ~]# usermod -aG wheel hduser<br/>[root@{hostname} ~]# passwd hduser</span></pre><p id="3699" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">上的<code class="du mb mc md me b">hduser</code>master</strong>应该能够通过<strong class="ih hj">无密码</strong> ssh连接到<strong class="ih hj">本身和其他服务器</strong>。我们应该在<strong class="ih hj">主</strong>服务器上为<code class="du mb mc md me b">hduser</code>创建一个ssh密钥，并将它复制到<strong class="ih hj">所有服务器，包括它自己</strong>:</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="6e6e" class="mj kq hi me b fi mk ml l mm mn">[hduser@master ~]# ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa<br/>[hduser@master ~]# ssh-copy-id -i .ssh/id_rsa.pub hduser@{master and workers}</span></pre><p id="6265" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以验证在运行最新的命令后，我们被允许使用<code class="du mb mc md me b">hduser</code>ssh所有服务器，而无需提供密码。完成这些简单的步骤后，服务器就准备好了，我们可以进入下一步了。</p><h1 id="c74f" class="kp kq hi bd kr ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm bi translated">安装Java</h1><p id="11bc" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi ls translated">在使用Apache Hadoop之前，我们应该安装Java，所以下载JDK或JRE(我更喜欢JDK)，并复制给workers。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="877b" class="mj kq hi me b fi mk ml l mm mn">[hduser@master ~]# wget {JDK LINK (I downloaded jdk-8u291-linux-x64.tar.gz)}<br/>[hduser@master ~]# scp jdk-8u291-linux-x64.tar.gz hduser@worker1:/home/hduser/jdk-8u291-linux-x64.tar.gz<br/>[hduser@master ~]# scp jdk-8u291-linux-x64.tar.gz hduser@worker2:/home/hduser/jdk-8u291-linux-x64.tar.gz</span></pre><p id="3614" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在所有服务器</strong>上，提取JDK，并创建一个软链接，以便将来能够简单地更改其版本。我将解压<code class="du mb mc md me b">/opt</code>目录中的所有文件。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="ee66" class="mj kq hi me b fi mk ml l mm mn">[hduser@{server} ~]# cd /opt<br/>[hduser@{server} /opt]# sudo tar xzf /home/hduser/jdk-8u291-linux-x64.tar.gz<br/>[hduser@{server} /opt]# sudo ln -s jdk1.8.0_291/ jdk<br/>[hduser@{server} /opt]# sudo chown -R hduser:hadoop jdk<br/>[hduser@{server} /opt]# sudo chown -R hduser:hadoop jdk1.8.0_291<br/><br/>[hduser@{server} /opt]# sudo update-alternatives --install /usr/bin/java java /opt/jdk/bin/java 100<br/>[hduser@{server} /opt]# sudo update-alternatives --install /usr/bin/javac javac /opt/jdk/bin/javac 100<br/>[hduser@{server} /opt]# # Check java is installed successfully<br/>[hduser@{server} /opt]# update-alternatives --display java<br/>[hduser@{server} /opt]# update-alternatives --display javac<br/>[hduser@{server} /opt]# java -version</span></pre><h1 id="4a86" class="kp kq hi bd kr ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm bi translated">配置HDFS和纱线</h1><p id="09f2" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi ls translated">目前，我们已经设置好了服务器和java！让我们下载Apache Hadoop，将其复制到所有服务器，创建链接，等等。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="f45a" class="mj kq hi me b fi mk ml l mm mn">[hduser@master ~]# wget {Apache Hadoop LINK (I downloaded hadoop-3.3.0-aarch64.tar.gz)}<br/>[hduser@master ~]# scp hadoop-3.3.0-aarch64.tar.gz hduser@worker1:/home/hduser/hadoop-3.3.0-aarch64.tar.gz<br/>[hduser@master ~]# scp hadoop-3.3.0-aarch64.tar.gz hduser@worker2:/home/hduser/hadoop-3.3.0-aarch64.tar.gz</span><span id="28ea" class="mj kq hi me b fi mo ml l mm mn">[hduser@{server} ~]# cd /opt<br/>[hduser@{server} /opt]# sudo tar xzf /home/hduser/hadoop-3.3.0-aarch64.tar.gz<br/>[hduser@{server} /opt]# sudo ln -s hadoop-3.3.0/ hadoop<br/>[hduser@{server} /opt]# sudo chown -R hduser:hadoop hadoop<br/>[hduser@{server} /opt]# sudo chown -R hduser:hadoop hadoop-3.3.0</span></pre><p id="bf0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在所有服务器</strong>上，创建一个目录，并将所有数据存储在那里。我将在<code class="du mb mc md me b">/data</code>目录下创建这个目录，所以如果您选择了另一个目录，不要忘记更改脚本。我们将创建四个目录:</p><ul class=""><li id="3559" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc mv ka kb kc bi translated"><code class="du mb mc md me b">data</code>:存储与<em class="mp"> DataNode </em>相关的数据</li><li id="a425" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc mv ka kb kc bi translated"><code class="du mb mc md me b">name</code>:存储与<em class="mp"> NameNode </em>相关的数据</li><li id="a1f9" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc mv ka kb kc bi translated"><code class="du mb mc md me b">pid</code>:创建<code class="du mb mc md me b">pid</code>文件</li><li id="f646" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc mv ka kb kc bi translated"><code class="du mb mc md me b">tmp</code>:允许Apache Hadoop创建临时文件</li></ul><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="69c7" class="mj kq hi me b fi mk ml l mm mn">/opt/hadoop/etc/hadoop<br/>[hduser@{server} /data]# sudo mkdir hadoop<br/>[hduser@{server} /data]# sudo chown -R hduser:hadoop hadoop<br/>[hduser@{server} /data/hadoop]# cd hadoop<br/>[hduser@{server} /data/hadoop]# mkdir data<br/>[hduser@{server} /data/hadoop]# mkdir name<br/>[hduser@{server} /data/hadoop]# mkdir pid<br/>[hduser@{server} /data/hadoop]# mkdir tmp</span></pre><p id="df62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有的Hadoop配置文件都在<code class="du mb mc md me b">/opt/hadoop/etc/hadoop</code>目录下，而且大部分配置在所有服务器上应该是相同的，所以我们将在<strong class="ih hj">主</strong>上逐个修改，然后复制到<strong class="ih hj">工作器</strong>上，并在工作器上做一些修改。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="ca94" class="mj kq hi me b fi mk ml l mm mn">[hduser@master <!-- -->~<!-- -->]# cd <!-- -->/opt/hadoop/etc/hadoop<br/>[hduser@master <!-- -->/opt/hadoop/etc/hadoop<!-- -->]# vi core-site.xml</span><span id="897e" class="mj kq hi me b fi mo ml l mm mn"># Add these elements:<br/>## Do not forget to change the tmp directory<br/>  &lt;property&gt;<br/>    &lt;name&gt;fs.defaultFS&lt;/name&gt;<br/>    &lt;value&gt;hdfs://{master}:9000&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br/>    &lt;value&gt;/data/hadoop/tmp&lt;/value&gt;<br/>  &lt;/property&gt;</span><span id="6e94" class="mj kq hi me b fi mo ml l mm mn">[hduser@master <!-- -->/opt/hadoop/etc/hadoop<!-- -->]# vi hadoop-env.sh</span><span id="81f7" class="mj kq hi me b fi mo ml l mm mn"># Add these commands:<br/>## Do not forget to change the PID directory<br/>export JAVA_HOME=/opt/jdk<br/>export HADOOP_PID_DIR=/data/hadoop/pid</span><span id="8280" class="mj kq hi me b fi mo ml l mm mn">[hduser@master <!-- -->/opt/hadoop/etc/hadoop<!-- -->]# vi hdfs-site.xml</span><span id="cb62" class="mj kq hi me b fi mo ml l mm mn"># Add these elements:<br/>## dfs.replication property specifies replication factor, you may want to change it. I prefer replication factor of two for small clusters.<br/>## Do not forget to change the data directory<br/>## Do not forget to change the name directory<br/>  &lt;property&gt;<br/>    &lt;name&gt;dfs.replication&lt;/name&gt;<br/>    &lt;value&gt;2&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;dfs.data.dir&lt;/name&gt;<br/>    &lt;value&gt;/data/hadoop/data&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;dfs.name.dir&lt;/name&gt;<br/>    &lt;value&gt;/data/hadoop/name&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;dfs.permission&lt;/name&gt;<br/>    &lt;value&gt;false&lt;/value&gt;<br/>  &lt;/property&gt;</span><span id="8c4d" class="mj kq hi me b fi mo ml l mm mn">[hduser@master <!-- -->/opt/hadoop/etc/hadoop<!-- -->]# vi mapred-site.xml</span><span id="bb4b" class="mj kq hi me b fi mo ml l mm mn"># Add these elements:<br/>  &lt;property&gt;<br/>    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br/>    &lt;value&gt;yarn&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;<br/>    &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;mapred.job.tracker&lt;/name&gt;<br/>    &lt;value&gt;master:54311&lt;/value&gt;<br/>    &lt;description&gt;The host and port that the MapReduce job tracker runs<br/>    at.  If "local", then jobs are run in-process as a single map<br/>    and reduce task.<br/>    &lt;/description&gt;<br/>  &lt;/property&gt;</span><span id="eeed" class="mj kq hi me b fi mo ml l mm mn">[hduser@master <!-- -->/opt/hadoop/etc/hadoop<!-- -->]# vi workers</span><span id="35aa" class="mj kq hi me b fi mo ml l mm mn"># Add the names of all workers<br/>master<br/>worker1<br/>worker2</span><span id="9f79" class="mj kq hi me b fi mo ml l mm mn">[hduser@master <!-- -->/opt/hadoop/etc/hadoop<!-- -->]# vi yarn-site.xml</span><span id="5ca2" class="mj kq hi me b fi mo ml l mm mn"># Add these lines<br/>## You may want to change the max-disk-utilization-per-disk-percentage.<br/>## Notice that the value of "yarn.nodemanager.hostname" property is {worker}. We leave it for now, and will change it later<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br/>    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;<br/>    &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage&lt;/name&gt;<br/>    &lt;value&gt;95&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br/>    &lt;value&gt;master&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt;<br/>    &lt;value&gt;{worker}&lt;/value&gt;<br/>  &lt;/property&gt;</span></pre><p id="d904" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好吧！搞定了。现在，我们将所有的配置文件复制到所有的工人，然后改变他们中的一些。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="6419" class="mj kq hi me b fi mk ml l mm mn">[hduser@master <!-- -->/opt/hadoop/etc/hadoop<!-- -->]# scp core-site.xml hadoop-env.sh hdfs-site.xml mapred-site.xml yarn-site.xml hduser@{workers}:/opt/hadoop/etc/hadoop/</span><span id="89de" class="mj kq hi me b fi mo ml l mm mn">[hduser@{server} <!-- -->/opt/hadoop/etc/hadoop<!-- -->]# vi yarn-site.xml</span><span id="d97a" class="mj kq hi me b fi mo ml l mm mn"># Change the value of "yarn.nodemanager.hostname" to the name of server.<br/>## On master: &lt;value&gt;master&lt;/value&gt;<br/>## On worker1: &lt;value&gt;worker1&lt;/value&gt;</span></pre><p id="98cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是时候在所有服务器上设置环境变量了。我建议在master上这样做，然后复制给其他人。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="bce7" class="mj kq hi me b fi mk ml l mm mn">[hduser@{server} <!-- -->~<!-- -->]# vi .bashrc</span><span id="7142" class="mj kq hi me b fi mo ml l mm mn"># Add these commands<br/># Java<br/>export JAVA_HOME=/opt/jdk</span><span id="ff15" class="mj kq hi me b fi mo ml l mm mn"># Haddop &amp; YARN<br/>export PDSH_RCMD_TYPE=ssh<br/>export HADOOP_HOME=/opt/hadoop<br/>export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop<br/>export HADOOP_MAPRED_HOME=/opt/hadoop<br/>export HADOOP_COMMON_HOME=/opt/hadoop<br/>export HADOOP_HDFS_HOME=/opt/hadoop<br/>export HADOOP_YARN_HOME=/opt/hadoop<br/>export PATH=$HADOOP_HOME/bin:$PATH<br/>export HDFS_NAMENODE_USER="hduser"<br/>export HDFS_DATANODE_USER="hduser"<br/>export HDFS_SECONDARYNAMENODE_USER="hduser"<br/>export YARN_RESOURCEMANAGER_USER="hduser"<br/>export YARN_NODEMANAGER_USER="hduser"<br/>export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native</span></pre><h1 id="4711" class="kp kq hi bd kr ks mq ku kv kw mr ky kz la ms lc ld le mt lg lh li mu lk ll lm bi translated">启动HDFS和纱线守护进程</h1><p id="df45" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi ls translated">下载，共享文件，配置所有服务器！现在，是启动集群的时候了。我们应该格式化命名节点，然后开始HDFS和纱线。如果你想启动HDFS和纱，启动HDFS，然后纱，当你想停止他们，停止纱先，然后HDFS。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="62f5" class="mj kq hi me b fi mk ml l mm mn">[hduser@master <!-- -->~<!-- -->]# hadoop namenode -format<br/>[hduser@master <!-- -->~<!-- -->]# /opt/hadoop/sbin/start-dfs.sh<br/>[hduser@master <!-- -->~<!-- -->]# /opt/hadoop/sbin/start-yarn.sh<br/>[hduser@master <!-- -->~<!-- -->]# /opt/hadoop/sbin/stop-yarn.sh<br/>[hduser@master <!-- -->~<!-- -->]# /opt/hadoop/sbin/stop-dfs.sh</span></pre><p id="1278" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您还可以创建服务来控制HDFS和纱线。</p><pre class="je jf jg jh fd mf me mg mh aw mi bi"><span id="6c39" class="mj kq hi me b fi mk ml l mm mn">[hduser@master <!-- -->~<!-- -->]# cd /etc/systemd/system<br/>[hduser@master /etc/systemd/system]# vi hdfs.service</span><span id="7b0c" class="mj kq hi me b fi mo ml l mm mn"># Add these lines<br/>## Do not forget to change the PID directory<br/>[Unit]<br/>Description=Hadoop DFS namenode and datanode<br/>After=syslog.target network.target remote-fs.target nss-lookup.target network-online.target<br/>Requires=network-online.target</span><span id="8f5f" class="mj kq hi me b fi mo ml l mm mn">[Service]<br/>User=hduser<br/>Group=hadoop<br/>Type=simple<br/>ExecStart=/opt/hadoop/sbin/start-dfs.sh<br/>ExecStop=/opt/hadoop/sbin/stop-dfs.sh<br/>WorkingDirectory=/home/hduser<br/>TimeoutStartSec=2min<br/>Restart=on-failure<br/>PIDFile=/data/hadoop/pid/hadoop-hduser-namenode.pid</span><span id="c4d1" class="mj kq hi me b fi mo ml l mm mn">[Install]<br/>WantedBy=multi-user.target</span><span id="a541" class="mj kq hi me b fi mo ml l mm mn">[hduser@master /etc/systemd/system]# vi yarn.service</span><span id="7ada" class="mj kq hi me b fi mo ml l mm mn"># Add these lines<br/>## Do not forget to change the PID directory<br/>[Unit]<br/>Description=YARN resourcemanager and nodemanagers<br/>After=syslog.target network.target remote-fs.target nss-lookup.target network-online.target<br/>Requires=network-online.target</span><span id="9d27" class="mj kq hi me b fi mo ml l mm mn">[Service]<br/>User=hduser<br/>Group=hadoop<br/>Type=simple<br/>ExecStart=/opt/hadoop/sbin/start-yarn.sh<br/>ExecStop=/opt/hadoop/sbin/stop-yarn.sh<br/>WorkingDirectory=/home/hduser<br/>TimeoutStartSec=2min<br/>Restart=on-failure<br/>PIDFile=/data/hadoop/pid/hadoop-hduser-resourcemanager.pid</span><span id="5908" class="mj kq hi me b fi mo ml l mm mn">[Install]<br/>WantedBy=multi-user.target</span></pre></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><p id="0167" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望这个教程是有用的。谢谢你。</p></div></div>    
</body>
</html>