<html>
<head>
<title>Understanding Dropout in Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解深层神经网络中的丢失</h1>
<blockquote>原文：<a href="https://medium.com/codex/understanding-dropout-in-deep-neural-networks-95e7d1b11c58?source=collection_archive---------7-----------------------#2021-01-06">https://medium.com/codex/understanding-dropout-in-deep-neural-networks-95e7d1b11c58?source=collection_archive---------7-----------------------#2021-01-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/cbc434878a2e5f5be0d7f03b844a5fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YhkwOC0AVYjj2msJ"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">照片由<a class="ae hv" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> JJ英</a>在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><div class=""/><p id="e841" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇文章旨在提供对一种非常流行的正则化技术的理解，这种技术叫做<strong class="ix hz"> <em class="jt">退出</em> </strong>。它假设预先了解模型训练、创建训练和测试集、过拟合、欠拟合和正则化等概念。</p><p id="ccc6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇文章从辍学的背景和原因开始。然后，它解释了辍学如何工作，以及它如何影响深度神经网络的训练。最后，它通过Keras的辍学层如何使用它。</p><h1 id="d25a" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">1.背景</h1><p id="eef0" class="pw-post-body-paragraph iv iw hy ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">深度神经网络是高度参数化的模型。通常，他们有数万甚至数百万个参数要学习。这些参数提供了大量的能力来学习一组不同的复杂数据集。这并不总是一件好事。这样的容量通常会导致<a class="ae hv" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过拟合</a>，这是一种训练集性能较高而测试集性能较差的情况(<em class="jt">低偏差、高方差</em>)。该模型可能有更高的测试错误率，因为它太依赖于训练数据。为了避免这种情况，我们尝试使用各种正则化技术来减少模型的学习能力。一种这样的正则化技术是<strong class="ix hz"> <em class="jt">退出</em> </strong>。正则化确保模型在看不见的数据上很好地概括。</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es kx"><img src="../Images/faebeefb22e17a224c6277672ddee5fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*5KizicdVpwvNCAo2Zy4hFQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图一。合身和过度合身的对比。来源:维基百科</figcaption></figure><p id="3602" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">图1显示了由<em class="jt">绿色</em>边缘表示的过拟合模型和由<em class="jt">黑色</em>边缘表示的正则化模型之间的对比。即使绿色边界似乎更适合训练数据，它也不太可能在看不见的实例(测试集)上表现得足够好。图1提供了过度拟合的一个很好的图像。</p><h1 id="acb0" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">2.输入辍学</h1><p id="016f" class="pw-post-body-paragraph iv iw hy ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">这是最流行的正则化技术之一，由Geoffrey Hinton于2012年在论文<a class="ae hv" href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener ugc nofollow" target="_blank">“通过防止特征检测器的共同适应来改善神经网络”</a>中提出。这是一个相当简单但非常有效的想法。</p><blockquote class="lc"><p id="216f" class="ld le hy bd lf lg lh li lj lk ll js dx translated">在每个训练步骤中，每个神经元被分配一个概率'<strong class="ak"> <em class="lm"> p </em> </strong>'，以暂时不参与训练过程(<em class="lm">退出</em>')。这里，“p”是一个超参数，称为辍学率，可以调整。</p></blockquote><figure class="lo lp lq lr ls hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ln"><img src="../Images/ceda65ad69b8bac2b632b673dd49b3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-cDHrjI5GOyhwYH32QdaWg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图二。上图显示了实施dropout对网络连接的影响。(左)具有密集连接的标准前馈网络。(右)由于辍学，连接数量急剧减少。来源:“辍学:防止神经网络过度拟合的简单方法”论文。</figcaption></figure><p id="df38" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，如果p=0.5，这意味着一个神经元有50%的机会在每个时期退出。如果一个神经元不参与训练步骤，它的所有连接都会被切断，这将影响下游层。这将大大降低神经网络中的连接密度(如图2所示)。丢弃可以应用于输入层和隐藏层，但不能应用于输出层。这是因为该模型必须总是为损失函数生成输出，以实现训练。退出过程仅在培训阶段进行。在推理阶段，网络中的所有神经元都充分参与。</p><p id="3813" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随意关闭神经元的做法可能会令人惊讶。有理由认为这可能会使培训过程变得非常不稳定。但实践证明，它在降低模型的复杂性方面非常有效。为了理解我为什么要引用《用Scikit-Learn和TensorFlow进行机器学习》这本书<a class="ae hv" href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?dchild=1&amp;keywords=Hands-On+Machine+Learning+with+Scikit-Learn%2C+Keras%2C+and+TensorFlow&amp;qid=1609912582&amp;s=books&amp;sr=1-3" rel="noopener ugc nofollow" target="_blank"/>中的一个例子。</p><blockquote class="lt lu lv"><p id="d0a8" class="iv iw jt ix b iy iz ja jb jc jd je jf lw jh ji jj lx jl jm jn ly jp jq jr js hb bi translated">如果让员工每天早上抛硬币来决定是否去上班，公司的表现会更好吗？谁知道呢。也许会！该公司显然将被迫调整其组织结构；它不能依靠任何一个人来填充咖啡机或执行任何其他关键任务，因此这种专业知识必须在几个人之间传播。员工必须学会与他们的许多同事合作，而不仅仅是少数几个。该公司将变得更有弹性。如果一个人退出，也不会有太大的影响。目前还不清楚这个想法是否真的适用于公司，但它肯定适用于神经网络。</p></blockquote><p id="3bc0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">类似地，在深度神经网络中，在训练过程中的每个时期，网络架构不同于前一个。此外，每个神经元被迫不要过于依赖少数几个输入连接，而是要关注所有的输入。这使得它们更能适应输入连接的变化。这种方式确保了一个更健壮的网络，可以更好地推广。</p><p id="c814" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">丢失中的可调超参数是<em class="jt">丢失率，</em>用<em class="jt"> p. </em>表示，其调节相当简单。</p><ul class=""><li id="fac3" class="lz ma hy ix b iy iz jc jd jg mb jk mc jo md js me mf mg mh bi translated">当你的模型过度拟合时，增加p</li><li id="60c9" class="lz ma hy ix b iy mi jc mj jg mk jk ml jo mm js me mf mg mh bi translated">当你的模型不合适时，减少它</li><li id="3737" class="lz ma hy ix b iy mi jc mj jg mk jk ml jo mm js me mf mg mh bi translated">对于较大的图层，保持较高的值；对于较小的图层，保持较低的值</li></ul><h1 id="3458" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">3.展示辍学的影响</h1><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/8355c01af30f8bbf2ba1c278e8c5c3a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*ODDS33VOEWaXEOrKnUYjbg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图3。丢失对MNIST数据集上训练的网络损失函数的影响</figcaption></figure><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/ab2fc6fbea125c0b5aac35eb2ba9f319.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*DP6jVXCnBeDR8YMJVeqr2w.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图4。辍学对MNIST数据集上训练的网络精度的影响</figcaption></figure><p id="4d2d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上图中可以清楚地看到辍学的影响(图3和图4)。这来自使用Keras的简单实验，其中前馈神经网络在MNIST数据集<strong class="ix hz"><em class="jt"/></strong>和<strong class="ix hz"> <em class="jt">上训练，而没有</em> </strong>丢失，保持所有其他因素恒定。蓝色线条表示<strong class="ix hz"> <em class="jt">有漏接</em> </strong>的型号，橙色线条表示<strong class="ix hz"> <em class="jt">无漏接</em> </strong>的型号。在图3中，“漏失对精度的影响”<strong class="ix hz"><em class="jt"/></strong>，可以清楚地观察到漏失增加了模型的损失。这不一定是坏事，但可能需要更长的时间来收敛。因此，在图4中精度下降。实验中使用的网络架构如下所示。</p><pre class="ky kz la lb fd mp mq mr ms aw mt bi"><span id="eccd" class="mu jv hy mq b fi mv mw l mx my">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense (Dense)                multiple                  401920    <br/>_________________________________________________________________<br/>dropout (Dropout)            multiple                  0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              multiple                  131328    <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          multiple                  0         <br/>_________________________________________________________________<br/>dense_2 (Dense)              multiple                  32896     <br/>_________________________________________________________________<br/>dropout_2 (Dropout)          multiple                  0         <br/>_________________________________________________________________<br/>dense_3 (Dense)              multiple                  1290      <br/>=================================================================<br/>Total params: 567,434<br/>Trainable params: 567,434<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="ed0b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> Github: </strong>生成上述图形的代码可从<a class="ae hv" href="https://github.com/mvshashank08/dropout/blob/main/implemeting-dropout.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><h1 id="aeee" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">4.Keras实施</h1><p id="2400" class="pw-post-body-paragraph iv iw hy ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">Keras使用<code class="du mz na nb mq b"><a class="ae hv" href="https://keras.io/api/layers/regularization_layers/dropout/" rel="noopener ugc nofollow" target="_blank">tf.keras.layers.Dropout</a></code>提供了一个脱落层。它将辍学率作为第一个参数。您可以在Keras的文档中找到更多详细信息。下面是一个小片段，展示了从动手ML书辍学的使用。</p><figure class="ky kz la lb fd hk"><div class="bz dy l di"><div class="nc nd l"/></div></figure><h1 id="110f" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">5.其他正则化技术</h1><p id="3315" class="pw-post-body-paragraph iv iw hy ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">除了丢弃之外，其他正则化技术也可以应用于神经网络。下面列出了一些最受欢迎的方法。</p><ul class=""><li id="a2b5" class="lz ma hy ix b iy iz jc jd jg mb jk mc jo md js me mf mg mh bi translated">l₁和l₂正规化</li><li id="e6ef" class="lz ma hy ix b iy mi jc mj jg mk jk ml jo mm js me mf mg mh bi translated"><a class="ae hv" href="https://en.wikipedia.org/wiki/Early_stopping" rel="noopener ugc nofollow" target="_blank">提前停止</a></li><li id="6085" class="lz ma hy ix b iy mi jc mj jg mk jk ml jo mm js me mf mg mh bi translated">辍学的其他变体(如<a class="ae hv" href="https://arxiv.org/pdf/1506.02142.pdf" rel="noopener ugc nofollow" target="_blank">蒙特卡洛辍学</a></li></ul><h1 id="675d" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">6.参考</h1><p id="ced0" class="pw-post-body-paragraph iv iw hy ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">以下是我用来写这篇文章的参考资料。关于辍学的原始论文(在下面的列表中)详细论述了辍学背后的理论以及为证明其有效性而进行的实验。</p><ul class=""><li id="9312" class="lz ma hy ix b iy iz jc jd jg mb jk mc jo md js me mf mg mh bi translated">Book: <a class="ae hv" href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?dchild=1&amp;keywords=Hands-On+Machine+Learning+with+Scikit-Learn%2C+Keras%2C+and+TensorFlow&amp;qid=1609912582&amp;s=books&amp;sr=1-3" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习</a></li><li id="5e7b" class="lz ma hy ix b iy mi jc mj jg mk jk ml jo mm js me mf mg mh bi translated">Github库:<a class="ae hv" href="https://github.com/mvshashank08/dropout" rel="noopener ugc nofollow" target="_blank">https://github.com/mvshashank08/dropout</a></li><li id="bf43" class="lz ma hy ix b iy mi jc mj jg mk jk ml jo mm js me mf mg mh bi translated">论文:“<a class="ae hv" href="https://arxiv.org/pdf/1207.0580.pdf" rel="noopener ugc nofollow" target="_blank">通过防止特征检测器的协同适应来改进神经网络</a></li><li id="a805" class="lz ma hy ix b iy mi jc mj jg mk jk ml jo mm js me mf mg mh bi translated">论文:<a class="ae hv" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">“辍学:防止神经网络过拟合的简单方法”</a></li><li id="965e" class="lz ma hy ix b iy mi jc mj jg mk jk ml jo mm js me mf mg mh bi translated">维基百科:<a class="ae hv" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过度拟合</a></li></ul><p id="cfce" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">谢谢你的时间。请在评论区留下任何建议。</p></div></div>    
</body>
</html>