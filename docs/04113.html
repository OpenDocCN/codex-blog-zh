<html>
<head>
<title>R-Drop, a simple trick to improve DropOut</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R-Drop，提高辍学率的简单方法</h1>
<blockquote>原文：<a href="https://medium.com/codex/r-drop-a-simple-trick-to-improve-dropout-d6f0bb64f302?source=collection_archive---------0-----------------------#2021-10-27">https://medium.com/codex/r-drop-a-simple-trick-to-improve-dropout-d6f0bb64f302?source=collection_archive---------0-----------------------#2021-10-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3f53637466530b08d4ec2380dd41f697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6gH33Xh3VhTnWdVC"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/@zoltantasi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Zoltan·塔斯</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="4f10" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">深度学习到处都在使用，但它们经常过度拟合训练数据，并且不能推广到看不见的样本。各种正则化技术防止这种过拟合。Dropout是最流行和最强大的正则化技术之一，无论网络架构和任务类型如何都可以使用。它是通过在训练期间简单地从神经网络中删除一定比例的隐藏单元来实现的。</p><p id="c7db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者提出了一种简单而强大的正则化技术:R-Drop，旨在利用辍学的随机性。在各种任务和数据集上的实验表明，R-Drop是普遍有效的。它甚至在WMT14翻译数据集上用vanilla Transformer模型实现了SOTA。</p><p id="467b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本帖中，我们将讨论:</p><ul class=""><li id="0a3d" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">R-Drop正规化训练管道。</li><li id="cd32" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">R-Drop理论分析。</li><li id="8768" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">大量实验中R-Drop效应的分析。</li><li id="7693" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">有趣的方面来看R-Drop。</li></ul><p id="e60f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://arxiv.org/pdf/2106.14448.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="kh">官方论文:R-Drop:神经网络的正则化辍学</em> </a></p></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><h2 id="2e65" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jg la lb lc jk ld le lf jo lg lh li lj bi translated">R-Drop正则化</h2><p id="c8fd" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">方法简单。在每个训练步骤中，每个数据样本都要经过一个模型两次。每一遍由不同的子模型处理，子模型通过丢弃进行采样。通过最小化两个输出之间的双向KL散度，训练两个输出分布P1(y | x)和p2(y | x)一致。所以与传统交叉熵训练的区别在于:</p><ul class=""><li id="5301" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">一个训练样本对数据进行两次检查。</li><li id="3dd6" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">双向KL发散被添加到损失项中。</li></ul><p id="ec7d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kh">*辍学可以被视为在每个训练步骤中创建不同的子模型</em></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/d214890ebffcb9c44a04fba9b3269515.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9OtamF3GwICQWSM9wyz4w.png"/></div></div></figure><p id="9f83" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最终损失项如下式所示，它结合了负对数似然损失(交叉熵)L_NLL和双向KL散度L_KL。在两侧测量KL散度:KL(P1，p2)，KL(p2，P1)并计算平均值。α是控制L_KL的权重系数。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/f8db2c1d4e24194ad6a0d010027fc114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K9_yQl2MhZhoLHG53g2m6A.png"/></div></div></figure><p id="a211" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在代码中，训练管道可以描述为:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/9404d81091c81f6755926d4c94e12f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2JW6Tp9x1P5co_bN8RBbWw.png"/></div></div></figure><p id="25eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者建议，为了节省训练成本，输入小批量x '是([x，x])的串联，并且推理实际上不被调用两次。</p><h2 id="a895" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jg la lb lc jk ld le lf jo lg lh li lj bi translated">理论分析</h2><p id="b2a2" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">好的，到目前为止还不错。但是为什么最小化一个额外的KL散度项会对模型的正则化产生影响呢？作者提供了一个理论分析，表明L_KL项限制了网络中参数的自由度。我们将从定义符号开始。</p><p id="2112" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">h (x):具有输入x的NN的第<em class="kh"> l </em>层的输出。</p><p id="b0d9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ξ:从伯努利分布B(p)中抽取的随机向量。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/41d135d08183c511dd037f5c5557cb20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*bjgp6dcU5ASeeHV79lWefg.png"/></div></figure><p id="3fdc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Dropout可以表示为h _ξ (x) = (1/p) ξ ⊙ h (x)，其中⊙是按元素的乘法运算符。神经网络表示为:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/6237d9507c77e424ef70087dbe1980cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fF5Vx9ZSCuz4glG2BU0diw.png"/></div></div></figure><p id="f14a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们进入证明。首先，之前已知，与不使用压差相比，通过控制模型的雅可比矩阵(Colin等人，2020年)，使用压差(L_NLL)最小化损耗<em class="kh">限制了模型的复杂性。我希望在以后的文章中更详细地回顾这篇论文。</em></p><p id="6ca2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者证明了对于全连接的神经网络，例如多层感知器和变压器，双向KL发散损失的约束等价于约束网络的所有参数相等。</p></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><p id="73fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑仅在<em class="kh">一个</em>隐藏节点处彼此不同的两个子结构P1和P2。只包含在P1的隐节点是h_i，只包含在P2的节点是h_j，当我们约束KL(P1 || P2)=0时，这等于P1(y|x) = P2(y|x)，h_i=h_j对于prior to hold应该为真。因此，当h_i=h_j时，KL(P1 || P2)最小</p><p id="2301" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这可以扩展到更一般的压差和架构。更严格的证明，参考原论文附录B节。</p><p id="9248" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">总之，优化混合损失L的约束优化问题寻求一个模型，该模型能够以最小的参数自由度最小化损失L_NLL，从而避免过拟合并提高泛化能力。</p></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><h2 id="73f7" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jg la lb lc jk ld le lf jo lg lh li lj bi translated">消融研究</h2><p id="857d" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">作者建议进行实验，以更好地了解R-Drop的影响，并调整一些超参数，以找到R-Drop的最佳形式。实验在IWSLT14 De→En语言翻译数据集上进行。我们关注数字。</p><ol class=""><li id="fb95" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js ly jz ka kb bi translated">R-Drop <em class="kh">真的</em>有助于调整吗？</li></ol><p id="cce0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与许多其他正则化技术类似，该模型在训练集上收敛缓慢，但最终最优结果在保留数据上更好。即使当相对于时间作图以考虑来自R-Drop的训练成本的增加时，具有R-Drop的模型在可忽略的时间段之后也足够好。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/a7035b39389ed1e43e28e1441f6ac5c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9V0RpQluV5CB4xoWIqKtg.png"/></div></div></figure><p id="f9d3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.如果我们每k步应用R-Drop而不是每次都应用呢？</p><p id="0c74" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者在每k={1，2，5，10}个训练步骤中应用R-Drop进行实验，并在下图中显示结果。虽然训练速度稍快，但这种方法不是很有效，而且网络很容易过拟合。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/0289a765e81ff69e6c8e72d179cfae66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zn3eOeWDSLgRYkxXgS-JQA.png"/></div></div></figure><p id="6025" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.如果我们在每个训练步骤中使用两个以上的输出分布会怎么样？</p><p id="4bea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者对3个输出进行了比较实验，得到BLEU分数为33.30，与2个输出的基本设置的33.25相近。这种收益不值得计算成本的增加。结果表明R-Drop已经具有足够强的正则化能力。</p><p id="90c2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.如果我们改变每一次考试的辍学率呢？</p><p id="ef60" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，P1可以从0.1的小辍学率抽样，而P2从0.3的辍学率抽样。虽然实验仅限于单个基准，但结果并不令人印象深刻。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/716502b89de17a880665426230e6118a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLcSsYuxd_LSdkMsHVs8Rg.png"/></div></div></figure><p id="3d69" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.权重系数α的不同值。</p><p id="5556" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在各种实验中使用了非常不同的α值。对于翻译，α=5被发现是最佳的，而α &lt;1 is used for most other tasks. Reasonably, the value of α depends on how easy overfitting happens, which then depends on the model and data size of each task. Note that the effects of different values of α are significant and is worth searching the correct value of α when applied to a different setting.</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/7854d702880339b9188f2ce3964ec8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*oIWPREfg_I6NKxD-3MeG9A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">Results of different α on translation with vanilla transformer.</figcaption></figure></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><h2 id="9d7e" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jg la lb lc jk ld le lf jo lg lh li lj bi translated">Experiments &amp; Results</h2><p id="3833" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">The authors provide experiments on 4 NLP and 1 CV task including:</p><ul class=""><li id="92e0" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">Neural machine translation(NMT): 6 dataset</li><li id="7614" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">Abstractive summarization: 1 dataset</li><li id="3b05" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">Language understanding: 8 dataset</li><li id="eb86" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">Language modeling: 2 dataset</li><li id="1978" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">Image classification: 2 dataset</li></ul><p id="0f92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Experiments for NMT are done on both low-resource and rich-resource translation tasks. The low-resource scenario is evaluated on the IWSLT translation dataset and the high-resource scenario is evaluated on the WMT translation task. The vanilla transformer model was used as the backbone.</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/6d7abf3be5dccc830ca193bef918c23d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGO7fXKePQRfmM4PUieB9g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Results on Low-resource Neural machine translation</figcaption></figure><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/85c856ff89708209519fec68610d61f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8YV3l5R0-BgXGHLWMj48Sg.png"/></div></div></figure><p id="891f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">R-Drop shows significant improvements in all sub-tasks of the two datasets. Surprisingly, when combined with R-Drop, even the vanilla transformer surpasses all complex models and achieves SOTA performance in the WMT dataset.</p></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><p id="b906" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">The GLUE benchmark of language understanding includes 8 different text classification or regression tasks. The BERT-base and RoBERTa-large pre-trained models are used as the backbone. Results are as the table below:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/e412041b3d5b3cf7987965291ad7b7ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ToNjF17LMqT-EyW4k08YcQ.png"/></div></div></figure><p id="8f08" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">For the regression task STS-B, the authors train the model with an MSE loss that replaces the bidirectional KL loss.</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/2aef5ffcff34e2dd1acd29c239afd9bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*CNYaKamMb7cOHSGi3PCt2g.png"/></div></figure></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><div class="lq lr ls lt fd ab cb"><figure class="mh ij mi mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/7074d055f5db0fe9c4be45cc7043ddf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*5NEO8Qr0gf0vGfXHSi4ZEA.png"/></div></figure><figure class="mh ij mn mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/c965665f59e497bd1b4ed18737203479.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*Hk2TnM1xbkEwyAQu_hkz-Q.png"/></div></figure><figure class="mh ij mo mj mk ml mm paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/9d6146f372ff35ea02c90e7b0b871cc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*oky0vntdIa_8nJdmrDffKw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx mp di mq mr translated">Left: abstractive summarization, Center: language modeling, Right: Image classification w/ViT</figcaption></figure></div><p id="48b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">R-Drop also demonstrates similar positive results in summarization, language modeling, and image classification tasks using transformer-based backbones.</p><h2 id="fa10" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jg la lb lc jk ld le lf jo lg lh li lj bi translated">Conclusion &amp; Thoughts</h2><p id="a8c0" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">In the <em class="kh">结论</em>，作者引用</p><blockquote class="ms mt mu"><p id="757f" class="iv iw kh ix b iy iz ja jb jc jd je jf mv jh ji jj mw jl jm jn mx jp jq jr js hb bi translated">…在这项工作中，我们重点关注基于变压器的模型。我们将把R-Drop应用于其他网络架构，如卷积神经网络。</p></blockquote><p id="5fcf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">…是不是只有我觉得在结论中写这个很奇怪？评估R-Drop与deep CNNs等非变压器网络架构结合使用时的性能会很有意思。不像MLP或变形金刚在整个网络主干中大量使用丢弃，用于图像分类的CNN通常仅在卷积特征提取器和全连接层之间使用一次丢弃。这可以使P1和P2之间的差异小得多，并减少KL损失项。可能有一些对策，如在CNN主干网上增加分层分支，使R-Drop有效。</p><p id="eb4d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一件有趣的事情是测试当R-Drop与其他正则化技术一起应用时，性能是如何受到影响的。好处会重叠还是叠加？</p><p id="c159" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在查看R-Drop的<a class="ae iu" href="https://github.com/dropreg/R-Drop" rel="noopener ugc nofollow" target="_blank"> Github repo </a>时，我发现了一个有趣的问题，即KL损失项在训练期间下降非常快:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/00d50a0e1efe7334ffa8d71fac13eaa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pI8Gz3gtrkSOlMZPZS6HzQ.png"/></div></div></figure><p id="8095" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者回答说</p><blockquote class="ms mt mu"><p id="9985" class="iv iw kh ix b iy iz ja jb jc jd je jf mv jh ji jj mw jl jm jn mx jp jq jr js hb bi translated">至于你的问题，不同的任务可能会得出不一致的结论。例如，在NMT任务中，KL损失从0.7减少到0.15。在我看来，判断R-DROP有效性的一个可能的标准是一个子模型adapt dropout是否对训练有重大影响。例如，KL损失将在没有R-Drop的基线中逐渐增加。</p></blockquote><p id="7bd7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但我认为这是值得注意的。我认为我们可以通过利用这种现象(只是我脑海中的一些东西)来加速培训管道。我首先想到的是每k步应用R-Drop，而不是每一步都应用R-Drop，作者发现这会损害性能。无论如何，这是一个值得注意的有趣观察。</p><p id="fd29" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇论文的一个特别之处是描述得很好的强有力的实验，这些实验是一致的和公平的。大量的实验让我(可能还有其他研究人员)对R-Drop的有效性产生了信任，并使这些观点更有说服力。我感受到了在研究论文中做好实验的重要性。</p></div></div>    
</body>
</html>