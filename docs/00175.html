<html>
<head>
<title>The U-Net for cell segmentation in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中用于细胞分割的U-Net</h1>
<blockquote>原文：<a href="https://medium.com/codex/the-u-net-for-cell-segmentation-in-pytorch-d34dddcdaccb?source=collection_archive---------1-----------------------#2020-12-30">https://medium.com/codex/the-u-net-for-cell-segmentation-in-pytorch-d34dddcdaccb?source=collection_archive---------1-----------------------#2020-12-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/342f2c88bc2f14bea027a44026df8a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvXoKMHoPJMKpKK7keZMEA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在Olaf Ronneberger等人的原始论文中提出的U-Net架构。艾尔。</figcaption></figure><p id="5689" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在本文中，我将介绍如何使用PyTorch实现原始的U-Net框架来分割医学图像。我将首先给出一个U-Net架构的概述，以及它是如何变魔术的；之后将介绍PyTorch的实现。</p><h1 id="3b29" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">U-Net，概述</h1><p id="3f23" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">U-Net于2015年由弗赖堡大学的一个研究小组首次推出。如果你还没有阅读这篇论文，我强烈建议你阅读一下<a class="ae kv" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj">链接</strong> </a>。从顶部可以看到的U-Net架构的图像可以清楚地看出“U-Net”这个名称的来源。U-Net是一个完全卷积网络，由称为编码器和解码器的两侧(左侧和右侧)组成。编码器通过应用核将图像编码到小维度的特征空间中，解码器将该信息映射到空间分类中以执行分割。U-Net的一个关键特征是每个编码器层的输入也连接到其相应解码器的输出，这被称为剩余连接或跳过连接。剩余连接让信息直接流过网络，而不需要应用非线性激活函数(<em class="kw">见下面的注释</em>)。通过在编码器中执行多个操作，丢失了一些空间信息，并且编码器和解码器之间的链接(剩余连接)能够恢复这种丢失的信息，从而产生分割特征。</p><p id="c9c2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">编码器和解码器以相反的方式不同，因为编码器执行所谓的下卷积，而解码器执行上卷积。这些是重要的，因为通过卷积使用填充和汇集技术，创建了图像的特征空间，并进一步将其转移回空间图，从而保持了图像中像素之间的关系。上下卷积是类似于将感兴趣的信息集中在下卷积中，然后用上卷积稀释，从而仅恢复感兴趣的信息的想法。</p><p id="0beb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="kw">注意:我明白剩余连接是如何工作的，但我不完全确定它们为什么工作，我听说它们类似于集合方法的想法，但我仍然觉得有点困惑。如果有人有明确的解释，欢迎在下面评论。</em></p><h1 id="c8c1" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">PyTorch实现</h1><p id="01cd" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">在本节中，首先将介绍核心U-Net架构，然后将介绍使用真实医学图像数据的示例训练循环。</p><p id="9b92" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> U网型号:</strong></p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="lb lc l"/></div></figure><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="f56c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">让它做点酷的事情:</strong></p><p id="aefa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这里，我们将在结肠组织学图像上训练上面提出的U-Net模型，该图像最初在GlaS竞赛中提出。这是一个研究竞赛，来自世界各地的研究团队在细分任务中竞争；查看链接下载数据，阅读更多关于大赛的信息，<a class="ae kv" href="https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/download/" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj">链接</strong> </a>。</p><p id="382c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，我们将定义一个从文件中获取图像数据的函数，将图像标准化并调整其大小(原始图像的大小各不相同，我注意到当图像大小调整为128X128时，结果更快更好)。I函数将两个文件夹(一个文件夹包含“原始”图像，另一个文件夹包含注释)所在的路径、函数应该从哪个图像编号开始以及函数应该从哪个图像停止作为输入。</p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="99b0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">查看一幅图像和相应的遮罩，如下所示:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/b23ffd9b89697f3e8ab992b00c2128ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*g8JOolSU-m8hCfo0PWzmMg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">原始图像(左)和它的面具(右)。</figcaption></figure><p id="d09c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">既然可以正确加载图像，我们现在可以开始训练网络。我使用Google Colab进行培训，因为他们提供了一个很好的笔记本风格的环境，已经安装了大多数标准包，并提供了12 GB的免费GPU使用。使用的训练循环如下所示:</p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="17f6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">训练后，在测试集上评估该模型，得到IoU分数为0.325，F1分数为0.789。模型损失以及原始和预测掩模的例子可以在下面看到。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es le"><img src="../Images/08d808723216ba4f13580844db7fb13e.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*0DakbrzMH40WR7_GkVCB5Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用BCE和Logits的损失。</figcaption></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/73e91ea95419457c90a30c1639b621a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W9Rr0apMGYS8GpHpZIDLgQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">真实遮罩(顶部)和预测遮罩(底部)。</figcaption></figure><h1 id="a5f8" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">结论</strong></h1><p id="a36b" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">有关使用的完整笔记本，请参见下面的链接。当诸如损失函数和图像尺寸的参数被调整时，U-Net的结果变化。U-Net是用于医学图像分割的最著名的架构之一，由于它的流行，在原始U-Net框架的基础上进行了开发，如<a class="ae kv" href="https://arxiv.org/abs/1807.10165" rel="noopener ugc nofollow" target="_blank"> UNet++ </a>和<a class="ae kv" href="https://arxiv.org/abs/1911.07067" rel="noopener ugc nofollow" target="_blank"> ResUNet++ </a>。我希望这篇文章对你有用，如果你有任何提示或建议，请在下面留下评论。</p><p id="ed59" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">完整笔记本:<a class="ae kv" href="https://colab.research.google.com/drive/1Nzv1rfI1_C-_j9yJGUk-OQ-OKMfkKAfE?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1 nzv 1 RFI 1 _ C-_ j 9 yj Guk-OQ-OKMfkKAfE？usp =共享</a></p></div></div>    
</body>
</html>