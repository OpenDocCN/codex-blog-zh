<html>
<head>
<title>Can CPUs leverage sparsity?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CPU可以利用稀疏性吗？</h1>
<blockquote>原文：<a href="https://medium.com/codex/can-cpus-leverage-sparsity-ddf23362350b?source=collection_archive---------4-----------------------#2021-06-14">https://medium.com/codex/can-cpus-leverage-sparsity-ddf23362350b?source=collection_archive---------4-----------------------#2021-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="605f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由机器学习架构的Numenta主管Lawrence Spracklen撰写</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/f697aa46bfe116a2d970da8949c3c66d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jXNlKYv8kFPPYrLJ.jpg"/></div></div></figure><p id="c52f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的<a class="ae jq" href="https://numenta.com/blog/2021/05/27/fast-and-accurate-sparse-networks" rel="noopener ugc nofollow" target="_blank">上一篇博文</a>中，我讨论了我们最近的公告，即我们已经利用极度稀疏性将深度神经网络推理性能提高了112倍以上。这种性能改进是在FPGA平台上实现的，FPGA固有的灵活性允许充分利用稀疏性带来的计算效率。</p><p id="1e68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然这显然是一个令人印象深刻的加速，但当我们的稀疏模型在FPGA上的绝对性能与在其他通用计算平台上使用稀疏模型所能实现的性能相比时，这种加速甚至更有影响力。</p><h1 id="bb66" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">什么是网络稀疏？</h1><p id="c394" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">大多数当前的深度学习网络被称为“密集”网络。在这些密集的网络中，神经元高度互联，高度活跃。这种高活动性和互连性的结合确保了计算每个神经元的输出需要大量的计算，使得使用这些密集的网络成为一项昂贵的任务，需要大量的计算能力，并导致不可持续的能源成本。</p><p id="b440" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Numenta，我们应用神经科学的见解来提高机器智能系统的性能和智能，我们很早就认识到这些密集的网络与人类的新大脑皮层几乎没有相似之处。在人类的新大脑皮层中，神经元相互连接非常稀少，每个神经元只连接到大脑皮层总神经元的一小部分。同样，在任何给定的时间点，只有百分之几的神经元是活跃的。我们可以将这些神经科学见解应用于深度学习，并创建“稀疏”的神经网络。令人难以置信的是，可以创建与可比的密集网络具有相同精度的稀疏网络，但是其中神经元连接到前一层中少至5%的神经元，并且神经元激活被限制在10%以下！</p><p id="10d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不难想象，创建连接数量比标准密集网络少20倍的稀疏网络，同时达到相当的准确性，需要复杂的技术。这是人工智能社区中一个活跃的研究领域，近年来有各种各样的技术被提出和改进。在许多情况下，稀疏网络是通过首先训练密集网络，然后作为最后一步，移除(或“修剪”)最不重要的神经元连接，直到达到期望的稀疏水平来创建的。通常，需要对稀疏网络进行某种程度的额外微调，以便在修剪后恢复精度。相比之下，Numenta专注于能够训练稀疏网络的技术，并且还开发了有效限制神经激活的技术，从而能够创建极其稀疏的深度神经网络。</p><p id="725c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与相应的密集网络相比，这些极其稀疏的网络将计算网络输出所需的计算量减少了一个数量级或更多。因此，使用它们将会带来显著的性能和功耗优势。这正是Numenta在FPGA上演示的内容。在这篇博客中，我研究了CPU上的稀疏网络是否也是如此。</p><h1 id="2951" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">CPU和ONNX引擎</h1><p id="53a2" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在这篇博文中，我将我们的FPGA稀疏模型性能与现代CPU上运行的稀疏模型性能进行了比较(我将在后续的博客中讨论GPU)。为了创建在CPU上使用的稀疏模型，我使用了微软的开源库来修剪预先训练好的密集网络。由此产生的CPU稀疏网络与Numenta <a class="ae jq" href="https://numenta.com/assets/pdf/research-publications/papers/Sparsity-Enables-100x-Performance-Acceleration-Deep-Learning-Networks.pdf" rel="noopener ugc nofollow" target="_blank">白皮书</a>中详述的稀疏网络具有相同的稀疏级别。</p><p id="bd53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为此，我使用PyTorch来训练在<a class="ae jq" href="https://numenta.com/assets/pdf/research-publications/papers/Sparsity-Enables-100x-Performance-Acceleration-Deep-Learning-Networks.pdf" rel="noopener ugc nofollow" target="_blank">白皮书</a>中描述的基线密集卷积神经网络(CNN)。该模型是在谷歌语音命令(GSC)数据集上训练的深度CNN模型，该数据集由成千上万个人说出的65，000个一秒长的关键词话语组成。任务是从音频信号中识别出正在说的单词。该模型具有大约250万个参数，并且实现了96%以上的准确性。</p><p id="0ac1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后使用<a class="ae jq" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> open ONNX模型交换格式</a>导出PyTorch GSC模型，并在24核(48个硬件线程)英特尔至强8275CL(在AWS上，该系统为C5.12xlarge)上调查峰值推理性能。选择这款处理器是因为它支持英特尔最新的DL Boost矢量神经网络指令，并提供充足的内存带宽，以确保计算性能不会受到内存瓶颈的不必要限制。一旦CNN网络被导出为ONNX格式，就有各种各样的开源和商用CPU推理引擎可以用来运行网络。使用几种常见的ONNX推理引擎和ML编译器对性能进行了研究，包括:</p><ul class=""><li id="f695" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><a class="ae jq" href="https://docs.openvinotoolkit.org/latest/index.html" rel="noopener ugc nofollow" target="_blank">英特尔OpenVINO </a></li><li id="b676" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><a class="ae jq" href="https://www.onnxruntime.ai/" rel="noopener ugc nofollow" target="_blank">微软ONNX运行时</a></li><li id="a7ef" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><a class="ae jq" href="https://tvm.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇TVM </a></li><li id="3a99" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><a class="ae jq" href="https://github.com/neuralmagic/deepsparse" rel="noopener ugc nofollow" target="_blank">神经魔深疏</a></li></ul><p id="17a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些推理引擎分析网络，执行各种复杂的优化，并产生定制的实现，该实现被定制为在目标硬件平台上运行特定的网络。因此，使用这些ONNX引擎的推理性能明显快于PyTorch中通常可以实现的性能(即使使用PyTorch TorchScript)。</p><h1 id="6777" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">密集网络性能</h1><p id="82cc" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">作为初始步骤，选择的CPU推理机用于运行基线密集GSC模型，并测量推理性能。这些实验的结果如图1所示。推断性能在很大程度上取决于批量大小(基本上就是有多少样本可供系统并行处理)。图1(a)显示了只包含一个样品的批次的性能，图1(b)显示了一批64个样品可用时的性能。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es li"><img src="../Images/a7ff2b0bfbf9b1f3d293fe9e6f0eab99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pf8hQLPyxw3dlrRHiaOlvA.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><em class="ln">图1:使用普通ONNX引擎的密集CNN推理性能。(a)显示了当一个批次包含单个样本时获得的结果，以及(b)显示了当批次大小增加到64个样本时的结果。</em></figcaption></figure><p id="b65f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图1(a)和图1(b)中，有两组不同的结果。最左边的“单线程”结果说明了当推理引擎被限制使用48个可用硬件线程中的一个时的推理性能。最右边的结果显示了允许引擎充分利用整个处理器时的性能(尽管它们通常需要手动指导来实现最佳性能)。</p><p id="491a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从图1中的结果可以得出一些有趣的观察结果:</p><ul class=""><li id="c010" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">即使当批量大小被限制为单个样本时，允许引擎使用多个硬件线程也提高了性能，说明引擎可以多线程化单个推理操作。</li><li id="9a40" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">当多个样本可用于并行处理时，引擎可以显著更高效地利用可用的计算资源，即使在受限于使用单个硬件线程时也是如此，如图2所示。尽管没有用图形显示空间限制，但我们发现，对于大多数推理引擎来说，存在与批量大小相关联的“幻数”。选择一个“好的”批量可以将性能提高20%或更多。选择的批量大小64是一个神奇的数字。</li><li id="6f44" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">TVM似乎对两个批量产生了相似的结果，虽然它是单样本推断的性能领导者，但它在64个样本运行中明显落后(如果Apache TVM项目的任何人有关于是否有我省略的酷标志/选项/开关来提高性能的指导，请ping我！).</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lo"><img src="../Images/7c3d2ac9386bf7ba4f28af7b22c5cbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bjx2gAaYH7lW5o4-PWulSw.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><em class="ln">图2:普通ONNX发动机批量较大时观察到的性能优势</em></figcaption></figure><h1 id="60cd" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">模型量化的影响</h1><p id="201c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">这个初始ONNX模型对模型权重和激活都使用32位浮点表示。FPGA上使用的基线密集网络使用8位整数权重和激活。为了在CPU和FPGA性能之间进行公平的比较，我使用ONNX引擎附带的工具将CPU GSC模型中的权重和激活量量化为8位整数。请记住，这款英特尔8275CL处理器支持AVX512 <a class="ae jq" href="https://en.wikichip.org/wiki/x86/avx512_vnni" rel="noopener ugc nofollow" target="_blank"> VNNI </a>(矢量神经网络指令)指令，能够高效处理INT8量化网络。</p><p id="540f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该量化网络的结果如图3所示，包括与密集网络的FPGA性能的直接比较(这些实验中使用的FPGA是Xilinx Alveo U250)。为了清楚起见，接下来我只显示64个批量的CPU结果，这在所有情况下都优于单个样本的结果。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es li"><img src="../Images/cd898b0db749e17a09f22630d0e1e72c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rjuwWdJqI8qyGMtn5DNHQ.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><em class="ln">图3:与FPGA密集网络结果相比，使用普通ONNX引擎的量化密集CNN的CPU性能</em></figcaption></figure><p id="5310" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从图3中，可以看到一些关键的观察结果。首先，对于标准的密集网络，24核CPU的性能远远超过FPGA。CPU针对密集的常规计算进行了优化，显然非常出色！其次，将图3中的结果与图1中的结果进行比较，可以清楚地看出，除了TVM结果之外，量化带来的性能改善并不明显。这与预期相反，预期性能提升高达4倍(我们正在从32位浮点运算转向8位整数运算，使处理器的512位AVX向量指令能够并行处理4倍以上的元素)。经过调查，很明显GSC网络中最大池层的存在给ONNX量化工具集带来了问题。在某些情况下，甚至需要手动禁用某些层的量化来实现加速！总的来说，ONNX中的量化支持似乎仍在发展，各种ONNX论坛上都有许多关于速度慢、脆弱和不可移植性的报告。</p><h1 id="2d45" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">稀疏模型性能</h1><p id="f975" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">最后，我们可以比较CPU上运行的稀疏网络与FPGA上运行的Numenta稀疏模型的性能。如前所述，在这篇博客中，我没有使用Numenta的技术来创建这些CPU网络。我们的稀疏模型针对FPGA进行了优化，不会在使用现有CPU工具的CPU上运行得更快。我们正在将我们的方法移植到CPU上。在这篇博客中，我使用当前可用的第三方工具来创建优化的CPU稀疏模型，即基于修剪。因此，为了创建CPU模型的稀疏版本，我使用微软神经网络智能软件来修剪原始的密集模型。CPU稀疏模型的总体稀疏度为95%，与我们的FPGA稀疏模型中使用的稀疏度一致。在图4中，运行在CPU上的稀疏模型的性能与运行在FPGA上的Numenta稀疏模型进行了比较。</p><p id="7203" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图4显示了两个稀疏FPGA结果，分别称为Numenta-SD和Numenta-SS。这些指的是Numenta创建的FPGA稀疏网络。Numenta-SD是我们的稀疏-密集模型，其中权重是稀疏的，但激活被认为是密集的，而Numenta-SS是我们的稀疏-稀疏模型，它充分利用了权重和激活的稀疏性。CPU上使用的稀疏ONNX模型是稀疏密集的。以一种高性能的方式同时利用CPU上的权重和激活稀疏性的能力仍然是一个公开的问题:零权重的位置是预先知道的，允许精心制作的利用它们的存在的内核被构造。零值“激活”的位置是动态的，迫使它们的位置在运行时被动态计算。据我所知，目前没有CPU推理引擎试图利用结合两种形式的稀疏性的乘法能力。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es li"><img src="../Images/8e754322ca9686445f8c4a7300a941ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsQlHX8akWNUkl9wIkcTCw.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><em class="ln">图4:使用不同ONNX引擎的稀疏CPU CNN模型的性能与Numenta的FPGA稀疏模型性能的比较</em></figcaption></figure><p id="ff4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图4中有许多有趣的观察结果。</p><ul class=""><li id="8c2f" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">在FPGA上运行的稀疏网络优于24核CPU。差很多！事实上，FPGA稀疏-稀疏(Numenta-SS)网络的性能比最佳CPU结果高出12倍以上。即使FPGA上较慢的稀疏-密集网络也比CPU快6倍！</li><li id="def5" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">虽然有点难以辨别，但是对于OpenVINO和OnnxRuntime来说，当迁移到稀疏网络时，观察到的性能改进为零。这个观察结果似乎并不是这个模型特有的，而且目前看来这些引擎并没有利用稀疏性的优势！</li><li id="d845" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">TVM和深度稀疏都利用了模型的稀疏性。对于TVM，目前不能利用GSC网络卷积层中的稀疏性，并且性能改善仅限于线性层。</li></ul><p id="2079" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然令人鼓舞的是，TVM和深度稀疏引擎利用网络稀疏性来降低推理成本，但通过利用CPU上的稀疏性获得的性能优势比在FPGA上观察到的要有限得多，如图5所示。不幸的是，在CPU上观察到的稀疏GSC网络的性能改善(即2–3倍)并不罕见，文献中讨论的性能改善也在类似的范围内。这个2–3倍与FPGA实现的112倍形成鲜明对比。这种显著差异的原因有两个:</p><ul class=""><li id="1647" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">正如我在<a class="ae jq" href="https://numenta.com/blog/2021/05/27/fast-and-accurate-sparse-networks" rel="noopener ugc nofollow" target="_blank">之前的博客</a>中所讨论的，稀疏性引入的零值权重和激活的有效随机位置很难在具有面向密集计算的宽向量引擎的CPU上有效利用。对于8位权重和激活的计算，向量VNNI指令允许通过单个指令执行64次乘累加(MAC)运算。因此，在所讨论的稀疏度水平上(20个元素中有1个是非零的)，与强力密集计算相比，即使与显式定位和处理非零开销相关联的较小开销也会迅速削弱性能优势。这与FPGA上的情况形成鲜明对比，FPGA的灵活特性允许有效利用稀疏性。</li><li id="fe93" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">FPGA上的稀疏-稀疏网络同时利用权重和激活稀疏性。并行利用两个维度的稀疏性可以成倍减少执行推理所需的计算量，从而极大地提高FPGA的性能。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lp"><img src="../Images/0cda14e01144d10b62fa3164ffec8813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LEpnnKPu_R36XSqMfN7fYA.jpeg"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><em class="ln">图5:与从密集CNN切换到95%稀疏版本模型相关的CPU上的推理性能优势。在FPGA上，Numenta使用稀疏技术实现了112倍的性能提升</em></figcaption></figure><h1 id="c43c" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">摘要</h1><p id="beab" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">让我们总结一下讨论的内容:</p><ul class=""><li id="e49d" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">深度神经网络通常是“密集的”和计算密集型的。</li><li id="24ac" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">有可能创建“稀疏”神经网络，其匹配密集网络的精度，但是通常需要10到100倍的较少计算来计算结果。理想情况下，我们期望看到与这种降低相称的性能改进和/或功率节省。</li><li id="226e" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">网络权重和激活可以从典型的32位浮点表示量化为8位整数。这不仅减少了模型的内存占用，还为使用向量指令的系统提供了加速，向量指令通常可以并行处理4倍以上的操作。</li><li id="490c" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">不幸的是，GSC CNN网络的量化改进是混合的，CPU对量化ONNX模型的强大支持似乎仍在发展。</li><li id="575d" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">对CPU和FPGA上的密集GSC网络性能的比较显示，CPU处于非常有利的位置。CPU针对密集的常规计算进行了优化，并擅长执行密集的网络。</li><li id="a2b5" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">遗憾的是，在CPU上观察到的稀疏网络的性能优势被弱化了。对于95%的稀疏CNN，相当于神经权重减少了20倍，推理性能的提高被限制在2-3倍左右。事实上，对于两个最著名的CPU推理引擎来说，相对于密集模型，没有任何改进。这不是一种异常现象，这些加速与CPU上稀疏网络通常观察到的情况一致。</li><li id="dac3" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">在FPGA上，Numenta通过稀疏性实现了112倍的性能提升。稀疏性带来的相对加速不仅令人印象深刻，在FPGA上运行的Numenta稀疏-稀疏网络的性能比在24核CPU上运行的稀疏网络高出12倍以上。</li></ul><p id="1cdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，从相对和绝对角度来看，稀疏性在FPGA上实现的令人印象深刻的性能优势清楚地表明了稀疏模型的潜力。不幸的是，虽然FPGA的灵活特性使其成为稀疏模型的理想平台，但观察到针对常规密集计算进行优化的CPU难以有效利用稀疏性带来的计算减少。需要新的稀疏技术来在CPU上实现更大的性能优势。</p><p id="3c54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在后续的博客中，我不仅会对比稀疏网络在GPU上的性能，还会讨论如何使用Numenta的稀疏技术来控制稀疏网络中非零元素的放置，同时保持模型的准确性，从而创建更适合在CPU上高效执行的稀疏模型…敬请关注…</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lq"><img src="../Images/85f13517b0a73031664b29ceb003288d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ON4aHffEwstiKI7t.png"/></div></div></figure><p id="a575" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢阅读这篇文章，请点击下面的“鼓掌”图标，推荐给你的关注者。要了解我们的工作如何帮助推进大脑理论和机器智能的发展，请访问我们的 <a class="ae jq" href="https://numenta.com/" rel="noopener ugc nofollow" target="_blank"> <em class="jd">网站</em> </a> <em class="jd">或关注我们的</em><a class="ae jq" href="https://twitter.com/Numenta" rel="noopener ugc nofollow" target="_blank"><em class="jd">Twitter</em></a><em class="jd">和</em> <a class="ae jq" href="https://www.facebook.com/OfficialNumenta/" rel="noopener ugc nofollow" target="_blank"> <em class="jd">脸书</em> </a> <em class="jd">。</em></p><p id="9f3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">原载于2021年6月14日numenta.com</em><em class="jd">的</em> <a class="ae jq" href="https://numenta.com/blog/2021/06/14/can-cpus-leverage-sparsity" rel="noopener ugc nofollow" target="_blank"> <em class="jd">。</em></a></p></div></div>    
</body>
</html>