<html>
<head>
<title>Decision trees in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的决策树</h1>
<blockquote>原文：<a href="https://medium.com/codex/decision-trees-in-python-98ca587f4329?source=collection_archive---------11-----------------------#2021-08-27">https://medium.com/codex/decision-trees-in-python-98ca587f4329?source=collection_archive---------11-----------------------#2021-08-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/2c9c7a4539a03c8dec98edb084795b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0kaTr3Cbtgr1RP4KVApKw.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">(图片来源:<a class="ae hv" href="http://500px.com/photo/77479191/angel-oak-by-daniela-duncan" rel="noopener ugc nofollow" target="_blank">丹妮拉·邓肯</a>)</figcaption></figure><div class=""/><p id="24a9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">决策树是一种<em class="jt">监督的</em>机器学习模型，用于分类和回归任务(CART)。它们易于实现和解释，是最常用的机器学习工具之一。</p><p id="442c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">树是<em class="jt">非参数</em> <em class="jt">非线性</em>模型，这意味着它们的函数可以根据需要采用任意多的参数<em class="jt"> ( </em>不同于固定在<em class="jt"> a </em> =截距、<em class="jt"> b </em> =斜率<em class="jt"> ) </em>上的线性回归，并生成<em class="jt"> X </em>到<em class="jt"> y </em>的非线性映射。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="0f1d" class="kb kc hy bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">分类和回归树</h1><p id="fd0e" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">决策树可以分为两类:</p><ol class=""><li id="d47d" class="le lf hy ix b iy iz jc jd jg lg jk lh jo li js lj lk ll lm bi translated"><em class="jt">回归树</em>:目标变量可以取连续值的决策树(<em class="jt">通常是数字</em>)。</li><li id="6089" class="le lf hy ix b iy ln jc lo jg lp jk lq jo lr js lj lk ll lm bi translated"><em class="jt">分类树</em>:目标变量取一组离散值的树模型(<em class="jt">类</em>)。</li></ol><p id="3837" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所有的树都由<em class="jt">节点</em>和<em class="jt">分支</em>组成。第一个节点也被称为<em class="jt">根节点</em>，最终节点(预测)被称为<em class="jt">叶节点</em>，中间的节点是<em class="jt"/><em class="jt">内部节点或决策节点。</em>节点由<em class="jt">分支连接。</em></p><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es ls"><img src="../Images/8a74b10b92bd6ee67b06a41258281f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*ekWgr-yVc-ba6DHC_9FeRA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图1 —决策树</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="1dfe" class="kb kc hy bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">主旨:信息增益</h1><p id="1de3" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">决策树将根据在该阶段实现最大信息增益的特征，将每个<em class="jt">父节点</em>分成两个<em class="jt">子节点</em>。这个过程将从根节点(第一次分裂)开始，并且将继续，直到所有子节点都是纯的，或者直到信息增益为0(除非我们手动设置一个“max_depth”参数)。</p><p id="8154" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">信息增益通过分割给定特征上的数据来衡量<em class="jt">熵</em>(数据集的<em class="jt">杂质</em>或<em class="jt">随机性</em>的度量)减少了多少，并且树通过连续分割特征来增长其分支，从最具预测性到最不具预测性。</p><p id="f058" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最小化熵相当于最大化信息增益，这正是树在每次分裂时要达到的目标。</p><p id="351c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在具有<em class="jt"> n </em>不同概率类别<em class="jt"> P、</em>的数据集中，测量节点<em class="jt"> I、</em>杂质的两个主要标准是<strong class="ix hz">基尼指数:</strong></p><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es lx"><img src="../Images/fbb84288d0a7e610ce497e1e06784bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/1*suPp3x4oOBkATtbUkbZ5Lw.gif"/></div></figure><p id="5a39" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">和<strong class="ix hz">熵:</strong></p><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/0a5bc299111a187ffb8bfb237593b000.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/1*2ztW2bSuilGRPQnnqbXJTg.gif"/></div></figure><p id="dc75" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基尼指数往往在计算上更有效，因此它最常用于决策树算法中。对任何有兴趣了解更多的人，我推荐这个视频。这帮助我将基尼指数背后的想法形象化，并最终更好地理解树形模型是如何工作的。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="cbf3" class="kb kc hy bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">用Scikit-Learn实现分类树</strong></h1><p id="cd4a" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">在下一节中，我将编写一些代码来演示分类树在众所周知的Iris数据集上的简单应用。这可以通过Scikit-Learn轻松访问，并随时进行处理:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="9c86" class="me kc hy ma b fi mf mg l mh mi">from sklearn import datasets<br/>iris = datasets.load_iris()</span></pre><p id="cdab" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">…但我将把它作为. csv文件导入，使这个应用程序更通用。</p><p id="bf0b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们开始导入所有模块:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="a59f" class="me kc hy ma b fi mf mg l mh mi">import pandas as pd<br/>import numpy as np<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_split<br/>import matplotlib.pyplot as plt</span></pre><p id="bb67" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">导入和读取数据集:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="0d33" class="me kc hy ma b fi mf mg l mh mi">df = pd.read_csv("Desktop/Iris.csv")<br/>print(df)</span></pre><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/029c579c7257025b7d5ccb24090e102e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*iP-OB0XpZHdHrJlrFX3lRg.jpeg"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图2—虹膜数据集</figcaption></figure><p id="2f5d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的特征<em class="jt"> X </em>是<em class="jt"> SepalLenghtCm，SepalWidthCm，PetalLenghtCM，PetalWidthCM </em>和<em class="jt">T9】我们的目标变量<em class="jt"> Y </em>是物种。</em></p><p id="63d5" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了使用我们的特性，我们需要创建一个矩阵<em class="jt"> X </em>，其中每行代表一个<em class="jt">实例</em>，每列代表一个<em class="jt">特性。</em>我们可以使用一个NumPy数组来实现这一点，该数组从感兴趣的列中获取数据，然后对其进行转置。我们还需要创建一个列向量<em class="jt"> y </em>，这是我们的目标变量(花卉种类):</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="b479" class="me kc hy ma b fi mf mg l mh mi">X = np.array([df.SepalLengthCm, df.SepalWidthCm, df.PetalLengthCm, df.PetalWidthCm]).T</span><span id="a37c" class="me kc hy ma b fi mk mg l mh mi">y = np.array(df.Species).reshape(-1,1)</span><span id="b35b" class="me kc hy ma b fi mk mg l mh mi">print(X.shape) -&gt; <em class="jt">Out: (150, 4)</em><br/>print(y.shape) -&gt; <em class="jt">Out: (150, 1)</em></span></pre><p id="39fd" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">(注意:树不需要特征缩放，所以我们不担心这个。)</em></p><p id="95fb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们将数据整理好，我们就可以开始创建决策树分类器了:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="3aba" class="me kc hy ma b fi mf mg l mh mi">dtc = DecisionTreeClassifier(max_depth = 4, random_state=0)</span></pre><p id="340d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，我们将数据集分为训练集和测试集:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="cec0" class="me kc hy ma b fi mf mg l mh mi">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span></pre><p id="88a2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使分类器适合训练集:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="6879" class="me kc hy ma b fi mf mg l mh mi">dtc = dtc.fit(X_train, y_train)</span></pre><p id="3422" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，对测试集上的分类器进行评分，以检查模型<em class="jt">的准确性</em>:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="d90f" class="me kc hy ma b fi mf mg l mh mi">score = dtc.score(X_test, y_test)<br/>print(score) -&gt; O<em class="jt">ut: 0.9736842105263158</em></span></pre><p id="7038" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的模型是完整的:<em class="jt"> 0.97%的准确率，</em>对于这样一个简单的分类器来说一点也不差！</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="ac96" class="kb kc hy bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">这是我们的树的样子</h1><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="40ac" class="me kc hy ma b fi mf mg l mh mi">fn=['sepal length','sepal width ','petal length ','petal width ']<br/>cn=['Setosa', 'Versicolor', 'Virginica']</span><span id="1e1c" class="me kc hy ma b fi mk mg l mh mi">plt.figure(figsize=(10,10))<br/>tree.plot_tree(dtc, feature_names = fn, class_names=cn, filled = True)</span></pre><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es ml"><img src="../Images/c6baeb090f17f7946a67f2b17f1fa6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*Pp7lak65qcfFDUh6PQjblw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图3 —我们的分类树</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="9da3" class="kb kc hy bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">使用我们的模型进行预测</h1><p id="d73d" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">给定一朵新的花(实例),我们可以这样开始预测它的种类:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="87d8" class="me kc hy ma b fi mf mg l mh mi">new_flower = np.array([5.5, 2.8, 5, 2]).reshape(1,-1)<br/>prediction = dtc.predict(new_flower)<br/>print(prediction) -&gt; <em class="jt">Out: 'Iris-virginica'</em></span></pre><p id="3d0b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的分类器似乎工作得很好，我们今天做得相当不错！</p><p id="cb12" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">决策树可以被调整(它们的<em class="jt">超参数</em>)、增强，也可以被组合来创建随机森林。还没完…</p><h1 id="94cb" class="kb kc hy bd kd ke mm kg kh ki mn kk kl km mo ko kp kq mp ks kt ku mq kw kx ky bi translated">编写基本决策树</h1><p id="670c" class="pw-post-body-paragraph iv iw hy ix b iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo ld jq jr js hb bi translated">总之，实现这个分类树的代码是:</p><pre class="lt lu lv lw fd lz ma mb mc aw md bi"><span id="ecc0" class="me kc hy ma b fi mf mg l mh mi">import pandas as pd<br/>import numpy as np<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_split</span><span id="e0e3" class="me kc hy ma b fi mk mg l mh mi">df = pd.read_csv("Desktop/Iris.csv")</span><span id="bf44" class="me kc hy ma b fi mk mg l mh mi">X = np.array([df.SepalLengthCm, df.SepalWidthCm, df.PetalLengthCm, df.PetalWidthCm]).T</span><span id="d3cd" class="me kc hy ma b fi mk mg l mh mi">y = np.array(df.Species).reshape(-1,1)</span><span id="2be7" class="me kc hy ma b fi mk mg l mh mi">dtc = DecisionTreeClassifier(max_depth = 4, random_state=0)</span><span id="fa20" class="me kc hy ma b fi mk mg l mh mi">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span><span id="df23" class="me kc hy ma b fi mk mg l mh mi">dtc = dtc.fit(X_train, y_train)</span><span id="6331" class="me kc hy ma b fi mk mg l mh mi">score = dtc.score(X_test, y_test) <br/>print(score)</span></pre></div></div>    
</body>
</html>