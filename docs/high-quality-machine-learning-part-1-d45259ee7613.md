# 高质量 ML —第 1 部分—功能验证

> 原文：<https://medium.com/codex/high-quality-machine-learning-part-1-d45259ee7613?source=collection_archive---------9----------------------->

![](img/54f74436f24475f30af4b51e0ce6b392.png)

[Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上 [AltumCode](https://unsplash.com/@altumcode?utm_source=medium&utm_medium=referral) 拍摄的照片

去年，业界更加关注将机器学习(ML)模型从实验室转移到现实生活中的部署。在他们对构建越来越酷的 ML 模型的热爱中，ML 团队没有对一些关键的软件工程方面给予足够的关注，而这些是我们在过去几十年中艰难地学习到的。可以作为商业软件一部分的 ML 模型应该是高质量的(功能性的)，健壮的，符合规范的，可解释的。本文快速浏览了数据科学生命周期(DSLC)中定义质量的这些方面。

实际上，DSLC 被视为三个迷你生命周期——数据管理、模型构建和部署管理。在本文中，我将部署管理阶段进一步分为两个子部分——接受服务(AIS)和实时服务(ILS)。AIS 包括必要的独立确认和验证功能，如集成测试、业务验收测试等。而 ILS 更多的是维护和监控生产系统中的 ML 模型的操作功能。

![](img/57044314d57120e98ff28f4f7d0f4be2.png)

DSLC 阶段

在企业设置中，通常没有有价值的项目是一次性的。交付的应用程序必须是可维护的，并在很长一段时间内持续开发。没有一个单独的组件能够提供商业价值。应该为 DSLC 设计一个经过深思熟虑的配置管理计划，为所有的项目工件提供长的生命周期。尽管我们不会明确地讨论它们，但在接下来的讨论中，必须记住这些基本方面。

以下主要方面应该是现场拍摄 ML 模型的 DSLC 的一部分。

*   功能验证—通过充分的测试，包括单元、集成、业务验收测试，确保 ML 模型的功能性能。代码级验证也是端到端验证策略的重要组成部分。
*   ML 公平:ML 模型是公平的，不偏袒或不偏袒任何一个群体或群体的集合
*   安全 DSLC:数据科学生命周期遵循安全数据和模型构建实践

除了数据隐私之外的上述三个需求(我们不在本文中讨论)从根本上形成了 ML 模型的质量基础。让我们分别来看一看。

# 功能验证

问题的早期识别和处理对于将质量成本保持在最佳水平至关重要。数据工程师对质量的看法将围绕标准的数据质量度量。询问任何一个 ML 模型工程师关于测试的问题——答案将主要涉及测试和验证数据集。但是 DSLC 必须为交付的 ML 模型提供全面的质量保证。

# 数据管理阶段

作为数据管道活动的一部分，数据工程师一直在进行必要的检查，以确保基本的数据质量目标，如重复检查、缺失值等。例如，Apache Nifi 和 Airflow 都有验证器机制。由于正在进行大数据/商业智能/分析项目，与其他阶段相比，此阶段的测试要求更加成熟。当前的重点是摆脱批处理模式思维，使数据管理阶段尽可能接近 CI/CD 要求。在过去的几年里，像 Great Expectations 和 Tensorflow Data Validation (TFDV)这样的专业框架已经建立起来，重点放在数据的早期测试上。这些框架具有专门的测试功能，同时将验证需求从数据管道的“编程”部分中分离出来。

# 模型构建阶段

在模型构建期间减少噩梦般诊断运行的一个重要步骤是在开始 ML 模型训练之前检查数据。虽然在接收期间应该对数据质量进行一定程度的保证，但是数据在数据管道中要经过许多级别的转换才能到达这一步。强烈建议在这个阶段进行防御性编程，进行数据形状验证、模式检查、数据异常等检查。这些可以通过混合使用 python 代码和数据库函数调用的内联编程来实现，例如像*display _ anomolies/validate _ statistics*这样的 TFDV 函数。

使用测试数据集的构建时间验证确保 ML 模型在原始数据分布的约束内工作。一个 ML 模型从开发到生产的准备状态不仅仅依赖于像 F1 分数、AUC、ROC 等综合性能指标。理论上，即使在高性能的 ML 模型中，有效的场景也可能在故障中被过度表现。只有当数据科学家研究结果和相关的测试数据(逐个样本)时，她/他才能发现这种偏差。手动验证 ML 模型性能的这个非常重要的方面经常被 ML 工程师跳过。

# 部署管理— AIS 阶段

集成测试:在基于 ML 的应用程序替换现有的非 ML 应用程序的项目中，集成测试的经验法则应该是成功标准不必因为 ML 的引入而改变。测试用例层次上的任何适应 ML 引入的变化都必须由测试经理清楚地分析和签署。

验收测试:验收测试的真正价值来自于最终用户在应用程序发布前对它的信任程度。用 ML-model-in-core 应用程序建立信任可能更加困难。由于其不确定性，基于 ML 模型的系统与传统的编程系统相比可能看起来太不稳定。最优选的方法是通过简单易用的界面给用户提供一种使用 ML 模型的方法，这种界面给了她/他对 ML 模型行为的直觉。通过让用户在理解的限度内改变特征值(产生受控扰动)来观察对预测的影响，可以推断出行为。如果用户创建一个扰动，期望推断从原始预测改变，行为*可能是*一个关注的原因。斜体是有意表示意外的 ML 模型行为不必在所有情况下都是问题。将这种实验与 ML 模型的可解释性结合起来，将使用户对 ML 模型的工作有更深的理解，并对新系统产生信心。IBM 的 AIX360 是一个很好的框架。除此之外，建立一种运行反事实的方法是窥视复杂 ML 模型神秘工作的另一种方式。Google 的 What-if 工具和 Cortex CertifAi 工具为实现这一目标提供了一个很好的框架。

# 部署管理— ILS 阶段

推理时间数据输入也必须不断评估。检查为训练服务的数据分布偏差有助于在问题触及部署的 ML 模型之前识别问题。为了进行这种评估，我们需要对推理数据和与活动 ML 模型相关联的基线训练数据进行持续的比较。不对称趋势可以提供一段时间内的偏差度量，并有助于在系统崩溃之前采取必要的措施。TFDV 提供了方便的*generate _ statistics*/*validate _ statistics*函数来创建训练基线并验证推理数据。对于非结构化数据，使用基于直方图&质心的基线的新方法开始在该领域得到应用。

使用初始现场测试，如 A/B、多臂 Bandit、Canary 测试等。在全面铺开之前，现在越来越普遍，而且成熟得很快。由于对 ML 模型性能的真正测试是用实时数据进行的，所以在可行的情况下建立这种实时随机控制实验是非常可取的。这些测试的成功显然取决于 KPI 的选择、群组选择和测试的长度。由于数据是实时的，所以很容易对指标做出快速反应。但是由于统计学的特质，这也是我们开始做这类测试的主要原因，最好让测试自然进行，研究结果并采取行动。

# 关键要点

*   强烈建议在数据管道阶段使用专业的测试框架，如 Great Expectations & TFDV
*   构建时验证是 ML 模型性能度量和故障模式场景手动检查的组合，以确保适当的 ML 行为
*   在验收期间，测试阶段业务用户应该被给予正确的工具来使用 ML 模型并建立信任
*   适当的 ML 操作策略包括使用基线训练数据和实时推断数据监控数据漂移，以防止实时 ML 模型故障
*   使用随机控制实验来确保 ML 模型在实际交通条件下表现良好

*我们将覆盖 ML 公平和安全 DSLC 的部分*[*2*](https://mukund-kannan.medium.com/high-quality-machine-learning-part-2-2643c144a2a7)*&*[*3*](https://mukund-kannan.medium.com/high-quality-machine-learning-part-3-c660b885233c)*。第 3 部分结尾的致谢和参考。*