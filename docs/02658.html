<html>
<head>
<title>How do Vision Transformers work? An Image is Worth 16x16 Words</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视觉变形金刚是如何工作的？一幅图像相当于16x16个字</h1>
<blockquote>原文：<a href="https://medium.com/codex/how-do-vision-transformers-work-an-image-is-worth-16x16-words-df47aed1b634?source=collection_archive---------7-----------------------#2021-07-30">https://medium.com/codex/how-do-vision-transformers-work-an-image-is-worth-16x16-words-df47aed1b634?source=collection_archive---------7-----------------------#2021-07-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/73d2e3bac444f2723b46ce94bfa130c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m8JUdhaNB1ZxpVkE"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">阿瑟尼·托古列夫在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="c0aa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">变形金刚，一个完全由注意力组成的架构，在发布后已经超越了竞争对手的NLP模型。这些强大的模型非常高效，随着最近GPT-4的发布，可以扩展到数十亿甚至数万亿个参数。他们受益于不断增长的数据集大小和计算限制。它们也可以很好地推广到其他应用程序，通过预训练的BERTs被微调并应用于许多应用程序的巨大成功就可以说明这一点。</p><p id="1926" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，先前全注意力网络在大规模计算机视觉中的应用并不成功。主要是因为先前提出的自注意机制在中/大图像中是不可行的，因为复杂度依赖于像素的数量。有些模型用GPU加速非常棘手，就像为什么循环模型跟不上变形金刚一样。</p><p id="a5f6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">向vision transformer (ViT)问好，它通过将图像拆分为多个补丁，以一种简单的方式将图像应用于变形金刚。视图像补丁为单词，ViT向转换器提供补丁的嵌入。ViT在中型数据集(如ImageNet和CIFAR100)中实现了有竞争力的性能。当应用于更大的数据集时，结果甚至会进一步改善，其中ViT能够实现类似的结果或在一些基准中击败CNN。</p><p id="9de4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，让我们来看看ViT是如何工作的，为什么他们在一些问题上有好有坏。该论文可通过以下链接获得。</p><p id="2277" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">一幅图像值16X16个字:大规模图像识别的变形金刚</a></p><p id="d5e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">免责声明:本帖不解释原变形金刚模型的概念，也不解释多头注意力等变形金刚的积木。</em></p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="41e9" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">ViT架构概述</h2><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/fa655fd1b9a8b9b37bea845a86560471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vz2o_py2CZKiX07aB8abAQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">ViT概述</figcaption></figure><p id="96ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输入图像首先被分割成大小为P×P的小块。每个小块被展平并线性映射到一个D维向量，称为ViT中的嵌入阶段。来自原始转换器和类标记的位置嵌入被添加到补丁嵌入中。由于基于x，y位置的2D位置嵌入对模型没有帮助，因此位置作为一个数字输入。上面的过程将图像补丁转换成令牌。处理后的令牌输入与常规的NLP任务没有什么不同。因此，不对编码器变压器模型进行修改。</p><p id="6abd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ViT还可以结合CNN以进一步提高性能。该论文提出了一种混合模型，通过输入由CNN计算的特征图来代替输入原始图像。</p><h2 id="0fe3" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">培养</h2><p id="445a" class="pw-post-body-paragraph iv iw hi ix b iy lb ja jb jc lc je jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">ViT在大型数据集上进行了预训练，并针对较小的数据集进行了时间调整。由于ViT可以接收任意长度的序列，因此面片数量会发生变化，以保持面片大小一致。这样，在数据集之间进行转换时，线性投影和输入图层将保持不变。由于不同的类类型，在更改数据集时，仅替换最终的预测图层。</p><h1 id="0ce2" class="lg kc hi bd kd lh li lj kh lk ll lm kl ln lo lp ko lq lr ls kr lt lu lv ku lw bi translated">实验</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/435ceebc5d57234aae70c08588ad3ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2bgxaDgdZD9PKQGqg27sQ.png"/></div></div></figure><p id="fd61" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文展示了基于BERT的实验中使用的ViT的三种变体。第一个实验改变了网络架构，并在许多数据集上与之前的《SOTA:吵闹的学生》进行了比较。当ViT-H模型(ViT-Huge)在大型JFT-300M数据集上进行预训练时，它在几乎所有指标上都优于以前的基线。甚至花费更少的时间来训练。下图描述了该实验。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/ea3de1dbc2c618684702b58b3c338042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hd78s2AVKR2SssReAwCg1A.png"/></div></div></figure><h2 id="c5f7" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">大型预训练数据集的需求</h2><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/0fef83861c18df03e4e9d27132eec13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zez69NR4saD5c7Ag4DDsGg.png"/></div></div></figure><p id="5ed6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所描述的ViT架构很少利用图像的2D结构。作者认为，这将导致比CNN更少的图像特定的归纳偏差，并且该模型必须从头学习空间关系。这自然增加了对更大数据集进行训练/预训练的需求。</p><p id="ae9f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上图说明了较大的模型(ViT-H，ViT-L)在相对较小的训练数据集上表现较差。他们似乎很容易过度适应数据集。考虑到大型模型被大量正则化，我们可以看到真正利用虚拟仪器的计算能力所需的图像数量非常大。</p><h2 id="31c1" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">为更大的变压器缩放模型</h2><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/fe18635bd649f2a893c91cb17a97311a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*octALT0tPcTAnNesOGPFJQ.png"/></div></div></figure><p id="0b5c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一个实验评估训练计算需求和准确性，通常被称为模型的<em class="jt">帕累托曲线</em>。在复杂性权衡方面，ViT总是优于其竞争对手CNN。当比较利用CNN特性的混合模型和不利用CNN特性的简单模型时，混合模型在小的设置上似乎确实优于ViT。然而，经过一些扩展后，性能差距似乎消失了。令人惊讶的是，维生素t可能不需要中枢神经系统。另一个发现是，性能图似乎还没有饱和，这与常规CNN的帕累托曲线不同。这就留下了进一步扩展以提高性能的可能性。</p><h1 id="6878" class="lg kc hi bd kd lh li lj kh lk ll lm kl ln lo lp ko lq lr ls kr lt lu lv ku lw bi translated">结论</h1><p id="6602" class="pw-post-body-paragraph iv iw hi ix b iy lb ja jb jc lc je jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">在这篇文章中，我们回顾了最初的视觉转换器架构和从实验中发现的vit的属性。ViT将图像补丁转换为标记，标准转换器直接应用于标记，将它们解释为单词嵌入。实验表明，与CNN相比，在图像分类方面取得了令人满意的结果。虽然它们似乎需要大数据集来进行有竞争力的训练和预训练，但它们有潜力进一步扩展。</p><p id="6164" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该论文还指出，ViTs的另一个挑战是应用于其他计算机视觉任务，如分割和对象检测。对我来说，这些任务不能用将图像裁剪成小块的ViT方法来解决。但是变形金刚在视觉上的应用正引起越来越多的兴趣。SOTA有线电视新闻网在许多领域已经被变压器取代。关注视觉变形金刚的进展。</p></div></div>    
</body>
</html>