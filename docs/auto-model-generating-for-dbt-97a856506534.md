# DBT çš„è‡ªåŠ¨æ¨¡å‹ç”Ÿæˆ

> åŸæ–‡ï¼š<https://medium.com/codex/auto-model-generating-for-dbt-97a856506534?source=collection_archive---------5----------------------->

## ç”¨äºå–æ¶ˆåµŒå¥—è½¬ç§»æ•°æ®(çº¢ç§»)çš„ç®€å• CLI

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ Python æ¥è‡ªåŠ¨åŒ–æˆ‘ä»¬çš„ dbt æ¨¡å‹ç”Ÿæˆæ­¥éª¤ã€‚

æˆ‘ç ”ç©¶å¾®æœåŠ¡å’Œ Kubernetes å·²ç»å¿« 2 å‘¨äº†ã€‚å› æ­¤ï¼Œæˆ‘ä¸èƒ½é¢ å€’æ–°çš„ä¸œè¥¿æ¥å†™ğŸ« ã€‚å®é™…ä¸Šï¼Œæˆ‘å·²ç»åœ¨æˆ‘æ­£åœ¨åšçš„é¡¹ç›®ä¸­å®ç°äº†ä¸€äº›æ–°çš„æŠ€æœ¯æ ˆã€‚ä¸ºäº†è®©ä½ å¯¹æœªæ¥æ„Ÿåˆ°å…´å¥‹ï¼Œæˆ‘ä¼šè¯´æˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å®ç° OpenSearch æˆ‘ä»¬çš„é¡¹ç›®æ¥åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„æœç´¢æœåŠ¡ã€‚ğŸ¥¸

![](img/9417a76af9a202161ff00e57a47734a1.png)

ç…§ç‰‡ç”±[å¢å¡æ–¯](https://unsplash.com/@lukash?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

# æˆ‘çš„æ—¥å¸¸ä»»åŠ¡ä¹‹ä¸€(æœ‰æ—¶ğŸ˜‹)

æˆ‘åœ¨ä¸€å®¶å’¨è¯¢å…¬å¸å·¥ä½œï¼Œæˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„ä¸€ä¸ªå®¢æˆ·å¼€å‘ä¸€ä¸ª IT åŸºç¡€è®¾æ–½é¡¹ç›®ã€‚å®¢æˆ·çš„æ•°æ®å›¢é˜Ÿéœ€è¦æ›´æ–°æš‚å­˜è¡¨å’Œæ•°æ®é›†å¸‚ï¼Œä»¥æ”¹è¿›ä»–ä»¬çš„æŠ¥å‘Šã€‚ä¸ºäº†æ›´æ–°ç›¸å…³çš„è¡¨ï¼Œæˆ‘éœ€è¦æ£€æŸ¥åµŒå¥—çš„æ•°æ®ç»“æ„(JSON æ ¼å¼)å¹¶æ›´æ–°æˆ‘çš„ dbt æ¨¡å‹ä¸­æ·»åŠ æˆ–åˆ é™¤çš„åˆ—ã€‚ç›´åˆ°å‡ºç°äº†ä¸€ä¸ªå‡ ä¹æœ‰ 75 åˆ—çš„è¡¨æ ¼ï¼Œæˆ‘æ‰è§‰å¾—å¥½å—äº›ã€‚ğŸ¤¯é‚£ä¸€æ¬¡ï¼Œæˆ‘ç¼–å†™äº†ä¸€ä¸ªç®€å•çš„ Python cliï¼Œè€Œä¸æ˜¯èŠ±è´¹æ—¶é—´é€è¡Œè½¬æ¢ 75 åˆ—ã€‚æˆ‘å·²ç»åœ¨æˆ‘ä»¬çš„ VCS ä¸Šå‘å¸ƒäº†å›è´­ï¼Œå£å·å¦‚ä¸‹ã€‚

> æˆ‘åˆ›å»ºäº†è¿™ä¸ªè„šæœ¬ï¼Œå› ä¸ºæˆ‘æ‡’å¾—ä¸€è¡Œä¸€è¡Œåœ°å†™:&

# DBT æ¨¡å‹ç”Ÿæˆå™¨

æˆ‘ä»¬çš„æ–‡ä»¶å¤¹æ ‘å¦‚ä¸‹æ‰€ç¤ºã€‚

```
â”œâ”€â”€ helpers
â”‚   â”œâ”€â”€ connection.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ parser.py
â”‚   â””â”€â”€ writer.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ script.py
```

æ‚¨å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç å®‰è£…æ‰€éœ€çš„åŒ…ã€‚

```
pip install black SQLAlchemy sqlalchemy-redshift psycopg2-binary
```

## è¿æ¥æ¨¡å—

é€šè¿‡ä½¿ç”¨ä¸‹é¢çš„å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°è·å¾—æ•°æ®åº“è¿æ¥ã€‚è¿™ä¸ªå‡½æ•°è¿”å›æ•°æ®åº“è¿æ¥ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`con`å¯¹è±¡æ¥æ‰§è¡Œ sql å‘½ä»¤ã€‚

```
# helpers/connection.py

from sqlalchemy import create_engine

def get_engine():
    """Returns Redshift engine

    Returns:
        engine: Redshift connected engine
    """
    return create_engine(
        "redshift+psycopg2://<YOUR_URI>" # ofcourse you can use env variables
    )
```

## åˆ†æå™¨æ¨¡å—

è§£æå™¨æ¨¡å—å¸®åŠ©æˆ‘ä»¬è§£ææ¯ä¸€åˆ—åŠå…¶ç±»å‹(å¯¹äºçº¢ç§»)ã€‚

```
# helpers/parser.py

import json
from datetime import datetime

class TypeChecker(object):
    @staticmethod
    def is_timestamp(val: str) -> bool:
        try:
            datetime.fromisoformat(val)
            return True
        except:
            return False

    @staticmethod
    def is_integer(val: str) -> bool:
        try:
            int(val)
            return True
        except:
            return False

    @staticmethod
    def is_decimal(val: str) -> bool:
        try:
            float(val)
            return True
        except:
            return False

    @staticmethod
    def is_varchar(val: str) -> bool:
        try:
            str(val)
            return True
        except:
            return False

def get_data_type(col_value: str) -> str:
    if TypeChecker.is_timestamp(col_value):
        return "timestamp"
    elif TypeChecker.is_integer(col_value):
        return "integer"
    elif TypeChecker.is_decimal(col_value):
        return "decimal(16,2)"
    elif (
        TypeChecker.is_varchar(col_value) == False
    ):  # if it's not varchar, it has to be examined
        return "UNDEFINED"
    else:
        return "varchar"

def get_parsed_data(res: str) -> dict:
    """Return passed data columns' types

    Args:
        res (str): JSON format data in STR!

    Returns:
        column_types (dict): columns are keys and data types are values
    """
    res = json.loads(res)

    column_types = {}

    for col in res:
        column_types[col] = get_data_type(res[col])

    return column_types
```

## å†™å…¥æ¨¡å—

è¿™ä¸ªæ¨¡å—å¸®åŠ©æˆ‘ä»¬ç”Ÿæˆ dbt æ¨¡å‹ç»“æ„(å­—ç¬¦ä¸²),å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

`json_identifier."col_name"::col_type as col`

```
# helpers/writer.py

def get_dbt_formatted_sql(data: dict, json_col_id: str) -> str:
    rows = [
        f'{json_col_id}."{col}"::{data[col]} as {col}'
        for col in data
        if col != "_extract_timestamp"
    ]
    output = ",\n".join(rows)
    output += f',\n{json_col_id}."_extract_timestamp"::timestamp as _extract_timestamp'
    return output
```

## ä¸»è„šæœ¬

æˆ‘ä»¬å¯ä»¥ç›´æ¥è¿è¡Œè¿™ä¸ªè„šæœ¬æ¥ä¸ºåµŒå¥—çš„æ•°æ®è¡¨ç”Ÿæˆ dbt æ¨¡å‹ã€‚è„šæœ¬æ‰§è¡Œä»¥ä¸‹æ“ä½œã€‚

*   ä»è¡¨æ ¼ä¸­é€‰æ‹©ç¬¬ä¸€è¡Œ
*   è§£æå®ƒçš„åˆ—åå’Œæ•°æ®ç±»å‹
*   ç”Ÿæˆ dbt æ¨¡å‹è¾“å‡º
*   å°†è¾“å‡ºå†™å…¥æ–‡æœ¬æ–‡ä»¶

```
# ./script.py

import sys
from helpers.connection import get_engine
from helpers.parser import get_parsed_data
from helpers.writer import get_dbt_formatted_sql

SCHEMA_PREFIX = "my_aws_schema"
JSON_PREFIX = "json_identifier"

def main():
    global SCHEMA_PREFIX
    global JSON_PREFIX

    table = sys.argv[1]

    try:
        SCHEMA_PREFIX = sys.argv[2]  # if 2nd argument is passed, it is schema!
    except:
        print(f"pre-defined schema is => {SCHEMA_PREFIX}")

    try:
        SCHEMA_PREFIX = sys.argv[
            3
        ]  # if 3rd argument is passed, it is JSON column identifier!
    except:
        print(f"pre-defined JSON identifier is => {JSON_PREFIX}")

    engine = get_engine()

    res = engine.execute(f"select * from {SCHEMA_PREFIX}.{table} limit 1;").fetchone()[
        0
    ]

    column_types = get_parsed_data(res)

    with open("raw_data.json", "w+", encoding="utf-8") as file:
        file.write(str(res))

    with open("raw_data_column_types.json", "w+", encoding="utf-8") as file:
        file.write(str(column_types))

    with open("dbt_model.txt", "w+", encoding="utf-8") as file:
        model_txt = get_dbt_formatted_sql(column_types, JSON_PREFIX)
        file.write(model_txt)

    print("Successfully completed!")
if __name__ == "__main__":
    main()
```

æˆ‘ä»¬å¯ä»¥åƒä¸‹é¢è¿™æ ·è¿è¡Œè„šæœ¬ã€‚

`python script.py table_name schema_name nested_identifier`

å®ƒå°†ç”Ÿæˆä¸€ä¸ªåä¸º`dbt_model.txt`çš„è¾“å‡ºæ–‡ä»¶ã€‚

```
nested_identifier."col1"::timestamp as col1,
nested_identifier."col2"::varchar as col2,
nested_identifier."col3"::timestamp as col3,
nested_identifier."_extract_timestamp"::timestamp as _extract_timestamp # col4
```

ç°åœ¨ï¼Œæ‚¨åªéœ€è¦å°†è¾“å‡ºå¤åˆ¶å¹¶ç²˜è´´åˆ°æ‚¨çš„ select è¯­å¥ä¸­ã€‚æ˜¾ç„¶ï¼Œæ‚¨å¯ä»¥æ”¹è¿›è„šæœ¬æ¥ç”Ÿæˆ select è¯­å¥ã€‚æˆ‘æŠŠå®ƒç•™ç»™ä½ ï¼ŒğŸ¥²

# æœ€å

å¸Œæœ›å®ƒèƒ½å¸®åŠ©æ‚¨æœ‰ä¸€ä¸ªç”Ÿæˆè‡ªåŠ¨ dbt æ¨¡å‹çš„æƒ³æ³•ï¼Œå¹¶è®©æ‚¨å…äºä¸€è¡Œä¸€è¡Œåœ°åµŒå¥—æ•°æ®ã€‚

è¯šæŒšçš„é—®å€™