<html>
<head>
<title>Optimization techniques in Deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的优化技术</h1>
<blockquote>原文：<a href="https://medium.com/codex/optimization-techniques-in-deep-learning-5ac07a6e552b?source=collection_archive---------11-----------------------#2021-08-31">https://medium.com/codex/optimization-techniques-in-deep-learning-5ac07a6e552b?source=collection_archive---------11-----------------------#2021-08-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c91cd4137136c0fbb920f8a1e155dc1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-_sHOCicIfemlOFw"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">坦维·马利克在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="1e62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我之前的<a class="ae iu" rel="noopener" href="/codex/the-way-i-learned-the-gradient-descent-cb408e6149e4">帖子</a>中，我讨论了梯度下降及其变体是如何工作的。但是，这些优化技术可能并不是在所有情况下都能很好地工作。没有什么理由SGD或批量SGD不值得每次都使用。</p><ol class=""><li id="455d" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">由于我们在深度学习中经常遇到的鞍点，这些优化技术可能不适用于非凸目标函数。</li></ol><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es kc"><img src="../Images/1e29df8a0e1bfc6e44776a8e4075f722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/0*aDc-kfxkT0OjtA22.jpg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">凸函数和非凸函数的区别</figcaption></figure><p id="d85a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.所有的特征不具有相同的频率，因此相同的学习率不能应用于所有的权重。</p><p id="0332" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.由于超调或偏离路径，不能使用大的学习速率。</p><p id="23cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇博客文章中，我们将讨论一些其他流行的可以解决上述问题的优化技术。</p><ol class=""><li id="f5dc" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">带动量的SGD</li><li id="62fa" class="jt ju hi ix b iy kh jc ki jg kj jk kk jo kl js jy jz ka kb bi translated">困扰</li><li id="c159" class="jt ju hi ix b iy kh jc ki jg kj jk kk jo kl js jy jz ka kb bi translated">阿达格拉德</li><li id="06b4" class="jt ju hi ix b iy kh jc ki jg kj jk kk jo kl js jy jz ka kb bi translated">RMSProp</li><li id="e9b1" class="jt ju hi ix b iy kh jc ki jg kj jk kk jo kl js jy jz ka kb bi translated">阿达德尔塔</li><li id="af06" class="jt ju hi ix b iy kh jc ki jg kj jk kk jo kl js jy jz ka kb bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li></ol><h1 id="1adb" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">1.带动量的SGD</h1><p id="b5c2" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">我们知道SGD或小批量SGD不使用全部数据来收敛。由于这种数据的缺乏，它试图近似实际路径以达到最佳点。到达目的地的方向是明确的，但在到达目的地的旅途中要经历许多风风雨雨。🙁</p><blockquote class="lp"><p id="67ef" class="lq lr hi bd ls lt lu lv lw lx ly js dx translated">动量允许我们使用梯度的指数加权平均来抑制振荡，</p></blockquote><figure class="ma mb mc md me ij er es paragraph-image"><div class="er es lz"><img src="../Images/5d1426f083ee440f56dd1326215f642e.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*VvzqANwsInjxK0PjTbaYaw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.willamette.edu/~gorr/classes/cs449/momrate.html" rel="noopener ugc nofollow" target="_blank"> SGD无动量</a></figcaption></figure><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/d104c30c7a2dc925d260b14bc9a3843e.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*nNus9FbKW99KJhrfJumVig.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms" rel="noopener ugc nofollow" target="_blank">带动量的新币</a></figcaption></figure><p id="8166" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有了动量，就不会再陷入局部极小值，也加快了收敛速度。</p><p id="6dee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们只改变更新函数来实现动量</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/15f04d0d08cef3340ae3001fea3781e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*lQcp_zo_Bi8HlkNXUikSxA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">更新SGD中的功能</figcaption></figure><p id="7c34" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看我之前的<a class="ae iu" rel="noopener" href="/codex/the-way-i-learned-the-gradient-descent-cb408e6149e4">博客</a>了解更多关于这个更新功能的信息。</p><p id="0b66" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">有两组等式来做指数平均。为了避免混淆，最好了解两组方程式。</strong></p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/8af5c72a42d831521c56015965a4c948.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*yXgp_iRxIWFM48flkwRqXw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">指数加权平均</figcaption></figure><p id="be9c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中，γ是已知的动量因子，η是学习率。(0 </p><p id="c4f7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">These both set of equations give the same result but with different γ, η values. But I prefer the second set of equations (i.e., eq (3) &amp; eq (4)) for now.</p><p id="c9f7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">By substituting eq (3) in eq (4), we get:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/9ea4e2444f4abe69200bc3c0248ace72.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*13dmD-LXH3F-6FwINgwIrA.png"/></div></figure><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/9c5920b725046a2fe121327115afc569.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*eeaqyE8xJIjDCbpWYFKouA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">[<a class="ae iu" href="https://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="4946" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的等式中，即等式(5)</p><p id="9d8f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">γV(t-1)是动量步长(绿线)，ηdL/dW(t-1)是梯度步长(红线)。实际步长(蓝线)是动量和梯度步长之和。我们知道动量是梯度的指数平均值。随着迭代次数的增加，动量步长增加(绿线的长度)，实际步长也增加(蓝线)。这就是它如何在每次迭代中收敛得更快。</p><p id="9a22" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看权重是如何随着动量更新的</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/8eecdc660617727a8b645b61d4f30373.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*7Fh5lWG7s06nTuPqCaWQkQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">[图片由作者提供]</figcaption></figure><p id="0ebf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是SGD与动量的工作方式和权重的更新方式。</p><h1 id="4902" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">2.内斯特罗夫加速梯度(NAG)</h1><p id="2de0" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">内斯特罗夫加速梯度类似于SGD +动量，但有微小的区别。在带动量的SGD中，我们计算梯度项(ηdL/dW(t-1))和动量项(γV(t-1))并求和以移动到下一个点。但是在内斯特罗夫加速梯度中，首先，我们计算动量并向其方向移动，然后计算该点的梯度(dL/dw’)(这与实际梯度(dL/wt-1)不同)，然后向最终到达实际点的方向移动，即wt。</p><p id="ebfe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">简单来说，我们首先从Wt-1到W '，然后到Wt。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/831e8ccafdf671b80c6cc91ad519eb0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*L0aAIkkk9SNfEDbSVGc8IA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">[ <a class="ae iu" href="https://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="5936" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从SGD提供的动力中，我们知道:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/bee21153de5a9ba5a66282ca8421e6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*FWyoMAXpDnEk1xS1IbUedg.png"/></div></figure><p id="6a86" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，为了只在动量步中运动，我们不应该在上面的方程中加入梯度项。</p><p id="2b76" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，没有梯度项，我们不会移动到Wt，相反，我们移动到W’。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/3ad40dc052be2f1eead431687602588a.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*mazHjgInNrFeSZws8wXivA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">注意！它没有梯度步骤。</figcaption></figure><p id="d0c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在计算从W '到Wt的梯度</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/62537572a7c93b98e6065239f88fc353.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*Y3VgO81v_ar_4ko8npJuLw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">“前瞻”渐变</figcaption></figure><p id="5f00" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以，最后的等式看起来像:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/7af0a73e767df402c1a85eccddd66312.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*DOTWNrDmdXiO3y5wIOHW9Q.png"/></div></figure><p id="0c4a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可能会怀疑我们最终会到达同一点(Wt ),那么用内斯特罗夫加速梯度代替SGD加动量有什么意义呢？</p><p id="1b27" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在W '处计算的梯度被称为“向前看”梯度，这有助于我们向前看我们将要到达的地方。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/fe0cbb4c0771364914376844d08d745d.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*el996lQUhX-mpsK4PNwNCw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">[ <a class="ae iu" href="https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="eb2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在图(a)中，对于每次更新，损耗减少，并且比之前的步骤行进更多的距离以收敛，这是很大的。但在第4次更新时，它超过了，后来，随着第5次和第6次更新，它又回到了最小损失点。直到前三次更新，SGD里的一切都是一样的有气势，有NAG。但是在更新4上，你可以看到NAG的重要性。我们知道，我们在NAG中计算两个部分步骤来达到下一个点。在第四次更新中，第一个部分步骤是4a，即，在动量方向上移动，在4b，我们计算前瞻梯度以查看我们正在前进的方向并移动到该点。因此，在更新4结束时，与具有动量的SGD相比，NAG中的过冲减少，并且仅再有一次更新，即第5次更新，它就达到最小值。因此，NAG需要5次更新，SGD需要6次更新才能到达同一个目的地。这就是NAG和SGD有动量的区别。</p><h1 id="cfb0" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">3.自适应梯度</h1><p id="55f4" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">到目前为止，我们讨论的所有先前的优化技术都具有恒定的学习速率。自适应梯度的核心思想是在每次迭代中对每个权重有不同的学习速率。但是为什么不同的学习速度如此重要呢？</p><p id="f6ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基本上，特征可以是稀疏的(包含很少的非零元素)和密集的(包含很少的零元素)。因此，您不能对稀疏和密集要素应用相同的学习速率，因为它们不会以相同的速率收敛。</p><p id="a471" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SGD的更新功能:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/42a0d293e44e9e9949d9b85d1091da2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*jHjD6zcb2nAzLmDxdAYDcA.png"/></div></figure><p id="2237" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，η是学习率，它对于所有迭代中的所有权重都是相同的。</p><p id="7c81" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用Adagrad更新功能:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/c81d5638aed9a3b1c1de92c21f8443a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*Pbh6urTPngcGOWsDrXoxcw.png"/></div></figure><p id="916a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里η’是学习率，它对于每次迭代中的所有权重都是不同的。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/945a4754cb450d8f70879d011751be4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*apjX2rKpnvHoPl0XM9KXHg.png"/></div></figure><p id="dd06" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ε是一个小正数，以避免被零整除的错误。</p><p id="c458" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">α(t-1)是前一个梯度的平方和。</p><p id="05c6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中η是恒定的学习速率，η是通过使用先前梯度的所有信息而改变的自适应学习速率。</p><p id="e622" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看一个例子，每个权重在一次迭代中有不同的学习率。</p><p id="99be" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑具有两个输入节点和一个输出节点的神经网络。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/65a69d57f3ea8e002cc1b5434459577a.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*ZFhzsr4aApKT17FuFI1BpA.png"/></div></figure><p id="28cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，α对于每个权重是不相同的，并且它的值对于每次迭代是变化的，因此它们对于每次迭代中的每个权重具有不同的学习速率。但是当我们使用SGD解决同一个例子时，它看起来是这样的:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/e6bdc89e2b56120561392ad66b390dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*JUhkSAqe6jJkBgy-mH7bwA.png"/></div></figure><p id="ee36" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">η是恒定的学习速率，直到最后一次迭代，它对于所有的权重都是相同的。</p><p id="98b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，adagrad的一切看起来都很好，但有一个不能忽视的主要缺点，即，</p><p id="d3e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随着迭代次数(t)的增加，α的值增加，然后η'减小。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/9ec67e95731bce7a53c98781500cc192.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*_5CL1SKaizedhgCxpcrXXQ.png"/></div></figure><p id="3b1d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">学习率η'随着每次迭代而降低，然后收敛变得更慢，并且在某一点上η'变得几乎为零。这个问题被称为消失梯度。Adadelta和RMSprop是试图有效解决这种消失梯度问题的其他优化技术。</p><h1 id="2276" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">4.RMSProp</h1><p id="530e" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">在adagrad中，为了计算α，我们取梯度平方的和。但是，在RMSProp中，我们对梯度平方取指数平均值。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/e1c9f1a57e03ee81f514c7116b1ea56e.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*LpEmIdbYuoB-0_m8RlBZbA.png"/></div></figure><p id="3d4a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们实际上是在控制η'中分母项的增长，通过执行指数平均来解决梯度消失的问题。</p><h1 id="0b0a" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">5.阿达德尔塔</h1><p id="7a9b" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">Adadelta与rmsprop在η'的分母部分做同样的事情。此外，在adadelta中，我们用delta的指数平均值替换默认学习速率η。</p><p id="aef8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">δ(D)为</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/6c546a7d9c17affdce2bd3bf0b08bb31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*qGWqcQmM-O645mAbpVEF7g.png"/></div></figure><p id="253b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看一眼这些方程，你可能会感到困惑。请花时间处理每一行。</p><h1 id="cc25" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">6.自适应矩估计</h1><p id="9f42" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">动量和内斯特罗夫加速梯度具有恒定的学习速率，并且仅关注于修改梯度部分。Adagrad、adadelta和RMSProp都是关于修改学习率的。Adam是momentum和RMSProp的结合，它侧重于修改两者，学习率和梯度，以获得更好的结果。</p><p id="db3d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在统计学中，均值常被称为一阶矩，方差常被称为二阶矩</p><p id="cf62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在RMSProp中，我们做了<em class="mx"> gₜ </em>的指数加权平均(EWA)来获得学习率η’，在SGD中，我们做了gₜ的指数加权平均(EWA)来获得动量。</p><p id="d27e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="mx"> gₜ </em>和<em class="mx"> gₜ </em>的EWA是均值(一阶矩)和方差(二阶矩)的估计值，将它们表示为<em class="mx"> mₜ </em>和<em class="mx"> vₜ </em></p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es my"><img src="../Images/0e686561b6d7dce04ab90f9ca4ae15e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*c6I3jRbAbmqm1qMulQHOzA.png"/></div></figure><p id="28cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是当我们用零初始化<em class="mx"> mₜ </em>和<em class="mx"> vₜ </em>时，它们偏向于零。因此，我们计算一阶和二阶矩中的偏差修正。(我强烈推荐你观看<a class="ae iu" href="https://www.youtube.com/watch?v=lWzo8CajF5s" rel="noopener ugc nofollow" target="_blank">吴恩达偏差修正视频</a>来更好地理解这一点)。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/9552bf000b48d086ff5bc52fadb64771.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*bfhMlvVpsEG9TeFffcWAjQ.png"/></div></figure><p id="9fc6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更新函数看起来与RMSProp的相似</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es na"><img src="../Images/2b98dd633fef20dff05dc5d157402926.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*mc6jnUDfXK9OuxOmOU3OmA.png"/></div></figure><p id="aecc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于这些协同修改，Adam在大多数情况下都比其他优化器工作得更好。</p><p id="81f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是了。我们已经走到了尽头。谢谢你一直读到最后，如果有任何建议或反馈，请在评论区告诉我。下次博客再见！😊</p><h2 id="58ce" class="nb kn hi bd ko nc nd ne ks nf ng nh kw jg ni nj la jk nk nl le jo nm nn li no bi translated">参考资料:</h2><ol class=""><li id="8576" class="jt ju hi ix b iy lk jc ll jg np jk nq jo nr js jy jz ka kb bi translated"><a class="ae iu" href="https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安·鲁德的博客</a></li><li id="0b5d" class="jt ju hi ix b iy kh jc ki jg kj jk kk jo kl js jy jz ka kb bi translated"><a class="ae iu" href="https://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank"> cs231讲义</a></li><li id="f640" class="jt ju hi ix b iy kh jc ki jg kj jk kk jo kl js jy jz ka kb bi translated">阿达格勒的参考资料</li></ol></div></div>    
</body>
</html>