<html>
<head>
<title>AWS Firehose Data Format Conversion — Failed Records Reprocessing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AWS消防软管数据格式转换—失败的记录重新处理</h1>
<blockquote>原文：<a href="https://medium.com/codex/aws-firehose-data-format-conversion-failed-records-reprocessing-ddc06e5c568d?source=collection_archive---------3-----------------------#2021-12-29">https://medium.com/codex/aws-firehose-data-format-conversion-failed-records-reprocessing-ddc06e5c568d?source=collection_archive---------3-----------------------#2021-12-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7eaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最近，我将我们的<a class="ae jd" href="https://docs.aws.amazon.com/glue/latest/dg/access-control-overview.html" rel="noopener ugc nofollow" target="_blank"> AWS Glue目录权限</a>迁移到AWS Lake Formation进行管理。<a class="ae jd" href="https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html" rel="noopener ugc nofollow" target="_blank"> AWS Lake Formation </a>增加了一层安全性，允许<a class="ae jd" href="https://docs.aws.amazon.com/lake-formation/latest/dg/lf-permissions-reference.html" rel="noopener ugc nofollow" target="_blank">细粒度权限</a>，管理数据湖<a class="ae jd" href="https://docs.aws.amazon.com/lake-formation/latest/dg/granting-location-permissions.html" rel="noopener ugc nofollow" target="_blank">资源位置</a> (s3)以及通过<a class="ae jd" href="https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html" rel="noopener ugc nofollow" target="_blank">标签(LF-Tag) </a>的控制权限，以及对IAM用户、角色、交叉帐户和SAML或QuickSight用户ARN的列级权限管理。AWS Lake Formation是在AWS Glue Catalog服务之后添加的，为了继续支持<a class="ae jd" href="https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started-setup.html#setup-change-cat-settings" rel="noopener ugc nofollow" target="_blank">向后兼容性</a>，Lake Formation默认授予<code class="du je jf jg jh b">IAMAllowedPrincipals</code>(基本上不评估权限)对Glue Catalog资源的访问权限，只要主体具有适当的IAM策略。</p><h1 id="eda9" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">设置</h1><p id="7d2a" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">产生事件/消息并发送到Kinesis数据流的服务集，由带有<a class="ae jd" href="https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html" rel="noopener ugc nofollow" target="_blank">记录格式转换</a>的Firehose交付流读取，能够将JSON格式数据从Kinesis流转换为压缩的Parquet(纵列)格式，以便使用Athena(和其他工具)查询数据在成本和速度方面得到优化。在这个设置中，数据库、表和模式是在Glue Catalog中配置的，Firehose引用它来将JSON转换为Parquet。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kl"><img src="../Images/7509acb28d5db9223ca6731f54ee7b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4XgasLXDbUZxIUscjfa4Yg.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">带有格式转换的消防软管引用胶水目录中的模式的示例设置</figcaption></figure><h1 id="de96" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">我们错过了什么</h1><p id="9d9c" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">我们确实注意向用户和用户承担的角色授予适当的权限，但是，我们只有少量AWS Kinesis Data Firehose流，这些流支持数据格式转换，将JSON转换为Parquet，引用Glue Catalog中注册的模式和表。不幸的是，我们忘记了在湖泊形成许可中包括消防软管承担的角色。当我们意识到Firehose无法将文件写入S3，因为它无法再从目录中读取Glue Table时，我们继续通过将Glue Table上的<code class="du je jf jg jh b">SELECT</code>权限授予Firehose承担的角色来修复这个问题。</p><p id="0304" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于这一事件，我们在S3出现了数据缺口。由于Firehose基于流(5-15分钟的批次或批次大小)工作，在窗口期间，当Firehose角色没有权限粘合表时，由于权限问题而无法写入的数据在parquet转换的输出位置不可用。对于任何合法的格式转换问题，firehose将失败的记录写入S3的子文件夹，除非您覆盖，否则它将在您的目标S3位置的<code class="du je jf jg jh b">format-conversion-failed</code>文件夹中(桶+前缀)。</p><h1 id="cef3" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">S3的数据(有失败记录)</h1><p id="2504" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">S3有定期流入的拼花转换数据，但是，突出显示的时间范围<em class="lb"> 2021/11/04/02 </em>和<em class="lb"> 2021/11/06/04 </em>之间存在间隙。</p><p id="fd5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据现在在parquet转换后的输出位置显示为parquet，我们发现那些文件作为<code class="du je jf jg jh b">raw data</code> (json)存在于<code class="du je jf jg jh b">format-conversion-failed</code>子目录(前缀)中的S3。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lc"><img src="../Images/709d19bca1ec3f31cca7d04b7190ae65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8HGWGyz9PDEFdrGd8uZGg.png"/></div></div></figure><h1 id="764d" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">解决办法</h1><p id="dece" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">我们在format-conversion-failed子目录中有原始数据，我们需要将其转换为parquet并放在parquet输出目录下，这样我们就可以填补权限问题造成的空白，这持续了大约两天。</p><ol class=""><li id="02d6" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">读取Glue schema，它也被Firehose用于格式转换</li><li id="d7f7" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">在我们发现问题的日期/时间范围内，从<code class="du je jf jg jh b">format-conversion-failed</code>子目录中一次读取一个文件</li><li id="57e2" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">使用从步骤1下载的模式，将每个JSON转换成parquet</li><li id="bf5d" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">将拼花文件写入各自的S3拼花输出目录</li></ol><h2 id="dc0f" class="lr jj hi bd jk ls lt lu jo lv lw lx js iq ly lz jw iu ma mb ka iy mc md ke me bi translated">从S3下载失败的JSON文件</h2><pre class="km kn ko kp fd mf jh mg mh aw mi bi"><span id="b144" class="lr jj hi jh b fi mj mk l ml mm"># date when issue started - just for local storage<br/>ISSUE_DATE=20211104<br/>ERROR_PREFIX=format-conversion-failed</span><span id="69d8" class="lr jj hi jh b fi mn mk l ml mm"># Create local directory - change `/tmp` prefix as appropriate<br/>mkdir -p "/tmp/${ISSUE_DATE}/${FIREHOSE_OUTPUT_PATH}"<br/>cd "/tmp/${ISSUE_DATE}/${FIREHOSE_OUTPUT_PATH}"</span><span id="d26b" class="lr jj hi jh b fi mn mk l ml mm"># Download failed files from S3 to local<br/>aws s3 --profile ${aws-profile} cp --recursive "s3://${BUCKET}/${FIREHOSE_OUTPUT_PATH}/${ERROR_PREFIX}/" "./${ERROR_PREFIX}"</span><span id="d269" class="lr jj hi jh b fi mn mk l ml mm"># Count all downloaded failed files<br/>find "./${ERROR_PREFIX}" -type f | wc -l</span></pre><h2 id="e1b8" class="lr jj hi bd jk ls lt lu jo lv lw lx js iq ly lz jw iu ma mb ka iy mc md ke me bi translated"><strong class="ak">从失败的JSON文件中提取</strong><code class="du je jf jg jh b"><strong class="ak">rawData</strong></code><strong class="ak"/></h2><p id="8d4d" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">Bash使用<code class="du je jf jg jh b">rawData</code>转换来自<code class="du je jf jg jh b">format-conversion-failed</code>出错对象的出错文件(base64编码)，并进行base64解码和写入。/json”目录。</p><pre class="km kn ko kp fd mf jh mg mh aw mi bi"><span id="0495" class="lr jj hi jh b fi mj mk l ml mm">mkdir json</span><span id="053d" class="lr jj hi jh b fi mn mk l ml mm"># Loop through each JSON file, grab rawData fields from each object, and perform base64 decode and write to a separate JSON file<br/>for f in $(find ./${ERROR_PREFIX} -type f | sort); do <br/>   json=$(echo $f | sed "s/format-conversion-failed/json/g");<br/>   dir=$(echo "${json%/*}"); mkdir -p $dir;<br/>   for r in $(cat $f | jq '.rawData' -r); do echo $r | base64 -d; done &gt; $json<br/>   echo $f:$(cat $f | wc -l):$json:$(cat $json | wc -l) | tee -a /tmp/failed-to-json-counts<br/>done</span><span id="f951" class="lr jj hi jh b fi mn mk l ml mm"># Count number of files with actual JSON extracted (and base64 decoded) from rawData field<br/>find "./json" -type f | sort | wc -l</span></pre><h2 id="c31b" class="lr jj hi bd jk ls lt lu jo lv lw lx js iq ly lz jw iu ma mb ka iy mc md ke me bi translated">为Spark准备JSON文件列表</h2><p id="357f" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">从<code class="du je jf jg jh b">./json</code>目录中，列出所有文件，按名称(包括路径)排序，并写入文件。(注意，我们也可以在spark中执行相同的步骤)</p><pre class="km kn ko kp fd mf jh mg mh aw mi bi"><span id="e08c" class="lr jj hi jh b fi mj mk l ml mm">find ./json -type f | sort &gt; files.txt</span></pre><h2 id="48af" class="lr jj hi bd jk ls lt lu jo lv lw lx js iq ly lz jw iu ma mb ka iy mc md ke me bi translated">下载消防软管使用的粘合模式</h2><p id="f2a0" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">从Glue目录中复制JSON格式的Avro模式到一个模式文件，<code class="du je jf jg jh b">schema.json</code></p><h2 id="348d" class="lr jj hi bd jk ls lt lu jo lv lw lx js iq ly lz jw iu ma mb ka iy mc md ke me bi translated">使用<code class="du je jf jg jh b">schema.json</code>将JSON转换成Parquet的Spark代码</h2><p id="667b" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">通过应用<code class="du je jf jg jh b">schema.json</code>，用拼花文件从<code class="du je jf jg jh b">./json</code>读取数据到<code class="du je jf jg jh b">./parquet</code></p><p id="0841" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">启动火花外壳</strong></p><pre class="km kn ko kp fd mf jh mg mh aw mi bi"><span id="57ad" class="lr jj hi jh b fi mj mk l ml mm">spark-shell — packages org.apache.spark:spark-avro_2.12:3.1.1</span></pre><p id="a99f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">火花码</strong></p><pre class="km kn ko kp fd mf jh mg mh aw mi bi"><span id="1625" class="lr jj hi jh b fi mj mk l ml mm">import spark.implicits._<br/>import org.apache.spark.sql.functions.unbase64<br/>import java.io._<br/>import java.nio.file._<br/>import org.apache.spark.sql.avro.functions._<br/>import org.apache.spark.sql.types._<br/>import org.apache.spark.sql.avro._<br/>import scala.reflect.io.Directory</span><span id="5cf6" class="lr jj hi jh b fi mn mk l ml mm">def getListOfFiles(dir: File, extensions: List[String]): List[File] = {<br/>    dir.listFiles.filter(_.isFile).toList.filter { file =&gt;<br/>        extensions.exists(file.getName.endsWith(_))<br/>    }<br/>}</span><span id="487e" class="lr jj hi jh b fi mn mk l ml mm">val SCHEMA_FILE = "{path-to-schema.json}"<br/>val jsonSchema = new String(Files.readAllBytes(Paths.get(SCHEMA_FILE)))<br/>val schema = spark.read.format("avro").option("avroSchema", jsonSchema).load().schema</span><span id="ef0d" class="lr jj hi jh b fi mn mk l ml mm">val filesDF = spark.read.text("files.txt")<br/>filesDF.show<br/>filesDF.count</span><span id="970c" class="lr jj hi jh b fi mn mk l ml mm">filesDF.take(filesDF.count.asInstanceOf[Int]).foreach { <br/> row =&gt; {<br/>  //row.toSeq.foreach{col =&gt; println(col + " = " + col.asInstanceOf[String])}<br/>  val jsonFile = row.get(0).asInstanceOf[String]<br/>  val parquetDir = jsonFile.replace("./json/", "./parquet/")<br/>  println(jsonFile + " = " + parquetDir)<br/>  <br/>  <br/>  val df = spark.read.schema(schema).json(jsonFile)<br/>  val numRecords = df.count()</span><span id="bcac" class="lr jj hi jh b fi mn mk l ml mm">df.coalesce(1).write.option("compression", "snappy").mode("overwrite").parquet(parquetDir)  <br/>  <br/>  val parquetFile = getListOfFiles(new File(parquetDir), List("parquet"))(0)<br/>  <br/>  println("Source: " + jsonFile + ", outputFile: " + parquetFile.toPath + " renamed to " + parquetDir + ".parquet" + ", numRecords: " + numRecords)<br/>  Files.move(parquetFile.toPath, new File(parquetDir + ".parquet").toPath, StandardCopyOption.ATOMIC_MOVE)<br/>  new Directory(new File(parquetDir)).deleteRecursively()<br/> } <br/>}</span></pre><p id="7fb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算拼花文件的数量</strong></p><pre class="km kn ko kp fd mf jh mg mh aw mi bi"><span id="4080" class="lr jj hi jh b fi mj mk l ml mm">find ./parquet -type f | sort | wc -l</span></pre><h2 id="7740" class="lr jj hi bd jk ls lt lu jo lv lw lx js iq ly lz jw iu ma mb ka iy mc md ke me bi translated">验证JSON和Parquet文件中的计数</h2><pre class="km kn ko kp fd mf jh mg mh aw mi bi"><span id="190b" class="lr jj hi jh b fi mj mk l ml mm"># Count of records per JSON file<br/>for f in $(find ./json -type f | sort); do echo $f: $(cat $f | wc -l); done &gt; /tmp/json.counts </span><span id="513a" class="lr jj hi jh b fi mn mk l ml mm"># Count of records per Parquet file<br/>for f in $(find ./parquet -type f | sort); do echo $f: $(parquet-tools rowcount $f | awk -F':' '{print $2}'); done | sed 's/parquet\//json\//g' | sed 's/.parquet//g' &gt; /tmp/parquet.counts</span><span id="60c0" class="lr jj hi jh b fi mn mk l ml mm"># Compare two files, each having counts from JSON and Parquet file<br/># Both files must have matching number of JSON and Parquet records<br/>diff /tmp/json.counts /tmp/parquet.counts</span></pre><h2 id="17a4" class="lr jj hi bd jk ls lt lu jo lv lw lx js iq ly lz jw iu ma mb ka iy mc md ke me bi translated">上传拼花文件到S3</h2><pre class="km kn ko kp fd mf jh mg mh aw mi bi"><span id="f7ba" class="lr jj hi jh b fi mj mk l ml mm"># Dry run to upload Parquet files<br/>aws s3 --profile ${aws-profile} cp --recursive --dryrun parquet/ "s3://${BUCKET}/${FIREHOSE_OUTPUT_PATH}/"</span><span id="3c11" class="lr jj hi jh b fi mn mk l ml mm"># Upload files after verifying Dry Run output<br/>aws s3 --profile ${aws-profile} cp --recursive parquet/ "s3://${BUCKET}/${FIREHOSE_OUTPUT_PATH}/"</span></pre><p id="3433" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">应用上述所有步骤后，S3中用于拼花输出的文件中的间隙应该不再存在。</p><p id="07ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，可以将bash脚本步骤转换成Spark。我刚刚概述了解决丢失数据问题的步骤。当我将这些步骤整合到一个Spark脚本中时，我将在这里更新它。如果你已经解决了这个问题，请告诉我，我可以在这里反映。</p><p id="9bb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一如既往，如果你遇到了这样的问题，并提出了更好的解决方法，请分享。</p></div></div>    
</body>
</html>