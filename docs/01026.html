<html>
<head>
<title>Analyzing data using Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用主成分分析(PCA)分析数据</h1>
<blockquote>原文：<a href="https://medium.com/codex/analyzing-data-using-principal-component-analysis-pca-836802e7e7fb?source=collection_archive---------8-----------------------#2021-03-31">https://medium.com/codex/analyzing-data-using-principal-component-analysis-pca-836802e7e7fb?source=collection_archive---------8-----------------------#2021-03-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="69df" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用Python了解和理解PCA的指南。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/d75c25b0f1a7a5306c1bdcbcdc08f9ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kON_4kMz4PJJu6-t"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">弗兰基·查马基在<a class="ae jn" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="c976" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">什么是主成分分析？</h1><p id="a749" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">主成分分析是一种无监督学习技术，缩写为PCA。它也被称为一般因素分析。它用于研究一组变量之间的相互关系，以便找出这些变量的潜在结构。它用于分析数据。</p><h1 id="5be4" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">它是如何工作的？</h1><p id="5a25" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">PCA产生几条正交线，它们很好地拟合了数据。正交线是n维空间中相互垂直的线。因此，如果创建了一条回归线，那么垂直于这条线的线将是正交线。现在组件的概念出现了。组件是一种线性变换，它为数据集选择一个变量系统，使数据集的第一个最大方差位于第一个轴上，第二个最大方差位于第二个轴上，依此类推。该程序减少了分析过程中使用的变量数量。</p><h1 id="b030" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">如何用Python实现？</h1><p id="684e" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">这里使用的数据集将是乳腺癌的内置scikit学习数据。使用主成分分析，数据将被转换，并找出哪些特征解释了数据中的最大差异。</p><p id="6067" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> →导入库</strong></p><p id="4907" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">导入了处理数据的基本库，如pandas和numpy。与它一起用于可视化的还有matplotlib和seaborn。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="09ec" class="lm jp hi li b fi ln lo l lp lq"><strong class="li hj">&gt;&gt;&gt; import</strong> pandas <strong class="li hj">as</strong> pd<br/><strong class="li hj">&gt;&gt;&gt; import</strong> numpy <strong class="li hj">as</strong> np<br/><strong class="li hj">&gt;&gt;&gt; import</strong> matplotlib.pyplot <strong class="li hj">as</strong> plt<br/><strong class="li hj">&gt;&gt;&gt; import</strong> seaborn <strong class="li hj">as</strong> sns<br/><strong class="li hj">&gt;&gt;&gt; %</strong>matplotlib inline</span></pre><p id="8ddc" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> →读取数据</strong></p><p id="e42a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">乳腺癌数据从sklearn导入。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="82a3" class="lm jp hi li b fi ln lo l lp lq"><strong class="li hj">&gt;&gt;&gt; from</strong> sklearn.datasets <strong class="li hj">import</strong> load_breast_cancer<br/>&gt;&gt;&gt; data <strong class="li hj">=</strong> load_breast_cancer()</span><span id="9922" class="lm jp hi li b fi lr lo l lp lq">&gt;&gt;&gt; data.keys()<br/>dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])</span><span id="02d8" class="lm jp hi li b fi lr lo l lp lq">&gt;&gt;&gt; df <strong class="li hj">=</strong> pd.DataFrame(data['data'],columns<strong class="li hj">=</strong>data['feature_names'])<br/>&gt;&gt;&gt; df.head()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ls"><img src="../Images/d44c4ad9f3a4f524d29abb63c11faf2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SO2dbSKVjUaDYbxo3nHk1w.png"/></div></div></figure><p id="f373" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> → PCA可视化</strong></p><p id="9191" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">PCA将用于找到前两个主成分，并在新的2-D空间中可视化数据。为此，将对数据进行缩放，以便每个要素都有一个单位方差。从sklearn导入标准标量模块，其对象适合数据。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="e570" class="lm jp hi li b fi ln lo l lp lq"><strong class="li hj">&gt;&gt;&gt; from</strong> sklearn.preprocessing <strong class="li hj">import</strong> StandardScaler<br/>&gt;&gt;&gt; scaler <strong class="li hj">=</strong> StandardScaler()</span><span id="9404" class="lm jp hi li b fi lr lo l lp lq">&gt;&gt;&gt; scaler.fit(df)<br/>StandardScaler()</span></pre><p id="1b1e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在，这些数据被转换。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="81d0" class="lm jp hi li b fi ln lo l lp lq">&gt;&gt;&gt; scaled_data <strong class="li hj">=</strong> scaler.transform(df)</span></pre><p id="5abc" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在从sklearn导入PCA，创建它的对象。指定了组件的数量。然后使用fit()方法找到主成分。然后使用transform()函数，进行旋转和降维。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="a601" class="lm jp hi li b fi ln lo l lp lq"><strong class="li hj">&gt;&gt;&gt; from</strong> sklearn.decomposition <strong class="li hj">import</strong> PCA<br/>&gt;&gt;&gt; pca <strong class="li hj">=</strong> PCA(n_components<strong class="li hj">=</strong>2)</span><span id="fdcd" class="lm jp hi li b fi lr lo l lp lq">&gt;&gt;&gt; pca.fit(scaled_data)<br/>PCA(n_components=2)</span></pre><p id="0409" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">数据被转换成它的前两个主成分。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="be31" class="lm jp hi li b fi ln lo l lp lq">&gt;&gt;&gt; x_pca <strong class="li hj">=</strong> pca.transform(scaled_data)</span></pre><p id="2fae" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在可以比较原始数据和转换后的数据，发现转换后的数据有两个部分。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="fea7" class="lm jp hi li b fi ln lo l lp lq">&gt;&gt;&gt; print(scaled_data.shape)<br/>&gt;&gt;&gt; print(x_pca.shape)<br/>(569, 30)<br/>(569, 2)</span></pre><p id="a5a0" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">所以现在从30维减少到2维。可以绘制这两个分量。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="2d07" class="lm jp hi li b fi ln lo l lp lq">&gt;&gt;&gt; plt.scatter(x_pca[:,0],x_pca[:,1],<br/>c<strong class="li hj">=</strong>data['target'],cmap<strong class="li hj">=</strong>'rainbow')<br/>&gt;&gt;&gt; plt.xlabel('First principal component')<br/>&gt;&gt;&gt; plt.ylabel('Second Principal Component')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/5c8b81cf585cbfe8edbe308681aede3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*2mtWKYyLBJIrLMl0b4P7SA.png"/></div></figure><p id="7042" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">因此，使用这两个组件，分类可以很容易地完成。</p><p id="265b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> →了解组件</strong></p><p id="bcbc" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这些组件是原始功能的组合。它们被存储为适合的PCA对象的特征。因此，当我们查看下面以NumPy矩阵形式出现的PCA分量时，每行代表一个主分量，每列对应于原始特征。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="4e1f" class="lm jp hi li b fi ln lo l lp lq">&gt;&gt;&gt; pca.components_<br/>array([[ 0.21890244,  0.10372458,  0.22753729,  0.22099499,  0.14258969,<br/>         0.23928535,  0.25840048,  0.26085376,  0.13816696,  0.06436335,<br/>         0.20597878,  0.01742803,  0.21132592,  0.20286964,  0.01453145,<br/>         0.17039345,  0.15358979,  0.1834174 ,  0.04249842,  0.10256832,<br/>         0.22799663,  0.10446933,  0.23663968,  0.22487053,  0.12795256,<br/>         0.21009588,  0.22876753,  0.25088597,  0.12290456,  0.13178394],<br/>       [-0.23385713, -0.05970609, -0.21518136, -0.23107671,  0.18611302,<br/>         0.15189161,  0.06016536, -0.0347675 ,  0.19034877,  0.36657547,<br/>        -0.10555215,  0.08997968, -0.08945723, -0.15229263,  0.20443045,<br/>         0.2327159 ,  0.19720728,  0.13032156,  0.183848  ,  0.28009203,<br/>        -0.21986638, -0.0454673 , -0.19987843, -0.21935186,  0.17230435,<br/>         0.14359317,  0.09796411, -0.00825724,  0.14188335,  0.27533947]])</span></pre><p id="35fa" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">可以绘制热图来显示特征和主要成分之间的关系。</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="6753" class="lm jp hi li b fi ln lo l lp lq">&gt;&gt;&gt; comp <strong class="li hj">=</strong> pd.DataFrame(pca.components_,<br/>columns<strong class="li hj">=</strong>data['feature_names'])<br/>&gt;&gt;&gt; comp</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lu"><img src="../Images/40d2a1cad967db01fec99746aa53ef57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXUhpjOzi_-K5ISMx1RUiA.png"/></div></div></figure><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="fb5a" class="lm jp hi li b fi ln lo l lp lq">&gt;&gt;&gt; sns.heatmap(comp,cmap<strong class="li hj">=</strong>'rainbow',)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/d9cf2493bd602fa5b6be8e30fe665944.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*zVXGQUncoKbs3PEaT7NPsw.png"/></div></figure></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><blockquote class="md"><p id="9c1f" class="me mf hi bd mg mh mi mj mk ml mm lb dx translated"><em class="mn">这里指笔记本</em><a class="ae jn" href="https://github.com/jayashree8/Machine_learning_PCA/blob/master/PCA.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="mn"/></a><em class="mn">。</em></p></blockquote><h2 id="f935" class="lm jp hi bd jq mo mp mq ju mr ms mt jy kp mu mv ka kt mw mx kc kx my mz ke na bi translated">初级机器学习书籍可以参考:</h2><div class="nb nc ez fb nd ne"><a href="https://amzn.to/3i3XU1A" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">Python机器学习:机器学习和深度学习的Python编程初学者指南</h2></div><div class="nl l"><div class="nm l nn no np nl nq jh ne"/></div></div></a></div><div class="nb nc ez fb nd ne"><a href="https://amzn.to/3fQc6IW" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">一百页的机器学习书</h2></div><div class="nl l"><div class="nr l nn no np nl nq jh ne"/></div></div></a></div><h2 id="ab42" class="lm jp hi bd jq mo ns mq ju mr nt mt jy kp nu mv ka kt nv mx kc kx nw mz ke na bi translated">可以参考的高级机器学习书籍:</h2><div class="nb nc ez fb nd ne"><a href="https://amzn.to/2SxwQNw" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">用Scikit-Learn、Keras和张量流进行机器学习:概念、工具和技术…</h2></div><div class="nl l"><div class="nx l nn no np nl nq jh ne"/></div></div></a></div><div class="nb nc ez fb nd ne"><a href="https://amzn.to/3wz62eE" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">模式识别和机器学习(信息科学和统计学)</h2></div><div class="nl l"><div class="ny l nn no np nl nq jh ne"/></div></div></a></div><blockquote class="nz oa ob"><p id="04d6" class="kg kh oc ki b kj lc ij kl km ld im ko od le kr ks oe lf kv kw of lg kz la lb hb bi translated"><em class="hi">联系我:</em> <a class="ae jn" href="https://www.linkedin.com/in/jayashree-domala8/" rel="noopener ugc nofollow" target="_blank"> <em class="hi"> LinkedIn </em> </a></p><p id="a568" class="kg kh oc ki b kj lc ij kl km ld im ko od le kr ks oe lf kv kw of lg kz la lb hb bi translated"><em class="hi">查看我的其他作品:</em> <a class="ae jn" href="https://github.com/jayashree8" rel="noopener ugc nofollow" target="_blank"> <em class="hi"> GitHub </em> </a></p></blockquote></div></div>    
</body>
</html>