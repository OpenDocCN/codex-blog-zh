<html>
<head>
<title>AlexNet Complete Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AlexNet完整架构</h1>
<blockquote>原文：<a href="https://medium.com/codex/alexnet-complete-architecture-dc3a9920cdd?source=collection_archive---------17-----------------------#2022-11-08">https://medium.com/codex/alexnet-complete-architecture-dc3a9920cdd?source=collection_archive---------17-----------------------#2022-11-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="095b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="eef8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">AlexNet由2012年  <strong class="jf hj"> <em class="kb"> ImageNet竞赛</em> </strong>冠军<strong class="jf hj"> <em class="kb">的<strong class="jf hj">辛顿</strong>和他的学生Alex Krizhevsky设计。也是在那一年之后，更多更深入的神经网络被提出，比如优秀的VGG，GoogleLeNet。其官方数据模型准确率<strong class="jf hj"> 57.1% </strong>，top 1-5达到<strong class="jf hj"> 80.2% </strong>。这对于传统的机器学习分类算法来说已经是相当出色的了。</em></strong></p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/331583638ceaaa36a67054a0fc6d8e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DW1yWUEfLtQnyuTp.png"/></div></div></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ko"><img src="../Images/b65c6f20bb0da0bf973d1e6385ae8fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q7NsZf4-U1q33XUf.png"/></div></div></figure><h2 id="7dc7" class="kp ig hi bd ih kq kr ks il kt ku kv ip jo kw kx it js ky kz ix jw la lb jb lc bi translated">下表解释了AlexNet的网络结构:</h2><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ld"><img src="../Images/38e62adac86e4e584162b52df464a179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0Dq28HZESo4Ligdi.jpeg"/></div></div></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es le"><img src="../Images/7958883a47e1f7ac9ebae4d4ce4b3881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/0*pZma8BU-5omwOeEQ.png"/></div></figure><h1 id="0f9a" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">为什么AlexNet能取得更好的效果？</h1><ol class=""><li id="874b" class="lf lg hi jf b jg jh jk jl jo lh js li jw lj ka lk ll lm ln bi translated"><strong class="jf hj">使用Relu激活功能:</strong></li></ol><p id="8a12" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated">使用ReLu的主要原因是因为<strong class="jf hj">它简单、快速，并且从经验上看它似乎工作得很好</strong>。根据经验，早期的论文观察到，用ReLu训练深度网络往往比用sigmoid激活训练深度网络收敛得更快更可靠。</p><p id="7fc6" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated"><strong class="jf hj"> Relu函数:f (x) = max (0，x) </strong></p><p id="7d70" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated">ReLu是<strong class="jf hj">用于多层神经网络或深度神经网络</strong>的非线性激活函数。这个函数可以表示为:其中x =一个输入值。根据等式，ReLu的输出是零和输入值之间的最大值。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lt"><img src="../Images/bf3af4756d1c10a5069793b6fcbf319e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7Z6LLv-pgD0rOplT.png"/></div></div></figure><p id="afd7" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated"><strong class="jf hj"> 2。标准化(本地响应标准化):</strong></p><p id="86bd" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated">要深入了解LRN，请参考:<a class="ae lu" href="https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac" rel="noopener" target="_blank">https://towards data science . com/difference-between-local-response-normalization-and-batch-normalization-272308 c 034 AC</a></p><p id="3081" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated">使用ReLU f (x) = max (0，x)后，你会发现激活函数后的值没有像tanh和sigmoid函数那样的值域，所以ReLU后一般会做一个归一化，LRU是一个稳健的建议(这里不确定，应该会提出？)神经科学中有一种方法叫做“侧抑制”，讲的是活动神经元对其周围神经元的作用。</p><p id="b591" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated">下面的等式指的是<strong class="jf hj">通道间LRN: </strong></p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lv"><img src="../Images/1cb80a018fa64de197cfe559771a9fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JoraQFQDPbJHCIHZ.png"/></div></figure><p id="b8b4" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated"><strong class="jf hj"> 3。辍学:</strong></p><p id="93a1" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated">Dropout也是常说的一个概念，可以有效防止神经网络的过拟合。与一般的线性模型相比，采用了一种常规的方法来防止模型过拟合。在神经网络中，通过修改神经网络本身的结构来实现退出。对于某一层神经元，在保持输入层和输出层神经元个体不变的情况下，随机删除一部分定义概率的神经元，然后根据神经网络的学习方法更新参数。在下一次迭代中，重新随机化，去掉一些神经元，直到训练结束。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lw"><img src="../Images/52d80dfeb9a016d3d33670586877d668.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*AhUGFY-kK7JnFTAo.jpg"/></div></figure><p id="07e1" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated"><strong class="jf hj"> 4。数据扩充:</strong></p><p id="18fa" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated">更深入的了解:<a class="ae lu" rel="noopener" href="/lansaar/what-is-data-augmentation-3da1373e3fa1">https://medium . com/lansaar/what-is-data-augmentation-3da 1373 E3 fa 1</a></p><p id="19e1" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated"><strong class="jf hj">在深度学习中，当数据量不够大时，一般有4种解决方案:</strong></p><ul class=""><li id="10e6" class="lf lg hi jf b jg lo jk lp jo lx js ly jw lz ka ma ll lm ln bi translated">数据扩充——人为地增加训练集的大小——通过翻译、翻转、噪声</li><li id="a308" class="lf lg hi jf b jg mb jk mc jo md js me jw mf ka ma ll lm ln bi translated">正则化— —相对少量的数据会导致模型过拟合，使得训练误差很小，测试误差特别大。通过在损失函数后增加一个正则项，可以抑制过拟合。缺点是需要引入手动调节的超参数。</li><li id="709b" class="lf lg hi jf b jg mb jk mc jo md js me jw mf ka ma ll lm ln bi translated">辍学-也是一种正规化的方法。但与上述不同的是，它是通过随机将某些神经元的输出设置为零来实现的</li><li id="304e" class="lf lg hi jf b jg mb jk mc jo md js me jw mf ka ma ll lm ln bi translated">无监督预训练-使用自动编码器或RBM卷积形式逐层进行无监督预训练，最后添加一个分类层进行监督微调</li></ul><h1 id="672f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">代码实现:</h1><pre class="kd ke kf kg fd mg mh mi mj aw mk bi"><span id="1f13" class="kp ig hi mh b fi ml mm l mn mo">!pip install tflearn</span><span id="46f0" class="kp ig hi mh b fi mp mm l mn mo">import keras<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Activation, Dropout, Flatten,\<br/> Conv2D, MaxPooling2D<br/>from keras.layers.normalization import BatchNormalization<br/>import numpy as np<br/>np.random.seed(1000)</span><span id="866c" class="kp ig hi mh b fi mp mm l mn mo"># (2) Get Data<br/>import tflearn.datasets.oxflower17 as oxflower17<br/>x, y = oxflower17.load_data(one_hot=True)</span><span id="f502" class="kp ig hi mh b fi mp mm l mn mo"># (3) Create a sequential model<br/>model = Sequential()</span><span id="71d2" class="kp ig hi mh b fi mp mm l mn mo"># 1st Convolutional Layer<br/>model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),\<br/> strides=(4,4), padding='valid'))<br/>model.add(Activation('relu'))<br/># Pooling <br/>model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))<br/># Batch Normalisation before passing it to the next layer<br/>model.add(BatchNormalization())</span><span id="2e11" class="kp ig hi mh b fi mp mm l mn mo"># 2nd Convolutional Layer<br/>model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))<br/>model.add(Activation('relu'))<br/># Pooling<br/>model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))<br/># Batch Normalisation<br/>model.add(BatchNormalization())</span><span id="09c4" class="kp ig hi mh b fi mp mm l mn mo"># 3rd Convolutional Layer<br/>model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))<br/>model.add(Activation('relu'))<br/># Batch Normalisation<br/>model.add(BatchNormalization())</span><span id="3db9" class="kp ig hi mh b fi mp mm l mn mo"># 4th Convolutional Layer<br/>model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))<br/>model.add(Activation('relu'))<br/># Batch Normalisation<br/>model.add(BatchNormalization())</span><span id="68e6" class="kp ig hi mh b fi mp mm l mn mo"># 5th Convolutional Layer<br/>model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))<br/>model.add(Activation('relu'))<br/># Pooling<br/>model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))<br/># Batch Normalisation<br/>model.add(BatchNormalization())</span><span id="1369" class="kp ig hi mh b fi mp mm l mn mo"># Passing it to a dense layer<br/>model.add(Flatten())<br/># 1st Dense Layer<br/>model.add(Dense(4096, input_shape=(224*224*3,)))<br/>model.add(Activation('relu'))<br/># Add Dropout to prevent overfitting<br/>model.add(Dropout(0.4))<br/># Batch Normalisation<br/>model.add(BatchNormalization())</span><span id="9549" class="kp ig hi mh b fi mp mm l mn mo"># 2nd Dense Layer<br/>model.add(Dense(4096))<br/>model.add(Activation('relu'))<br/># Add Dropout<br/>model.add(Dropout(0.4))<br/># Batch Normalisation<br/>model.add(BatchNormalization())</span><span id="9362" class="kp ig hi mh b fi mp mm l mn mo"># 3rd Dense Layer<br/>model.add(Dense(1000))<br/>model.add(Activation('relu'))<br/># Add Dropout<br/>model.add(Dropout(0.4))<br/># Batch Normalisation<br/>model.add(BatchNormalization())</span><span id="d8ef" class="kp ig hi mh b fi mp mm l mn mo"># Output Layer<br/>model.add(Dense(17))<br/>model.add(Activation('softmax'))</span><span id="cf4d" class="kp ig hi mh b fi mp mm l mn mo">model.summary()</span><span id="9758" class="kp ig hi mh b fi mp mm l mn mo"># (4) Compile <br/>model.compile(loss='categorical_crossentropy', optimizer='adam',\<br/> metrics=['accuracy'])</span><span id="3d77" class="kp ig hi mh b fi mp mm l mn mo"># (5) Train<br/>model.fit(x, y, batch_size=64, epochs=1, verbose=1, \<br/>validation_split=0.2, shuffle=True)</span></pre><p id="8f18" class="pw-post-body-paragraph jd je hi jf b jg lo ji jj jk lp jm jn jo lq jq jr js lr ju jv jw ls jy jz ka hb bi translated">在这里，我使用不同的数据集和使用批量标准化和不同的输入维数来减少可训练参数，以便它可以在CPU上运行。最初的架构有11亿个参数，这将需要GPU平稳运行。</p></div></div>    
</body>
</html>