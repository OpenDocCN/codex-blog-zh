<html>
<head>
<title>Autonomous Navigation using Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度强化学习的自主导航</h1>
<blockquote>原文：<a href="https://medium.com/codex/implementation-of-deep-reinforcement-learning-algorithms-to-solve-banana-collector-unity-ml-agent-55a7c7d92f3e?source=collection_archive---------16-----------------------#2021-09-10">https://medium.com/codex/implementation-of-deep-reinforcement-learning-algorithms-to-solve-banana-collector-unity-ml-agent-55a7c7d92f3e?source=collection_archive---------16-----------------------#2021-09-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/2e76c864bca45413f57d4307bb5e8267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*J_oGW-dtrSyvSeBoXD3z4A.png"/></div><figcaption class="hn ho et er es hp hq bd b be z dx translated"><strong class="bd hr">自主导航采用深度强化学习(图片来源:</strong><a class="ae hs" href="https://waypointrobotics.com/blog/what-autonomous-robots/" rel="noopener ugc nofollow" target="_blank"><strong class="bd hr">【https://waypointrobotics.com/blog/what-autonomous-robots/】</strong></a><strong class="bd hr">)</strong></figcaption></figure><div class=""/><h1 id="34e7" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">实现深度Q学习算法来解决香蕉收集器Unity ML-Agent导航问题语句</h1><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jp"><img src="../Images/34098fe88d9782d62d3734e120337bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kQmReqDybDk4jPZ8zJfFjQ.gif"/></div></div></figure><h1 id="dea9" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">问题陈述的简要介绍</h1><p id="62fb" class="pw-post-body-paragraph jy jz hv ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">使用香蕉收集器Unity ML-Agent环境的简化版本，该项目的目标是训练一个Agent在一个大的正方形世界中导航并只收集黄色香蕉。收集一个黄色香蕉提供+1的奖励，收集一个蓝色香蕉提供-1的奖励(即惩罚)。因此，代理的目标是收集尽可能多的黄色香蕉，同时避免蓝色香蕉。代理的观察空间是37维的，代理的动作空间是4维的(向前、向后、向左转和向右转)。任务是插曲式的，为了解决环境，代理人必须在连续100集内获得+13的平均分。</p><h1 id="cae8" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">实施的强化算法:</h1><h2 id="7dd5" class="kw it hv bd hr kx ky kz ix la lb lc jb kj ld le jf kn lf lg jj kr lh li jn lj bi translated">a)香草深度Q-学习</h2><h2 id="9298" class="kw it hv bd hr kx ky kz ix la lb lc jb kj ld le jf kn lf lg jj kr lh li jn lj bi translated">b)双重深度Q学习</h2><h2 id="4587" class="kw it hv bd hr kx ky kz ix la lb lc jb kj ld le jf kn lf lg jj kr lh li jn lj bi translated">c)双重深度Q学习，优先体验重放</h2><h1 id="1255" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">结果展示:</h1><h2 id="e6b0" class="kw it hv bd hr kx ky kz ix la lb lc jb kj ld le jf kn lf lg jj kr lh li jn lj bi translated"><strong class="ak">未经训练的代理在环境中随机导航</strong></h2><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lk"><img src="../Images/5819db3cfed205b909d4c1c2264c90cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*QIOlwS0jVxVLPWsddP83ZQ.gif"/></div></div></figure><h2 id="83bf" class="kw it hv bd hr kx ky kz ix la lb lc jb kj ld le jf kn lf lg jj kr lh li jn lj bi translated">一个训练有素的代理导航的目标是收集黄色香蕉和避免蓝色香蕉</h2><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jp"><img src="../Images/34098fe88d9782d62d3734e120337bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kQmReqDybDk4jPZ8zJfFjQ.gif"/></div></div></figure><h1 id="4439" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">状态空间:</h1><p id="2744" class="pw-post-body-paragraph jy jz hv ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">观察是在37维连续空间中进行的，对应于代理前进方向周围物体的35维基于射线的感知和2维速度。光线感知的35个维度分解为:7条光线从代理人以下列角度投射(并以同样的顺序返回):【20，90，160，45，135，70，110】其中90°在代理人正前方。每条光线都是5维的，它被投射到场景上。如果它遇到四个可检测对象之一(即黄香蕉、墙、蓝香蕉、试剂)，则数组中该位置的值被设置为1。最后有一个距离度量，它是射线长度的一部分。每条光线都是[黄香蕉，墙，蓝香蕉，代理，距离]。代理的速度是二维的:左/右速度和前/后速度。观察空间是完全可观察的，因为它包括了关于障碍物类型、到障碍物的距离和智能体速度的所有必要信息。因此，不需要增加观测值就可以完全观测到。因此，输入的观测值可以直接用作状态表示。</p><h1 id="f371" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">行动空间:</h1><p id="9619" class="pw-post-body-paragraph jy jz hv ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">动作空间是四维的。四个离散动作对应于:</p><p id="efc9" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">a) 0:向前移动</p><p id="48fc" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">b) 1:向后移动</p><p id="4831" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">c) 2:向左移动</p><p id="0f33" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">d) 3:向右移动</p><h1 id="b397" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">解决方案标准:</h1><p id="c69a" class="pw-post-body-paragraph jy jz hv ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">当代理在连续100集内平均得分为+13时，环境被视为已解决。</p><h1 id="299e" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">相关概念</h1><p id="1421" class="pw-post-body-paragraph jy jz hv ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">本节提供了一个理论背景，描述了当前在这一领域的工作，以及在这项工作中使用的概念和技术。</p><p id="0dee" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> a) </strong> <strong class="ka hw">强化学习:</strong>随着AlphaGo等最近的突破和对雅达利2600游戏的掌握，强化学习变得非常流行。强化学习(RL)是一个学习策略的框架，该策略通过与环境的交互来最大化代理的长期回报。策略将情况(状态)映射到操作。在每次状态转换后，代理会收到一个即时的短期奖励。一个国家的长期回报是由一个价值函数决定的。状态的值大致对应于代理从该状态开始可以累积的总奖励。对应于在状态s中采取行动a之后的长期奖励的行动值函数通常被称为Q值，并且形成了被称为Q学习的最广泛使用的RL技术的基础</p><p id="7302" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> b)时间差异学习:</strong>时间差异学习是现代RL的中心思想，并且通过基于其他估计更新动作值函数的估计来工作。这确保代理不必等到完成一集后的实际累积奖励来更新其估计，而是能够从每个动作中学习。</p><p id="8ffd" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw">c)Q-学习:</strong>Q-学习是一种偏离策略的时间差分(TD)控制算法。非策略方法评估或改进不同于决策策略的策略。因此，这些决策可以由人类专家或随机策略做出，生成(状态、动作、奖励、新状态)条目以从中学习最优策略。</p><pre class="jq jr js jt fd lq lr ls lt aw lu bi"><span id="8c3f" class="kw it hv lr b fi lv lw l lx ly">Q-learning learns a function Q that approximates the optimal action-value function. It does this by randomly initializing Q and then generating actions using a policy derived from Q, such as e-greedy. An e-greedy policy chooses the action with the highest Q value or a random action with a (low) probability of , promoting exploration <br/>as e (epsilon) increases. With this newly generated (state (St), action (At), reward (Rt+1), new state (St+1)) pair,<br/>Q is updated using rule 1.<br/> <br/>This update rule essentially states that the current estimate must be updated using the received immediate reward plus a discounted estimation of the maximum action-value for the new state. It is important to note here that the update is done immediately after performing the action using an estimate instead of waiting for the true cumulative reward, demonstrating TD in action. The learning rate α decides how much to alter the current estimate and the<br/>discount rate γ decides how important future rewards (estimated action-value) are compared to the immediate reward.</span></pre><p id="d48d" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> d)经验重放:</strong>经验重放是在固定大小的缓冲区中存储以前的经验(St，At，Rt+1，St+1)的机制。然后，对小批次进行随机采样，添加到当前时间步长的经验中，并用于增量训练神经网络。这种方法可以对抗灾难性遗忘，通过多次训练来更有效地利用数据，并表现出更好的收敛行为</p><p id="f2cb" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> e)固定的Q目标:</strong>在使用函数逼近器的Q学习算法中，TD目标还取决于正在学习/更新的网络参数w，这可能导致不稳定性。为了解决这个问题，使用了具有相同架构但不同权重的独立网络。并且这个单独的目标网络的权重每隔几步更新一次，以等于持续更新的本地网络。</p><h1 id="3935" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">所用学习算法的描述</h1><ol class=""><li id="db80" class="lz ma hv ka b kb kc kf kg kj mb kn mc kr md kv me mf mg mh bi translated"><strong class="ka hw">深度Q学习算法:</strong>在现代Q学习中，使用神经网络来估计函数Q，该神经网络将状态作为输入，并输出每个可能动作的预测Q值。它通常用Q(S，A，θ)表示，其中θ表示网络的权重。用于控制的实际策略可以随后通过估计给定当前状态的每个动作的Q值并应用ε-贪婪策略从Q中导出。深度Q学习简单地意味着使用多层前馈神经网络或者甚至卷积神经网络(CNN)来处理原始像素输入</li><li id="f749" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">双深度Q学习算法:</strong>深度Q学习是基于Q学习算法，以深度神经网络作为函数逼近器。然而，Q-learning遇到的一个问题是在其更新方程中过高估计了TD目标。期望值总是大于或等于期望值的贪婪动作。因此，Q-learning最终会高估Q值，从而降低学习效率。为了解决这个问题，我们使用双Q学习算法，其中有两个独立的Q表。在每个时间步，我们随机决定使用哪个q表，并使用一个q表中的贪婪动作来评估另一个q表的q值</li><li id="6250" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">使用双深度Q学习算法的优先化体验重放:</strong>对于记忆重放，代理收集元组(状态、奖励、下一状态、动作、完成)并重用它们用于将来的学习。在优先重放的情况下，代理必须根据每个元组对学习的贡献来分配优先级。之后，这些元组基于它们的优先级被重用，从而导致更高效的学习。</li></ol><p id="801b" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">为此实现引入了两个新参数:</p><p id="427b" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> a) ALPHA : </strong>优先级指数，可以调整该指数，以确定可以重新引入多少因子随机采样，从而避免仅通过使用优先级经验样本而过度拟合。ALPHA的值1对应于仅使用优先级经验样本。ALPHA的值0对应于仅随机使用经验样本</p><p id="ba71" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw">b)β:</strong>重要性采样权重指数，用于确定在训练时Q-net模型的权重被修改了多少因子，β参数的值可以在训练期间逐渐增加，以在模型最终收敛到预期结果时，在训练的后期阶段给予更新的权重更多的重要性</p><h1 id="6fcd" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">使用的神经网络架构:</h1><p id="36df" class="pw-post-body-paragraph jy jz hv ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">使用具有2个隐藏层的多层前馈神经网络体系结构，每个隐藏层具有64个隐藏神经元。在2个隐藏层的输入上使用ReLU(校正线性单元)激活函数。在一个实现中，我也尝试初始化权重，以查看模型的学习是否增加，但是在没有神经网络层的权重初始化的情况下，在所获得的结果中没有发现太大差异。在修改的实现中，我也尝试降低学习速率，以获得更快的结果，而在模型的训练中没有任何显著的改进</p><p id="8045" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">此外，在双重深度Q-Net实现中，在训练期间实现的奖励被剪切到-1到1的范围内，以去除训练期间的异常值</p><h1 id="cb4a" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">使用的超参数:</h1><ol class=""><li id="da6e" class="lz ma hv ka b kb kc kf kg kj mb kn mc kr md kv me mf mg mh bi translated"><strong class="ka hw">集数:</strong> 5000</li><li id="d4df" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">最大时间步长:</strong> 1000</li><li id="c2ae" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">Eps _ start:</strong>1(e-greedy策略中使用的起始ε值)</li><li id="2d47" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">Eps _ End:</strong>0.01(e-greedy策略中使用的ε值下限)</li><li id="d66a" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">Eps _ Decay:</strong>0.995(ε值减少的系数)</li><li id="e073" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw"> BUFFER_SIZE : </strong> int(1e5)</li><li id="5515" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">批量大小:</strong> 64</li><li id="68d5" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">伽玛:</strong> 0.99(贴现率)</li><li id="e39e" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw"> TAU : </strong> 1e-3(用于目标参数的软更新)</li><li id="c8f9" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw">目标得分:</strong>大于等于16</li></ol><h1 id="4615" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">优先体验重放实施的额外参数:</h1><ol class=""><li id="9f42" class="lz ma hv ka b kb kc kf kg kj mb kn mc kr md kv me mf mg mh bi translated"><strong class="ka hw"> ALPHA : </strong> 0.6(优先级指数)</li><li id="30a1" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated"><strong class="ka hw"> INIT_BETA : </strong> 0.4(重要性采样指数)</li></ol><h1 id="18d4" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">每集奖励图</h1><ol class=""><li id="43d2" class="lz ma hv ka b kb kc kf kg kj mb kn mc kr md kv me mf mg mh bi translated"><strong class="ka hw">深度Q学习算法结果(使用学习率1e-4获得的最佳结果):</strong></li></ol><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/c19ec7c19fa94e068d8230351a7d9c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*fThshdpAlcY4kIpvab_P1w.png"/></div></figure><p id="7bc3" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">使用学习率1e-4，在964集中获得+16分</p><p id="405e" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> 2。双重深度Q学习算法结果(通过在-1到1的范围内剪裁奖励获得的最佳结果):</strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/ba2e5272ed9311080bb856a5d6d0f9e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*lkuz9c8h_fCSEPMbSHQ4rA.png"/></div></figure><p id="5976" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">816集的+16分</p><p id="a70d" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> 3。具有双重深度Q学习算法结果的优先体验重放:</strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/36d11f35009df02f144ad4a83f539460.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*USUhNOCrdy4GC_iWrNWQag.png"/></div></figure><p id="e708" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">在732集里获得了+16的分数</p><h1 id="7424" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">结论</h1><p id="f8ec" class="pw-post-body-paragraph jy jz hv ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">从上面通过实施3种不同算法实现的结果可以明显看出，具有双重深度Q-net算法的优先体验重放在仅仅732集内执行了与目标分数的最佳转换，而双重深度Q-net算法在816集内收敛到目标分数，并且简单深度Q-net算法实施花费最大集数，即964集来达到+16的目标分数</p><p id="d31f" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated">在所有3个实现中，学习率为1e-4时获得了最佳结果，并且在双重深度Q-Net实现的情况下，在训练期间获得的奖励被剪切到-1到1的范围内，以去除训练期间的异常值</p><h1 id="0061" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">设置项目的安装说明:</h1><h1 id="0d43" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">1)设置Python环境:</h1><pre class="jq jr js jt fd lq lr ls lt aw lu bi"><span id="ff25" class="kw it hv lr b fi lv lw l lx ly">a) Download and install Anaconda 3 (latest version 5.3) from this link (https://www.anaconda.com/download/)for the specific Operating System and Architecture (64-bit or 32-bit) being used for Python 3.6 + version onwards<br/>    <br/> b) Create (and activate) a new environment with Python 3.6.:<br/>    Open Anaconda prompt and then execute the below given commands<br/> <br/>    Linux or Mac:<br/>    conda create --name drlnd python=3.6<br/>    source activate drlnd<br/>    <br/>    Windows:<br/>    conda create --name drlnd python=3.6 <br/>    activate drlnd<br/>    <br/> c) Minimal Installation of OpenAi Gym Environment<br/>    Below are the instructions to do minimal install of gym :</span><span id="8cd4" class="kw it hv lr b fi mq lw l lx ly">    git clone https://github.com/openai/gym.git<br/>    cd gym<br/>    pip install -e .<br/>     <br/>A minimal install of the packaged version can be done directly from PyPI: pip install gym<br/>     <br/> d) Clone the repository (https://github.com/udacity/deep-reinforcement-learning.git) and navigate to the python/ folder.<br/>Then, install several dependencies by executing the below commands in Anaconda Prompt Shell :<br/>      <br/>git clone https://github.com/udacity/deep-reinforcement-learning.git<br/>cd deep-reinforcement-learning/python<br/>pip install . (or pip install [all] )<br/>      <br/> e) Create an Ipython Kernel for the drlnd environment :<br/>python -m ipykernel install --user --name drlnd --display-name "drlnd"<br/>      <br/> f) Before running code in a notebook, change the kernel to match the drlnd environment by using the drop-down Kernel menu.</span></pre><h1 id="b3eb" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">2)安装Unity ML-代理相关的库/模块:</h1><pre class="jq jr js jt fd lq lr ls lt aw lu bi"><span id="2425" class="kw it hv lr b fi lv lw l lx ly">Clone the GitHub Repository (https://github.com/Unity-Technologies/ml-agents.git)and install the required libraries by running the below mentioned commands in the Anaconda Prompt :</span><span id="691d" class="kw it hv lr b fi mq lw l lx ly">git clone https://github.com/Unity-Technologies/ml-agents.git<br/>cd ml-agents/ml-agents (navigate inside ml-agents subfolder)<br/>pip install . or (pip install [all]) (install the modules required)</span></pre><h1 id="25c2" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">3)下载Unity环境:</h1><pre class="jq jr js jt fd lq lr ls lt aw lu bi"><span id="8462" class="kw it hv lr b fi lv lw l lx ly">a)For this project, Unity is not necessary to be installed because readymade built environment has already been provided, and can be downloaded from one of the links below as per the operating system being used:</span><span id="3ce4" class="kw it hv lr b fi mq lw l lx ly">Linux: https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip<br/>Mac OSX: https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip<br/>Windows (32-bit): https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip<br/>Windows (64-bit): https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip<br/> <br/>Place the downloaded file in the p1_navigation/ as well as python/ folder in the DRLND GitHub repository, and unzip (or decompress) the file.</span><span id="eb21" class="kw it hv lr b fi mq lw l lx ly">b)(For AWS) If the agent is to be trained on AWS (and a virtual screen is not enabled), then please use this link (https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux_NoVis.zip) to obtain the "headless" version of the environment. Watching the agent during training is not possible without enabling a virtual screen.(To watch the agent, follow the instructions to enable a virtual screen (https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)<br/>and then download the environment for the Linux operating  system above.)</span></pre><h1 id="483c" class="is it hv bd hr iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">运行代码以训练代理/测试已训练代理的详细信息:</h1><ol class=""><li id="4bee" class="lz ma hv ka b kb kc kf kg kj mb kn mc kr md kv me mf mg mh bi translated">首先在本地系统上克隆这个存储库(<a class="ae hs" href="https://github.com/PranayKr/Deep_Reinforcement_Learning_Projects.git" rel="noopener ugc nofollow" target="_blank">https://github . com/PranayKr/Deep _ Reinforcement _ Learning _ projects . git</a>)。</li><li id="c58a" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">还要在本地系统上克隆前面提到的存储库(【https://github.com/udacity/deep-reinforcement-learning.git】T2)。</li><li id="e765" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">现在，将这个克隆的GitHub Repo中存在的所有源代码文件和预训练模型权重放入Deep-RL克隆存储库文件夹的python/文件夹中。</li><li id="68a0" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">接下来，将包含已下载的用于Windows (64位)操作系统的unity环境文件的文件夹放入Deep-RL克隆存储库文件夹的python/文件夹中。</li><li id="50c8" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">打开Anaconda提示符shell窗口，并在Deep-RL克隆存储库文件夹中的python/文件夹内导航。</li><li id="b036" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">在Anaconda提示符shell窗口中运行命令“jupyter notebook ”,在浏览器中打开jupyter notebook web-app工具，从该工具中可以找到笔记本中提供的任何培训和测试源代码。ipynb文件)可以打开。</li><li id="057b" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">在笔记本中运行/执行代码之前，使用下拉内核菜单更改内核(为drlnd环境创建的IPython内核)以匹配drlnd环境。</li><li id="1ab9" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">提供的培训和测试笔记本中的源代码(。ipynb文件)也可以在各自的新python文件(.py文件)，然后使用命令“python <filename.py>”从Anaconda提示符shell窗口直接执行。</filename.py></li></ol><p id="1cb8" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw">注:</strong></p><ol class=""><li id="14cd" class="lz ma hv ka b kb ll kf lm kj mr kn ms kr mt kv me mf mg mh bi translated">通过选择内核选项卡中的选项(重新启动并全部运行),可以同时执行所有单元</li><li id="4e88" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">请更改(*的名称。pth)文件，模型权重在训练期间保存在该文件中，以避免用相同的文件名覆盖现有的预训练模型权重</li></ol><p id="12aa" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> a)普通深度Q-net算法训练/测试细节(使用的文件):</strong></p><pre class="jq jr js jt fd lq lr ls lt aw lu bi"><span id="9770" class="kw it hv lr b fi lv lw l lx ly"><strong class="lr hw">For Training :</strong> Open either of the below mentioned Jupyter Notebook and execute all the cells<br/>    <br/>    1) <strong class="lr hw">DeepQ-Net_Navigation_Solution-LR_(1e-4).ipynb</strong> (using Learning Rate Hyperparameter val : 1e-4)<br/>    2) <strong class="lr hw">DeepQ-Net_Navigation_Solution-LR_(5e-4).ipynb</strong> (using Learning Rate Hyperparameter val : 5e-4)<br/>    3) <strong class="lr hw">DeepQ-Net_Navigation_Solution-LR_(5e-5).ipynb</strong> (using Learning Rate Hyperparameter val : 5e-5)<br/>    <br/>    <strong class="lr hw">Neural Net Model Architecture file Used :</strong> <strong class="lr hw">NN_Model.py</strong><br/>    <strong class="lr hw">The Unity Agent file used :</strong> <strong class="lr hw">DeepQN_Agent.py</strong><br/>    <br/>    <strong class="lr hw">For Testing :</strong> open the Jupyter Notebook file <strong class="lr hw">"DeepQNet_Test.ipynb"</strong> and run the code to test the results obtained using Pre-trained model weights<br/>                  <br/>    <strong class="lr hw">Pretrained Model Weights provided :</strong> 1)<strong class="lr hw">DQN_Checkpoint.pth</strong><br/>                                        2)<strong class="lr hw">DQN_Checkpoint_2.pth</strong><br/>                                        3)<strong class="lr hw">DQN_Checkpoint_3.pth</strong></span></pre><p id="d43c" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> b)双深度Q-net算法训练/测试细节(使用的文件):</strong></p><pre class="jq jr js jt fd lq lr ls lt aw lu bi"><span id="7d37" class="kw it hv lr b fi lv lw l lx ly"><strong class="lr hw">For Training :</strong> Open either of the below mentioned Jupyter Notebook and execute all the cells<br/>    <br/>    1) <strong class="lr hw">DoubleDeepQ-Net_Navigation_Solution.ipynb </strong><br/>    2) <strong class="lr hw">DoubleDeepQ-Net_Navigation_Solution2.ipynb </strong><br/>    3) <strong class="lr hw">DoubleDeepQ-Net_Navigation_Solution-RewardsClipped.ipynb</strong><br/>    4) <strong class="lr hw">DoubleDeepQ-Net_Navigation_Solution-RewardsClipped-LRDecay.ipynb</strong><br/>    <br/>    <strong class="lr hw">Neural Net Model Architecture file Used :</strong> <strong class="lr hw">DDQN_NN_Model.py</strong><br/>    <strong class="lr hw">The Unity Agent file used :</strong> <br/>    1) <strong class="lr hw">DoubleDeepQN_Agent.py</strong><br/>    2) <strong class="lr hw">DoubleDeepQN_Agent_WeightsInitialized.py</strong><br/>    <br/>    <strong class="lr hw">For Testing :</strong> open the Jupyter Notebook file <br/><strong class="lr hw">"DoubleDeepQ-Net_Test.ipynb"</strong> and run the code to test the results obtained using Pre-trained model weights<br/>                  <br/>    <strong class="lr hw">Pretrained Model Weights provided :</strong> <br/>    1)<strong class="lr hw">DoubleDQN_Checkpoint_1.pth</strong><br/>    2)<strong class="lr hw">DoubleDQN_Checkpoint_1_RewardsClipped.pth</strong><br/>    3)<strong class="lr hw">DoubleDQN_Checkpoint_2.pth</strong></span></pre><p id="aaa8" class="pw-post-body-paragraph jy jz hv ka b kb ll kd ke kf lm kh ki kj ln kl km kn lo kp kq kr lp kt ku kv hb bi translated"><strong class="ka hw"> c)具有优先经验的双深度Q-net重放算法训练/测试细节(使用的文件):</strong></p><pre class="jq jr js jt fd lq lr ls lt aw lu bi"><span id="857e" class="kw it hv lr b fi lv lw l lx ly"><strong class="lr hw">For Training :</strong> Open the below mentioned Jupyter Notebook and execute all the cells<br/><strong class="lr hw">PrioritizedExpReplaay_DoubleDeepQ-Net_Navigation_Solution.ipynb </strong><br/>   <br/><strong class="lr hw">Neural Net Model Architecture file Used</strong> : <strong class="lr hw">DDQN_NN_Model.py</strong><br/><strong class="lr hw">The Unity Agent file used </strong>: <strong class="lr hw">PriorityExpReplay_DoubleDeepQN_Agent.py</strong><br/>    <br/><strong class="lr hw">For Testing :</strong> open the Jupyter Notebook file <strong class="lr hw">"PriorityExp_DoubleDeepQ-Net_Test.ipynb"</strong> and run the code to test the results obtained using Pre-trained model weights.<br/>                  <br/><strong class="lr hw">Pretrained Model Weights provided</strong> : <strong class="lr hw">PriorityExpDoubleDQN_Checkpoint_1.pth</strong></span></pre><h2 id="bae5" class="kw it hv bd hr kx ky kz ix la lb lc jb kj ld le jf kn lf lg jj kr lh li jn lj bi translated"><strong class="ak">注:</strong></h2><ol class=""><li id="5622" class="lz ma hv ka b kb kc kf kg kj mb kn mc kr md kv me mf mg mh bi translated">这篇文章也发表在LinkedIn(<a class="ae hs" href="https://www.linkedin.com/pulse/implementation-deep-reinforcement-learning-algorithms-pranay-kumar" rel="noopener ugc nofollow" target="_blank">https://www . LinkedIn . com/pulse/implementation-deep-reinforcement-learning-algorithms-pranay-Kumar</a>)</li><li id="98a5" class="lz ma hv ka b kb mi kf mj kj mk kn ml kr mm kv me mf mg mh bi translated">如果你想联系我，请通过LinkedIn<strong class="ka hw"/>(【https://www.linkedin.com/in/pranay-kumar-02b35524/】)<strong class="ka hw"/>给我留言，或者发邮件到pranay.scorpio9@gmail.com</li></ol></div></div>    
</body>
</html>