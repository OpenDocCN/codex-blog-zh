<html>
<head>
<title>NLP Deep Learning Training on Downstream tasks using Pytorch Lightning — IMDB Classification — Part 2 of 7</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pytorch Lightning对下游任务进行NLP深度学习培训— IMDB分类—第2部分，共7部分</h1>
<blockquote>原文：<a href="https://medium.com/codex/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-imdb-classification-cbb6c62789c3?source=collection_archive---------12-----------------------#2021-07-23">https://medium.com/codex/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-imdb-classification-cbb6c62789c3?source=collection_archive---------12-----------------------#2021-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/9f4ecacc9aa360dd72d8d96b84b63e7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*ABm3J22Z9RfHQvCAWRqHUQ.png"/></div></figure><p id="c6e9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是第2部分，也是本系列的继续。请点击这里的<a class="ae jk" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-intro-part-1-of-6-c338a05f86e6" rel="noopener">介绍文章</a>了解这个系列的动机。正如简介中提到的，我们将查看<a class="ae jk" href="https://github.com/kswamy15/NLP_Tasks_PyLightning/blob/main/Bert_NLP_Pytorch_IMDB_v4.ipynb" rel="noopener ugc nofollow" target="_blank"> IMDB分类器Colab笔记本</a>的各个部分，并对每个部分做出适当的评论。</p><ol class=""><li id="fb37" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><strong class="io hj">下载并导入库</strong>——这里没什么重要的。只需下载并导入常规的Pytorch和Pytorch Lightning库</li><li id="34b8" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">下载数据</strong>—IMDB数据集经过预处理，可从变压器数据集库中下载。但是出于演示的目的，笔记本从公共存储库中下载数据，并以对培训有用的格式准备数据。该数据有25000个训练样本和25000个测试样本。</li><li id="9251" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">定义预训练模型</strong> —这里使用的预训练模型是基于蒸馏的模型。它是BERT的精华版本，速度快60%，内存轻40%，但仍保留了BERT 97%的性能。一旦您成功地完成了这方面的训练，就可以通过更改model_checkpoint变量来尝试其他预训练的模型。</li><li id="3c6e" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">定义预处理函数或数据集类</strong> —这里我们定义Pytorch数据集继承类IMDBDataset，它将创建数据加载器所需的数据集格式的训练、val和测试数据。Pytorch使用DataLoader类将数据构建成小批。在这个类中，使用预先训练的标记器对数据进行标记。</li><li id="bcbe" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">定义数据模块类</strong> —这是一个Pytorch Lightning定义的类，包含使用数据加载器准备小批量数据所需的所有代码。在训练开始时，训练器类将首先调用<em class="jz">准备_数据</em>和<em class="jz">设置</em>功能。在<em class="jz"> prepare_data </em>功能中，根据以下事实为数据定义目标，即评论按照前12，500条属于正面评论，其余12，500条属于负面评论的顺序排列。训练数据被分成75/25训练和验证数据。这里有一个<em class="jz"> collate </em>函数，用于填充小批量。Bert类模型要求小批量的所有输入数据长度相同。collate函数不是将输入数据填充到整个数据集的最大长度，而是帮助将小批量的输入数据填充到该小批量中最大长度的数据。这提供了更快的训练和更少的内存使用。</li><li id="8135" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">定义模型类</strong>—DL模型的正向函数在此定义。可以看出，来自最后一个隐藏层的输出(以及作为CLS令牌的输出的第一个元素的输出)取自Bert模型，并且在最终通过具有两个输出的线性层发送用于二进制分类之前，通过线性、Relu、丢弃层发送。CLS令牌的输出被认为代表整个句子的意思。如果我们想从微调模型中为其他下游应用程序获取最后一个隐藏层的令牌嵌入，则定义get_outputs函数。</li></ol><figure class="ka kb kc kd fd ij"><div class="bz dy l di"><div class="ke kf l"/></div></figure><ul class=""><li id="d757" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj kg jr js jt bi translated"><strong class="io hj">定义Pytorch Lightning模块类别</strong> —这是定义培训、验证和测试步骤功能的地方。在阶跃函数中计算模型损耗和精度。这里还定义了优化器和调度器——可以定义多个优化器和调度器，然后根据一些标准在阶跃函数中使用/切换。对于这个简单的例子，我们只使用一个Adam优化器和OneCycleLR调度器。</li><li id="e3c8" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj kg jr js jt bi translated"><strong class="io hj">定义训练器参数</strong> —所有必需的训练器参数和训练器回调都在这里定义。我们定义了3种不同的回调——提前停止、学习速率监控和检查点。最新的Pytorch Lightning更新允许使用. yaml文件定义参数，而不是使用argparse来定义参数。yaml文件可以作为一个参数提供给python。py文件。这样，教练参数可以与训练代码分开维护。因为我们使用Colab笔记本进行演示，所以我们坚持使用argparse方法。</li><li id="8556" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj kg jr js jt bi translated"><strong class="io hj">训练模型</strong> —这是使用Trainer.fit()方法完成的。可以在训练器参数中定义一个分析器，以提供有关训练运行时间的更多信息。</li><li id="f12f" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj kg jr js jt bi translated"><strong class="io hj">评估模型性能</strong> —二元或多类分类是训练语言模型最容易的下游任务之一。您可以看到，在IMDB测试数据集上，仅一个历元之后，我们就获得了超过92.6%的准确率——使用Bert-base模型而不是DistilBert模型将进一步提高准确率。IMDB分类上的<a class="ae jk" href="https://paperswithcode.com/sota/sentiment-analysis-on-imdb" rel="noopener ugc nofollow" target="_blank"> SOTA </a>在2019年达到了97.4%，但SOTA使用了额外的训练数据。即使是准确率为92.8%的DistilBert也使用了额外的训练数据。使用额外的训练数据，Bert-Large获得了95.49%的准确率。</li><li id="1f94" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj kg jr js jt bi translated"><strong class="io hj">对训练好的模型运行推理</strong> —使用predict方法向模型发送一个示例批处理文本，以从训练好的模型获得预测。这可以用于构建ML推理管道。</li><li id="a2e0" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj kg jr js jt bi translated"><strong class="io hj"> TensorBoard日志数据</strong> —这将在Colab笔记本中打开TensorBoard，让您查看各种TensorBoard日志。Pytorch Lightning日志默认为TensorBoard，这可以使用Logger回调来更改。</li></ul><p id="b777" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，我们将在本系列的第3部分的<a class="ae jk" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-ner-on-conll-data-part-fe1512ae4183" rel="noopener">中了解令牌分类或命名实体识别(NER)任务培训。</a></p></div></div>    
</body>
</html>