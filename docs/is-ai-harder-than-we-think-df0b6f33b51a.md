# AI 比我们想象的难吗？

> 原文：<https://medium.com/codex/is-ai-harder-than-we-think-df0b6f33b51a?source=collection_archive---------18----------------------->

了解当前人工智能面临的挑战及其在现实世界中的应用。

即使有了所有看似人工智能的突破，机器人、聊天机器人、自动驾驶汽车等长期承诺的技术最终变得更加困难，并被推迟到未来。我们大多数人都对人工智能的总体进展感到困惑，我们做得好还是比预期落后？

最近发表的论文《为什么 AI 比我们想象的要难？在巩固当前人工智能发展的挑战方面做得很好。许多专家声称，对人工智能技术的过度乐观导致了乐观预测的“人工智能春天”和失望和资金减少的“人工智能冬天”的循环。从外部看，人们认为我们是通过阅读诸如“*人工智能算法解决蛋白质折叠*”或“人工智能击败国际象棋大师”之类的标题做到的，但人工智能专家认为，深度学习(用数据训练的神经网络模型)已经是一个很大的进步，但我们需要另一个突破，才能将其提升到一个新的水平。为了理解其中的原因，我们先来看看论文中提出的我们 AI 进展中的四个谬误。

![](img/409623376097c6be50137aef5dee936c.png)

由[安德里亚·德·森蒂斯峰](https://unsplash.com/@santesson89)在 [Unsplash](https://unsplash.com) 上拍摄

谬误 1:狭隘的智力和一般的智力是连续的

第一个受欢迎的里程碑，下棋计算机深蓝被视为我们迈向通用人工智能的第一步。随着道路上许多这样的里程碑，我们认为我们正在慢慢改进，以建立我们无所不能的通用智能机器。但这种想法与事实相去甚远。事实上，我们不知道如何制造一台万能的机器。但是我们可以建立模型来完成比人类表现更好的特定任务。我们可以做诸如识别图像中的物体(猫、狗)、情感预测等任务。但是这些模型有局限性，例如，如果你试图理解情感模型预测一个句子在情感上是积极的原因，它不能理解其中的**因果**。

**谬误 2:容易的事容易，难的事难**

人类可以在拥挤的人行道上行走，避开其他人，但教机器学习这一点，运动功能和思维过程是如此复杂，甚至不可能用深度学习术语来定义这个问题陈述。相反，机器擅长处理千兆字节的数据，提出见解，并掌握复杂的游戏，如围棋、国际象棋，但我们可能会发现这些是不可能做到的。

研究人员称赞 AlphaGo 在掌握围棋这种极具挑战性的游戏方面取得的胜利，但这提出了一个问题:向谁挑战？唉，人类。但如果你玩猜字谜游戏，我们必须理解表演技能、视觉技能和知识的工作理论，我们可以很容易地做到这一点，但它超出了今天机器能做的任何事情。

这就是所谓的*莫拉维克悖论*，它指出“让计算机在智力测试或玩跳棋时表现出成人水平的表现相对容易，而在感知和移动性方面，让它们拥有一岁儿童的技能却很难或不可能。”

**谬误 3:一厢情愿的记忆术的诱惑**

这个谬误指出了机器学习中使用的术语，如理解，如何影响我们对这些能力的一般程度的概念。例如，当一篇新论文在标准问答数据集上取得了最先进的结果，并超过了人类的表现，它就会成为头条新闻，如“新的人工智能模型在问答方面超过了人类的表现。”但问题是，该基准是对有限版本的问题回答的测试，并不衡量问题回答和自然语言理解的总体能力。

事实上，深度学习模型即使在一项任务中超越了人类的智能，也不会在人类层面的理解中理解这项任务。当面对与训练数据不同的情况时，他们表现出脆性，不可预测的错误，有时甚至在最轻微的方面。这是因为他们容易受到**捷径学习的影响，也就是说，他们通过制作数据的模式而不是真正理解数据来学习，问题是捷径不会导致良好的概括。**

![](img/6ff316d94dcd292401f4ce8c295949dc.png)

示例:使用对象检测模型，我们可以看到少量噪声如何完全脱离模型。但是对于人眼来说，这种变化甚至是不可察觉的(来自 [Goodfellow 等人](https://arxiv.org/pdf/1412.6572.pdf)的反面例子)

**谬误 4:智力全在大脑**

当前的人工智能研究是一种只针对大脑的方法，即它基于这样一种思想:所有的认知都发生在大脑中，我们的身体有必要的感官输入，我们可以复制大脑和任何认知功能，而没有身体的概念。研究人员一直认为，随着更多的硬件修复和更好的算法，我们正在朝着创造我们的一般智能取得进展，但也许我们还没有。

研究人员现在正在思考只有大脑的模型，并探索另一种方法。几十年来，许多认知科学家一直在争论身体在所有认知活动中的中心地位。这种研究具身认知的想法表明，“我们的思想是以感知、行动和情感为基础的，或者说与感知、行动和情感有着千丝万缕的联系，我们的大脑和身体共同工作来拥有认知。”这得到了多个学科的支持，如心理学和神经科学，其中研究表明控制认知的神经结构与控制感觉和运动系统的神经结构密切相关。

人工智能研究员斯陶特·拉塞尔(Staurt Russell)担心使用超级智能解决人类问题的可能结果:“如果超级智能气候控制系统的工作是将二氧化碳浓度恢复到工业化前的水平，认为解决方案是将人类人口减少到零，那会怎么样？…如果我们将错误的目标插入机器，而它比我们更聪明，我们就输了。”(类似于丹·布朗的《地狱》)。

这些想法假设 AI 系统可以是没有任何人类基本常识和情感的超级智能。但迄今为止的研究表明，人类智能似乎是一个强有力的集成系统，具有相互关联的属性，包括情感、文化、欲望、对世界的常识性理解，不知道这些属性是否可以分离。

# 人工智能使用中的挑战

人工智能在现实世界中的使用正在急剧上升，试图将当前的发展融入多个应用程序中。人工智能的实现伴随着它自己的挑战。在人工智能的使用中解决这些问题的关键在于纳入社会规范，并为每个规范确定精确的定义。这里的挑战是，隐私、道德等社会规范有些主观，可以根据情况而变化，但重要的是要明确定义您的使用/应用，并融入其中。

**数据隐私**

早先，敏感属性的匿名化被用于数据隐私，但是这种方法被发现容易受到链接攻击。今天，可区分隐私的概念被广泛采用。为了理解差分隐私，考虑这样一个例子，其中你的医疗记录是机器学习模型的输入。当您在两种情况下训练模型时:一种情况具有包括您的完整输入，另一种情况没有您的医疗记录输入，如果两种情况下的输出差异可以忽略不计，则认为该模型保留了差异隐私。这是因为第三方只能访问模型输出，如果他们不能衡量两种情况之间的差异，他们就不可能预测您的医疗输入。

这仍然是一个挑战，如果这些公司因为数据泄露而受到攻击，我们的个人信息将会暴露。因此，一个名为联合学习的新研究领域正在兴起，在这个领域中，你的数据甚至不会离开你的设备。它的工作方式如下:当前的模型被下载到你的设备上，模型被你的数据改进，汇总的变化被发送回服务器。

**公平**

与数据隐私不同，公平没有单一的定义。关于所使用的敏感属性，有不同的公平概念。

> 为什么机器学习一开始就是不公平的？

为了理解为什么，让我们考虑一个例子，其中一所大学想要使用 SAT 分数和高中 GPA 这两个属性从他们的申请中预测学生的成功。成功标准是客观设定的(例如，毕业时 gpa 高于 3 的学生),符合该标准的学生被标记为积极。我们试图用大学录取学生的 GPA 和 SAT 成绩来预测成功与否。

![](img/38c5db411f5726801355292d2ac565c8.png)![](img/2b285aacf01657471dbc2b7b38d1fdc3.png)

图 a、b

![](img/a7d879a95f023d3df5c235d08108c183.png)![](img/a3286a35efc581ae0e8ed277187d14c9.png)

图 c，d. Ref: [亚马逊对话](https://www.youtube.com/watch?v=I1y-I8-BMds&list=PL2yQDdvlhXf9pBU8bAO_EseecE4_jaYZb&index=28)

考虑一下图 a 中的学生，结果和预期的一样，大量的阳性结果在高 sat 和高 GPA 范围内，有一些异常值。对该数据训练线性分类器将产生红线分隔符。现在，考虑图 b 中用蓝色显示的另一组学生。与黄色相比，这些蓝色学生的 SAT 分数较低。一种可能性是，黄种学生有更多的 SAT 辅导资源，并通过多次重考来获得更高的 SAT 分数。对蓝色数据训练线性分类器将产生蓝线分隔符。

现在，当您在图 c 中的整个人口上训练模型时，边界将类似于只有黄色人口的边界。这个模型有一个错误拒绝率，最有害的一种 100%的不公平，对于 blue 来说可以看作是不公平的。那么，这个怎么解决呢？如果我们考虑黄色和蓝色的独立模型，我们会得到更好的结果，但如果这两个人口统计数据被任何敏感属性如种族分开，这将是非法的。

那么如何让它变得更好呢？

我们试图通过添加特殊的约束来降低错误拒绝率，然后将边界放得更低以包括蓝色阳性。但是你也会添加一些黄色负片，影响整体的准确性。这种公平性和准确性之间的权衡是这一领域的普遍趋势。

**可解释性&偏差**

数据在制造 AI 系统中起着重要的作用。这些来自人类的数据和我们一样有偏差。我们没有合适的方法来识别模型从数据集学习到的偏见。即使我们学习了这些，要防止模型学习任何偏差也是一个挑战。当前的解决方案对性别、年龄、种族等敏感属性进行建模，并添加特定的约束条件，以优化并最小化围绕这些属性的偏差。此外，在工业应用中，尽可能提高数据质量是一个普遍的共识，因为它可以提供比模型交换更高的增益。

**道德问题**

考虑一下著名的悖论，决策中的电车难题是“如果你不动杠杆，你救了一个人，但杀死了五个人，但如果你移动杠杆，只有一个人被杀死，五个人得救。”这是一个非常复杂的场景，当你移动控制杆时，你会被认为是谋杀了一个人吗？当人工智能决策涉及人类在自动驾驶、医疗保健、司法系统等应用中时，这种伦理问题就出现了。

![](img/e8616b2dcbff62645b6db22397af54c1.png)

电车困境。考虑杠杆处的人应该做什么？如果是人工智能呢？

如果一辆自动驾驶汽车发生事故，谁该负责？车里的人，算法设计师还是汽车制造商？这些关于**责任**的问题也需要解决。人工智能的这些问题阻止我们完全控制人工智能系统。最终，这是一个信任赤字的问题当模型不能解释为什么做出那个决定时，我们怎么能让模型做出一个包含一定风险的决定呢？

**引人深思:**

到现在，每个人都明白，像终结者这样的情况离现实相当遥远。问题是，我们不知道意识在我们体内是如何工作的，更不用说将其纳入 AI 了。人们提出这个问题，就好像我们正在走向*星际旅行*的情形(人类屈从于科学和探索的充满希望的未来)或*黑客帝国*的情形(令人沮丧的未来，机器统治世界，人类被喂入虚拟现实，而我们甚至没有意识到这不是现实)。你怎么想呢?

**来源:** 论文:[为什么 AI 比我们想象的要难？](https://arxiv.org/abs/2104.12871)
亚马逊谈:[伦理算法](https://www.youtube.com/watch?v=I1y-I8-BMds&list=PL2yQDdvlhXf9pBU8bAO_EseecE4_jaYZb&index=28)

# AI # ML #人工智能