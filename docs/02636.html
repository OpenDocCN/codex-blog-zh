<html>
<head>
<title>Real-Time Neural Style Transfer in Quake3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">地震中的实时神经类型转移3</h1>
<blockquote>原文：<a href="https://medium.com/codex/real-time-neural-style-transfer-in-quake3-71cd5f6c3e4?source=collection_archive---------16-----------------------#2021-07-29">https://medium.com/codex/real-time-neural-style-transfer-in-quake3-71cd5f6c3e4?source=collection_archive---------16-----------------------#2021-07-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/5205d8c309c7d35e9e9671699fc09da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*eFYWt2IfzT6_-Qbnx7nERg.png"/></div></figure><p id="f491" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本文中，我将演示如何使用Ubuntu操作系统在开源游戏ioQuake3上设置实时神经风格传输(NST)管道。这取决于你有多少计算能力，但如果我们这样说，我们可以做一个关于单个RTX 2060的幻灯片。</p><p id="e511" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们需要做的第一件事是按需导出屏幕缓冲区，幸运的是，这对于使用OpenGL渲染的开源视频游戏来说相对简单。我们将使用<a class="ae jk" href="https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/glReadPixels.xhtml" rel="noopener ugc nofollow" target="_blank"> glReadPixels() </a>以极快的速度导出JPEGs，然后使用python中的Tensorflow Keras来设置一个守护程序，该守护程序将读取这些导出的屏幕缓冲区，并尽可能快地将它们转换为NST帧。</p><p id="ca98" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先是克隆ioQuake3存储库，将目录更改为它，安装SDL2 Dev，然后像这样编译项目；</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="86b8" class="ju jv hi jq b fi jw jx l jy jz">git clone <a class="ae jk" href="https://github.com/ioquake/ioq3" rel="noopener ugc nofollow" target="_blank">https://github.com/ioquake/ioq3</a><br/>cd ioq3<br/>sudo apt install libsdl2-dev<br/>make</span></pre><p id="60a6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">希望你已经顺利通过这个过程，没有任何错误，如果没有，你可以加入我们的官方<a class="ae jk" href="https://discord.com/invite/Gt4zqNcU" rel="noopener ugc nofollow" target="_blank"> ioQuake3 discord </a>并寻求一些帮助。</p><p id="6c8e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在的下一步是修改ioQuake3源代码，按需导出JPEG屏幕缓冲区。为此，您需要导航到<code class="du ka kb kc jq b">ioq3/code/renderergl2/tr_backend.c</code>这是因为ioQuake3默认使用GL2渲染器。在适当的文本编辑器中打开这个文件，例如<a class="ae jk" href="https://code.visualstudio.com/" rel="noopener ugc nofollow" target="_blank"> Visual Studio Code </a>或<a class="ae jk" href="https://help.gnome.org/users/gedit/stable/" rel="noopener ugc nofollow" target="_blank"> Gedit </a>，这是Ubuntu正式发行版预装的。</p><p id="61fe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这个文件中，我们需要做的是导航到处理交换屏幕缓冲区的那段代码，这段代码叫做<code class="du ka kb kc jq b">RB_SwapBuffers</code>，目前，在2021年7月29日，这段代码从<a class="ae jk" href="https://github.com/ioquake/ioq3/blob/main/code/renderergl2/tr_backend.c#L1346" rel="noopener ugc nofollow" target="_blank">行1346 </a>开始。你需要在纹理交换测试后添加两行代码，就像这样；</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kd"><img src="../Images/c964258046fd38584e3353d37f8600ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8hRkQvaVvYEz75e-Djt3rA.png"/></div></div></figure><p id="efc1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为我们正在使用C函数<a class="ae jk" href="https://linux.die.net/man/2/access" rel="noopener ugc nofollow" target="_blank"> access() </a>，所以您还需要在文件的开头第<a class="ae jk" href="https://github.com/ioquake/ioq3/blob/main/code/renderergl2/tr_backend.c#L21" rel="noopener ugc nofollow" target="_blank">行第21 </a>之后包含<a class="ae jk" href="https://en.wikipedia.org/wiki/Unistd.h" rel="noopener ugc nofollow" target="_blank"> unistd头文件</a>，如下所示:</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/a8e82d155c86fc78854e4003e303e663.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*aSsvKqocga3qmV-QZe_CFg.png"/></div></figure><p id="efce" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以回到我们添加的两行代码，让我们解释一下它是如何工作的；</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kj"><img src="../Images/4f4d666b4023214ecb4bde9f91f623c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UB16M6NSoSNHZ7m-Rf9yQA.png"/></div></div></figure><p id="639b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们在这里做的是一个简单的检查文件是否存在，如果文件不存在，我们将截图导出为JPEG格式。这允许我们在每次删除旧的屏幕截图时触发ioQuake3游戏生成新的屏幕截图！尽管记住<code class="du ka kb kc jq b">RB_TakeScreenshotJPEG()</code>函数会根据载入的mod保存到一个相对目录，但默认情况下，在一个没有mod的普通Quake3游戏中，这个目录将是你主文件夹中的<code class="du ka kb kc jq b">ioq3/baseq3</code>目录。因此，您需要将<code class="du ka kb kc jq b">access()</code>功能中的用户名“v”修改为您自己的用户名。</p><p id="e042" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du ka kb kc jq b">RB_TakeScreenshotJPEG()</code>函数使用<code class="du ka kb kc jq b">glReadPixels()</code>函数读取屏幕缓冲区，正如你在<code class="du ka kb kc jq b">RB_ReadPixels()</code>函数中看到的；</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kk"><img src="../Images/026235db9755a49bb8f3106d0aec106e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YmI_sSd5sVjzUtW5-xGHtg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">请记住，如果使用glReadPixels()将原始缓冲区导出为训练数据的GL_FLOAT，如果您不打算对缓冲区进行均值/样本标准化，则数据已经为您进行了0–1标准化。</figcaption></figure><p id="e756" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你还需要做一件事，我们现在还不能从<code class="du ka kb kc jq b">tr_backend.c</code>文件中访问<code class="du ka kb kc jq b">RB_TakeScreenshotJPEG()</code>函数。我们将需要在适当的头文件中暴露它，指向<code class="du ka kb kc jq b">ioq3/code/renderergl2/tr_local.h</code>，并在<a class="ae jk" href="https://github.com/ioquake/ioq3/blob/main/code/renderergl2/tr_local.h#L1974" rel="noopener ugc nofollow" target="_blank">行1974 </a>之后弹出我们的函数原型，就像这样；</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/775ded2cd26e69620d53553e7d0a78a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*mCqNSyVM5hFnauaVBqZu_g.png"/></div></figure><p id="c68f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">太棒了。我们完成了，现在您可以在控制台中键入<code class="du ka kb kc jq b">make</code>并编译我们的更改。</p><p id="aa66" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在是创建Python守护进程的时候了，它将尽可能快地运行<em class="kq">(在1毫秒的无限循环上)</em>读取输出jpg，对其运行NST神经网络，然后将结果输出到我们的tmp文件夹，作为<code class="du ka kb kc jq b">/tmp/newbuff.bmp</code>或您想要的任何图像类型。为此，我们将使用<a class="ae jk" href="https://keras.io/examples/generative/neural_style_transfer/" rel="noopener ugc nofollow" target="_blank"> Keras.io代码示例页面</a>中的NST神经网络。这段代码只需要最小的修改，这样无限循环只做它需要做的事情，这样算法就可以在ioQuake3游戏的输出截图上尽可能快地实时运行。</p><p id="025d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们最终希望我们的循环看起来像这样；</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kr"><img src="../Images/444d83c694fa28a3ed455ce77731363b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFNhN-FoIPJqvL15bbt5Fg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">完整的脚本文件可从这里获得:<a class="ae jk" href="https://pastebin.com/WF6aasEz" rel="noopener ugc nofollow" target="_blank">https://pastebin.com/WF6aasEz</a></figcaption></figure><p id="cb3f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">最后，我们将NST修改后的图像作为<code class="du ka kb kc jq b">/tmp/newbuff.bmp</code>输出到tmp目录，并移除/删除触发ioQuake3游戏输出新图像的旧截图，然后重复该过程。默认的Ubuntu图像查看器是gnome之眼，但是在Xubuntu上我使用Ristretto图像查看器，它会自动实时加载对图像文件的修改，基本上允许你在游戏窗口旁边有第二个屏幕。侏儒之眼也应该这样做，但包住它你不知道该怎么办。</p><p id="fd08" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">假设您已经为自己节省了一点时间，并从<a class="ae jk" href="https://pastebin.com/WF6aasEz" rel="noopener ugc nofollow" target="_blank">这里</a>获取了修改前的脚本，那么您只需要修改开头附近的配置变量:</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/6de4cf46ba7d711a43d937eb3ddf72fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*qNTkY0MqNqFe0dQ0z-SETQ.png"/></div></figure><p id="e2ca" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您需要像以前一样用您的用户主目录更新<code class="du ka kb kc jq b">inim</code>路径，方法是用您的用户名替换“v”。你需要输入一个你想要用作<code class="du ka kb kc jq b">stim</code>变量的样式图像的路径，它不一定是JPEG。<code class="du ka kb kc jq b">img_nrows</code>变量将改变你的输出图像的大小，<code class="du ka kb kc jq b">iterations</code>变量定义NST神经网络<em class="kq">最终结果的“深度”</em>(我个人倾向于使用6次迭代，但如果你能以10–33次迭代运行，那就更好了】。这也是决定网络输出新图像速度的主要因素，数字越高，输出质量越好，但生成图像所需的时间越长。</p><p id="fc42" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在你需要做的就是运行ioQuake3游戏，在一个单独的控制台窗口中运行Python NST脚本，然后在你选择的图像查看器中打开<code class="du ka kb kc jq b">/tmp/newbuff.bmp</code>文件，如果你已经正确地完成了所有这些，你应该会得到这样的结果；</p><figure class="jl jm jn jo fd ij"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">这是我的RTX 2060 OC在6次迭代中的渲染速度，最后稍微高一点。</figcaption></figure><p id="06eb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">基本优化注释；你可能想要使用<a class="ae jk" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img" rel="noopener ugc nofollow" target="_blank">TF . keras . preprocessing . image . load _ img</a>而不是<a class="ae jk" href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file" rel="noopener ugc nofollow" target="_blank"> tf.keras.utils.get_file </a>，因为后者在技术上是用于从网络下载图像的，所以不使用该函数在本地加载文件可能会有一些性能优势。此外，考虑从ioQuake3游戏中导出图像的格式，将它们作为导入python，然后将新图像作为输出。保存和加载不同类型的图像文件都有自己的开销。如果您可以将原始像素从ioQuake3导出为GL_FLOAT，并将其作为numpy数组加载到Python中，您将节省多余的JPEG save &amp;加载。</p><p id="d491" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你不想从视频游戏中抓取屏幕缓冲区，比如你可能想抓取一个闭源游戏的屏幕缓冲区，那么你可以看看<a class="ae jk" href="https://stackoverflow.com/a/38298349" rel="noopener ugc nofollow" target="_blank">这个stackoverflow答案</a>，它展示了在Linux上抓取整个屏幕截图的最快方法<em class="kq">(记住这段代码最慢的部分是</em> <code class="du ka kb kc jq b"><em class="kq">savepng()</em></code> <em class="kq">函数)</em>我的基准测试显示这段代码在没有对抓取的图像进行处理的情况下以3000+FPS的速度运行，或者用<code class="du ka kb kc jq b">savepng()</code>以6 FPS的速度运行<a class="ae jk" href="https://pastebin.com/DvDHa4gK" rel="noopener ugc nofollow" target="_blank"> <em class="kq">(此处检查基准)</em> </a></p><p id="539b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您还可以尝试<a class="ae jk" href="https://www.tensorflow.org/lite/convert" rel="noopener ugc nofollow" target="_blank">将这个Keras模型转换为TFLite模型</a>，并使用<a class="ae jk" href="https://github.com/tensorflow/tflite-micro" rel="noopener ugc nofollow" target="_blank"> TFLite Micro </a>将TFLite直接集成到ioQuake3源代码中。我不确定那会有多好，但也许值得一查。</p><p id="79da" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">编辑:这里有一个是实时运行的。</p><figure class="jl jm jn jo fd ij"><div class="bz dy l di"><div class="kt ku l"/></div></figure><figure class="jl jm jn jo fd ij"><div class="bz dy l di"><div class="kv ku l"/></div></figure><figure class="jl jm jn jo fd ij"><div class="bz dy l di"><div class="kt ku l"/></div></figure><figure class="jl jm jn jo fd ij"><div class="bz dy l di"><div class="kt ku l"/></div></figure></div></div>    
</body>
</html>