<html>
<head>
<title>ReLU (Rectified Linear Unit) linear or non-linear, that is the question…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性或非线性，这是个问题…</h1>
<blockquote>原文：<a href="https://medium.com/codex/relu-rectified-linear-unit-linear-or-non-linear-that-is-the-question-2b18f419464?source=collection_archive---------3-----------------------#2021-05-21">https://medium.com/codex/relu-rectified-linear-unit-linear-or-non-linear-that-is-the-question-2b18f419464?source=collection_archive---------3-----------------------#2021-05-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/88604cfe380a4151311d22898bbf8796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cLcmvFW41JxPfIRT"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@sajad_sqs9966b?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">萨贾德·诺里</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="10b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">激活函数是神经网络的组成部分。它用于激活各层中的神经元或节点。隐藏层中使用的激活函数主要控制模块的学习。根据激活函数的输出值，在反向传播中进一步改进权重和偏差。输出激活函数决定了模型或网络将产生哪种类型的输出，无论是二分类、多分类还是多标签分类，它完全取决于我们在输出层选择的激活函数。</p><h2 id="efdd" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak"> <em class="ko">线性激活功能的问题</em> </strong></h2><p id="5b72" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">在回归问题的情况下，线性激活函数很有帮助，在回归问题中，模型必须预测真实值(不仅仅是0/1 ),例如预测比赛得分或股票价格。但在卷积神经网络或多层感知器网络中，这是行不通的。在这个网络中，线性激活函数或多或少是无用的，因为节点的输出将是输入的线性函数。该功能不会帮助网络从数据中学习。因此，Sigmoid、tanh函数等非线性函数是更好的选择。</p><h2 id="0d3f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak"> <em class="ko">但是乙状结肠就足够好了吗？</em>T11】</strong></h2><p id="c78f" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">通常使用深层网络而不是浅层网络来训练模型。在任何网络中使用激活函数，并且在反向传播期间，计算节点的导数。现在对于一个更深的网络，一个名为“消失梯度”的问题可能会到来。类似sigmoid的激活函数将输入从大范围挤压到(0，1)范围。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="a9b0" class="jt ju hi kz b fi ld le l lf lg">                          <em class="lh">f(x)=1/(1+e^(-x))</em></span></pre><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es li"><img src="../Images/df38b41a7bc5aa75e7a577366f625fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*5Tymxs56B3_PtzNaLa-XVQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图1:Sigmoid函数及其导数</figcaption></figure><p id="45ac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，输入的巨大变化也会导致函数输出的微小变化。因此，梯度下降将接近于0。对于浅层网络，这可能不是一个问题，因为我们没有这么多的层。但是对于深度网络，当我们在反向传播期间迭代各层并将激活函数的输出乘以权重时，梯度收敛到0。</p><h2 id="453d" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak"><em class="ko">ReLU优于乙状结肠</em> </strong></h2><p id="77ea" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">对于ReLU(校正线性单位),曲线是弯曲的，而不是弯曲的，因此函数弯曲的地方不定义导数。这是一个问题，因为梯度下降需要所有点的导数，并且根据这些值，我们必须估计权重和偏差以进一步改进模型/网络。</p><h2 id="9ce8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak"> <em class="ko">在ReLU </em>中修改</strong></h2><p id="adc2" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">虽然ReLU可以解决渐变消失的问题，但它提出了另一个问题，称为死ReLU。这是因为ReLU函数的不连续性。值&gt; 0时ReLU的导数为1，值&lt;0 but the derivative at value=0 is unknown. Hence, a modified function is nowadays used which is known as LReLU. The change in the equation is we will use a value close to 0 instead of 0.</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="ee3f" class="jt ju hi kz b fi ld le l lf lg"><strong class="kz hj"><em class="lh">                           </em></strong><em class="lh">f(x)=max(0.01*x,x)</em></span></pre><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es li"><img src="../Images/dabf745ff0afc04b21ffcdbfa9f62aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*zCvT4JZ9IvmxOmya-4yXYg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">Fig(2): The difference between ReLU &amp; LReLU</figcaption></figure><h2 id="bf13" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">T5时导数为0结论T7】</strong></h2><p id="62ee" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">虽然在很多地方我们看到ReLU被用作线性函数，但实际上不是。线性函数允许您将特征平面分成两半，但是ReLU的非线性可以在特征平面中创建任意形状。</p><h2 id="0085" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak"> <em class="ko">参考文献</em> </strong></h2><p id="6802" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated"><a class="ae iu" href="https://datascience.stackexchange.com/questions/26475/why-is-relu-used-as-an-activation-function" rel="noopener ugc nofollow" target="_blank">1 . https://data science . stack exchange . com/questions/26475/why-is-ReLU-used-as-an-activation-function</a></p><p id="8948" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/#:~:text=Linear%20Output%20Activation%20Function,instead%20returns%20the%20value%20directly" rel="noopener ugc nofollow" target="_blank"> 2。https://machine learning mastery . com/choose-an-activation-function-for-deep-learning/#:~:text = Linear % 20 output % 20 activation % 20 function，而不是% 20直接返回% 20 the % 20 value % 20</a>。</p><p id="070e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484" rel="noopener" target="_blank"> 3。https://towardsdatascience . com/the-vanishing-gradient-problem-69 BF 08 b 15484</a></p></div></div>    
</body>
</html>