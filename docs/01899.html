<html>
<head>
<title>The Mathematical background of Lasso and Ridge Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">套索和岭回归的数学背景</h1>
<blockquote>原文：<a href="https://medium.com/codex/mathematical-background-of-lasso-and-ridge-regression-23b74737c817?source=collection_archive---------0-----------------------#2021-06-13">https://medium.com/codex/mathematical-background-of-lasso-and-ridge-regression-23b74737c817?source=collection_archive---------0-----------------------#2021-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7738bba7d414aedd4f3dd4f703cf0fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zrEZS-KHpr9T7Z2MPXJl3A.jpeg"/></div></div></figure><p id="a416" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在某些情况下，简单的模型可能无法概括数据，当在训练数据上实现非常复杂的模型时，它会过度拟合。正则化是一种用于解决模型过拟合问题的技术。正则化只不过是将系数项(β)添加到成本函数中，从而使这些项受到惩罚并且数量很小。这在本质上有助于捕捉数据中的趋势，同时通过不让模型变得太复杂来防止过度拟合。(即在训练数据上表现良好的同时尽可能简单的模型)。通过正则化，人们试图在保持模型简单和不使其过于天真之间取得微妙的平衡。</p><p id="0552" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">两种最常见的正则化技术:</p><p id="b6e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">①L1或拉索回归</p><p id="816e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(2) L2或岭回归</p><h2 id="f824" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated"><strong class="ak">线性回归:</strong></h2><p id="8135" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">在线性回归设置中，给我们yᵢ)}ᴺᵢ₌₁{(xᵢ的n个样本，其中每个xᵢ = (xᵢ₁,…,xᵢₚ)是特征的p维向量，每个yᵢ ∈ ℝ是相关的响应变量。我们的目标是使用特征的线性组合来近似响应变量yᵢ。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/5ddccd4a754b60b52a5438e45b6cc7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*8uWfAqG_Xz-dWntsq4m0cw.png"/></div></figure><p id="be9a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该模型由回归权重向量β = (β₁,…,βₚ) ∈ ℝᵖ和截距(或偏差)项β₀ ∈ ℝ来参数化</p><p id="07af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了评估最佳拟合线，需要优化相应的成本函数(或损失函数)。例如，考虑均方误差(MSE)作为成本函数</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/142405ac0f8834f608c7c94fb7684268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*xwn1KzfgzyfADc6gPHBNSg.png"/></div></figure><p id="0ff8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上述成本函数的系数由以下封闭形式的解确定。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/fd65f94fde060b56c84809d6921eaa21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*eDfS49_xRS_EyiYIGM1N4A.png"/></div></figure><h2 id="776d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">岭回归或L2回归:</h2><p id="0583" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">在岭回归中，“系数平方和”的附加项被添加到成本函数中[2]。岭回归本质上是试图最小化误差项的总和以及我们试图确定的系数的平方和。系数的平方和被称为“正则项”,它也具有由λ表示的正则化系数。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/3acef2c1afa89598cd051563e060fce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*kZL56tkhW6WcGXb75JyeuA.png"/></div></figure><p id="19c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上述成本函数的系数值由以下封闭形式的解确定。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/1e9ffa41db8a33e40c8c3a5d70ef0b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*cDeu1idB3xpZ1EG3K9db7Q.png"/></div></figure><h2 id="f7dc" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">拉索或L1回归:</h2><p id="f542" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">在lasso(最小绝对收缩选择算子)回归中，将“系数绝对值之和”的正则化项添加到成本函数中[2]。Lasso将冗余变量的系数修剪为零，因此也直接执行特征选择。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/698de3ddd8131a330cf01fdaedfd6653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*Zcbx-devZbkbD5mgnIWsUg.png"/></div></figure><p id="9955" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，一般形式的脊和套索的成本函数由下式给出:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/ee1bbc751a5f4d78891609caa80e13de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*Jsvje9T2eQinF4JSDrHXeQ.png"/></div></figure><p id="f9ab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里E(β)是误差项，R(β)是正则项。如果我们使用λ的高值，我们将会在控制模型的复杂性上付出很多额外的代价。它根本不允许系数太大。如果我们允许λ接近于零，则意味着没有正则化，并且很有可能过度拟合。这里λ是目标函数中的超参数，我们将其最小化用于正则化回归。</p><p id="2c15" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Lasso将冗余变量的系数修剪为零，因此也直接执行特征选择。另一方面，Ridge将系数降低到任意低值，尽管不是零。</p><p id="d7b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在lasso中，我们用系数的绝对值之和作为正则项，这使得事情有点不方便，绝对值在为零的情况下是不可微的。因此，我们不再像在岭回归[4]中那样得到任何封闭形式的解</p><p id="4f3b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们了解Lasso的正则化项如何有助于特征选择。套索回归产生稀疏解-这意味着许多模型系数自动精确为零，βⱼ = 0。</p><p id="9cf1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果β*是我们最终得到的最佳模型，则为:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/b61e7c2571787ea6d101a5d7218fc482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*pPgEyJ5fV-5vIs5d81rJhA.png"/></div></figure><p id="342a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里argmin计算表达式E(β)+ λR(β)最小的β值。稀疏度(β*)随着λ的增加而增加，其中模型的稀疏度(β*)由β*中恰好等于零的参数的数量来定义。在现实世界的问题中，我们经常有大量的特征，但是我们希望模型能够只选取最有用的特征(因为我们不想要不必要的复杂模型)。由于套索正则化产生稀疏解，它会自动执行要素选择。</p><h2 id="7b71" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated"><strong class="ak">为什么Lasso提供了稀疏解，而Ridge没有？</strong></h2><p id="e76c" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">为了理解为什么lasso最终得到了稀疏解，而ridge没有，我们需要理解这两个损失函数的性质。如果我们能把这些功能形象化，这将会很方便。如图1所示，函数f(x，y)以各种方式可视化。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/6f995bfb8604709d81d3ff7bb793ccb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*VVIqOB7nc2LG5sZFz5YWXA.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">图1:功能的可视化</figcaption></figure><p id="990f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于一个给定为第三轴的函数f(x，y ),比方说Z用来绘制函数的输出值，一旦我们连接所有的值，我们就得到一个曲面。</p><p id="904d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还有另一种方法，使用下图所示的等高线来显示函数。例如，写出f(x，y) = x + y和x + y</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/8c639bba37b9d96626d0b3cdea56194e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVzLbWb5jAB9YbQ_Keoujg.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">图2:函数f(x + y)(左)和f(x + y)(右)的等高线</figcaption></figure><p id="29f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于等高线，我们连接函数值相同的所有点，最终得到该函数的等高线，这是该函数的可视化表示[图2]。没有两条等高线可以相交，这意味着一个函数不能有两个不同的x和y的值，但是两条等高线可以相切。</p><p id="47cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图3描述了只有两个参数时的套索(左)和岭回归(右)。残差平方和具有椭圆轮廓，以完全最小二乘估计为中心。套索的正则化区域是菱形|β₁|+|β₂|≤ t，而脊的正则化区域是圆盘|β₁ |+|β₂ |≤ t。这两种方法都找到椭圆轮廓碰到正则化区域的第一个点。与圆盘不同，钻石有角；如果解出现在拐角处，那么它的单参数βⱼ等于零。当p&gt;2(大于2个特征)时，菱形变成菱形，并有许多角、平边和面；估计参数为零的机会更多。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/795a31c18218ef6c7fc163f3be206b7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4BqFCKRXiW8fqODs-1G-Lw.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">图3:<em class="lg">lasso(左)和ridge regression(右)的估计图。显示的是误差和正则化函数的等高线。实心蓝色区域是正则化区域，而红色椭圆是误差函数</em>的轮廓</figcaption></figure><p id="700c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以推广岭和套索，并将其视为贝叶斯估计。考虑标准</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/571c2770e2ed5aa80c0bd1448d1e401e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*a08_pmBoKGA2Tcjp4HNRrQ.png"/></div></div></figure><p id="e317" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中q=1对应于套索，而q=2对应于岭回归</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es li"><img src="../Images/c074bdbc544c33e0e3552ab6e2da12eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*SPCcOfqYuAAkEkDdNpFDJw.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated"><em class="lg">图4:对于给定的q值</em>，恒定值的等高线(来自方程9)</figcaption></figure><h2 id="34ab" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">参考资料:</h2><p id="a640" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">[1]j . Friedman、t . Hastie和r . TiB shirani(2001年)。《统计学习的要素》(第1卷，第10期)。纽约:统计学中的斯普林格系列。</p></div></div>    
</body>
</html>