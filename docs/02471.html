<html>
<head>
<title>Review — MUNIT: Multimodal Unsupervised Image-to-Image Translation (GAN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述— MUNIT:多模态无监督图像到图像翻译(GAN)</h1>
<blockquote>原文：<a href="https://medium.com/codex/review-munit-multimodal-unsupervised-image-to-image-translation-gan-10e2c08a1b6e?source=collection_archive---------4-----------------------#2021-07-22">https://medium.com/codex/review-munit-multimodal-unsupervised-image-to-image-translation-gan-10e2c08a1b6e?source=collection_archive---------4-----------------------#2021-07-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="cdee" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用MUNIT，从单一图像生成多样式图像</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/def9ae606ab6758f61fe3b58a1da487d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1RKmG_MrcxT2GJj_7Go4ZA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">使用MUNIT进行动物图像翻译</strong></figcaption></figure><p id="fa9e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi kk translated"><span class="l kl km kn bm ko kp kq kr ks di">在</span>这个故事中，回顾了由康乃尔大学和英伟达合作的<strong class="jq hj">多模态无监督图像到图像翻译</strong>、【MUNIT】。在本文中:</p><ul class=""><li id="78dd" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">在MUNIT中，假设<strong class="jq hj">图像表示</strong>可以分解成一个<strong class="jq hj">内容代码</strong>和一个<strong class="jq hj">样式代码</strong>，其中内容代码<strong class="jq hj">是域不变的，样式代码</strong>捕获特定于域的属性。</li><li id="5bf7" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">为了将图像翻译到另一个域，其内容代码<strong class="jq hj">与从目标域的样式空间采样的随机样式代码</strong>重新组合。</li><li id="d712" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">最后，MUNIT允许用户通过提供示例样式图像来控制翻译输出的样式。</li></ul><p id="f8fa" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是一篇发表在<strong class="jq hj"> 2018 ECCV </strong>的论文，引用超过<strong class="jq hj"> 1100次</strong>。(<a class="lh li ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----10e2c08a1b6e--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="4a3c" class="lq lr hi bd jn ls lt lu lv lw lx ly lz io ma ip mb ir mc is md iu me iv mf mg bi translated">概述</h1><ol class=""><li id="6b10" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj mm kz la lb bi translated"><strong class="jq hj">简介&amp;假设</strong></li><li id="069f" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj mm kz la lb bi translated"><strong class="jq hj">概述</strong></li><li id="968c" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj mm kz la lb bi translated"><strong class="jq hj">损失函数</strong></li><li id="1be7" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj mm kz la lb bi translated"><strong class="jq hj">理论分析</strong></li><li id="6408" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj mm kz la lb bi translated"><strong class="jq hj"> MUNIT:网络架构</strong></li><li id="5b25" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj mm kz la lb bi translated"><strong class="jq hj">实验结果</strong></li></ol></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="1f30" class="lq lr hi bd jn ls lt lu lv lw lx ly lz io ma ip mb ir mc is md iu me iv mf mg bi translated"><strong class="ak"> 1。简介&amp;假设</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mn"><img src="../Images/c62680429717b9b5bda3f2f3338eb637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*qNmgRGABzywtXT-jGlWpaA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn"> MUNIT:多模态无监督图像到图像翻译</strong></figcaption></figure><ul class=""><li id="0f54" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">假设<strong class="jq hj">图像的潜在空间可以分解为内容空间<em class="mo"> C </em>和风格空间<em class="mo"> S </em> </strong>，如上图所示。</li><li id="1b2c" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">然后进一步假设不同域中的<strong class="jq hj">图像共享一个共同的内容空间，但不共享风格空间。</strong></li><li id="110a" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><strong class="jq hj">为了将</strong>图像翻译到目标域，<strong class="jq hj">其内容代码与目标样式空间</strong>中的随机样式代码重新组合。</li></ul><blockquote class="mp mq mr"><p id="8f84" class="jo jp mo jq b jr js ij jt ju jv im jw ms jy jz ka mt kc kd ke mu kg kh ki kj hb bi translated">内容代码对翻译过程中应该保留的信息进行编码，而样式代码表示输入图像中不包含的剩余变化。</p></blockquote><ul class=""><li id="b938" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated"><strong class="jq hj">通过对不同的风格代码</strong>进行采样，我们的模型能够产生<strong class="jq hj">多样化和多模态的输出。</strong></li></ul></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="4e61" class="lq lr hi bd jn ls lt lu lv lw lx ly lz io ma ip mb ir mc is md iu me iv mf mg bi translated">2.<strong class="ak">概述</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mv"><img src="../Images/22ee13b8b077ce14b551bce6178ba928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*duPZ8m_irR-s9arUxxiGTw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">弹药:概述</strong></figcaption></figure><ul class=""><li id="76ac" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">翻译模型由一个<strong class="jq hj">编码器<em class="mo">Ei</em>T3<em class="mo">T5】和一个<strong class="jq hj">解码器<em class="mo"> Gi </em> </strong> <em class="mo"> </em>组成，用于每个<strong class="jq hj">域<em class="mo"> Xi </em> </strong> ( <em class="mo"> i </em> = 1，2)。</em></strong></li><li id="381f" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><strong class="jq hj"> (a) </strong>将每个自动编码器的潜码分解成一个<strong class="jq hj">内容码<em class="mo"> ci </em> </strong> <em class="mo"> </em>和一个<strong class="jq hj">风格码<em class="mo"> si </em> </strong>，其中:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/5266ef388329dd749bad958afc610e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*htEA66NOze4JPoJwouZmdg.png"/></div></figure><ul class=""><li id="90f8" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated"><strong class="jq hj"> (b)通过交换编码器-解码器对来执行图像到图像的转换</strong>。例如，为了<strong class="jq hj">将一个图像<em class="mo"> x </em> 1 ∈ <em class="mo"> X </em> 1平移到<em class="mo"> X </em> 2 </strong>，其<strong class="jq hj">内容潜码<em class="mo">c</em>1 =<em class="mo">Ec</em>1(<em class="mo">X</em>1)被提取</strong>并且一个<strong class="jq hj">样式潜码<em class="mo"> s </em> 2被从先前随机抽取</strong></li><li id="745e" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">然后用<strong class="jq hj"> <em class="mo"> G </em> 2产生最终输出图像<em class="mo">x</em>1→2</strong>=<em class="mo">G</em>2(<em class="mo">c</em>1，<em class="mo"> s </em> 2)。</li><li id="ff14" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">注意，尽管先验分布是单峰的，但是由于解码器的非线性，输出图像分布可以是多峰的。</li></ul></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="8e4a" class="lq lr hi bd jn ls lt lu lv lw lx ly lz io ma ip mb ir mc is md iu me iv mf mg bi translated">3.<strong class="ak">损失函数</strong></h1><h2 id="b6e3" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">3.1.双向重建损失</h2><ul class=""><li id="1f42" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">为了学习彼此相反的编码器和解码器对，我们使用目标函数来促进在<strong class="jq hj">图像→潜像→图像</strong>和<strong class="jq hj">潜像→图像→潜像</strong>两个方向上的重建:</li></ul><h2 id="432f" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">3.1.1.图像重建</h2><ul class=""><li id="14ae" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">给定<strong class="jq hj">从数据分布</strong>中采样的图像，我们应该能够在编码和解码之后<strong class="jq hj">重建它:</strong></li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/60422082166beb5fcfe833d00e83bb6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*y8X0cGO0cpiO2Pj63j_MRQ.png"/></div></figure><h2 id="f40a" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">3.1.2.潜在重建</h2><ul class=""><li id="1cdb" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">给定<strong class="jq hj">在翻译时从潜在分布</strong>中采样的潜在代码(样式和内容)，我们应该能够在解码和编码后<strong class="jq hj">重建它。</strong></li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/39282f797755154c39a0475fff81c9f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*J0ljdO0Y5t2V4B9lM1sb4Q.png"/></div></figure><ul class=""><li id="e040" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">其中<em class="mo"> q </em> ( <em class="mo"> s </em> 2)为先验<em class="mo"> N </em> (0，<em class="mo">I</em>)<em class="mo">p</em>(<em class="mo">c</em>1)由<em class="mo">c</em>1 =<em class="mo">Ec</em>1(<em class="mo">x</em>1)和<em class="mo"> x </em> 1给出</li><li id="8e1a" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">其他损失项<strong class="jq hj"><em class="mo">Lx</em>2 _<em class="mo">recon</em></strong>、<strong class="jq hj"><em class="mo">Lc</em>2 _<em class="mo">recon</em></strong>和<strong class="jq hj"><em class="mo">Ls</em>1 _<em class="mo">recon</em></strong>的定义方式类似。</li><li id="cdc7" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><strong class="jq hj"> L1重建损失</strong>被用作它<strong class="jq hj">鼓励清晰输出图像</strong>。</li><li id="c6e7" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><strong class="jq hj">风格重建损失<em class="mo"> Lsi_recon </em> </strong>对<strong class="jq hj">有影响，给定不同的风格码，鼓励不同的输出</strong>。</li><li id="fa38" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><strong class="jq hj">内容重建损失<em class="mo">Lci _ recon</em>T13】促使翻译后的图像<strong class="jq hj">保留输入图像的语义内容</strong>。</strong></li></ul><h2 id="e286" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">3.2.对抗性损失</h2><ul class=""><li id="1a32" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">GAN试图将翻译图像的分布与目标数据分布相匹配。<strong class="jq hj">由模型生成的图像应该与目标域中的真实图像没有区别。</strong></li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es no"><img src="../Images/f8154c44d8ac9d1a5e397bf520e93360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*HXKkWjZtcvpAMbblOkeHdw.png"/></div></figure><ul class=""><li id="14de" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">其中<em class="mo"> D </em> 2是试图在<em class="mo"> X </em> 2中区分翻译图像和真实图像的鉴别器。鉴频器<em class="mo"> D </em> 1和损耗<em class="mo"> Lx </em> 1_ <em class="mo"> GAN </em>定义类似。</li><li id="5483" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">这是<a class="ae nn" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">干</a>中相当标准的对抗性输。</li></ul><h2 id="6057" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">3.3.全损</h2><ul class=""><li id="3d80" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated"><strong class="jq hj">编码器、解码器和鉴别器被联合训练</strong>以优化最终目标，该目标是对抗损失和双向重建损失项的加权和<strong class="jq hj"/></li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/f93cb16cf393ef4c583c0c7005a471f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*iFZkmzM9U-CGr5G7IRiOog.png"/></div></figure><ul class=""><li id="fbd7" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">其中<em class="mo"> λx </em>、<em class="mo"> λc </em>、<em class="mo"> λs </em>是控制重构项重要性的权重。</li></ul></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="c57e" class="lq lr hi bd jn ls lt lu lv lw lx ly lz io ma ip mb ir mc is md iu me iv mf mg bi translated"><strong class="ak"> 4。理论分析</strong></h1><ul class=""><li id="fbb8" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">一些命题成立。</li></ul><h2 id="4a7b" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">4.1.命题1(当损耗最小化时发现优化的编码器和发生器)</h2><ul class=""><li id="5be4" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">最小化所提出的损失函数导致1)在编码和生成期间潜在分布的匹配，2)由我们的框架引起的两个联合图像分布的匹配，以及3)实施弱形式的循环一致性约束。</li><li id="cd7d" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">假设存在<em class="mo"> E </em> 1*，<em class="mo"> E </em> 2*，<em class="mo"> G </em> 1*，<em class="mo"> G </em> 2*，则:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/706603f75893550e3cf66ea386d2432e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*zUgSAq15F2tAOAbsnA63jA.png"/></div></figure><ul class=""><li id="5f21" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">然后:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nr"><img src="../Images/421154571a7667a0b009cfba214df040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*aMMksdkpyB_zXt3Sh2w67g.png"/></div></figure><h2 id="58bb" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">4.2.命题2(潜在分布匹配)</h2><ul class=""><li id="6bce" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated"><strong class="jq hj">如果解码器在生成期间接收到非常不同的潜在分布，则自动编码器训练将无助于</strong><a class="ae nn" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"><strong class="jq hj">GAN</strong></a><strong class="jq hj">训练。</strong>尽管损失函数不包含明确鼓励潜在分布匹配的项，但它具有隐式匹配它们的效果。</li><li id="4065" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">当达到最佳状态时，我们有:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/d1e9bf34eb3ba7eb6a794105a8cd4346.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*qd-wUmUPoJPsgDApIviHlA.png"/></div></figure><ul class=""><li id="2c74" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">上述命题表明<strong class="jq hj">在最优性时，编码风格分布与其高斯先验相匹配。</strong></li><li id="e297" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">这表明内容空间变得领域不变。</li></ul><h2 id="029a" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">4.3.命题3(联合分布匹配)</h2><ul class=""><li id="4cf8" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">该模型学习两种条件分布</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nt"><img src="../Images/c6c4623f91c955a59a31d61228b098bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*KAapWONNrW-OCyz4gmQ8mg.png"/></div></figure><ul class=""><li id="2392" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">它与数据分布一起定义了两种联合分布:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nu"><img src="../Images/955660872c4cb54e8ee79cbaee656dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*MXpOHvwqNZE-x6-lDAVX9g.png"/></div></figure><ul class=""><li id="9ce6" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">由于两者都被设计成<strong class="jq hj">近似相同的底层节理分布<em class="mo"> p </em> ( <em class="mo"> x </em> 1，<em class="mo"> x </em> 2) </strong>，所以最好<strong class="jq hj">与</strong>一致，即:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nv"><img src="../Images/291b7f3d8516935c3f5bdf9cc5cce95b.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*k2LtidWKj6tuV0mhZcnc_Q.png"/></div></figure><ul class=""><li id="e783" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">联合分布匹配为无监督的图像到图像翻译提供了一个重要的约束条件，并且是许多最近方法成功的原因。所提出的模型在最优性上匹配联合分布。当达到最佳状态时，我们有:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nw"><img src="../Images/c841d9688f3303b67fe5f89756cfdc9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*h6kG3Z5ud4RUALRFjrtIDA.png"/></div></figure><h2 id="57a6" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">4.4.命题4(风格增强的周期一致性)</h2><ul class=""><li id="a265" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">联合分布匹配可以通过<strong class="jq hj">周期一致性约束</strong>实现，如<a class="ae nn" href="https://sh-tsang.medium.com/review-cyclegan-unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks-1c2602805be2" rel="noopener"> CycleGAN </a>所示。然而，这个约束对于多模态图像翻译来说太强了。如果实施循环一致性，转换模型将<strong class="jq hj">退化为确定性函数</strong>。</li><li id="c27e" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><strong class="jq hj"> MUNIT框架在图像风格的联合空间之间承认一种更弱形式的循环一致性</strong>，称为<strong class="jq hj">风格增强循环一致性</strong>，这<strong class="jq hj">更适合多模态图像翻译</strong>。</li><li id="d51d" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">当达到最佳状态时，我们有:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nx"><img src="../Images/446160de8cfee04b7b58108c34cfccbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/format:webp/1*TBz-KnjW-dC6QaPFoCeMTA.png"/></div></figure><ul class=""><li id="8dac" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">直观上，风格增强的循环一致性意味着，如果我们将图像翻译到目标域，并使用原始风格将其翻译回来，我们应该获得原始图像。</li><li id="eb5a" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><strong class="jq hj">建议的双向重建损失</strong>暗示了样式增强的周期一致性，但是明确地实施它对于一些数据集可能是有用的:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ny"><img src="../Images/9b10ea0fcf3c5f4be077b81ba37d2d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*v6bVkmfmZ_-LfEWLsC2MaQ.png"/></div></figure><ul class=""><li id="8e96" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">(如果感兴趣，论文中有更多关于这些命题的细节。)</li></ul></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="1f95" class="lq lr hi bd jn ls lt lu lv lw lx ly lz io ma ip mb ir mc is md iu me iv mf mg bi translated">5.<strong class="ak"> MUNIT:网络架构</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nz"><img src="../Images/1fb0f4e7582a42f5046d8f246cacfc67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3xZrGenycxFizloen3EnDQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn"> MUNIT:网络架构</strong></figcaption></figure><h2 id="866b" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">5.1.风格编码器</h2><ul class=""><li id="b752" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">样式编码器包括几个<strong class="jq hj">步进conv </strong>可选层，接着是一个<strong class="jq hj">全局平均池</strong> (GAP)层和一个<strong class="jq hj">全连接</strong> (FC)层。</li><li id="98fa" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><strong class="jq hj">样式编码器中不使用实例标准化(in)层</strong>，因为IN移除了代表重要样式信息的原始特征均值和方差。</li></ul><h2 id="0de1" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">5.2.解码器</h2><ul class=""><li id="6480" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">解码器<strong class="jq hj">从输入图像的内容和样式代码中重建输入图像。</strong></li><li id="e0cf" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">它通过<strong class="jq hj">一组残差块</strong>处理内容码，最后通过<strong class="jq hj">几个上采样和卷积层产生重建图像。</strong></li><li id="88f2" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">受最近在归一化层中使用仿射变换参数来表示样式的工作的启发[54，72-74]，我们为残差块配备了<strong class="jq hj">自适应实例归一化(AdaIN)</strong>【54】层，其<strong class="jq hj">参数由多层感知器(MLP)从样式代码中动态生成。</strong></li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oa"><img src="../Images/3e3a1589d64ab1ca6bf24c92c9c265fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*x9aaTbgNgNJDNtYMOfBp7w.png"/></div></figure><ul class=""><li id="dfbf" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">其中<em class="mo"> z </em>是前一卷积层的激活，<em class="mo"> μ </em>和<em class="mo"> σ </em>是信道均值和标准差，<em class="mo"> γ </em>和<em class="mo"> β </em>是MLP产生的参数。</li></ul><h2 id="e3b3" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">5.3.鉴别器</h2><ul class=""><li id="36e4" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">使用<strong class="jq hj"> LSGAN物镜</strong>和<strong class="jq hj">多尺度鉴别器</strong>来指导生成器产生逼真的细节和正确的全局结构。</li><li id="c60f" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">(LSGAN使用最小二乘法作为损失函数，希望我将来能写一个关于它的故事。)</li></ul><h2 id="deca" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">5.4.领域不变感知损失</h2><ul class=""><li id="5174" class="kt ku hi jq b jr mh ju mi jx mj kb mk kf ml kj ky kz la lb bi translated">感知损失，通常计算为输出和参考图像之间的VGG特征空间中的距离。</li><li id="84c1" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">使用这种损失的修改版本，其更具有域不变性。</li><li id="4f11" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">具体来说，<strong class="jq hj">在计算距离之前，对VGG特征</strong>进行实例归一化，以去除包含许多领域特定信息的原始特征均值和方差。</li></ul></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="591f" class="lq lr hi bd jn ls lt lu lv lw lx ly lz io ma ip mb ir mc is md iu me iv mf mg bi translated">6.实验结果</h1><h2 id="e3c7" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">6.1.<strong class="ak">定性比较</strong></h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ob"><img src="../Images/5e6c13441d2c4242f3af316395832b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dlzJwqADEqv7v-GGzj7-kg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">边缘定性对比→鞋子</strong></figcaption></figure><ul class=""><li id="ae67" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">下面的每一列显示了一种方法的3个随机输出。</li><li id="25cf" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">尽管注入了随机性，但<a class="ae nn" href="https://sh-tsang.medium.com/review-unit-unsupervised-image-to-image-translation-networks-gan-4a25ced6d078" rel="noopener">单元</a>和<a class="ae nn" href="https://sh-tsang.medium.com/review-cyclegan-unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks-1c2602805be2" rel="noopener">循环</a>(有或无噪声)都无法产生不同的输出。</li><li id="8c11" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">没有<em class="mo"> Lx </em> _ <em class="mo"> recon </em>或者<em class="mo"> Lc </em> _ <em class="mo"> recon </em>的情况下，MUNIT的画质不尽人意。没有<em class="mo"> Ls_recon </em>，模型会遭受部分模式崩溃，许多输出几乎相同(例如，前两行)。</li></ul><blockquote class="mp mq mr"><p id="9ead" class="jo jp mo jq b jr js ij jt ju jv im jw ms jy jz ka mt kc kd ke mu kg kh ki kj hb bi translated">完整的模型产生的图像<strong class="jq hj">既多样又真实</strong>，类似于自行车，但不需要监督。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oc"><img src="../Images/f4a0486fa5c11c63c09887edf52dc623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*92ELeLIr0h0B1T3OSJX82w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">更多示例结果</strong></figcaption></figure><h2 id="a6dc" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">6.2.<strong class="ak">定量</strong>比较</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es od"><img src="../Images/0772722205bb658cb73f39838d19ea78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYUfaZ9AotSyLko_rW2QKA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">边缘定量评价→鞋/手袋</strong></figcaption></figure><ul class=""><li id="8815" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">人的偏好用于衡量质量，LPIPS距离用于评估多样性。</li><li id="82b2" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated"><a class="ae nn" href="https://sh-tsang.medium.com/review-unit-unsupervised-image-to-image-translation-networks-gan-4a25ced6d078" rel="noopener">单元</a>和<a class="ae nn" href="https://sh-tsang.medium.com/review-cyclegan-unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks-1c2602805be2" rel="noopener"> CycleGAN </a>根据LPIPS距离产生非常小的分集。</li><li id="467d" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">从MUNIT中移除<em class="mo"> Lx_recon </em>或<em class="mo"> Lc_recon </em>会导致质量显著下降。没有<em class="mo"> Ls_recon </em>，质量和多样性都变差。</li></ul><blockquote class="mp mq mr"><p id="f8eb" class="jo jp mo jq b jr js ij jt ju jv im jw ms jy jz ka mt kc kd ke mu kg kh ki kj hb bi translated">完整模型获得的<strong class="jq hj">质量和多样性可与完全监督的BicycleGAN </strong>相媲美，并且明显优于所有非监督基线。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oe"><img src="../Images/33c65cae79465427d4b91d1c72c6bc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q_ctbcMx6k9esHIk1NT6RQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">动物形象翻译的定量评估</strong></figcaption></figure><ul class=""><li id="da05" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated"><strong class="jq hj">初始得分(IS) </strong>测量所有输出图像的<strong class="jq hj">多样性，</strong>而<strong class="jq hj">条件IS (CIS) </strong>测量以单个输入图像为条件的<strong class="jq hj">输出的多样性。</strong></li><li id="c870" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">根据CIS和IS，MUNIT模型获得最高分。</li><li id="2c1f" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">特别是，所有基线都获得了非常低的CIS，表明它们未能从给定的投入中产生多式输出。</li></ul><h2 id="b282" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">6.3.其他数据集</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es of"><img src="../Images/44e357a27175253316cca27512a1e9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SZSPc-0DzDth2kkvrT7-gQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">街景翻译的示例结果</strong></figcaption></figure><ul class=""><li id="75ff" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">该模型能够从给定的城市景观图像生成具有<strong class="jq hj">不同渲染(例如，下雨、下雪、日落)</strong>的SYNTHIA图像，并且从给定的SYNTHIA图像生成具有<strong class="jq hj">不同照明、阴影和道路纹理</strong>的城市景观图像。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es og"><img src="../Images/1db37b3ab3a3fe2fc465e2e52a65b044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*areNbrhS-iamTDh10g4aCA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">约塞米蒂夏季↔冬季的示例结果(高清分辨率)。</strong></figcaption></figure><ul class=""><li id="c081" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">类似地，它从给定的夏季图像生成具有不同雪量的冬季图像，并且从给定的冬季图像生成具有不同叶子量的夏季图像。</li></ul><h2 id="74d8" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">6.4.示例引导的图像翻译</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oh"><img src="../Images/a7dcddcc6f73a43b7e5941d9013de530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LjRJc-MDjgFqwGs3pOufTg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">图像翻译</strong></figcaption></figure><ul class=""><li id="a46d" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">每行都有相同的内容，而每列都有相同的样式。生成的鞋子的颜色和生成的猫的外观可以通过提供示例样式图像来指定。</li><li id="41c5" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">也可以从参考图像中<strong class="jq hj">提取样式代码，而不是从先前采样样式代码。</strong></li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oi"><img src="../Images/8a1b1101bf00143c768e545743d699ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y1yAdYNz0Rvjj7cG8tF4kg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">与现有风格转移方法的比较</strong></figcaption></figure><ul class=""><li id="df62" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">还比较了经典的风格转换算法。</li><li id="23d0" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">MUNIT生成的结果<strong class="jq hj">更加真实可信，</strong>因为它使用<a class="ae nn" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75"> GAN </a> s学习目标域图像的分布。</li></ul></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h2 id="acd5" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">参考</h2><p id="edcd" class="pw-post-body-paragraph jo jp hi jq b jr mh ij jt ju mi im jw jx oj jz ka kb ok kd ke kf ol kh ki kj hb bi translated">【2018 ECCV】【MUNIT】<br/><a class="ae nn" href="https://arxiv.org/abs/1804.04732" rel="noopener ugc nofollow" target="_blank">多模态无监督图像到图像翻译</a></p><h2 id="8eac" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated">生成对抗网络</h2><p id="da7a" class="pw-post-body-paragraph jo jp hi jq b jr mh ij jt ju mi im jw jx oj jz ka kb ok kd ke kf ol kh ki kj hb bi translated"><strong class="jq hj">图像合成</strong> [ <a class="ae nn" rel="noopener" href="/@sh.tsang/review-gan-generative-adversarial-nets-gan-e12793e1fb75">甘</a> ] [ <a class="ae nn" rel="noopener" href="/@sh.tsang/review-cgan-conditional-gan-gan-78dd42eee41"> CGAN </a> ] [ <a class="ae nn" rel="noopener" href="/@sh.tsang/review-lapgan-laplacian-generative-adversarial-network-gan-e87200bbd827">拉普甘</a>[<a class="ae nn" href="https://sh-tsang.medium.com/review-aae-adversarial-autoencoders-gan-e8fda9160542" rel="noopener">AAE</a>][<a class="ae nn" rel="noopener" href="/@sh.tsang/review-dcgan-deep-convolutional-generative-adversarial-network-gan-ec390cded63c">DCGAN</a>][<a class="ae nn" href="https://sh-tsang.medium.com/review-cogan-coupled-generative-adversarial-networks-gan-273f70b340af" rel="noopener">CoGAN</a>][<a class="ae nn" href="https://sh-tsang.medium.com/review-simgan-learning-from-simulated-and-unsupervised-images-through-adversarial-training-gan-86a7003add50" rel="noopener">辛甘</a> ] [ <a class="ae nn" href="https://sh-tsang.medium.com/review-bigan-adversarial-feature-learning-gan-535eb76be2ca" rel="noopener">甘比</a> ] [ <a class="ae nn" href="https://sh-tsang.medium.com/review-ali-adversarially-learned-inference-gan-6e4677667914" rel="noopener">阿里</a><br/><strong class="jq hj">图像到图像的翻译</strong><a class="ae nn" href="https://sh-tsang.medium.com/review-pix2pix-image-to-image-translation-with-conditional-adversarial-networks-gan-ac85d8ecead2" rel="noopener"> <br/> <strong class="jq hj">超分辨率</strong></a><a class="ae nn" rel="noopener" href="/@sh.tsang/review-srgan-srresnet-photo-realistic-super-resolution-gan-super-resolution-96a6fa19490">SRGAN&amp;SRResNet</a><a class="ae nn" rel="noopener" href="/@sh.tsang/reading-enhancenet-automated-texture-synthesis-super-resolution-8429635aa75e">EnhanceNet</a><a class="ae nn" rel="noopener" href="/towards-artificial-intelligence/reading-esrgan-enhanced-super-resolution-generative-adversarial-networks-super-resolution-e8533ad006b5">ESR gan</a><br/><strong class="jq hj">模糊检测</strong><a class="ae nn" href="https://sh-tsang.medium.com/review-dmenet-deep-defocus-map-estimation-using-domain-adaptation-blur-detection-20fdcaf5e384" rel="noopener">DMENet</a><br/><strong class="jq hj">摄像头篡改检测</strong><a class="ae nn" href="https://sh-tsang.medium.com/review-mantinis-visapp-19-generative-reference-model-and-deep-learned-features-camera-f608371c9854" rel="noopener">曼蒂尼的VISAPP’19</a><strong class="jq hj"><br/>视频编码【T55</strong></p><h2 id="a6b0" class="mx lr hi bd jn my mz na lv nb nc nd lz jx ne nf mb kb ng nh md kf ni nj mf nk bi translated"><a class="ae nn" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>