<html>
<head>
<title>NLP Deep Learning Training on Downstream tasks using Pytorch Lightning — Intro — Part 1 of 7</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pytorch Lightning对下游任务进行NLP深度学习培训—简介—第1部分，共7部分</h1>
<blockquote>原文：<a href="https://medium.com/codex/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-intro-part-1-of-6-c338a05f86e6?source=collection_archive---------7-----------------------#2021-07-23">https://medium.com/codex/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-intro-part-1-of-6-c338a05f86e6?source=collection_archive---------7-----------------------#2021-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f4109cc4093b5b0a3a39507bebd24184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bjirIJDTGCmwGltnC6ubgA.png"/></div></div></figure><p id="884f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基于大型转换器的语言模型，如伯特、GPT、玛丽安、T5等。被开发和训练以对他们被训练的语言/文本语料库具有统计理解。它们以自我监督的方式(没有人为的数据标记)被训练，使用诸如屏蔽记号预测、下一句预测等技术。这些模型对于特定的实际NLP任务不是很有用，直到它们经历了一个叫做迁移学习的过程。在迁移学习过程中，这些模型通过添加一个Head(由几个神经层组成，如linear、dropout、Relu等)以监督的方式对给定的任务进行微调。)添加到特定的预训练语言模型(不确定为什么他们称其为头部而不是尾部，因为监督层被添加到预训练模型的底部)。本系列是关于使用Pytorch Lightning框架来微调不同NLP特定任务的语言模型。</p><p id="7be9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有预训练NLP模型的主要维护者，如HuggingFace、FastAI、SparkNLP等，都有训练器API来使用发布的数据集或您自己的标记数据集微调语言模型。HuggingFace中用于在IMDB数据集上微调Bert的示例训练器API如下所示:</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="25bb" class="jx jy hi jt b fi jz ka l kb kc">python run_glue.py \<br/>  --model_name_or_path bert-base-cased \<br/>  --dataset_name imdb  \<br/>  --do_train \<br/>  --do_predict \<br/>  --max_seq_length 128 \<br/>  --per_device_train_batch_size 32 \<br/>  --learning_rate 2e-5 \<br/>  --num_train_epochs 3 \<br/>  --output_dir /tmp/imdb/</span></pre><p id="a907" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然API通过提供带有配置细节的一行命令似乎可以轻松地微调特定任务的语言模型，但您放弃了一些微调控制，如改变语言模型头部的架构(如添加另一个丢弃层或另一个密集层)、使用不同的学习速率调度程序、使用不同的损失函数(可能是不平衡数据的加权损失函数)、使用不同的度量来测量模型性能等。理论上，您可以对python API脚本文件进行修改以满足您的需求，但这并不容易。这些python脚本文件中的大部分注释都很糟糕。撰写本系列的动机是展示一种更好的方法，以一种有组织的方式使用Pytorch Lightning框架来微调语言模型。本系列文章显然是面向那些希望更好地控制NLP训练过程的高级实践者、研究人员和学生的。</p><p id="5403" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该系列将展示以下下游任务的微调:</p><figure class="jo jp jq jr fd ij er es paragraph-image"><div class="er es kd"><img src="../Images/da913ea9d5a48a16fe6dcaa6a47dbb0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*HuH0ePgNPPpwC0ym6DlyIQ.png"/></div><figcaption class="ke kf et er es kg kh bd b be z dx translated">下游任务和用于该任务的预训练模型</figcaption></figure><p id="13ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有Pytorch Lightning Colab笔记本都按照如下所示的方式组织成不同的部分。每个系列中的帖子将围绕与培训的特定任务相关的每个部分做出适当的评论。在这个系列的每一部分都会重复某些评论，以使每一部分在完整性方面独立存在。</p><ol class=""><li id="8241" class="ki kj hi is b it iu ix iy jb kk jf kl jj km jn kn ko kp kq bi translated"><strong class="is hj">下载并导入库</strong></li><li id="ed18" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">下载数据</strong></li><li id="aaf2" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">定义预训练模型</strong></li><li id="960f" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">定义预处理函数或数据集类</strong></li><li id="f1f6" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">定义数据模块类</strong></li><li id="a84e" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">定义模型类</strong></li><li id="23bb" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">定义Pytorch Lightning模块类</strong></li><li id="72f8" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">定义训练器参数</strong></li><li id="c616" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">训练模型</strong></li><li id="82a6" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">评估模型性能</strong></li><li id="1576" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">对训练好的模型进行推理</strong></li><li id="dde1" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated"><strong class="is hj">打开张量板日志</strong></li></ol><p id="cd51" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Pytorch Lightning framework提供了一种更易于管理的方法来组织围绕训练或微调预训练模型的代码。它混淆了许多围绕设置训练、验证循环、将模型/数据转移到Cuda等的垃圾工作，并允许ML数据科学家专注于训练或微调模型的重要方面。将代码组织成单独的类/节使其更具可读性和可理解性。Pytorch Lightning在过去12个月中也进行了大量更新，使其更加灵活——例如，要使用在每个训练步骤中改变学习率的调度程序，您必须在training_step和training_epoch_end函数中放置调度程序特定的代码，但现在不再需要了。</p><p id="cd9c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">转到本系列的第二部分<a class="ae kw" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-imdb-classification-cbb6c62789c3" rel="noopener">我们将微调一个DistilBert模型来对IMDB电影评论数据进行二进制分类。不同的部分可以在这里访问:</a></p><ol class=""><li id="da68" class="ki kj hi is b it iu ix iy jb kk jf kl jj km jn kn ko kp kq bi translated">第二部分— <a class="ae kw" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-imdb-classification-cbb6c62789c3" rel="noopener"> IMDB电影评论二进制分类</a></li><li id="6d8f" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated">第3部分—<a class="ae kw" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-ner-on-conll-data-part-fe1512ae4183" rel="noopener">CoNLL 2003</a>数据上的命名实体或令牌识别</li><li id="d730" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated">第4部分— <a class="ae kw" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-multiple-choice-on-swag-eb6a50498307" rel="noopener">关于Swag </a>数据的多项选择答案</li><li id="0095" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated">第5部分— <a class="ae kw" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-question-answering-on-17d2a0965733" rel="noopener">关于1.1班</a>数据的问题回答</li><li id="db0d" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated">第6部分—<a class="ae kw" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-summarization-on-xsum-3b4ffd5db91d" rel="noopener">XSum</a>数据汇总</li><li id="af17" class="ki kj hi is b it kr ix ks jb kt jf ku jj kv jn kn ko kp kq bi translated">第7部分—<a class="ae kw" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-translation-on-english-to-ab90af97b9c2" rel="noopener">wmt 16英语到罗马尼亚语数据的翻译</a></li></ol><p id="68e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">需要注意的一点是，本系列中显示的所有代码示例都在Colab笔记本上，因为GPU在那里是免费的。出于演示目的，这是可以的。在Colab笔记本上运行研究实验或生产培训是不实际的，因为NLP任务肯定需要多个GPU来使用更大的Transformer模型。我将很快把代码转换成可以在AWS Sagemaker或Azure ML上运行的Python脚本，并在完成后更新这个介绍。</p></div></div>    
</body>
</html>