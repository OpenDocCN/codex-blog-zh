<html>
<head>
<title>ICF, the missing ingredient of TF-IDF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ICF，TF-IDF缺失的成分</h1>
<blockquote>原文：<a href="https://medium.com/codex/icf-the-missing-ingredient-of-tf-idf-d9f715c9946f?source=collection_archive---------13-----------------------#2021-08-14">https://medium.com/codex/icf-the-missing-ingredient-of-tf-idf-d9f715c9946f?source=collection_archive---------13-----------------------#2021-08-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="abe2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">针对文本分类任务进行优化</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/563f4eef50d35b295c4d3765aa35ea40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PBQWTWWnpbLHdKDljc7HAg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图片来自<a class="ae jn" href="https://freeimages.com" rel="noopener ugc nofollow" target="_blank">自由影像</a></figcaption></figure><p id="cb3e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">许多人都知道<a class="ae jn" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>是一个基本的自然语言处理工具，通常用于简单的文本分类任务，如垃圾邮件过滤。没有多少人知道普通的TF-IDF不是为这种应用而优化的，并且通过应用简单的算法改变，TF-IDF可以被显著地改进。也就是说，考虑改用TF-ICF。</p><h1 id="4938" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">TF-IDF，根据文档重要性对术语进行排序</h1><p id="2888" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">TF-IDF起源于文本搜索/索引领域，是一种在给定文档语料库的情况下评估术语对文档重要性的方法。首先，确定每个文档的关键术语，然后，给定一个搜索查询，根据查询术语TF-IDF得分对最相关的文档进行排序。TF-IDF公式相当直观，是两个因素的产物:</p><ul class=""><li id="ac90" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated">TF —术语频率，术语在文档中出现的次数。一个术语越常见，它就越重要。但是如果这个术语在其他文档中也很常见呢？。以单词“the”为例，它可能在许多语料库文档中非常常见。这样的单词对于区分文档和其他文档是没有用的。TF应该与其他文档的外观标准化，以使其成为术语对文档(在语料库中)重要性的真实指标。</li><li id="d8dc" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">IDF —逆文档频率，日志(文档数/包含该术语的文档数)。超级常见的术语将得到log(1) = 0。词条越稀有，语料库中的文档越多，日志得到的响应就越高。</li></ul><p id="3cdd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用IDF，我们可以将TF得分标准化；将TF乘以IDF，我们考虑在给定其他文档的术语的情况下，一个术语对一个文档有多重要。一个术语在文档中的常见程度乘以该术语在语料库中的独特程度。对于分类任务，TF-IDF通常与单词包一起出现。</p><h1 id="5598" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">BOW —单词包，对文本进行分类</h1><p id="4239" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">为了对文档进行分类，我们首先需要将其规范化为分类模型能够处理的格式。BOW是一种向量化文本的技术；特征是语料库术语(单词)的子集，值是术语TF，1/0(以二进制模式)或TF-IDF分数。</p><p id="fecd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">“词汇袋”因为缺乏语境；不知道<strong class="jq hj">在哪里</strong>每个单词出现，但<strong class="jq hj">是否</strong>它出现。为了选择要考虑的X个术语(从N个语料库可能的单词中),简单的方法将是在一些文本处理步骤(去除停用词、词干等)之后采取k个最常见的..).不太简单的方法使用诸如TF-IDF的试探法，假设TF-IDF突出语料库中最重要的术语。<a class="ae jn" rel="noopener" href="/xeneta/boosting-sales-with-machine-learning-fbcf2e618be3#.33uhr93sw">这里的</a>就是这样一个使用TF-IDF和BOW的文本分类管道的例子。</p><h1 id="85e9" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">TF — ICF，以类而不是文档为目标</h1><p id="62b1" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">如前所述，TF-IDF最初的动机之一是支持语料库的文档搜索；对于每个文档，在语料库中找到最能代表它的关键词。这就是为什么TF-IDF更喜欢使用<strong class="jq hj">罕见术语</strong>(假设它们可以更好地将文档与其他术语区分开来)而不是常见术语。问题是文本分类，<strong class="jq hj">不总是最稀有的记号是最重要的记号</strong>。例如，考虑一个二进制分类任务，其中一个术语出现在每个正面文档中，而在负面文档中则完全不出现。从概念上讲，这样一个术语对于文档分类非常有用，因此我们假设TF-IDF将其排在较高的位置。但是由于这个术语非常常见(出现在一半的语料库文档中)，TF-IDF将降低其排名，并且很可能倾向于其他术语。内在的原因是TF-IDF没有考虑documents <strong class="jq hj">类</strong>，因此(简单的)修复方法是让它知道这些类。王和张(2010，<a class="ae jn" href="https://arxiv.org/abs/1012.2609" rel="noopener ugc nofollow" target="_blank">此处</a>)介绍了一种解决该问题的优雅方案——TF-ICF:</p><ul class=""><li id="2693" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated"><strong class="jq hj"> TCF-ICF </strong> —将每个类别的文档合并成一个主文档，然后计算TF-IDF分数。TCF-ICF；每个类别的术语计数*每个类别的术语重要性(关于其在其他类别的分布)。使TF-IDF考虑到该术语对于分类的重要性。但是要使它真正有用，我们需要重新添加每个文档的重要性；例如，考虑具有两个类的分类任务，每个类100个文档，以及在类合并的主文档中最常见但同时仅在少数文档中出现的术语。当TCF查看合并文档统计数据时，它会倾向于该术语，从而获得较高的TCF-ICF得分。问题是从类的角度来看，这样的术语没有用(与类的其他文档无关)。</li><li id="9d2d" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated"><strong class="jq hj">TF</strong>-<strong class="jq hj">ICF</strong>——解决方案将是用TF替换TCF，又名TF-ICF；考虑每个文档术语频率(TF ),同时支持类别唯一术语(ICF)。使用TF-ICF，我们支持文档和类中最重要的术语，使分数对分类更有用。</li></ul><h2 id="9276" class="lv kl hi bd km lw lx ly kq lz ma mb ku jx mc md kw kb me mf ky kf mg mh la mi bi translated">慎用，并考虑合奏</h2><p id="b865" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">尽管TF-ICF似乎是文本分类任务中最直观的TF-IDF，但在某些情况下，最好的解决方案是也考虑其他版本，使用这些方法的集合。主要原因是大多数数据集并不像我们在例子中描述的那样尖锐地分布，在许多情况下，我们可以从混合不同的方法中受益；TF-ICF将突出与大部分类别文档相关的术语。在某些情况下，拥有一个特征来区分甚至是类的一部分，对于一个模型来说示例可能是有用的。TCF-ICF将强调这一点。类似地，在某些情况下，TF-IDF发现文档中的重要术语(相对于TF-ICF类术语)对分类模型很有用。底线是我们应该选择最合适的版本，同时也要记住其他选项。</p><p id="c3c3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">并让最优秀的TF-IDF胜出；)</p></div></div>    
</body>
</html>