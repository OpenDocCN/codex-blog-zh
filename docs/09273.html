<html>
<head>
<title>Realtime Data Scraping with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python进行实时数据采集</h1>
<blockquote>原文：<a href="https://medium.com/codex/realtime-data-scraping-with-python-517bf5a5eb84?source=collection_archive---------0-----------------------#2022-10-09">https://medium.com/codex/realtime-data-scraping-with-python-517bf5a5eb84?source=collection_archive---------0-----------------------#2022-10-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6cc6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">利用Selenium和Beautifulsoup进行实时更新</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/e61be27329580dc3a5427957c6bbe5b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LnpP48bW0phPbHsA"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jn" href="https://unsplash.com/@aronvisuals?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Aron视觉</a>拍摄的照片</figcaption></figure><h2 id="9686" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">语境</h2><p id="e7f5" class="pw-post-body-paragraph km kn hi ko b kp kq ij kr ks kt im ku jz kv kw kx kd ky kz la kh lb lc ld le hb bi translated">供应链弹性对于那些依靠供应链来生产商品的公司来说是一个重要的话题。想象一下这样一种情况，你的供应链经理要求实时更新世界某些地方的当前天气状况，以及可能的气候警告和警报，可在<a class="ae jn" href="https://severeweather.wmo.int/v2/list.html" rel="noopener ugc nofollow" target="_blank">这里</a>获得。你决定通过实时抓取网站来解决这个问题。为此，您将创建一个Python脚本来收集所有需要的数据，然后安排它每30分钟运行一次来接收实时更新。</p><blockquote class="lf lg lh"><p id="76e6" class="km kn li ko b kp lj ij kr ks lk im ku ll lm kw kx ln lo kz la lp lq lc ld le hb bi translated">本文最适合熟悉Python的程序员。</p></blockquote><h2 id="0564" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">擦</h2><ol class=""><li id="7344" class="lr ls hi ko b kp kq ks kt jz lt kd lu kh lv le lw lx ly lz bi translated">我们需要做的第一件事是安装必要的库，比如BeautifulSoup和Selenium</li></ol><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="ed11" class="jo jp hi mb b fi mf mg l mh mi">pip install bs4<br/>pip install selenium</span></pre><p id="30bf" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">为了给出一个简单的区别，我们将需要<em class="li"> Selenium </em>去一个网站，通过点击按钮与浏览器交互，并等待元素出现。然后，<em class="li"> BeautifulSoup </em>用于迭代HTML并提取实际数据(即您所看到的)。</p><p id="02cf" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">2.我们现在探索<a class="ae jn" href="https://severeweather.wmo.int/v2/list.html" rel="noopener ugc nofollow" target="_blank">网站</a>。正如您在下图中看到的，在数据被正确加载之前，需要大约5秒的等待时间。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/71690efbf2167dac9fd0a771d01c163a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*v58OSFcxGsBC8U38iIiLMA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">数据正在加载(图标)</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/9bdecdce445e809b0239ff4acfc9bacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XI95kQiYU01qg8JfUwSlTw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">HTML格式的加载数据</figcaption></figure><p id="357e" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">因此，直接用BeautifulSoup开始抓取将导致没有条目，因为我们需要等待数据出现在HTML中。我们通过在获取数据后创建的元素上设置监听器来解决这个问题。</p><p id="3cad" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">右键点击网站上的“Inspect Element”按钮，我们在检查界面中看到需要等待的元素是带有类<em class="li"> dataTables_scrollBody </em>的<div>。</div></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ml"><img src="../Images/27b144d4495dd3c2d8a890eb14ebcf82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*9ggjkrQBmBAGHSNInBNdpw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">检查元素结果</figcaption></figure><p id="6b00" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">要抓取一个网站，库<em class="li"> Selenium </em>需要我们有谷歌Chrome浏览器(你也可以用别的浏览器)。因此，我们告诉Selenium启动Google Chrome</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="0866" class="jo jp hi mb b fi mf mg l mh mi">from selenium import webdriver</span><span id="f6b7" class="jo jp hi mb b fi mm mg l mh mi">driver = webdriver.Chrome(ChromeDriverManager().install())</span></pre><p id="834a" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">并通过传递网址告诉司机我们的网站在哪里</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="8bc2" class="jo jp hi mb b fi mf mg l mh mi">driver.get("https://severeweather.wmo.int/v2/list.html")</span></pre><p id="6ebf" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">现在，我们可以设置上面提到的监听器，让驱动程序等待带有<em class="li">dataTables _ scroll body</em><strong class="ko hj">类</strong>的<div>元素出现在HTML中</div></p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="e0ea" class="jo jp hi mb b fi mf mg l mh mi">try:        <br/>  <strong class="mb hj">elem = WebDriverWait(driver, 30).until(    EC.presence_of_element_located((By.CLASS_NAME, "dataTables_scrollBody")))</strong><br/> finally:        <br/>  print('loaded')</span></pre><p id="5976" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">我们将我们的抓取函数定义为<em class="li"> scrapeWeather </em>，此时我们的代码应该如下所示:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="dda1" class="jo jp hi mb b fi mf mg l mh mi">### imports<br/>import pandas as pd</span><span id="4059" class="jo jp hi mb b fi mm mg l mh mi">from bs4 import BeautifulSoup</span><span id="1dc6" class="jo jp hi mb b fi mm mg l mh mi">from selenium import webdriver<br/>from selenium.webdriver.common.by import By<br/>from selenium.webdriver.support.ui import WebDriverWait<br/>from selenium.webdriver.support import expected_conditions as EC<br/>from webdriver_manager.chrome import ChromeDriverManager</span><span id="b70c" class="jo jp hi mb b fi mm mg l mh mi">###</span><span id="95b1" class="jo jp hi mb b fi mm mg l mh mi">def scrapeWeather(): # Our function for scraping   <br/> driver = webdriver.Chrome(ChromeDriverManager().install())</span><span id="beb9" class="jo jp hi mb b fi mm mg l mh mi"> #url request<br/>driver.get("https://severeweather.wmo.int/v2/list.html") </span><span id="ecca" class="jo jp hi mb b fi mm mg l mh mi"> try:        <br/>  elem = WebDriverWait(driver, 30).until(    EC.presence_of_element_located((By.CLASS_NAME, "dataTables_scrollBody")))<br/> finally:        <br/>  print('loaded')</span></pre><p id="d81b" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">3.现在数据已经在HTML中了，我们可以选择想要用BeautifulSoup抓取的条目。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mn"><img src="../Images/2b502c1d08a7ddc74d41ff33a5c4fe20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*eKxIHYHRUJo8xNAjqJJDOA.png"/></div></figure><p id="5d7a" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">从检查中我们可以看到，所有的数据都在<tbody>标签中。每个<tr>标签在表格中包含一个条目(行)。因此，我们必须找到正确的</tr></tbody><tbody>，并开始循环所有的<tr>标签。我们用函数<strong class="ko hj"> findAll，</strong>来完成这项工作，它查找一个HTML标签的所有条目。</tr></tbody></p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="fa79" class="jo jp hi mb b fi mf mg l mh mi">soup = BeautifulSoup(driver.page_source, 'html.parser')    </span><span id="7352" class="jo jp hi mb b fi mm mg l mh mi">"""Scraper getting each row"""    <br/>all = soup.<strong class="mb hj">findAll</strong>("tbody")[2] #the &lt;tbody&gt; we want is the third one<br/>row = all.<strong class="mb hj">findAll</strong>('tr')</span></pre><p id="f816" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">由于我们将条目保存到一个CSV文件中，我们将:</p><ul class=""><li id="5ccd" class="lr ls hi ko b kp lj ks lk jz mo kd mp kh mq le mr lx ly lz bi translated">创建一个空数组，我们将用表中每一行的数据填充它，</li><li id="2874" class="lr ls hi ko b kp ms ks mt jz mu kd mv kh mw le mr lx ly lz bi translated">迭代每一行(I)，迭代行(I)的每一列(j)，以及</li><li id="a98f" class="lr ls hi ko b kp ms ks mt jz mu kd mv kh mw le mr lx ly lz bi translated">将信息保存到正确的变量中。</li></ul><p id="f60c" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">代码将如下所示:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="8499" class="jo jp hi mb b fi mf mg l mh mi">rest_info = [] # empty array populated with the info of each row</span><span id="73dd" class="jo jp hi mb b fi mm mg l mh mi">for i in rows: #i is a row<br/>        infos_row = i.findAll('td')   # get the info of a single row<br/>        for index, j in enumerate(infos_row): #j is a col of row i <br/>            info = None<br/>            if index == 0: #in this case the first col has the event information<br/>                info = j.find('span') #the info is within a <em class="li">span</em><br/>                event = info.text #we extract the text from the <em class="li">span</em></span><span id="fc7f" class="jo jp hi mb b fi mm mg l mh mi">            if index == 4:<br/>                info = j.find('span')<br/>                areas = info.text</span><span id="25c7" class="jo jp hi mb b fi mm mg l mh mi">            if index == 1:<br/>                issued_time = j.text<br/>            if index == 3:<br/>                country = j.text</span><span id="7085" class="jo jp hi mb b fi mm mg l mh mi">            if index == 5:<br/>                regions = j.text</span><span id="238e" class="jo jp hi mb b fi mm mg l mh mi">            if index == 2:<br/>                continue<br/>        #finally we append the infos to the list (for each row) <br/>        rest_info.append([event,issued_time,country,areas,regions)])</span></pre><p id="94cb" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">现在我们已经将信息保存在列表中，让我们将它推送到一个CSV中。</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="1a55" class="jo jp hi mb b fi mf mg l mh mi">df = pd.DataFrame(rest_info, columns=<br/> ['Event_type','Issued_time','Country','Areas','Regions','Date'])</span><span id="b574" class="jo jp hi mb b fi mm mg l mh mi"><strong class="mb hj">df.to_csv</strong>("scraped_weather.csv",mode='a', index=False,header=False)</span></pre><p id="9754" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">CSV文件应该如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mx"><img src="../Images/4cb85444a48dbfe1c13e0a52ee1ad364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-bsN_B-QEkfmFZDINRaphw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">从网站上搜集的数据</figcaption></figure><p id="2d5a" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">恭喜你！你已经抓取了网站。现在让我们看看如何自动化这个过程。</p><h2 id="3874" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.实时自动化</h2><p id="c9d1" class="pw-post-body-paragraph km kn hi ko b kp kq ij kr ks kt im ku jz kv kw kx kd ky kz la kh lb lc ld le hb bi translated">为了安排每<strong class="ko hj"> X </strong>分钟进行一次抓取(取决于您的需要)，我们需要使用一个调度程序。这里我们有许多选项中的两个:</p><ul class=""><li id="d8ef" class="lr ls hi ko b kp lj ks lk jz mo kd mp kh mq le mr lx ly lz bi translated">GitHub操作</li><li id="1b62" class="lr ls hi ko b kp ms ks mt jz mu kd mv kh mw le mr lx ly lz bi translated">谷歌云调度程序</li></ul><p id="d97d" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">对于本教程，我们将使用GitHub Actions，因为我认为它是最简单易懂的。</p><ol class=""><li id="1780" class="lr ls hi ko b kp lj ks lk jz mo kd mp kh mq le lw lx ly lz bi translated">首先，我们需要稍微修改一下代码，以便能够通过GitHub上的<strong class="ko hj"> Selenium打开谷歌Chrome。</strong>我们需要安装模块<em class="li"> pyvisualdisplay </em>作为</li></ol><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="85b7" class="jo jp hi mb b fi mf mg l mh mi">pip install PyVirtualDisplay</span></pre><p id="cb20" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">然后，我们需要对现有代码进行以下更改:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="6669" class="jo jp hi mb b fi mf mg l mh mi">import pandas as pd</span><span id="407a" class="jo jp hi mb b fi mm mg l mh mi">from bs4 import BeautifulSoup</span><span id="95cd" class="jo jp hi mb b fi mm mg l mh mi">from selenium import webdriver<br/>from selenium.webdriver.common.by import By<br/>from selenium.webdriver.support.ui import WebDriverWait<br/>from selenium.webdriver.support import expected_conditions as EC<br/>from webdriver_manager.chrome import ChromeDriverManager</span><span id="6232" class="jo jp hi mb b fi mm mg l mh mi"><strong class="mb hj">from selenium.webdriver.chrome.options import Options<br/>from selenium.webdriver.chrome.service import Service<br/>import chromedriver_autoinstaller</strong></span><span id="ffdc" class="jo jp hi mb b fi mm mg l mh mi"><strong class="mb hj">from pyvirtualdisplay import Display</strong><br/><strong class="mb hj">display = Display(visible=0, size=(800, 800))  <br/>display.start()</strong></span><span id="3beb" class="jo jp hi mb b fi mm mg l mh mi"><strong class="mb hj">chromedriver_autoinstaller.install(</strong>)  # Check if the current version of chromedriver exists</span><span id="364c" class="jo jp hi mb b fi mm mg l mh mi"><strong class="mb hj">chrome_options = webdriver.ChromeOptions() </strong>     <br/><strong class="mb hj">options = [<br/>  # Define window size here<br/>   "--window-size=1200,1200",<br/>    "--ignore-certificate-errors"<br/>]</strong></span><span id="10ab" class="jo jp hi mb b fi mm mg l mh mi"><strong class="mb hj">for option in options:<br/>    chrome_options.add_argument(option)</strong></span><span id="c41a" class="jo jp hi mb b fi mm mg l mh mi"><strong class="mb hj">driver = webdriver.Chrome(options = chrome_options)</strong></span></pre><p id="5222" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">在<em class="li"> scrapeWeather </em>类中，我们不再需要调用ChromeDriver安装程序</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="5910" class="jo jp hi mb b fi mf mg l mh mi">def scrapeWeather():    <br/> <strong class="mb hj">#</strong>driver = webdriver.Chrome(ChromeDriverManager().install()) <strong class="mb hj">#not needed anymore!</strong></span><span id="39be" class="jo jp hi mb b fi mm mg l mh mi"> driver.get("https://severeweather.wmo.int/v2/list.html")<br/> ....</span></pre><p id="f549" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">2.我们已经准备好将代码部署到GitHub并对其进行调度。为此，我们需要:</p><ul class=""><li id="8432" class="lr ls hi ko b kp lj ks lk jz mo kd mp kh mq le mr lx ly lz bi translated">创建存储库</li><li id="4df5" class="lr ls hi ko b kp ms ks mt jz mu kd mv kh mw le mr lx ly lz bi translated">推送python脚本</li><li id="4249" class="lr ls hi ko b kp ms ks mt jz mu kd mv kh mw le mr lx ly lz bi translated">创建并推送一个<em class="li"> requirements.txt </em>文件(<code class="du my mz na mb b">pip install pipreqs</code>并在您的脚本所在的终端文件夹中运行<code class="du my mz na mb b">pipreqs</code></li><li id="bbbc" class="lr ls hi ko b kp ms ks mt jz mu kd mv kh mw le mr lx ly lz bi translated">创建一个工作流:在你的GitHub仓库-&gt;动作-&gt;新建工作流。在工作流中，我们需要添加以下代码(复制粘贴并根据您的设置进行更改):</li></ul><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="3419" class="jo jp hi mb b fi mf mg l mh mi">name: scrap3</span><span id="9902" class="jo jp hi mb b fi mm mg l mh mi">on:<br/>  <strong class="mb hj">schedule:<br/>    - cron: '*/30 * * * *' #the schedule, in this case every 30 mins, in cron time (URL CRON)</strong></span><span id="7e94" class="jo jp hi mb b fi mm mg l mh mi">jobs:<br/>  build:<br/>    runs-on: ubuntu-latest<br/>    steps:</span><span id="eb39" class="jo jp hi mb b fi mm mg l mh mi">- name: checkout repo content<br/>        uses: actions/checkout@v2</span><span id="06a2" class="jo jp hi mb b fi mm mg l mh mi">- name: setup python<br/>        uses: actions/setup-python@v2<br/>        with:<br/>          python-version: '3.7.7' # install the python version needed<br/>          <br/>      - name: install python packages<br/>        run: |<br/>          python -m pip install --upgrade pip<br/>          pip install -r requirements.txt<br/>          <br/>      - name: execute py script<br/>        <strong class="mb hj">run: python scrape.py #NAME OF YOUR FILE HERE!!</strong><br/>          <br/>      - name: commit files<br/>        run: |<br/>          git config --local user.email "<a class="ae jn" href="mailto:action@github.com" rel="noopener ugc nofollow" target="_blank">action@github.com</a>"<br/>          git config --local user.name "GitHub Action"<br/>          git add -A<br/>          git commit -m "update data" -a<br/>          <br/>      - name: push changes<br/>        uses: ad-m/github-push-action@v0.6.0<br/>        with:<br/>          github_token: ${{ secrets.GITHUB_TOKEN }}<br/>          branch: main</span></pre><p id="e506" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">完美。现在，您的脚本将每30分钟运行一次，并将数据添加到CSV中。您现在可以从另一个端点调用GitHub上托管的这个CSV文件，并获得实时天气更新！</p></div><div class="ab cl nb nc gp nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="hb hc hd he hf"><p id="7b48" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">这是一个使用BeautifulSoup、Selenium和GitHub动作从web上抓取数据的例子。我在上周末参加的<a class="ae jn" href="https://hackzurich.com/" rel="noopener ugc nofollow" target="_blank"> HackZurich </a>项目中使用了这个脚本。它是欧洲最大的黑客马拉松，在48小时内，我们建立了一个供应链预警应用程序，这使我们赢得了挑战。你可以在这里看到我们的<a class="ae jn" href="https://app.creatorspace.dev/monopoli/projects/fxdQWIJCum3gfqUx" rel="noopener ugc nofollow" target="_blank">应用</a>和包含所有代码<a class="ae jn" href="https://github.com/HackZurichDreamTeam/scraping-repo" rel="noopener ugc nofollow" target="_blank">的GitHub库</a>。</p><p id="e279" class="pw-post-body-paragraph km kn hi ko b kp lj ij kr ks lk im ku jz lm kw kx kd lo kz la kh lq lc ld le hb bi translated">感谢您花宝贵的时间阅读本文。记得在媒体上<strong class="ko hj">关注</strong>我，如果有任何问题，请在<a class="ae jn" href="https://www.linkedin.com/in/gioele-monopoli/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上<strong class="ko hj">联系</strong>我。下次见！</p></div></div>    
</body>
</html>