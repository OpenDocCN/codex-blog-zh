<html>
<head>
<title>Logistic Regression and Maximum Likelihood Estimation Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归和最大似然估计函数</h1>
<blockquote>原文：<a href="https://medium.com/codex/logistic-regression-and-maximum-likelihood-estimation-function-5d8d998245f9?source=collection_archive---------3-----------------------#2021-04-09">https://medium.com/codex/logistic-regression-and-maximum-likelihood-estimation-function-5d8d998245f9?source=collection_archive---------3-----------------------#2021-04-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/af8a70ccd0e11f91a3260f6d2e61e56f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gxv1uLye5I5W3sqq"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">约翰尼斯·格罗尔在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><p id="6e91" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我试图以最简单的方式解释逻辑回归算法及其背后的数学原理。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es jt"><img src="../Images/d2ed39a1dee21cf2b2062b0fa545aff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*mQFvY74WcMgjrwFP5g3wHA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">代表二项式分类的Sigmoid曲线</figcaption></figure><p id="d3b7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归是机器学习的分类算法，其中输出变量是分类的。它属于监督学习方法，其中使用带有标签的过去数据来建立机器学习模型。</p><p id="b4b0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为什么叫逻辑回归而不是逻辑分类？实质上，逻辑回归模型输出与预测变量具有线性关系的概率(或logit形式的对数优势比)。当您将阈值附加到这些概率值时，它会将结果分类为1或0(二项式逻辑回归)。因此，即使逻辑回归是一种分类算法，它也有回归这个词。</p><p id="ccd1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上图所示的S形曲线是一条S形曲线。逻辑回归函数也称为sigmoid函数。逻辑回归函数的表达式为:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es jy"><img src="../Images/1595a1c7c9155afbbb7ef322ef0a054c.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*UO95vOImJER5Zhjp0NfmMQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">逻辑回归函数</figcaption></figure><p id="254c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中:</p><p id="5143" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> y = β0 + β1x(单变量Logistic回归情况下</strong></p><p id="5d0f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> y = β0 + β1x1 + β2x2 … +βnxn </strong>(多元逻辑回归情况下)</p><p id="f340" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">单变量逻辑回归意味着仅使用一个预测变量预测输出变量，而多变量逻辑回归意味着使用多个预测变量预测输出变量。</p><p id="d383" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归函数将<strong class="ix hz"> logits </strong>也称为<strong class="ix hz"> log-odds </strong>的值从<strong class="ix hz">∞到+∞ </strong>转换为介于<strong class="ix hz"> 0和1 </strong>之间的范围。</p><p id="acee" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们试着简化我们所说的。设<strong class="ix hz"> P </strong>为事件发生的概率。所以事件不会发生的概率是<strong class="ix hz"> 1-P. </strong></p><p id="6f95" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">几率</strong>定义为某一特定事件发生的概率与该事件不发生的概率之比。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es jy"><img src="../Images/e75affb07fa6143f69e6cf8946d27fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*mOejgk8PerUflFfHCGHCJQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">赔率表达式</figcaption></figure><p id="ab17" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们知道逻辑回归函数给我们概率值。所以我们可以写:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es jy"><img src="../Images/0a5a8c6ca838f590100a5a99c33aa284.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*9Gwp_xlUQDeR8Nf2pG7CgQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">逻辑回归函数的简化表达式</figcaption></figure><p id="9455" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在既然我们提到了<strong class="ix hz">对数赔率</strong>，让我们取<strong class="ix hz">赔率</strong>等式两边的自然对数，代入<strong class="ix hz"> P. </strong>的值</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es jz"><img src="../Images/ac466aed1cdc371c88d2e926c6c34166.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*9WDAnF12J1Cd7Eg2oZzdpQ.png"/></div></figure><p id="c984" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我们得到了更简化形式的逻辑回归函数方程，并且我们可以说<strong class="ix hz">对数优势</strong>与预测变量<strong class="ix hz"> x </strong>具有线性关系。</p><p id="2276" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">为什么逻辑回归优于线性回归？</strong></p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ka"><img src="../Images/e82c1a289aa253a0cb550b5b4c23d663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*d87HYkbxq_Y6WBaUDiclaw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">逻辑回归和线性回归的区别</figcaption></figure><p id="b950" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当涉及二项式分类(0/1)时，我们需要在被分类为0或1的值之间创建一个边界。在线性回归中，我们知道输出是一个连续变量，所以画一条直线来创建这个边界似乎是不可行的，因为值可能从∞到+∞。</p><p id="ea6f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于逻辑回归模型使用sigmoid函数输出概率，可以映射到0或1，因此在分类的情况下，它优于线性回归。</p><p id="132c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你想知道更多关于线性回归的工作原理，可以看看我写的关于线性回归的文章。</p><div class="hh hi ez fb hj kb"><a rel="noopener follow" target="_blank" href="/mlearning-ai/linear-regression-simple-explanation-with-example-fba51b2c181d"><div class="kc ab dw"><div class="kd ab ke cl cj kf"><h2 class="bd hz fi z dy kg ea eb kh ed ef hx bi translated">线性回归——简单举例说明！！</h2><div class="ki l"><h3 class="bd b fi z dy kg ea eb kh ed ef dx translated">我试图用最简单的方法和例子来解释线性回归。</h3></div><div class="kj l"><p class="bd b fp z dy kg ea eb kh ed ef dx translated">medium.com</p></div></div><div class="kk l"><div class="kl l km kn ko kk kp hp kb"/></div></div></a></div><div class="hh hi ez fb hj kb"><a rel="noopener follow" target="_blank" href="/mlearning-ai/the-mathematics-behind-linear-regression-fb4db1ebd7b5"><div class="kc ab dw"><div class="kd ab ke cl cj kf"><h2 class="bd hz fi z dy kg ea eb kh ed ef hx bi translated">线性回归背后的数学。</h2><div class="ki l"><h3 class="bd b fi z dy kg ea eb kh ed ef dx translated">在本文中，我将尽可能简单地解释与线性回归相关的各种数学概念…</h3></div><div class="kj l"><p class="bd b fp z dy kg ea eb kh ed ef dx translated">medium.com</p></div></div><div class="kk l"><div class="kq l km kn ko kk kp hp kb"/></div></div></a></div><h1 id="2c8a" class="kr ks hy bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">最大似然估计</h1><p id="acd6" class="pw-post-body-paragraph iv iw hy ix b iy lp ja jb jc lq je jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">为了让我们的模型预测输出变量为0或1，我们需要找到最佳拟合的sigmoid曲线，该曲线给出β系数的最佳值。也就是说，我们需要在0和1值之间创建一个有效的边界。</p><p id="ed8f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在一个成本函数告诉你你的值和实际值有多接近。因此，这里我们需要一个成本函数，最大化获得期望输出值的可能性。这样的代价函数被称为<strong class="ix hz">最大似然估计(MLE) </strong>函数。</p><p id="4152" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让这个代价函数表示为P(Y；z)。我们有一些由<strong class="ix hz"> Y </strong>表示的样本数据点。所以<strong class="ix hz"> Y </strong>代表‘n’个观察值，比如Y1，Y2…yn。我们需要找到这个未知参数<strong class="ix hz"> z，</strong>，使得观察到<strong class="ix hz"> Y </strong>的概率最大化。<strong class="ix hz"> z </strong>是决定变量取值为0还是1的偏差。</p><p id="2115" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">记住这是一个监督学习算法。所以我们会有实际观测值和预测观测值。我们的成本函数应该使预测值接近实际值的概率最大化。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lu"><img src="../Images/672d747e3a73b9d182f040fed748253f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Mos0Z3LecdpHE_zZBqomKA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">Sigmoid曲线和概率</figcaption></figure><p id="90af" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上图中，我们可以看到被分类为0或1的点以及与它们相关联的各自的概率。从P1到P7有7个点和7个相关概率。</p><p id="595c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了正确分类，对于0分，我们需要P1、P2和P4的概率尽可能小，对于1分，我们需要P3、P5、P6和P7的概率尽可能高。我们也可以说，(1-P1)，(1-P2)，P3，(1-P4)，P5，P6，P7应该尽可能的高。</p><p id="4f93" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">联合概率只不过是概率的乘积。所以乘积:[<strong class="ix hz">(1-P1)*(1-P2)* P3 *(1-P4)* P5 * P6 * P7]</strong>应该是最大值。</p><p id="001b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个联合概率函数只不过是我们的成本函数，为了得到最佳拟合的sigmoid曲线，应该将它最大化。或者我们可以说预测值接近实际值。</p><p id="6cbd" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在来看我们的成本函数，让<strong class="ix hz"> J(z) </strong>是<strong class="ix hz"> z </strong>的函数，使得</p><p id="005f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">J(z)= P(Y；z) = P(Y1，Y2…Yn；z) </strong></p><p id="94bd" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里的假设是所有的Y都是独立的。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lv"><img src="../Images/3c8c067f4f1dde7f9cfd829c634b6e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*nNJDb7RAUFQs-cfC3zsoRA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">成本函数是所有概率P(Yi)的乘积</figcaption></figure><p id="8c00" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">两面取自然原木:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lw"><img src="../Images/4309adec9333f93c1749adee6ef25370.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*RkSxc1Jy7Fm-SdqpqrkIzQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">记录两边的日志</figcaption></figure><p id="7eb9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为乘积的对数变成总和:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lx"><img src="../Images/73aafc8266f2e4c81c56dce1670b4ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*-jsrWK0EWuYUJvO51LFFfg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">乘积的对数是总和</figcaption></figure><p id="53ac" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">J(z)也可以写成L(z|Yi) (L为似然)。对于给定值<strong class="ix hz"> z </strong>和观察样本Yi，该函数给出观察样本值的概率。因此，如果Yi=1，表达式变为<strong class="ix hz"> z </strong>，如果Yi为0，表达式变为<strong class="ix hz"> 1-z </strong>:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ly"><img src="../Images/2edeb7a769f46edc38f14693b25f65e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C66A2pyIWPngvsujPGoRLw.png"/></div></div></figure><p id="ac8e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">进一步解这个方程，我们得到:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lz"><img src="../Images/4f151d025049dce19fd52f399b39c68c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PbkGaoBPa0p4rSwb86--tA.png"/></div></div></figure><p id="5cea" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将该方程相对于<strong class="ix hz"> z </strong>进行微分并将导数设置为零，我们使用封闭形式的解计算最大值:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/2d197e4191b291758ffdf741ec74e08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2__SCFIWGlE8u-uCWz-UFQ.png"/></div></div></figure><p id="16fb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">进一步求解，这个方程变成:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mb"><img src="../Images/31bc7473911b2cd8818682043ee51e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*958v37-ky9sn3IIx3i43eA.png"/></div></figure><p id="c330" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">右边的项表示1的数量与0的数量之比。因此，该函数在以下情况下达到最大值:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mc"><img src="../Images/219cc0887dcf704b28bb886e34b02caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*g6Udu_DIgfXgaA9QHobR1w.png"/></div></figure><p id="2913" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望你喜欢阅读这篇文章。请随意评论并给出您的反馈。</p><p id="89d5" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="md">参考文献:</em></p><ol class=""><li id="53fd" class="me mf hy ix b iy iz jc jd jg mg jk mh jo mi js mj mk ml mm bi translated"><a class="ae hv" href="https://faculty.washington.edu/ezivot/econ583/mleLectures.pdf" rel="noopener ugc nofollow" target="_blank"><em class="md">https://faculty . Washington . edu/ezivot/econ 583/mle lectures . pdf</em></a></li><li id="9d18" class="me mf hy ix b iy mn jc mo jg mp jk mq jo mr js mj mk ml mm bi translated"><a class="ae hv" href="https://stats.stackexchange.com/questions/127042/why-isnt-logistic-regression-called-logistic-classification" rel="noopener ugc nofollow" target="_blank"><em class="md">https://stats . stack exchange . com/questions/127042/why-nots-logistic-regression-called-logistic-class ification</em></a></li></ol><p id="fe63" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> <em class="md">可以在LinkedIn上联系我:</em></strong><a class="ae hv" href="https://www.linkedin.com/in/pathakpuja/" rel="noopener ugc nofollow" target="_blank"><strong class="ix hz"><em class="md">https://www.linkedin.com/in/pathakpuja/</em></strong></a></p><p id="fe3d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> <em class="md">请访问我的GitHub简介获取python代码:</em></strong><a class="ae hv" href="https://github.com/pujappathak" rel="noopener ugc nofollow" target="_blank"><strong class="ix hz"><em class="md">https://github.com/pujappathak</em></strong></a></p></div></div>    
</body>
</html>