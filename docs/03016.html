<html>
<head>
<title>Training Vision Transformers from Scratch for Malware Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为恶意软件分类从头开始训练视觉转换器</h1>
<blockquote>原文：<a href="https://medium.com/codex/training-vision-transformers-from-scratch-for-malware-classification-ccdae11d7236?source=collection_archive---------5-----------------------#2021-08-15">https://medium.com/codex/training-vision-transformers-from-scratch-for-malware-classification-ccdae11d7236?source=collection_archive---------5-----------------------#2021-08-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bb67749872b87f32312b6580d526b9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SwuZ1bTmd1ja8hdomCD99Q.png"/></div></div></figure><blockquote class="iq ir is"><p id="54a5" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一张图片值16x16个字，一个恶意软件值多少？也许一个恶意软件值66x66的图像。</p></blockquote></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="3e56" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"># 1.背景</h1><p id="b230" class="pw-post-body-paragraph it iu hi iw b ix kx iz ja jb ky jd je kz la jh ji lb lc jl jm ld le jp jq jr hb bi translated"><strong class="iw hj">任务&amp;数据集描述</strong></p><p id="e233" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">最近，在<a class="ae lf" href="https://challenge.xfyun.cn/topic/info?type=malware-classification" rel="noopener ugc nofollow" target="_blank">2021 iFLYTEK a . I . Developer Challenge</a>中启动了恶意软件分类追踪。竞赛提供已知的恶意软件数据，并要求参赛者预测每个恶意软件样本数据所属的类别(家族)。这是一个包含9个恶意软件类别的多类别问题，由0到8标识。</p><p id="bcca" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">比赛数据由训练集和测试集组成，总数据量超过10w，包含70个字段，其中id是每个样本的唯一标识符，标签是样本所属的恶意软件类别。为了保证比赛的公平性，选取了5万个样本作为训练集，8000个样本作为测试集，并对部分领域进行了脱敏处理。特别是特征字段主要是asm文件信息，比如“line_count_asm”表示asm文件的行数，“size_asm”表示asm文件的大小，其余关于asm的特征字段都以“asm_commands”为前缀，理解为asm中的一个命令。与2015年<a class="ae lf" href="https://www.kaggle.com/c/malware-classification/overview" rel="noopener ugc nofollow" target="_blank">微软恶意软件分类挑战</a>不同，这个数据集只有操作码词频和文件大小信息，即我只能使用静态特征进行分析和建模。评估指标是准确性参考来自<a class="ae lf" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html" rel="noopener ugc nofollow" target="_blank"> sklearn.metrics </a>。</p><p id="af95" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">这里只是简单的展示类的分布。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/ad091e7ebcc420feacdb06e3e825fcc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*hopsV4uBtDmclrU7pa6mHg.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">训练数据集中类的分布</figcaption></figure><p id="1fe6" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">很明显0，1，2类的样本比其他的少很多。</p><p id="bcc2" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated"><strong class="iw hj">动机</strong></p><p id="89e1" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">如今，神经网络方法已经达到了可能超过以前机器学习方法的极限的水平，大多数基于图像的恶意软件分类技术[1]都是用卷积神经网络(CNN)实现的。它巧妙地将恶意软件分类问题转移到图像分类问题。然而，<a class="ae lf" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">Vision Transformer(ViT)</a>【2】将Transformer架构的应用从自然语言处理扩展到计算机视觉，已经在许多计算机视觉基准测试中逐渐获得了最先进的结果，并且已经成为现有CNN架构的替代方案。</p><p id="7ae9" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">受同一家族的恶意软件样本之间的视觉相似性和ViT在视觉任务上的成功的激励，我们提出了MalwareViT用于将视觉变形器应用于恶意软件分类，这是一种基于从Asm作为图像提取的操作码频率获得的共生矩阵的文件不可知深度学习方法，以有效地将恶意软件分组到家族中。</p><p id="2170" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">在下文中，我将通过应用ViT来介绍基于操作码频率作为图像的恶意软件分类方法。</p><h1 id="6ca5" class="jz ka hi bd kb kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw bi translated"># 2.恶意软件图像生成</h1><p id="d977" class="pw-post-body-paragraph it iu hi iw b ix kx iz ja jb ky jd je kz la jh ji lb lc jl jm ld le jp jq jr hb bi translated">在ViT中，一幅图像相当于16x16的文字，同样，恶意软件在MalwareViT中相当于66x66的图像。</p><p id="2c49" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">加工步骤如下。在计算了恶意二进制文件反编译得到的66个操作码的词频后，我们将它们按总频率的升序排序，归一化到0到255的区间，将其视为像素值，并将其水平和垂直各排列成一列，形成二维数组。由于一些<a class="ae lf" href="https://vx-underground.org/archive/other/VxHeavenPdfs/Opcodes%20as%20Predictor%20for%20Malware.pdf" rel="noopener ugc nofollow" target="_blank">研究</a> [3]表明词频越小，越有利于区分恶意软件，所以我进行了一个“逆频”运算，即255/(freq+1)，使操作码频率越小，灰度值越大。另外，矩阵中的列和行的交集的值被取为它们之间的最大值，以获得共生矩阵。最后，我们将这些矩阵保存为大小为66x66的图像。</p><p id="a6e8" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">特征处理是最重要的步骤之一。基于卷积或注意力机制的属性构建特征对深度学习模型具有重要影响。这里使用局部操作码补丁和全局位置分布信息来构造图像特征，生成的图像如下。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/9a643b50151ae3579f1aa12e4ee32a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y4NbTEzgFPX4dsFw1pWFvg.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">恶意软件图像和标签</figcaption></figure><h1 id="4a93" class="jz ka hi bd kb kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw bi translated"># 3.ViT概述</h1><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/4a9ea5f1a489ead9baf92b8d7831bfab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5dLl6ZeNUBxWvKzWFkOEg.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">ViT型号概述</figcaption></figure><p id="be67" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">上图(来自<a class="ae lf" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">论文</a>【3】)展示了视觉变压器是如何工作的。</p><p id="2e6d" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">在论文中，他们提出了一种不关注像素而是关注图像小区域的方法。通过使用线性投影矩阵来展平输入图像中的每个颜色块，并且向其添加位置嵌入。这是必要的，因为转换器处理所有的输入而不考虑顺序，所以拥有这个位置信息有助于模型正确地评估注意力的权重。附加的类标签作为分类任务中要预测的类的占位符连接到输入(图像中的位置0)。我们可以把它作为全球信息的补充。</p><p id="5fcf" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">该代码基于Keras网站上使用ViT 的示例<a class="ae lf" href="https://keras.io/examples/vision/image_classification_with_vision_transformer/" rel="noopener ugc nofollow" target="_blank">图像分类。</a></p><div class="lw lx ez fb ly lz"><a href="https://keras.io/examples/vision/image_classification_with_vision_transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab dw"><div class="mb ab mc cl cj md"><h2 class="bd hj fi z dy me ea eb mf ed ef hh bi translated">Keras文档:使用视觉转换器进行图像分类</h2><div class="mg l"><h3 class="bd b fi z dy me ea eb mf ed ef dx translated">Keras文档</h3></div><div class="mh l"><p class="bd b fp z dy me ea eb mf ed ef dx translated">:使用Vision Transformer Keras documentation Keras . io进行图像分类</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn io lz"/></div></div></a></div><h1 id="b7ef" class="jz ka hi bd kb kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw bi translated"># 3.实现MalwareViT</h1><p id="6cb6" class="pw-post-body-paragraph it iu hi iw b ix kx iz ja jb ky jd je kz la jh ji lb lc jl jm ld le jp jq jr hb bi translated"><strong class="iw hj">进口包</strong></p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="ffed" class="mt ka hi mp b fi mu mv l mw mx"><em class="iv"># environment: Colab Tensorflow 2.5.0</em><br/><em class="iv"># !pip install -U tensorflow-addons</em><br/>import tensorflow as tf<br/>print(tf.__version__)<br/>import os<br/>import sys<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from tensorflow import keras<br/>from tensorflow.keras import layers<br/>import tensorflow_addons as tfa<br/><br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/><br/>import random<br/>SEED=42<br/>os.environ['PYTHONHASHSEED']=str(SEED)<br/>os.environ["CUDA_VISIBLE_DEVICES"] = "0"<br/>random.seed(SEED)<br/>np.random.seed(SEED)<br/>tf.random.set_seed(SEED)</span></pre><p id="b407" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated"><strong class="iw hj">准备数据</strong></p><p id="8c36" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">使用ImageDataGenerator从包含图像类别的文件夹中导入数据</p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="7682" class="mt ka hi mp b fi mu mv l mw mx">path = './dataset/'<br/>train_path = path+'/train'<br/>img_width, img_height = 66,66<br/>img_size = (img_width, img_height)<br/>def img_data_gen(imgs_path,img_size,batch_size,rescale,shuffle=False):<br/>    return ImageDataGenerator(rescale=rescale).flow_from_directory(imgs_path,target_size=img_size,batch_size=batch_size,class_mode='categorical',shuffle=shuffle)<br/><br/>train_gen = img_data_gen(imgs_path=train_path,img_size=img_size,batch_size=50000,rescale=1. / 255,shuffle=True)<br/><br/>imgs, labels = next(train_gen)<br/>print(f"imgs.shape:{imgs.shape},labels.shape:{labels.shape}")</span></pre><p id="c538" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">将用于模型训练和验证的数据除以0.25的测试规模</p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="cbd5" class="mt ka hi mp b fi mu mv l mw mx">from sklearn.model_selection import train_test_split<br/>X_train, X_val, y_train, y_val = train_test_split(imgs,labels, test_size=0.25,stratify=labels, random_state=SEED)<br/>print(f"X_train.shape:{X_train.shape},X_val.shape:{X_val.shape}")<br/>print(f"y_train.shape:{y_train.shape},y_val.shape:{y_val.shape}")</span></pre><p id="8932" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated"><strong class="iw hj">配置超参数</strong></p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="5a16" class="mt ka hi mp b fi mu mv l mw mx">num_classes = labels.shape[1]<br/>input_shape = imgs.shape[1:]<br/><br/>learning_rate = 0.0005<br/>weight_decay = 0.0001<br/>batch_size = 256<br/>num_epochs = 150<br/>patience = 30 # After patience epoch stop if not improve<br/>image_size = 66  # We'll resize input images to this size<br/>patch_size = 11  # Size of the patches to be extract from the input images<br/>num_patches = (image_size // patch_size) ** 2<br/># Here input_shape=(66,66),patch shape=(11,11) -&gt; 36 patches<br/>projection_dim = 36<br/>num_heads = 6<br/>transformer_units = [<br/>    projection_dim * 2,<br/>    projection_dim,<br/>]  # Size of the transformer layers<br/>transformer_layers = 8<br/>mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier</span></pre><p id="6015" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated"><strong class="iw hj"> ViT建模</strong></p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="0cfd" class="mt ka hi mp b fi mu mv l mw mx">def mlp(x, hidden_units, dropout_rate):<br/>    for units in hidden_units:<br/>        x = layers.Dense(units, activation=tf.nn.gelu)(x)<br/>        x = layers.Dropout(dropout_rate)(x)<br/>    return x<br/><br/>class Patches(layers.Layer):<br/>    def __init__(self, patch_size):<br/>        super(Patches, self).__init__()<br/>        self.patch_size = patch_size<br/><br/>    def call(self, images):<br/>        batch_size = tf.shape(images)[0]<br/>        patches = tf.image.extract_patches(<br/>            images=images,<br/>            sizes=[1, self.patch_size, self.patch_size, 1],<br/>            strides=[1, self.patch_size, self.patch_size, 1],<br/>            rates=[1, 1, 1, 1],<br/>            padding="VALID",<br/>        )<br/>        patch_dims = patches.shape[-1]<br/>        patches = tf.reshape(patches, [batch_size, -1, patch_dims])<br/>        return patches<br/>    # refer: https://stackoverflow.com/questions/58678836/notimplementederror-layers-with-arguments-in-init-must-override-get-conf<br/>    def get_config(self):<br/>        config = super().get_config().copy()<br/>        config.update({<br/>            'patch_size': self.patch_size,<br/>        })<br/>        return config<br/><br/>class PatchEncoder(layers.Layer):<br/>    def __init__(self, num_patches, projection_dim):<br/>        super(PatchEncoder, self).__init__()<br/>        self.num_patches = num_patches<br/>        self.projection = layers.Dense(units=projection_dim)<br/>        self.position_embedding = layers.Embedding(<br/>            input_dim=num_patches, output_dim=projection_dim<br/>        )<br/><br/>    def call(self, patch):<br/>        positions = tf.range(start=0, limit=self.num_patches, delta=1)<br/>        encoded = self.projection(patch) + self.position_embedding(positions)<br/>        return encoded<br/>    def get_config(self):<br/>        config = super().get_config().copy()<br/>        config.update({<br/>            'num_patches': self.num_patches,<br/>            'projection': self.projection,<br/>            'position_embedding': self.position_embedding,<br/>        })<br/>        return config<br/><br/>def create_vit_classifier():<br/>    inputs = layers.Input(shape=input_shape)<br/>    # Augment data.<br/>    # augmented = data_augmentation(inputs)<br/>    # Create patches.<br/>    patches = Patches(patch_size)(inputs)<br/>    # Encode patches.<br/>    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)<br/><br/>    # Create multiple layers of the Transformer block.<br/>    for _ in range(transformer_layers):<br/>        # Layer normalization 1.<br/>        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)<br/>        # Create a multi-head attention layer.<br/>        attention_output = layers.MultiHeadAttention(<br/>            num_heads=num_heads, key_dim=projection_dim, dropout=0.1<br/>        )(x1, x1)<br/>        # Skip connection 1.<br/>        x2 = layers.Add()([attention_output, encoded_patches])<br/>        # Layer normalization 2.<br/>        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)<br/>        # MLP.<br/>        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)<br/>        # Skip connection 2.<br/>        encoded_patches = layers.Add()([x3, x2])<br/><br/>    # Create a [batch_size, projection_dim] tensor.<br/>    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)<br/>    representation = layers.Flatten()(representation)<br/>    representation = layers.Dropout(0.5)(representation)<br/>    # Add MLP.<br/>    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)<br/>    logits = layers.Dense(num_classes)(features)<br/>    model = keras.Model(inputs=inputs, outputs=logits)<br/>    return model</span></pre><p id="e91c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated"><strong class="iw hj">可视化补丁</strong></p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="5336" class="mt ka hi mp b fi mu mv l mw mx">Image size: 66 X 66<br/>Patch size: 11 X 11<br/>Patches per image: 36<br/>Elements per patch: 363</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es my"><img src="../Images/f475a4504fbe885e6f485b0f6cdfd57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*xBPOqyagxR5TaDlt4W9wDg.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">尺寸为66x66的原始图像</figcaption></figure><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/3de316c94af0e1f3f73ebcb05c72aebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*Z-g_HlK7qwCh1OPAw8PC8g.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">36块大小为11x11的补丁</figcaption></figure><p id="ad9f" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated"><strong class="iw hj">训练、评估和预测</strong></p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="ca9b" class="mt ka hi mp b fi mu mv l mw mx">def run_experiment(model):<br/>    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)<br/>    model.compile(<br/>        optimizer=optimizer,<br/>        loss=keras.losses.CategoricalCrossentropy(from_logits=True),<br/>        metrics=[<br/>            keras.metrics.CategoricalAccuracy(name="accuracy"),<br/>            keras.metrics.TopKCategoricalAccuracy(5, name="top-5-accuracy"),<br/>        ],<br/>    )<br/>    model_name = "keras_trained_MalwareViT.h5"<br/>    log_dir = os.path.join(os.getcwd(), 'logs')<br/>    filepath='ViT.{epoch:02d}-{val_loss:.4f}.h5'<br/>    ck_path = os.path.join(log_dir, filepath)<br/>    if not os.path.isdir(log_dir):<br/>      os.makedirs(log_dir)<br/>    mc = keras.callbacks.ModelCheckpoint(ck_path, monitor='val_loss',save_best_only=True,save_weights_only=True)<br/>    es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=0) # when patience epoch val_loss not improve, stop train<br/>    # tb = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)<br/>    callbacks = [es, mc]<br/><br/>    history = model.fit(<br/>        x=X_train,<br/>        y=y_train,<br/>        batch_size=batch_size,<br/>        epochs=num_epochs,<br/>        validation_data=(X_val, y_val),<br/>        # validation_split=0.1,<br/>        # shuffle=True,<br/>        callbacks=callbacks,<br/>    )<br/>    # To see history keys for visualization<br/>    print(history.history.keys())<br/><br/>    # save model and weight<br/>    # model_path = os.path.join(log_dir, model_name)<br/>    # model.save(model_path)<br/><br/>    return history<br/><br/>vit_classifier = create_vit_classifier()<br/><br/># show and save <!-- -->model structure<br/># vit_classifier.summary()<br/># keras.utils.plot_model(vit_classifier, show_shapes=True)<br/><br/># train<br/>history = run_experiment(vit_classifier)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/dd7774415c03e4e30c58bde20ad7147f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vOBLcMllBGCsSYAsJ2hMrA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">损耗和精度曲线</figcaption></figure><p id="2b58" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">评估</p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="1e3b" class="mt ka hi mp b fi mu mv l mw mx"># load best model<br/>vit_classifier.load_weights("/content/logs/best_ViT.h5")<br/><br/>_, accuracy, top_5_accuracy = vit_classifier.evaluate(X_val, y_val)<br/>print(f"Test accuracy: {round(accuracy * 100, 2)}%")<br/>print(f"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%")</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/0179583232437512d9865591ec6726fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lV4NNe_e8o3ddgRF.png"/></div></div></figure><p id="4743" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">预测</p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="19a7" class="mt ka hi mp b fi mu mv l mw mx">sub_path = "./dataset/test"<br/>sub_gen = img_data_gen(imgs_path=sub_path,img_size=img_size,batch_size=1,rescale=1. / 255)<br/>sub_pred = vit_classifier.predict(sub_gen)<br/>sub_pred_class = np.argmax(sub_pred, axis = 1)</span></pre><p id="0fa8" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">使服从</p><pre class="lh li lj lk fd mo mp mq mr aw ms bi"><span id="baf5" class="mt ka hi mp b fi mu mv l mw mx">df_sub = pd.read_csv("./dataset/sample_submit.csv")<br/>df_sub['label'] = sub_pred_class<br/>df_sub.to_csv("submit_MalwareViT.csv",index=False)</span></pre><h1 id="0a09" class="jz ka hi bd kb kc lp ke kf kg lq ki kj kk lr km kn ko ls kq kr ks lt ku kv kw bi translated"># 5.改进建议</h1><p id="664d" class="pw-post-body-paragraph it iu hi iw b ix kx iz ja jb ky jd je kz la jh ji lb lc jl jm ld le jp jq jr hb bi translated">由于这里的ViT实现不使用预训练，如果你想获得更高的精度，可以尝试训练更多轮次，使用更深的层，改变输入图像大小，改变面片大小，增加投影维度，还可以考虑改变学习速率，切换到优化器，使用权重衰减等训练策略。</p><p id="fb2d" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">因为图像相对简单，太复杂的模型往往会过度拟合。当然，这个数据集和15年的微软恶意软件分类数据集很可能是同源的，人们也可以考虑从大型的微软恶意软件分类数据集进行训练，以获得预训练的模型，然后在这个数据集上进行微调可能效果很好。</p><p id="ac1e" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">在<a class="ae lf" href="https://www.sciencedirect.com/science/article/pii/S1389128619304736" rel="noopener ugc nofollow" target="_blank">论文</a>【4】中已经表明，在ImageNet上使用几个经典CNN架构的预训练模型进行微调也可以产生出色的结果，其中使用彩色图像，并且使用图像处理方法的图像数据增强来提高模型的稳健性，这与灰度图像相比更好。(在论文实验中，在基于灰度的模型的比较中，ResNet50作为预训练模型工作得最好。我随后尝试了基于几个经典CNN架构的预训练方案，它真的很好，MobileNetV2训练得更快更好，但都很容易过度适应)</p><p id="be8c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">从文献[4]中，我们还得到了一些启示，我们可以通过引入基于代码级混淆技术的对抗训练来提高模型的健壮性，如死代码插入、代码转置、寄存器重分配、指令替换等。在这种情况下，可以通过随机增加总体操作码词频、随机增加jmp和调用的数量等来操纵表格数据集。可以通过增加灰度值、改变亮度、整体缩放等来操纵图像数据。</p><p id="dc0c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">除了表格类的数据可以使用树模型，图像数据使用神经网络模型，然后进行模型融合，还可以把这个分类问题做为序列分类，简单的把一列词频特征组合成一列，和NLP方向的思路一样，会根据词频逆向得到asm文档，然后嵌入，作为文本分类任务。</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><div class="lh li lj lk fd lz"><a href="https://github.com/rickyxume/MalwareViT" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab dw"><div class="mb ab mc cl cj md"><h2 class="bd hj fi z dy me ea eb mf ed ef hh bi translated">GitHub - rickyxume/MalwareViT:从零开始为恶意软件分类训练视觉转换器</h2><div class="mg l"><h3 class="bd b fi z dy me ea eb mf ed ef dx translated">一张图片值16x16个字，一个恶意软件值多少？…</h3></div><div class="mh l"><p class="bd b fp z dy me ea eb mf ed ef dx translated">github.com</p></div></div><div class="mi l"><div class="nc l mk ml mm mi mn io lz"/></div></div></a></div><p id="4721" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">这里是我的github资源库，只是抛砖引玉，欢迎点击⭐.如果您有任何问题，请随时与我沟通。</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="6424" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">参考</h1><p id="90bc" class="pw-post-body-paragraph it iu hi iw b ix kx iz ja jb ky jd je kz la jh ji lb lc jl jm ld le jp jq jr hb bi translated">[1] Nataraj L，Karthikeyan S，Jacob G，等.恶意软件图像:可视化和自动分类.美国计算机学会，2011年。<a class="ae lf" href="https://dl.acm.org/doi/10.1145/2016904.2016908" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/doi/10.1145/2016904.2016908</a></p><p id="6802" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">[2] Dosovitskiy A，Beyer L，科列斯尼科夫A，等.一幅图像相当于16x16个字:大规模图像识别的变形金刚[J].2020.https://arxiv.org/abs/2010.11929<a class="ae lf" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"/></p><p id="c8c5" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">[3] Bilar D .操作码作为恶意软件的预测器[J].国际电子安全和数字取证杂志，2007，1(2):156–168。<a class="ae lf" href="https://vx-underground.org/archive/other/VxHeavenPdfs/Opcodes%20as%20Predictor%20for%20Malware.pdf" rel="noopener ugc nofollow" target="_blank">https://VX-underground . org/archive/other/VxHeavenPdfs/Opcodes % 20 as % 20 predictor % 20 for % 20 malware . pdf</a></p><p id="e743" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je kz jg jh ji lb jk jl jm ld jo jp jq jr hb bi translated">[4]瓦桑D，阿拉扎布M，瓦桑S，等. IMCFN:基于图像的恶意软件分类用微调卷积神经网络体系结构[J].计算机网络，2020，171:107138。<a class="ae lf" href="https://www.sciencedirect.com/science/article/pii/S1389128619304736" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/pii/s 1389128619304736</a></p></div></div>    
</body>
</html>