<html>
<head>
<title>Pyspark RDD</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">派斯帕克·RDD</h1>
<blockquote>原文：<a href="https://medium.com/codex/pyspark-for-beginners-part-4-pyspark-rdd-7b5587347b4c?source=collection_archive---------8-----------------------#2022-08-15">https://medium.com/codex/pyspark-for-beginners-part-4-pyspark-rdd-7b5587347b4c?source=collection_archive---------8-----------------------#2022-08-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="e589" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">弹性分布式数据集(rdd)是Pyspark的基本构建块，Pyspark是一种分布式内存抽象，可帮助程序员在大型集群上以容错方式执行内存计算。在本教程中，我们将重点介绍，RDD的特点，对RDD，RDD和其他概念的转换和行动。</p></blockquote><p id="8582" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">要了解更多关于Pyspark的知识并成为一名认证的Pyspark开发人员，请阅读第1部分:Pyspark初学者。</p><div class="jk jl ez fb jm jn"><a href="https://blog.devgenius.io/pyspark-for-beginners-part-1-introduction-638fb16c5092" rel="noopener  ugc nofollow" target="_blank"><div class="jo ab dw"><div class="jp ab jq cl cj jr"><h2 class="bd hj fi z dy js ea eb jt ed ef hh bi translated">Pyspark初学者|第1部分:简介</h2><div class="ju l"><h3 class="bd b fi z dy js ea eb jt ed ef dx translated">PySpark是Apache Spark的Python API。使用PySpark，我们可以在分布式集群上并行运行应用程序…</h3></div><div class="jv l"><p class="bd b fp z dy js ea eb jt ed ef dx translated">blog.devgenius.io</p></div></div><div class="jw l"><div class="jx l jy jz ka jw kb kc jn"/></div></div></a></div><p id="319b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">RDD是一个类似于Python中的列表的对象集合，不同之处在于RDD是在分布于多个物理服务器(也称为集群中的节点)的多个进程上计算的，而Python集合只在一个进程中运行和处理。默认情况下，它提供并行性。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/bceb77098426551f27fa694c0953a4e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*syoD_A3UVVvkcaQr.png"/></div></div></figure><p id="808d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">rdd主要用于I/O操作，因为在内存中，通过rdd共享数据要快10-100倍。</p><p id="cfca" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">RDD的特色:</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ko"><img src="../Images/80cbd6cae9cf0c784a36ed0276095453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L7QBAWAGAnCNZhMshahjaQ.png"/></div></div></figure><p id="a8e9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">创造RDD </strong></p><p id="88b0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">可以从sparkSession的sparkContext中使用parallelize()方法创建RDD。在2.0版本之后，sparkSession可以用来创建sparkContext。</p><p id="8373" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">让我们首先创建一个sparkSession</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="fed9" class="ku kv hi kq b fi kw kx l ky kz">from pyspark.sql import SparkSession</span><span id="2cf0" class="ku kv hi kq b fi la kx l ky kz">spark = SparkSession.builder.appName("Practice").getOrCreate()</span></pre><p id="aaa9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">创建sparkSession后，让我们创建RDD</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="26a5" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize(["chandu",1,"rohith",2,"minu",3,"karthik",4])</span></pre><p id="2566" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">这里，我们使用并行化方法来创建RDD</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lb"><img src="../Images/ee033f641d9c2c691b5e061cbabb1df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*G9JstEgZ01l-WGyK.png"/></div></div></figure><p id="b581" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">该函数将驱动程序中的现有集合加载到并行化RDD中。对于生产应用程序，我们通常使用外部存储系统来创建RDD，如<code class="du lc ld le kq b">HDFS</code>、<code class="du lc ld le kq b">S3</code>、<code class="du lc ld le kq b">HBase</code>等</p><ol class=""><li id="d7bf" class="lf lg hi il b im in iq ir jh lh ji li jj lj jg lk ll lm ln bi translated">使用并行化方法:</li></ol><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="8137" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize(["chandu",1,"rohith",2,"minu",3,"karthik",4])</span><span id="a97c" class="ku kv hi kq b fi la kx l ky kz">myRDD.take(4)</span><span id="aac5" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># ['chandu', 1, 'rohith', 2, 'minu', 3, 'karthik', 4]</span></pre><p id="b96e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">2.使用文本文件-要从. txt文件创建RDD，我们将使用sparkContext中的文本文件</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="8bc1" class="ku kv hi kq b fi kw kx l ky kz">New_RDD = spark.sparkContext.textFile("Sample.txt")</span></pre><p id="d234" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">3.使用CSV文件-创建RDD。我们将使用的csv文件。地图转换，我们将在后面讨论。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="6880" class="ku kv hi kq b fi kw kx l ky kz">CSV_RDD = (spark.sparkContext.textFile("file:///home/edureka/Downloads/fifa_players.csv".map(lambda element: element.split(",")))</span></pre><p id="c4e9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">对于本文，我们将使用通过并行化方法创建的myRDD</p><h1 id="e72e" class="lo kv hi bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated"><strong class="ak"> pySpark RDD行动:</strong></h1><p id="66b9" class="pw-post-body-paragraph ii ij hi il b im ml io ip iq mm is it jh mn iw ix ji mo ja jb jj mp je jf jg hb bi translated">动作是在RDD上应用的操作，用于指示Apache Spark应用计算并将结果传递回驱动程序。</p><p id="2d8d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。take(n) — </strong>从RDD返回前n个元素</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="a076" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize(["chandu",1,"rohith",2,"minu",3,"karthik",4])</span><span id="90ab" class="ku kv hi kq b fi la kx l ky kz">myRDD.take(4)</span><span id="a929" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># ['chandu', 1, 'rohith', 2]</span></pre><p id="0c37" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。collect() — </strong>用于返回RDD的所有元素。一般来说，当数据较多时，从驱动程序加载所有数据并返回到控制台是非常繁忙的。所以不使用collect()，而是使用take()方法。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="d211" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize(["chandu",1,"rohith",2,"minu",3,"karthik",4])</span><span id="b9df" class="ku kv hi kq b fi la kx l ky kz">myRDD.collect()</span><span id="5c5a" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># ["chandu",1,"rohith",2,"minu",3,"karthik",4]</span></pre><p id="6ef7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。count() </strong> —用于获取RDD中元素的数量</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="8d80" class="ku kv hi kq b fi kw kx l ky kz">myRDD.count()</span><span id="8b40" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># 8</span></pre><p id="d398" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">。<strong class="il hj"> first() </strong> —用于返回RDD的第一个元素</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="9183" class="ku kv hi kq b fi kw kx l ky kz">myRDD.first()</span><span id="8964" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># "chandu"</span></pre><p id="0467" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。top(n) </strong> —用于返回RDD中最大的n个元素</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="43f2" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4])<br/>myRDD.top(2)</span><span id="15f4" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># [4, 3]</span></pre><p id="68c9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。min()，。max() </strong> —用于返回RDD的最小、最大元素</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="8bef" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4])<br/>print(myRDD.min(),myRDD.max())</span><span id="15c7" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># 1 4</span></pre><p id="7f50" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。reduce()</strong>-使用指定的二元运算符减少数据集的元素。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="1291" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4])</span><span id="0567" class="ku kv hi kq b fi la kx l ky kz">add = lambda x,y: x + y<br/>myRDD.reduce(add)</span><span id="28e6" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># 10</span></pre><p id="6bc8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。fold() </strong> —类似于reduce，但reduce和fold的区别在于，reduce取第一个和第二个元素，而fold取第一个元素。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="4466" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4])</span><span id="670a" class="ku kv hi kq b fi la kx l ky kz">from operator import add</span><span id="5602" class="ku kv hi kq b fi la kx l ky kz">myRDD.fold(0, add)</span><span id="2922" class="ku kv hi kq b fi la kx l ky kz"># Output: 10</span></pre><p id="b944" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。saveAsTextFile() </strong> —用于将生成的RDD作为文本文件提供。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="cf02" class="ku kv hi kq b fi kw kx l ky kz">myRDD.saveAsTextFile('file.txt')</span></pre><p id="773d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。getNumPartitions() — </strong>返回分区的数量</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="9e48" class="ku kv hi kq b fi kw kx l ky kz">myRDD.getNumPartitions()</span><span id="1167" class="ku kv hi kq b fi la kx l ky kz"># Output<br/># 2</span></pre><h1 id="9bfe" class="lo kv hi bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated"><strong class="ak">派斯帕克RDD变换:</strong></h1><p id="33f6" class="pw-post-body-paragraph ii ij hi il b im ml io ip iq mm is it jh mn iw ix ji mo ja jb jj mp je jf jg hb bi translated">PySpark RDD变换是惰性求值，用于从一个RDD变换到另一个。由于RDD在本质上是不可变的，转换总是创建新的RDD而不更新现有的，因此，RDD转换链创建了RDD谱系或RDD算子图，或RDD依赖图。</p><p id="7038" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。map() </strong> —转换接受一个匿名函数，并将该函数应用于RDD中的每个元素</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="e09b" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4])</span><span id="90a0" class="ku kv hi kq b fi la kx l ky kz">myRDD = myRDD.map(lambda x: x + 1)<br/>myRDD.collect()</span><span id="4b32" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># [2, 4, 3, 5]</span></pre><p id="db25" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。filter() </strong> —转换接受一个匿名函数，用于过滤出满足函数中特定条件的元素。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="9f12" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4])</span><span id="9c21" class="ku kv hi kq b fi la kx l ky kz">myRDD = myRDD.filter(lambda x: x&gt;2)<br/>myRDD.collect()</span><span id="0bc8" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># [3,4]</span></pre><p id="6338" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。distinct() </strong> —此转换返回RDD中的不同元素</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="24e7" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4,3,5,2,4])</span><span id="61f6" class="ku kv hi kq b fi la kx l ky kz">myRDD = myRDD.distinct()<br/>myRDD.collect()</span><span id="b9f6" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># [2, 4, 1, 3, 5]</span></pre><p id="e65b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。union() </strong> —合并两个RDD并返回一个RDD</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="fab5" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4])</span><span id="21b2" class="ku kv hi kq b fi la kx l ky kz">myRDD1 = myRDD.filter(lambda x: x&gt;2)  # [3,4]</span><span id="6ab3" class="ku kv hi kq b fi la kx l ky kz">myRDD2 = myRDD.filter(lambda x: x%2==0) # [2,4]</span><span id="ca10" class="ku kv hi kq b fi la kx l ky kz">unionRDD = myRDD1.union(myRDD2)<br/>unionRDD.collect()</span><span id="d246" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># [2,4,3,4]</span></pre><p id="43cb" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。flatMap() </strong> —执行与<strong class="il hj">相同。map() </strong>转换并返回原始RDD中每个元素的单独值。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="102e" class="ku kv hi kq b fi kw kx l ky kz">flatmap_rdd = spark.sparkContext.parallelize(["Hey there", "This is PySpark RDD"])</span><span id="f54c" class="ku kv hi kq b fi la kx l ky kz">flatmap_rdd.flatMap(lambda x: x.split(" ")).collect()</span><span id="d4de" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># ['Hey', 'there', 'This', 'is', 'PySpark', 'RDD']</span></pre><p id="8391" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。重新分区(n) </strong> —在RDD上创建n个分区</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="fea9" class="ku kv hi kq b fi kw kx l ky kz">myRDD = spark.sparkContext.parallelize([1,3,2,4])</span><span id="0548" class="ku kv hi kq b fi la kx l ky kz">print("Number of Partitions Before: ",myRDD.getNumPartitions())</span><span id="d49a" class="ku kv hi kq b fi la kx l ky kz">myRDD = myRDD.repartition(3)</span><span id="f12c" class="ku kv hi kq b fi la kx l ky kz">print("Number of Partitions Before: ",myRDD.getNumPartitions())</span><span id="9a2e" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># Number of Partitions Before:  2 <br/># Number of Partitions Before:  3</span></pre><p id="8c6a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。coalesce() </strong> —当我们想要减少分区时，类似于repartition by操作更好。与所有节点相比，通过重新分区，从更少的节点对数据进行重新排序，可以实现更好的性能。</p><h1 id="70d8" class="lo kv hi bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">PySpark对RDD操作</h1><p id="757d" class="pw-post-body-paragraph ii ij hi il b im ml io ip iq mm is it jh mn iw ix ji mo ja jb jj mp je jf jg hb bi translated">Pair RDDs是PySpark中一种特殊的数据结构，采用键值对的形式。键被称为标识符，而值被称为数据。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="dd78" class="ku kv hi kq b fi kw kx l ky kz">marks = [('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78)]</span><span id="905d" class="ku kv hi kq b fi la kx l ky kz">pairRDD = spark.sparkContext.parallelize(marks)</span></pre><p id="3622" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。countByKey() </strong>操作— <strong class="il hj"> </strong>返回基于键值对的成对RDD的计数</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="86e5" class="ku kv hi kq b fi kw kx l ky kz">marks_rdd = spark.sparkContext.parallelize([('Rahul', 25), ('Swati', 26), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])</span><span id="b028" class="ku kv hi kq b fi la kx l ky kz">dict_rdd = marks_rdd.countByKey()</span><span id="15a3" class="ku kv hi kq b fi la kx l ky kz">dict(dict_rdd)</span><span id="533f" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># {'Abhay': 1, 'Rahul': 2, 'Rohan': 2, 'Shreya': 1, 'Swati': 2}</span></pre><p id="b148" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。reduceByKey() </strong>转换-将每个键的值与指定的函数合并。</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="c1c3" class="ku kv hi kq b fi kw kx l ky kz">marks_rdd = spark.sparkContext.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])</span><span id="ae92" class="ku kv hi kq b fi la kx l ky kz">print(marks_rdd.reduceByKey(lambda x, y: x + y).collect())</span><span id="9085" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># [('Rahul', 48), ('Shreya', 50), ('Rohan', 44), ('Swati', 45), ('Abhay', 55)]</span></pre><p id="6b2d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。sortByKey() </strong>转换-根据关键字对RDD元素进行排序</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="3555" class="ku kv hi kq b fi kw kx l ky kz">marks_rdd = spark.sparkContext.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])</span><span id="3106" class="ku kv hi kq b fi la kx l ky kz">print(marks_rdd.sortByKey('ascending').collect())</span><span id="f6f0" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/># [('Abhay', 29), ('Abhay', 26), ('Rahul', 25), ('Rahul', 23), ('Rohan', 22), ('Rohan', 22), ('Shreya', 22), ('Shreya', 28), ('Swati', 26), ('Swati', 19)]</span></pre><p id="6b87" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。groupByKey() </strong>转换-根据关键字对RDD元素进行分组</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="2023" class="ku kv hi kq b fi kw kx l ky kz">marks_rdd = spark.sparkContext.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])</span><span id="aea8" class="ku kv hi kq b fi la kx l ky kz">marks_grouped_rdd = marks_rdd.groupByKey()</span><span id="a4cc" class="ku kv hi kq b fi la kx l ky kz">for i,j in marks_grouped_rdd.collect():<br/>    print(i,list(j))</span><span id="c788" class="ku kv hi kq b fi la kx l ky kz"># Output:<br/>Rahul [25, 23] <br/>Shreya [22, 28] <br/>Rohan [22, 22] <br/>Swati [26, 19] <br/>Abhay [29, 26]</span></pre><p id="80f5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj"> RDD到数据帧，反之亦然</strong></p><p id="684f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">我们可以使用以下语法将RDD数据帧和RDD数据帧进行转换:</p><pre class="ke kf kg kh fd kp kq kr ks aw kt bi"><span id="5b3e" class="ku kv hi kq b fi kw kx l ky kz"># Converts RDD to DataFrame<br/>dfFromRDD1 = rdd.toDF()<br/># Converts RDD to DataFrame with column names<br/>dfFromRDD2 = rdd.toDF("col1","col2")<br/># using createDataFrame() - Convert DataFrame to RDD<br/>df = spark.createDataFrame(rdd).toDF("col1","col2")<br/># Convert DataFrame to RDD<br/>rdd = df.rdd</span></pre><p id="39e4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">这就是我认为我们已经涵盖了大部分的RDD概念。</p><p id="25af" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">下一步要了解更多关于Pyspark DataFrame的信息，请参考<a class="ae mq" href="https://muttinenisairohith.medium.com/pyspark-for-begineers-part-2-pyspark-dataframe-60008da53e30" rel="noopener"> <strong class="il hj">这里。</strong>T41】</a></p><p id="5a75" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">快乐编码和学习。</p></div></div>    
</body>
</html>