<html>
<head>
<title>Tensor Basics in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中的张量基础</h1>
<blockquote>原文：<a href="https://medium.com/codex/tensor-basics-in-pytorch-252a34288f2?source=collection_archive---------14-----------------------#2021-07-09">https://medium.com/codex/tensor-basics-in-pytorch-252a34288f2?source=collection_archive---------14-----------------------#2021-07-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7c82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">张量是PyTorch库的基本数据结构。</p><p id="c43e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di"> D </span> eep学习是监督机器学习的主流之一，使用输入和标记输出，我们的目标是开发一个将输入映射到输出的函数。<em class="jm">深度学习</em>是<em class="jm">深度的</em>，确切地说，它的模型学习许多<em class="jm">层</em>的转换，其中每一层提供一个级别的表示[1]。深度学习框架在传播思想方面发挥了至关重要的作用。第一代允许简单建模的框架包括<a class="ae jn" href="https://github.com/BVLC/caffe" rel="noopener ugc nofollow" target="_blank"> Caffe </a>、<a class="ae jn" href="https://github.com/torch" rel="noopener ugc nofollow" target="_blank"> Torch </a>和<a class="ae jn" href="https://github.com/Theano/Theano" rel="noopener ugc nofollow" target="_blank">the ano</a>【1】。在本教程中，我们将讨论PyTorch中张量的基础知识。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jo"><img src="../Images/4fc2d2bffd247d756fdd4f0645d164c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oLcN6Vlpa-PrxnRYJGnXDQ.png"/></div></div></figure><h1 id="9191" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">安装PyTorch</h1><p id="268c" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated"><strong class="ih hj"> PyTorch </strong>是一个基于Torch库的开源机器学习库，用于计算机版本和自然语言处理等应用，主要由脸书的AI研究实验室(FAIR)开发。</p><p id="3ada" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PyTorch有两个高级特性:</p><ul class=""><li id="a7c9" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">张量计算(如<a class="ae jn" href="https://en.wikipedia.org/wiki/NumPy" rel="noopener ugc nofollow" target="_blank"> NumPy </a>)通过<a class="ae jn" href="https://en.wikipedia.org/wiki/Graphics_processing_unit" rel="noopener ugc nofollow" target="_blank">图形处理器</a> (GPU)实现强大加速</li><li id="1183" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/Deep_neural_networks" rel="noopener ugc nofollow" target="_blank">深度神经网络</a>建立在基于类型的<a class="ae jn" href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="noopener ugc nofollow" target="_blank">自动微分</a>系统上</li></ul><p id="2cd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于安装指南，请参考PyTorch网站<a class="ae jn" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank">这里</a>。在为该版本选择了一些特性(如操作系统、包和语言)后，您可以按照出现在<code class="du lr ls lt lu b">Run this Command:</code>中的命令或者按照给出的URL下载并安装源代码。为了利用CUDA功能，您的系统应该配备NVIDIA GPU。为了检查你的系统是否支持CUDA，请点击查看<a class="ae jn" href="https://developer.nvidia.com/cuda-gpus" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="33f4" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">张量</h1><p id="84df" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">PyTorch引入了一个基本的数据结构:张量。对于那些来自数学、物理或工程的人来说，术语<em class="jm">张量</em>与空间、参考系统以及它们之间的转换概念捆绑在一起【2】。在深度学习方面，张量是向量和矩阵在任意维数上的推广。换句话说，<strong class="ih hj">张量是一个多维数组。</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lv"><img src="../Images/701f38d54d2b1735c19fb83fdc36d8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eu8deI-v8ttd6GeLHu03fw.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">张量[2]</figcaption></figure><p id="d581" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上图可以看出，张量代表一个(可能是多维的)数值数组。对于一个轴，张量(在数学上)对应于一个<em class="jm">向量</em>。对于两个轴，一个张量对应一个<em class="jm">矩阵</em>。多于两个轴的张量没有特殊的数学名称。</p><p id="cec7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PyTorch不是唯一处理多维数组的库。NumPy和tensor中的多维数组有几个共同点。然而，当谈到加速器时，GPU很好地支持加速计算，而NumPy只支持CPU计算。第二，张量类支持自动微分。这些性质使得张量类适合深度学习。下面，我们将带您了解PyTorch中张量的一些主要操作。</p><h1 id="74e6" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">将PyTorch库导入Python脚本</h1><p id="4c33" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">你可以用</p><p id="c9dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lr ls lt lu b">import torch</code></p><p id="2af6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在代码实现中使用Pytorch。</p><h1 id="ddbd" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">检查您的系统中是否有CUDA</h1><p id="9a26" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">使用以下代码检查您的系统是否支持CUDA进行GPU加速。</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="2708" class="me kb hi lu b fi mf mg l mh mi">torch.cuda.is_available()</span></pre><p id="fd99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出为布尔型，其中<code class="du lr ls lt lu b">True</code>表示有CUDA，而<code class="du lr ls lt lu b">False</code>表示没有CUDA。</p><h1 id="ce79" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">构建张量</h1><p id="3e66" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">你可以直接调用张量变量</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="af7d" class="me kb hi lu b fi mf mg l mh mi">#an empty multi-dimensional (2 * 3 * 2) tensor<br/>torch.empty(2, 3, 2)</span><span id="8bee" class="me kb hi lu b fi mj mg l mh mi"># a 1*12 vector<br/>torch.arange(12)</span><span id="59d6" class="me kb hi lu b fi mj mg l mh mi">#a random 1*2 matrix <br/>torch.rand(1, 2)</span><span id="00b7" class="me kb hi lu b fi mj mg l mh mi">#a random 1*2 matrix drawn from Gaussian distribution <br/>torch.randn(1, 2)</span><span id="98f1" class="me kb hi lu b fi mj mg l mh mi">#a zero-filled 1*2 matrix <br/>torch.zeros(1, 2)</span><span id="dc8c" class="me kb hi lu b fi mj mg l mh mi">#a 1*2 matrix filled with only 1<br/>torch.ones(1, 2)</span><span id="8ed6" class="me kb hi lu b fi mj mg l mh mi">#Specifying the type of the elements of the tensor<br/>torch.ones(1, 2, dtype=torch.int)</span></pre><p id="6640" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或将列表或NumPy数组转换成张量</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="f2b2" class="me kb hi lu b fi mf mg l mh mi">#converting list into tensor<br/>x = [1, 2, 3, 4, 5]<br/>torch.tensor(x)</span><span id="2f1f" class="me kb hi lu b fi mj mg l mh mi">#converting NumPy into tensor</span><span id="ef07" class="me kb hi lu b fi mj mg l mh mi">import numpy as np<br/>x = np.array([1, 2, 3, 4, 5])<br/>y = torch.from_numpy(x)</span><span id="8ce6" class="me kb hi lu b fi mj mg l mh mi">#<em class="jm"> Assign a copy of `x` to `y` by allocating new memory<br/>y = x.clone()</em></span></pre><blockquote class="mk ml mm"><p id="0576" class="if ig jm ih b ii ij ik il im in io ip mn ir is it mo iv iw ix mp iz ja jb jc hb bi translated"><em class="hi">请注意，在将NumPy转换为张量时，它们都指向内存中的同一个位置。换句话说，如果你改变了NumPy变量，</em> <code class="du lr ls lt lu b"><em class="hi">x</em></code> <em class="hi">这里的张量版本，</em> <code class="du lr ls lt lu b"><em class="hi">y</em></code> <em class="hi">，也随之改变。</em></p></blockquote><h1 id="4d17" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">张量的类型和形状</h1><p id="36b2" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">使用下面的代码，你可以检查张量的类型及其组成元素。</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="3ef9" class="me kb hi lu b fi mf mg l mh mi">x = torch.ones(1, 2, dtype=torch.int) </span><span id="3dfb" class="me kb hi lu b fi mj mg l mh mi">#type of variable<br/>type(x)</span><span id="eded" class="me kb hi lu b fi mj mg l mh mi">#type of the elements of tensor<br/>x.dtype</span><span id="b60b" class="me kb hi lu b fi mj mg l mh mi">#type of the elements of tensor<br/>x.type()</span><span id="5f7b" class="me kb hi lu b fi mj mg l mh mi">#shape of tensor<br/>x.shape</span><span id="8438" class="me kb hi lu b fi mj mg l mh mi">#total number of elements in a tensor<br/>x.numel()</span></pre><h1 id="34ae" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">限幅</h1><p id="1985" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">可以通过以下方式访问给定维度中的给定元素或元素子集</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="72bc" class="me kb hi lu b fi mf mg l mh mi">x = torch.rand(5, 3)<br/>#tensor([[0.4788, 0.8171, 0.7119],<br/>         [0.9143, 0.4804, 0.0356],<br/>         [0.7858, 0.5312, 0.0178],<br/>         [0.9417, 0.1743, 0.0691],<br/>         [0.7302, 0.6352, 0.8515]]</span><span id="2cd2" class="me kb hi lu b fi mj mg l mh mi">#first column<br/>x[:, 0]</span><span id="a18e" class="me kb hi lu b fi mj mg l mh mi"># first row<br/>x[0, :]</span><span id="4d13" class="me kb hi lu b fi mj mg l mh mi">#row:1, column:3<br/>x[1,3]</span></pre><p id="e8ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要访问非张量类型格式的张量的单个元素，我们可以使用<code class="du lr ls lt lu b">item()</code>方法，如下所示</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="34c7" class="me kb hi lu b fi mf mg l mh mi">#tensor format - tensor(0.4804)<br/>x[1,3]</span><span id="a3af" class="me kb hi lu b fi mj mg l mh mi">#scalar format<br/>x[1, 3].item()</span><span id="1e16" class="me kb hi lu b fi mj mg l mh mi">#scalar format with type<br/>float(x[1, 3]) <br/>int(x[1, 3])</span></pre><p id="2d34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意<code class="du lr ls lt lu b">item</code>方法只适用于张量的一个元素。如果将它应用于多维数组张量，就会产生误差。</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="d75e" class="me kb hi lu b fi mf mg l mh mi">x[:,1].item()</span></pre><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es mq"><img src="../Images/776288900182899d687bb565d16580a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nMm0PfLzf5DYc4nLwYAgEA.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">如果“item”用于多维数组，则出现错误</figcaption></figure><h1 id="f6cb" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">调整大小/整形</h1><p id="21ce" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">我们可以通过以下方式调整预定义张量的大小</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="2198" class="me kb hi lu b fi mf mg l mh mi"># a 3*5 tensor<br/>x = torch.rand(3, 5)</span><span id="619b" class="me kb hi lu b fi mj mg l mh mi">#resize x to 1*15<br/>y = x.view(15)</span><span id="ca9f" class="me kb hi lu b fi mj mg l mh mi">#PyTorch can set the dimension automatically if -1 is given<br/>y = x.view(-1, 3)</span><span id="367e" class="me kb hi lu b fi mj mg l mh mi">#another way to reshape the tensor<br/>x.reshape(5, 3)</span></pre><p id="6015" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，调整后的张量维数乘积应该保留在输出中。例如，您可以将3*5的大小调整为2*8，因为第一个大小是15，第二个大小是16。我们通过为我们希望张量自动推断的维度放置<code class="du lr ls lt lu b">-1</code>来调用自动维度发现功能。</p><h1 id="6406" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">串联</h1><p id="e763" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">我们可以使用concatenate操作按行(dim=0)或按列(dim=1)堆叠矩阵。</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="58df" class="me kb hi lu b fi mf mg l mh mi">#concatenate along row<br/>torch.cat((X, Y), dim=0)</span><span id="2d0f" class="me kb hi lu b fi mj mg l mh mi">#concatenate along column<br/>torch.cat((X, Y), dim=1)</span></pre><h1 id="07a9" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">算术运算</strong></h1><p id="1550" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">张量是PyTorch处理不同操作和存储数据的主要模块。不同的算术运算可以用不同的格式实现。</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="5746" class="me kb hi lu b fi mf mg l mh mi">#defining two 2*2 tensor<br/>x = torch.rand(2,2)<br/>y = torch.rand(2,2)</span><span id="4581" class="me kb hi lu b fi mj mg l mh mi">#Addition<br/>x+y<br/>torch.add(x,y)</span><span id="88a8" class="me kb hi lu b fi mj mg l mh mi">#Subtraction<br/>x-y<br/>torch.sub(x, y)</span><span id="b278" class="me kb hi lu b fi mj mg l mh mi">#Multiplication<br/>x*y<br/>torch.mul(x, y)</span><span id="632d" class="me kb hi lu b fi mj mg l mh mi">#division<br/>x / y<br/>torch.div(x, y)</span><span id="b592" class="me kb hi lu b fi mj mg l mh mi">#<em class="jm">exponentiation, x to power y<br/>x**y</em></span><span id="01e9" class="me kb hi lu b fi mj mg l mh mi">#functional operation<br/>torch.exp(x)</span></pre><p id="639f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PyTorch还支持就地操作。它使您能够将操作的输出直接赋给给定的变量。在机器学习模型处理成千上万个参数的情况下，这可以为你节省大量内存。此外，我们可能会从多个变量指向相同的参数。如果我们不就地更新，其他引用仍然会指向旧的内存位置，这使得我们的部分代码可能会无意中引用过时的参数。操作的就地版本如下:</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="d3b8" class="me kb hi lu b fi mf mg l mh mi">#add x to y and replace it into y<br/>y.add_(x)</span><span id="da88" class="me kb hi lu b fi mj mg l mh mi">#also<br/>y += x</span><span id="990b" class="me kb hi lu b fi mj mg l mh mi">#subtract<br/>y.sub_(x)<br/>#also<br/>y -= x</span><span id="b665" class="me kb hi lu b fi mj mg l mh mi">#multiplication<br/>y.mul_(x)<br/>#also<br/>y *=x</span><span id="4754" class="me kb hi lu b fi mj mg l mh mi">#division<br/>y.div_(x)<br/>#also<br/>y /=x</span></pre><blockquote class="mk ml mm"><p id="57f4" class="if ig jm ih b ii ij ik il im in io ip mn ir is it mo iv iw ix mp iz ja jb jc hb bi translated">一般来说，当PyTorch中的任何方法中有下划线<code class="du lr ls lt lu b">_</code>时，表示修改就地发生。</p></blockquote><h1 id="b030" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">将张量转移到CPU或GPU</h1><p id="3f42" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">张量可以驻留在CPU或GPU中。为了利用GPU的能力，张量应该转移到GPU范围。</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="8bb6" class="me kb hi lu b fi mf mg l mh mi">if torch.cuda.is_available():   # first check if CUDA is available<br/>    device =torch.device("cuda") # set the device to CUDA<br/>    x = torch.ones(5, device=device) #move the tensor to GPU</span><span id="1c77" class="me kb hi lu b fi mj mg l mh mi">#another form of GPU transfer<br/>y = torch.ones(5)<br/>y = y.to(device)</span></pre><p id="98da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们想把变量传送回CPU，我们可以用<code class="du lr ls lt lu b">to('cpu')</code>方法作为</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="3027" class="me kb hi lu b fi mf mg l mh mi">#move back to the CPU<br/>x = x.to("cpu")</span></pre><p id="3848" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CPU后移对于执行一些操作是必要的，特别是当从张量转换到NumPy时，这将在下面讨论。</p><p id="2132" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">张量到数字</strong></p><p id="bf23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">只有当变量驻留在CPU上时，才能将张量转换为NumPy格式。如果张量位于GPU中，会抛出错误。要想成功，首先要把张量传入CPU。</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="6f69" class="me kb hi lu b fi mf mg l mh mi">#tesnor resides in CPU<br/>x = torch.ones(1, 5)</span><span id="11f2" class="me kb hi lu b fi mj mg l mh mi">#convert tensor to NumPy<br/>z = x.numpy()</span></pre><p id="8575" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果张量驻留在GPU中，首先应该将其移回CPU，然后转换为NumPy:</p><pre class="jp jq jr js fd ma lu mb mc aw md bi"><span id="6efc" class="me kb hi lu b fi mf mg l mh mi">if torch.cuda.is_available():   <br/>    device =torch.device("cuda") <br/>    x = torch.ones(5, device=device) #move the tensor to GPU</span><span id="d7ac" class="me kb hi lu b fi mj mg l mh mi">#Since x is located in GPU, it should move back to CPU<br/>x = x.to("cpu")<br/>z = x.numpy()</span></pre><p id="7aac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样！</p><h1 id="2f5d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">参考</h1><p id="1633" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">【1】<a class="ae jn" href="https://d2l.ai/chapter_introduction/index.html" rel="noopener ugc nofollow" target="_blank">https://d2l.ai/chapter_introduction/index.html</a></p><p id="b792" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]史蒂文斯、伊莱、卢卡·安提加和托马斯·维赫曼。<em class="jm">用PyTorch进行深度学习</em>。曼宁出版公司，2020。</p></div></div>    
</body>
</html>