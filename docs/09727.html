<html>
<head>
<title>K-Means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k均值</h1>
<blockquote>原文：<a href="https://medium.com/codex/k-means-c5763e50898e?source=collection_archive---------3-----------------------#2022-11-05">https://medium.com/codex/k-means-c5763e50898e?source=collection_archive---------3-----------------------#2022-11-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f0e7136048ec8d1a774aa9d2fc24fcf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*g4rjG_dRiif-3Mal"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae iu" href="https://unsplash.com/ja/@writecodenow?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Boitumelo Phetla </a>拍摄的照片</figcaption></figure><p id="375e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一种聚类算法，旨在将相似的实体分组到一个聚类中，并且适用于数值数据。</p><h1 id="40c8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">步骤</strong></h1><p id="0f59" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">1.选择簇的数量(k)</p><p id="a502" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.它随机初始化每个聚类的聚类中心，即该算法从第一组随机选择的质心开始，即随机选择几个点，将其视为聚类的质心。</p><p id="6083" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.对于每个数据点，计算距所有质心的欧几里德距离，并根据到所有质心的最小距离分配聚类。质心是代表群集中心的虚拟或真实位置</p><p id="f55a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.这一步是移动质心步。k表示通过取聚类中所有数据点的平均值来移动每个聚类的质心。换句话说，该算法计算每个聚类的新质心(聚类中所有点的平均值)</p><p id="34b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当出现以下任一情况时，它会停止创建和优化集群:</p><p id="d274" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">-质心已经稳定，即它们的值没有变化，因为聚类已经成功</p><ul class=""><li id="bbcf" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">已达到定义的迭代次数。</li></ul><h1 id="0329" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">寻找最佳聚类数的方法:</h1><p id="a9b4" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><strong class="ix hj">肘法:</strong></p><p id="b21c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是确定最佳聚类数的最常用方法。<em class="lf"> </em>该方法基于计算不同数量的组(k)的组内误差平方和(WSS ),并选择WSS变化首先开始减小的k。</p><p id="4d2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">肘形法背后的思想是，对于少量的聚类，所解释的变化迅速变化，然后它变慢，导致曲线中形成肘形。拐点是我们可以用于聚类算法的聚类数。关于这种方法的更多细节可以在袁春辉和杨海涛的论文中找到。</p><p id="1127" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用<a class="ae iu" href="https://www.scikit-yb.org/en/latest/" rel="noopener ugc nofollow" target="_blank"> YellowBrick </a>库，它可以用几行代码实现elbow方法。它是Scikit-Learn的包装器，并且有一些很酷的机器学习可视化！</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="2792" class="lp ju hi ll b fi lq lr l ls lt">pip install yellowbrick(if not installed)</span><span id="1cd9" class="lp ju hi ll b fi lu lr l ls lt">from yellowbrick.cluster import KElbowVisualizer</span><span id="092b" class="lp ju hi ll b fi lu lr l ls lt">model = KMeans()</span><span id="7b34" class="lp ju hi ll b fi lu lr l ls lt"># k is range of number of clusters.</span><span id="4d30" class="lp ju hi ll b fi lu lr l ls lt">visualizer = KElbowVisualizer(model, k=(2,30), timings= True)</span><span id="38e0" class="lp ju hi ll b fi lu lr l ls lt">visualizer.fit(cluster_df)        # Fit data to visualizer</span><span id="7295" class="lp ju hi ll b fi lu lr l ls lt">visualizer.show()        # Finalize and render figure</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/e7ec457649e5e9f9cb8b71c065099484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/0*Csmhfbs-PJnwUkmG"/></div></figure><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/742a601a056c99b3a0420164aceba391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/0*sScWk1pO4OEmcW5P.png"/></div></figure><p id="8778" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">KElbowVisualizer函数适用于2到30之间的分类值范围的KMeans模型。肘点是通过8个聚类实现的，这8个聚类是由函数本身突出显示的。该函数还通过绿线告知我们为不同数量的聚类绘制模型需要多少时间。</p><p id="96e3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">轮廓系数:</strong></p><p id="5b91" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">点<em class="lf"> i </em>的轮廓系数定义如下:</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/4d643991c0199f1ca60ba48e7d000f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*q5_cVoiiWI-Nbnbj.png"/></div></figure><p id="9436" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<em class="lf"> b(i) </em>是点<em class="lf"> i </em>到任何其他聚类中所有点的最小平均距离，而<em class="lf"> a(i) </em>是<em class="lf"> i </em>到其聚类中所有点的平均距离。例如，如果我们只有3个聚类A、B和C，并且I属于聚类C，那么通过测量<em class="lf"> i </em>到聚类A中每一点的平均距离，即I到聚类B中每一点的平均距离，并取最小结果值来计算<em class="lf"> b(i) </em>。数据集的轮廓系数是单个点的轮廓系数的平均值。</p><p id="30de" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">轮廓系数告诉我们各个点是否被正确地分配到它们的簇中。使用轮廓系数时，我们可以使用以下经验法则:</p><ol class=""><li id="ebe7" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js ly lc ld le bi translated"><em class="lf"> S(i) </em>接近0表示该点在两个聚类之间</li><li id="baf0" class="kw kx hi ix b iy lz jc ma jg mb jk mc jo md js ly lc ld le bi translated">如果它更接近-1，那么我们最好将它分配给其他集群</li><li id="0668" class="kw kx hi ix b iy lz jc ma jg mb jk mc jo md js ly lc ld le bi translated">如果<em class="lf"> S(i) </em>接近1，则该点属于“正确的”聚类</li></ol><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="f25d" class="lp ju hi ll b fi lq lr l ls lt">from yellowbrick.cluster import KElbowVisualizer</span><span id="386a" class="lp ju hi ll b fi lu lr l ls lt">model = KMeans()</span><span id="d49b" class="lp ju hi ll b fi lu lr l ls lt"># k is range of number of clusters.</span><span id="e444" class="lp ju hi ll b fi lu lr l ls lt">visualizer = KElbowVisualizer(model, k=(2,30),metric='silhouette', timings= True)</span><span id="4994" class="lp ju hi ll b fi lu lr l ls lt">visualizer.fit(cluster_df)        # Fit the data to the visualizer</span><span id="7e5e" class="lp ju hi ll b fi lu lr l ls lt">visualizer.show()        # Finalize and render the figure</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es me"><img src="../Images/9597f09057398b8da630511972d70e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/0*YK-0u8xOie9cSgA2.png"/></div></figure><p id="d217" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">差距统计:</strong></p><p id="4728" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">差距统计是由斯坦福大学的研究人员Tibshirani、Walther和Hastie在他们2001年的论文中提出的。他们的方法背后的想法是找到一种方法来比较聚类紧密度与数据的零引用分布，即没有明显聚类的分布。他们对最佳聚类数的估计是原始数据上的聚类紧密度落在该参考曲线之下最远的值。此信息包含在以下差距统计公式中:</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/d64250953c83942dc57e1b9d0aed1a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/0*ng0vK7Ir7T2pTuZX"/></div></figure><p id="9197" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中，Wk是基于类内误差平方和(WSS)的聚类紧密度的度量:</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/ccc3a1fb4d5ef708a33066768cebae07.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/0*I-pd_eK2VU3JtRgs"/></div></figure><p id="5249" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">类内误差平方和由KMeans函数的inertia_ attribute计算，如下所示:</p><ul class=""><li id="c724" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">每个点到聚类中心的距离的平方(误差平方)</li><li id="3b74" class="kw kx hi ix b iy lz jc ma jg mb jk mc jo md js lb lc ld le bi translated">WSS分数是所有点的这些平方误差的总和</li></ul><p id="cd78" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在python中计算k均值聚类的间隙统计包括以下步骤:</p><ul class=""><li id="ac59" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">将观察到的数据在不同数量的聚类上进行聚类，并计算我们的聚类的紧密度</li><li id="ee28" class="kw kx hi ix b iy lz jc ma jg mb jk mc jo md js lb lc ld le bi translated">生成参考数据集，并用不同数量的聚类对每个数据集进行聚类。参考数据集是使用random_sample函数根据“连续均匀”分布创建的。</li><li id="579a" class="kw kx hi ix b iy lz jc ma jg mb jk mc jo md js lb lc ld le bi translated">计算参考数据集上聚类的平均紧密度</li><li id="f268" class="kw kx hi ix b iy lz jc ma jg mb jk mc jo md js lb lc ld le bi translated">根据参考数据和原始数据的聚类紧密度差异计算间隙统计数据</li></ul><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="d7d8" class="lp ju hi ll b fi lq lr l ls lt">def optimalK(data, nrefs=3, maxClusters=15):</span><span id="0c6e" class="lp ju hi ll b fi lu lr l ls lt">"""</span><span id="3009" class="lp ju hi ll b fi lu lr l ls lt">Calculates KMeans optimal K using Gap Statistic</span><span id="66a1" class="lp ju hi ll b fi lu lr l ls lt">Params:</span><span id="e5a3" class="lp ju hi ll b fi lu lr l ls lt">data: ndarry of shape (n_samples, n_features)</span><span id="58e8" class="lp ju hi ll b fi lu lr l ls lt">nrefs: number of sample reference datasets to create</span><span id="13e8" class="lp ju hi ll b fi lu lr l ls lt">maxClusters: Maximum number of clusters to test for</span><span id="a21e" class="lp ju hi ll b fi lu lr l ls lt">Returns: (gaps, optimalK)</span><span id="3929" class="lp ju hi ll b fi lu lr l ls lt">"""</span><span id="7586" class="lp ju hi ll b fi lu lr l ls lt">gaps = np.zeros((len(range(1, maxClusters)),))</span><span id="75f1" class="lp ju hi ll b fi lu lr l ls lt">resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})</span><span id="5381" class="lp ju hi ll b fi lu lr l ls lt">for gap_index, k in enumerate(range(1, maxClusters)):</span><span id="7517" class="lp ju hi ll b fi lu lr l ls lt"># Holder for reference dispersion results</span><span id="0414" class="lp ju hi ll b fi lu lr l ls lt">refDisps = np.zeros(nrefs)</span><span id="34ac" class="lp ju hi ll b fi lu lr l ls lt"># For n references, generate random sample and perform kmeans getting resulting dispersion of each loop</span><span id="085b" class="lp ju hi ll b fi lu lr l ls lt">for i in range(nrefs):</span><span id="87a1" class="lp ju hi ll b fi lu lr l ls lt"># Create new random reference set</span><span id="10e7" class="lp ju hi ll b fi lu lr l ls lt">randomReference = np.random.random_sample(size=data.shape)</span><span id="cc64" class="lp ju hi ll b fi lu lr l ls lt"># Fit to it</span><span id="339a" class="lp ju hi ll b fi lu lr l ls lt">km = KMeans(k)</span><span id="657a" class="lp ju hi ll b fi lu lr l ls lt">km.fit(randomReference)</span><span id="f7f3" class="lp ju hi ll b fi lu lr l ls lt">refDisp = km.inertia_</span><span id="3314" class="lp ju hi ll b fi lu lr l ls lt">refDisps[i] = refDisp</span><span id="d455" class="lp ju hi ll b fi lu lr l ls lt"># Fit cluster to original data and create dispersion</span><span id="f493" class="lp ju hi ll b fi lu lr l ls lt">km = KMeans(k)</span><span id="68c7" class="lp ju hi ll b fi lu lr l ls lt">km.fit(data)</span><span id="9739" class="lp ju hi ll b fi lu lr l ls lt">origDisp = km.inertia_</span><span id="d4c1" class="lp ju hi ll b fi lu lr l ls lt"># Calculate gap statistic</span><span id="d809" class="lp ju hi ll b fi lu lr l ls lt">gap = np.log(np.mean(refDisps)) - np.log(origDisp)</span><span id="d186" class="lp ju hi ll b fi lu lr l ls lt"># Assign this loop's gap statistic to gaps</span><span id="8dcf" class="lp ju hi ll b fi lu lr l ls lt">gaps[gap_index] = gap</span><span id="cc23" class="lp ju hi ll b fi lu lr l ls lt">resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)</span><span id="b845" class="lp ju hi ll b fi lu lr l ls lt">return (gaps.argmax() + 1, resultsdf)</span><span id="25a4" class="lp ju hi ll b fi lu lr l ls lt">score_g, df = optimalK(cluster_df, nrefs=5, maxClusters=30)</span><span id="0667" class="lp ju hi ll b fi lu lr l ls lt">plt.plot(df['clusterCount'], df['gap'], linestyle='--', marker='o', color='b')</span><span id="0e83" class="lp ju hi ll b fi lu lr l ls lt">plt.xlabel('K')</span><span id="9cc4" class="lp ju hi ll b fi lu lr l ls lt">plt.ylabel('Gap Statistic')</span><span id="4fcc" class="lp ju hi ll b fi lu lr l ls lt">plt.title('Gap Statistic vs. K')</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/a466ee4a22bd37bd828221f93f1f07b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/0*tAoG0NU9bQHyn97Y.png"/></div></figure><p id="01c0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">树状图:</strong></p><p id="a90a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种技术是特定于凝聚层次聚类方法的。聚类的凝聚层次方法首先将每个点视为一个单独的聚类，然后开始根据点和聚类的距离以层次方式将点连接到聚类。在另一篇博客中，我们将重点介绍这种方法的细节。为了获得分层聚类的最佳聚类数，我们使用了一个树形图，这是一个显示聚类合并或分裂顺序的树形图。</p><p id="eb32" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果两个聚类合并，树状图会将它们连接成一个图，连接的高度就是这些聚类之间的距离。我们将使用scipy库中的树状函数来绘制图形。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="a4b5" class="lp ju hi ll b fi lq lr l ls lt">import scipy.cluster.hierarchy as shc</span><span id="48ec" class="lp ju hi ll b fi lu lr l ls lt">from matplotlib import pyplot</span><span id="0b29" class="lp ju hi ll b fi lu lr l ls lt">pyplot.figure(figsize=(10, 7))</span><span id="8f14" class="lp ju hi ll b fi lu lr l ls lt">pyplot.title("Dendrograms")</span><span id="0768" class="lp ju hi ll b fi lu lr l ls lt">dend = shc.dendrogram(shc.linkage(cluster_df, method='ward'))</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/99bbdc8665ebd3ce1fa057c1dca8c5ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*nXmwci42K-n5YqvF.png"/></div></figure><p id="93b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以根据树状图的层次结构来选择最佳的聚类数。正如其他聚类验证指标所强调的那样，4个聚类也可以考虑用于聚集层次结构。</p><p id="d98d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">贝叶斯信息准则:</strong></p><p id="a010" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">贝叶斯信息准则(BIC)评分是一种对使用最大似然估计框架的模型进行评分的方法。BIC统计数据的计算方法如下:</p><p id="9092" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lf">BIC =(k * ln(n))——(2ln(L))</em></p><p id="7c52" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lf">其中L是模型的似然函数的最大值，k是参数的数量，n是记录的数量</em></p><p id="a2b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BIC分数越低，模型越好。我们可以将BIC分数用于聚类的高斯混合建模方法。我们将在单独的博客中讨论这个模型的细节，但这里需要注意的关键是，在这个模型中，我们需要选择聚类的数量以及协方差的类型。我们尝试了参数的各种组合，并选择了具有最低BIC分数的模型。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="5b3b" class="lp ju hi ll b fi lq lr l ls lt">from sklearn.mixture import GaussianMixture</span><span id="c2a3" class="lp ju hi ll b fi lu lr l ls lt">n_components = range(1, 30)</span><span id="687a" class="lp ju hi ll b fi lu lr l ls lt">covariance_type = ['spherical', 'tied', 'diag', 'full']</span><span id="5364" class="lp ju hi ll b fi lu lr l ls lt">score=[]</span><span id="1117" class="lp ju hi ll b fi lu lr l ls lt">for cov in covariance_type:</span><span id="e5e6" class="lp ju hi ll b fi lu lr l ls lt">for n_comp in n_components:</span><span id="36ee" class="lp ju hi ll b fi lu lr l ls lt">gmm=GaussianMixture(n_components=n_comp,covariance_type=cov)</span><span id="418d" class="lp ju hi ll b fi lu lr l ls lt">gmm.fit(cluster_df)</span><span id="a51c" class="lp ju hi ll b fi lu lr l ls lt">score.append((cov,n_comp,gmm.bic(cluster_df)))</span><span id="c9cc" class="lp ju hi ll b fi lu lr l ls lt">score</span></pre></div></div>    
</body>
</html>