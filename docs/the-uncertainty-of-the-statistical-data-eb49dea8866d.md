# 统计数据的不确定性

> 原文：<https://medium.com/codex/the-uncertainty-of-the-statistical-data-eb49dea8866d?source=collection_archive---------2----------------------->

![](img/69a8e3f44a4a174a1588160b974a4aee.png)

PIRO4D 在 pixabay 上的照片。

摘要:任何结果都可以随机产生，任何随机结果都是无用的。传统方法将不确定性定义为真实值附近的离差的度量，并基于这样的假设，即任何偏离一致性的情况都是确定性事件的结果。这种方法的问题是，即使是非均匀分布也可能随机产生，并且这种事件的概率随着测试的假设数量的增加而增加。因此，存在将随机且不可重复的假设视为确定性假设的风险。事实上，据信这种作用方式是大量不可再现结果的原因。因此，我们认为随机获得相等或更好结果的概率才是统计数据的真正不确定性。因为它表示数据有用的概率，因此任何其他分析的有效性都取决于该参数。

**简介**

任何结果都可以随机产生，任何随机结果都是无用的。传统方法[1]和[2]将不确定性定义为真实值附近的离差的度量，并基于这样的假设，即任何偏离一致性的情况都是确定性事件的结果。这种方法的问题是，即使是非均匀分布也可能随机产生，并且这种事件的概率随着测试的假设数量的增加而增加。因此，存在将随机且不可重复的假设视为确定性假设的风险。事实上，这种作用方式被认为是大量不可重现结果的原因[3]和[4]。因此，我们认为随机获得相同或更好结果的概率是统计数据的真正不确定性，因为它代表了数据有用的概率，因此任何其他分析的有效性都取决于该参数。

此外，我们还将解决确定计算随机获得相同或更好结果的概率的正确方法的问题。关于这个主题，我们将看到，在计算这个概率值时，基本点是考虑依赖于由所有测试的假设产生的所有其他数据的统计数据。

在统计分析中，将统计数据视为非独立数据具有重要意义。事实上，我们所有的随机行动不仅是无用的，而且会增加统计数据的不确定性。为此，在下面的文章[5]中，我们强调了在统计中有意识地行动的重要性。

此外，统计数据的不确定性评估只有通过了解所有的尝试才有可能。在实践中，不确定性的计算是非常困难的，因为我们不仅必须考虑我们所有的尝试，而且还必须考虑与我们执行相同任务的每个其他人所做的尝试。这样，我们的统计数据的不确定性也取决于从事我们自己的分析的人所采取的行动。事实上，属于一个研究网络的一群人，他们都有相同的声誉，都致力于同一问题，可以认为是一个人完成了所有的尝试。因此，不确定性的计算变成了依赖于我们所拥有的信息的相对事物。

**不确定度的定义**

我们将要给出的统计数据不确定性定义的目的是确定一个与结果的可重复性相关联的参数，该参数是通用的，因此独立于我们进行统计分析的系统。

> 我们将统计数据的不确定性定义为随机获得相同或更好结果的概率。

该定义将统计数据视为预测，因此只有在生成预测的过程是非随机的情况下，预测才是可重复的。因此，不确定度的计算包括确定产生结果的过程类型。我们可以通过随机产生不可重复结果的统计特性来区分认知过程和随机过程。事实上，通过使用我们正在进行测量的系统上的信息，我们可以增加预测的概率，这导致随机获得相同结果的概率随之降低。

有趣的是，统计数据的可重复性和产生统计数据的过程的非随机性是两个等价的概念。事实上，信息导致结果的可重复性，同时产生不能随机再现的结果。

为了理解给出的定义，我们报告下面的例子:我们必须分析一个统计数据，该数据由对一个事件的 1000 个预测表示，而该事件只能有两个结果。1000 次预测分为 600 次成功和 400 次失败。为了计算以随机方式获得相同或更好结果的概率，我们使用二项式分布，并获得以下值 1.4 108%。

现在，让我们考虑一个由 10 个预测代表的统计数据，这 10 个预测分为 8 个成功和 2 个失败。在这种情况下，随机获得相等或更好结果的概率是 5.5%。

比较这两个结果，我们注意到，在第一种情况下，虽然成功的次数只有 60%，但不确定性几乎为零，而在第二种情况下，成功的概率为 80%，不确定性要高得多。这种差异是因为，如上所述，给出的定义只涉及结果的可重复性，而不涉及其准确性。因此，它是一个随着结果重复次数的增加而减小的值。本文介绍的方法与经典方法有很大不同，经典方法将不确定性视为数据相对于真实值的离差度量。

> 要理解的基本点是，统计数据完全随机的概率及其随机分量的估计值(真实值周围的离差)是两个仅部分相互依赖的参数。第一个随着测量重复次数的增加而减少，第二个则不会，这就是为什么传统的不确定度定义在许多情况下对于结果的可重复性并不重要的原因之一。

正如我们在例子中所看到的，问题在于，一个完全随机的过程产生结果的概率总是或大或小的。在这种情况下，任何分析都被证明是错误的，因此，该值被认为是统计结果的真实不确定性。

**统计数据不确定度的计算**

正确计算随机获得相同或更好结果的概率需要改变我们的统计方法。统计学中常用的方法是将一种方法产生的数据与不同方法产生的数据分开考虑。这种处理方式似乎是唯一可能的，但是，正如我们将在下面的悖论中显示的，它会导致一个不合逻辑的结果，而解决这个问题的方法是将数据视为非独立的。

我们认为拥有一台具有巨大计算能力的计算机，可以用来对我们想要研究的现象提出假设。计算机的工作原理如下:它创建一个随机假设，然后进行统计检验。在这一点上，我们问自己以下问题:是否可以有一个有用的统计测试来评估所产生的假设的结果？

如果我们回答是，我们会得到一个不合逻辑的结果，因为我们的计算机总是能够通过生成大量的随机假设，找到一个通过统计测试的假设。这样，我们得出了一个荒谬的结论:随机创造知识是可能的，因为有一台非常强大的计算机和一个统计测试就足以理解每一个现象。

如果我们回答不，我们会得到另一个不合逻辑的结果，因为我们是在说没有假设可以被评估。在实践中，不同假设的结果都是等价的，不可区分的。

怎么才能解决这个逻辑悖论呢？在不出现不合逻辑的情况下，回答这个问题的唯一方法是考虑从相互依赖的不同方法中获得的结果。满足这个条件的函数是随机得到相等或更好结果的概率。事实上，这种概率的计算意味着对所有行为的随机模拟。因此，随机尝试增加了执行操作的数量，从而增加了随机获得相同或更好结果的可能性。由于这个原因，产生随机假设是没有用的，因此，如果你使用这个参数作为不确定性的度量，就有可能评估数据，同时，不可能通过产生随机假设来创造知识。

> 将统计数据视为非独立数据是正确计算不确定性的基本条件。随机得到相等或更好结果的概率满足这个条件。

统计数据之间的相互依赖对统计学有着深远的影响，这将在下一节讨论。

**统计数据非独立性的后果**

在计算不确定性时考虑统计数据的相互依赖性会导致统计学中的三个基本结果。

> 统计数据的非独立性的第一个基本结果是:我们的每一个随机行为总是会增加统计数据的不确定性。

示例:我们需要分析一个统计数据，该数据由关于一个事件的 10 个预测表示，而该事件只能有两个结果。10 次预测分为 8 次成功和 2 次失败。为了计算随机获得相同或更好结果的概率，我们使用二项式分布，并得到以下值 5.5%。如果在做这 10 个预测之前，我们测试了一个不同的假设，用这个假设我们做了 10 个其他的预测，分为 5 个成功和 5 个失败，我们结果的不确定性就改变了。事实上，在这种情况下，我们必须通过执行两次随机尝试(每次 10 次预测)来计算获得成功次数大于或等于 8 的结果的概率。在这种情况下，概率变成 10.6%，所以首先测试一个随机假设的事实几乎是我们第二个假设的不确定性的两倍。因此，增加随机假设会增加我们必须做出的预测的数量，在真实假设下，有一个可接受的不确定性。

> 统计数据的非独立性的第二个基本结果是:我们以及与我们地位相当的其他人的每一个随机行为，总是会增加统计数据的不确定性。

对于同等术语，我们指的是与我们具有相同声誉的人，因此同等人产生的数据用相同的权重来判断。

例如:10 个人参与一个项目，该项目的目标是开发一种能够预测只有两种结果的事件结果的算法。一个不参与项目但知道参与者所做的每一次尝试的外部人员评估所获得的统计数据。所有参与者做出 100 个预测，9 个有 50%的成功几率，一个有 65%的成功几率。获得成功概率为 65%的参与者的静态数据的不确定性是通过计算获得成功次数大于或等于 65 的结果的概率而获得的，其中成功次数大于或等于 65 的结果是通过执行 10 次随机尝试而获得的，每一次随机尝试包括 100 次预测。以这种方式获得的概率是 16%,而如果他是项目中唯一的参与者，概率将是 0.18%,因此大约低 100 倍。

> 统计数据非独立性的第三个基本后果是:不确定性的计算因所拥有的信息而异。

例如:10 个人参与一个项目，该项目的目标是开发一种能够预测只有两种结果的事件结果的算法。在这种情况下，人们不知道其他参与者，并认为他们是唯一参与项目的人。所有参与者做出 100 个预测，9 个有 50%的成功几率，一个有 65%的成功几率。获得 65%成功概率的参与者独立计算所得结果的不确定性。在不知道其他人正在参与项目的情况下，通过执行由 100 次预测组成的单次随机尝试，计算获得成功次数大于或等于 65 次的结果的概率；得到的概率是 0.18%。知道参与者所做的每一次尝试的外部人员计算参与者的统计数据的不确定性，这获得 65%的成功概率。然后，它通过进行 10 次随机尝试(每次 100 次预测)来计算获得成功次数大于或等于 65 的结果的概率。以这种方式获得的概率是 16%，比参与者计算的不确定性高得多。外部人员使用更多信息计算的不确定性值比个体参与者计算的不确定性值更准确。因此，必须始终考虑利用最大数量的信息获得的不确定性值，在本例中，最准确的不确定性为 16%。

统计数据的非独立性的第一个和第二个基本突出结果可以通过突出动作的非随机性来重新定义。

> 统计数据的非独立性的第一个基本结果:我们的每一个非随机行为总是涉及统计数据不确定性的减少。
> 
> 统计数据的非独立性的第二个基本结果是:我们和与我们地位相当的其他人的每一个非随机行为，总是涉及统计数据不确定性的减少。

**结论**

不确定性的传统定义意味着考虑真实的，对于非均匀数据分散，结果不是完全随机的假设。我们认为这个假设是不确定性定义的主要问题。事实上，无论获得什么样的统计数据，都有可能是完全随机的，因此是无用的。

这一错误源于这样一个事实，即不确定性的定义是在每种方法都有很强的确定性成分的环境中形成的。因此，随机计算获得相同或更好结果的概率似乎是没有用的。然而，当我们在金融等随机成分占主导地位的领域应用统计数据时，传统的不确定性方法被证明是不成功的。它失败的原因很简单，它所基于的假设可能不真实。为此，我们将统计数据的不确定性定义为随机获得相同或更好结果的概率。因为这个不确定性的定义与任何假设都没有联系，所以它是普遍适用的。这个概率值的正确计算意味着考虑相互依赖的统计数据。正如我们通过一个悖论所展示的那样，这一假设使得给定的不确定性定义与不可能随机创造知识的逻辑原则相一致。

统计数据的非独立性意味着所采取的每一项措施都会对不确定性的计算产生影响。有趣的是，不同人执行的动作之间也会产生依赖性。因此，不确定性的计算依赖于我们所拥有的信息，所以它成为一种相对的东西，只有在完全了解信息的情况下才能绝对确定。

**参考书目:**

[1] Bich，w .，Cox，M. G .和 Harris，P. M.《测量不确定度表示指南》的演变。计量学，43(4):S161–S166，2006。

[2] Grabe，M .，“科学和技术中的测量不确定性”，Springer，2005 年。

[3] Munafò，m .，Nosek，b .，Bishop，d .等人，“可再生科学宣言”。10021(2017 年)。[https://doi.org/10.1038/s41562-016-0021.](https://doi.org/10.1038/s41562-016-0021.)

[4]约安尼迪斯，J. P. A .“为什么大多数发表的研究结果是假的”。公共科学图书馆医学版。2、e124 (2005)。[5]Andrea Berdondini，“意识的统计科学”(2021 年 8 月 30 日)。可在 https://ssrn.com/abstract=3914134[SSRN](https://ssrn.com/abstract=3914134)买到。

[5]Andrea Berdondini，“意识的统计科学”(2021 年 8 月 30 日)。可在 https://ssrn.com/abstract=3914134.[SSRN](https://ssrn.com/abstract=3914134.)买到