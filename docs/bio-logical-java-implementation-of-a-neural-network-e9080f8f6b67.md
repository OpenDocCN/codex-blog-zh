# 生物逻辑:神经网络的 Java 实现

> 原文：<https://medium.com/codex/bio-logical-java-implementation-of-a-neural-network-e9080f8f6b67?source=collection_archive---------9----------------------->

今天，我很高兴向你们介绍一个高度生物启发的神经网络范例的创建和使用，我称之为生物逻辑。这将是一个相当长的帖子，详细介绍每个对象，从树突到轴突等等。我也希望那些从未创建过神经网络的人能够理解这一点。出于这些原因，我将花时间介绍我们试图模仿的东西；神经元。

![](img/872efdf4e9993f9f12cdba66ebd76122.png)

基本神经元(在本文中，体细胞将被称为细胞核)

把上图想象成一个从左到右处理数据的管子。树突收集来自先前神经元的输入信号，并试图刺激细胞核。在成功的刺激下，细胞核会将输出信号发送到自己的轴突，穿过突触间隙，到达另一个神经元的树突。这一过程将连续进行。

想象每一个神经元都在做一个决定。核心可以被认为是一个保险提供商，或代理人，试图确保他们只为通过检查的房屋提供保险，以便尽可能多赚钱。树突可以被认为是代理人在电话中的所有同事，在重要的新建筑上给代理人一个是/否的投票，试图影响代理人的决定。代理人然后拿起老板组的电话，或轴突，并转达他或她的最终决定。代理人可以自由地给每个来电分配一个“信任”号码。由于从未见过任何同事，代理人不可能知道该信任他们多少。然而，当老板打电话回来告诉代理人这是不是一个好的保险时，代理人会记得哪个同事说了“是”或“不是”，并标记他们的“信仰”数字，如果他们是对的或错的。当一个关于新房子的新电话打进来时，一个同事以 0.9(负)的信念数投票“是”，而另一个同事以 1.1(正)的信念数投票“否”，那么代理很可能会打电话给老板小组的线路，回答“否”。这确保了做出最佳的分类。请记住，比起某人的正确，更有可能相信某人的不正确。如果电话是以-1.4 的“是”和 0.8 的“是”来回答的，代理人仍然理所当然地总体投“否”，仅仅因为第一个同事有很强的犯错历史。这些“信念”数字是每个神经元的输入权重。

我想用这个类比作为一个引子，引入几个不一定从上面的简单情况中显现出来，但通过它的镜头很容易看到的主题。这些主题是通过“屏蔽”，激活函数，动量，梯度裁剪，重量衰减，和小批量的辍学。

辍学是一个惊人而简单的概念，它可以防止代理过度依赖任何一个同事。从概念上讲，我们将阻止一些同事给代理打电话。在实践中，这是通过用滤波器屏蔽输入来实现的，该滤波器被设计成“归零”并阻止某些输入。代理人可能有 100 个同事，在传达她的决定之前，她预计他们会打电话来。掉线率为 20%,合理的假设是，在随机选择中，只有 80 个电话打给了她。这迫使代理使用更小的同事子网来做出她的决定，并将她的“信念”更新限制在她收到呼叫的几个人。像这样扰乱系统可能从一开始就没有好处，但很快代理人就会意识到更合适的“信念”值来应用于每个同事，而不是依赖单个值来判断是错还是对。与正常情况相比，这种方法会使代理花费更长的时间从 boss 组接收足够的反馈来调整这些值。辍学的最后一个诀窍是在训练后忽视它。在本例中，这意味着每个电话呼叫现在都在工作。现在，代理已经在不过度依赖的情况下微调了她的“信念”值，她将开始接到每个同事的电话，都经过仔细权衡，然后再做出决定并挂断下一个电话。

激活函数本身就是一个独特的主题。你会经常听到激活函数提供非线性。但这意味着什么呢？我们为什么需要它？什么是线性模型？至此，我基本上描述了一个线性模型，也就是说一个身份激活的模型。身份激活很简单(in=out)，在得到结果之前不做任何修改。我现在提出对一个决定感到兴奋或渴望的概念。这是代理真正发挥作用的地方。如果没有他或她自己对这件事的想法，老板们还不如直接绕过代理人，询问同事们自己。中介不需要在转发之前简单地收集同事的答案。在实践中，任何带有身份激活的隐藏层基本上都可以被移除，网络变得更浅。一行中的几个身份激活层相当于单个身份层。为了更清楚地看到这一点，让我们使代理人非常兴奋。我将看一下 SoftPlus 激活，以了解这一点。

![](img/6c4811925afbef34825d8c1acebcaff7.png)

Desmos.com/calculator 的好意

我们可以从图中看到，基于输入的汇总，代理人的决策发生了什么变化。将代理推向“不”的输入被减轻，而将代理推向 0 或“绝对不”需要巨大的影响力。然而，我们看到任何积极的传入影响都会得到热切的认可。这将给出代理人自己对输入的观点，向老板证明他或她确实对决策有影响，不需要被解雇。就 boss 组的修正而言，代理人做出的积极决策会比消极决策更新得更积极。从曲线图的斜率可以看出这一点。线的斜率越大，神经元更新用于生成决策的权重就越多。回到类比中，代理人越愿意为决策冒名誉风险，就越注意更新同事的“信念”值。更恰当地说，在 SoftPlus 中，代理通过输出零来表示“不”的决策在这里更类似于简单地将自己从决策中移除。由于没有参与决策，代理人不会招致老板的愤怒，而且在更新“信念”价值观时也没有什么可做的。是的，绝对存在允许负值通过的激活函数，它们将允许代理人强有力地做出“不”的决定。无论老板们说什么，代理人都不会采取什么行动来更新“信念”的价值观。这对于像 Sigmoid 这样的激活函数来说是很常见的，它对一个决定很快“饱和”,变得非常确定。乙状结肠看起来像这样:

![](img/7ccfadd1e3685d3bcd332af46c67a266.png)

Desmos.com/calculator 的好意

这使得代理人要么退出决策，要么投出适度的一票，两者都非常封闭。在这个项目中，我包含了几个激活函数，包括一些我自己想出来的。然而，要知道，让你的保险公司充斥着容易激动的代理人会导致一些严重的问题。我建议将大声活跃的代理与多层平静的代理配对，以减轻他们的过度兴奋。通过这个过程，我发现*比简单地对每一层使用相同的激活函数*要成功得多。

动量使权重更新过程变得平滑，并且如果几次更新是在同一个方向上，动量可以加速权重更新过程。同样，它可以抑制没有其他类似更新支持的不稳定更新。动量通常不仅会加速网络的训练，还会允许网络在损失图中找到更有效的最小值。运行示例中的代理可以对每个同事进行“一致性”评分。他们的对错越一致，代理在更新他们的“信念”评级时就越放心。不一致的同事将收到较小的更新，而可靠错误的同事将收到较大的负面更新，这意味着总是做与他或她建议相反的事情。这具有取更新的“运行平均值”并将运行平均值应用于权重而不是直接应用更新的效果。这将暴露同事输入的时间线上的共识，允许代理将它们分类为“一般错误”或“一般正确”,而不是如果它们在一个时间线上是正确的，而在下一个时间线上是错误的，则撤销更新。动量将来自[0，1]的一个因子。零动量表示更新将被直接应用，而 0.99 动量意味着严重依赖于任何单个更新的平均值。对于初学者来说，从没有动量开始似乎是最有利的，看看添加动量会如何影响结果。通常，动量值选择为 0.9。

渐变裁剪虽然简单，但在类比中可能看起来更简单。梯度是代理人获得的关于他们做出的决定有多错误或正确的信息，以及它应该在多大程度上影响代理人的“信念”值。在这里，剪切此值会强制使用绝对最大值或最小值。这可以防止 boss 小组通过大规模更新来粗暴地扰乱代理的“信仰”价值观。使用渐变剪裁，代理收到的反馈总是限制在两个值之间。网络的当前状态使用非常自由的梯度范围[-5，5]。

在类比中，权重衰减直接对应于“信念值”衰减。这意味着什么？网络中不会有固定的权重，即使它没有接收更新。每一轮训练，体重都会减少一小部分，接近于零。这意味着较大的重量比较小的重量受到更严重的影响。这是防止过度依赖任何单个同事的另一种方式，并且将需要在给定方向上的多次更新，以支持该方向上的大权重，防止衰减。关于这方面的进一步阅读，请看神经网络中的 L2 正则化，因为权重衰减是它的实现。该网络使用恒定权重衰减值 0.0001。

在编码之前，我想讨论的最后一个主题是网络中小批量的实现。虽然该网络一次只能分析一个样品，但它具有跨运行累积数据的功能，并通过对小批量的值进行平均来更新每个小批量的运行。这种方法提供了在训练过程中即时改变小批量的可能性。虽然我还在工作，但我还没有展示任何令人惊讶的用途，但它对用户来说肯定是可用的。请注意，较大的小批量虽然提供了更稳定的解决方案，但可能会导致网络需要更长的时间来收敛，主要原因有一个:网络现在在进行单次更新之前，通过相关的训练对“小批量”进行多次采样。谨慎的步骤变成了缓慢的步骤。

对于神经网络来说，生物逻辑并不具有最大的吞吐量。然而，它被分解成小的构建块，并带有构建它们的工具，同时试图模拟神经元背后的过程及其上面的保险类比。对许多人来说，这是面向对象编程的核心。对我来说，因为这个原因，生物学是我最喜欢的学科之一。这也有一个不幸的副作用，就是在我的文件空间中创建一个更大的项目。我在这里为您提供了一个由 12 个网络子目录类组成的自给自足的分组，在 Math 子目录中还有一个 Vector 类，在测试中还有一个示例 XORGate 类。项目的 x 测试子目录。

我通常从“自上而下”开始，我们首先看项目是如何组合在一起的，然后跟踪依赖关系。然而，我发现如果从“内部到外部”来看，这个项目更容易管理，我将从最基础的类开始，Vector。它只是容纳一个数据数组，并提供对它或其他向量及其各自的数据数组进行操作的功能。

![](img/06875080732fe38ff6c9c2a3ee644a3c.png)![](img/787a323cb089618baf77b817cb09b82d.png)![](img/af8b2180b79b8e61aa488a80ba177bd4.png)![](img/9c8997f53cd5b34c34d8b51c65aac21b.png)![](img/7e1f881155a2c6c97fa3f2e102b36976.png)

太棒了。现在我们有了项目的主要工具。几乎所有的计算都会使用这些向量。我希望这一点很快会变得更加明显。

我们“从内向外”旅程的下一步将是定义我们在网络中使用的激活函数。这里我们只是标记类型，而不是对定义它们的数学进行编码。该计算将在稍后应用激活的 Nucleus 类中进行。

![](img/8b4385e8d6630aff009a4baa6cb0ea03.png)

正如你所看到的，我已经提供了相当多的潜在激活功能在网络中使用。这些大多都很普通，然而 SArSinH 是我自己创造的。这是一种非常不稳定的激活，因此不应该在连续层中使用。它的剧烈激活需要通过另一层逐渐减弱的激活来缓解。事实上，列表中有一个非常适合，因为它可以优雅地处理大量输入。我通过将 SArSinH(比例双曲反正弦)与交替正弦激活配对，比简单地将 ELU(指数线性单位)或 SELU(比例指数线性单位)链接起来获得了更大的成功。我真心鼓励你尝试尽可能多的类型。如果感兴趣的话，你甚至可以自动化这个过程，类似于我在另一篇关于[遗传算法和并行训练](https://rwhildreth09.medium.com/a-java-implementation-genetic-algorithm-to-develop-and-test-neural-networks-with-parallel-training-7adb302d4250)的文章中介绍的方法。如果你想在这个项目中加入你自己的激活，这个方法相当简单。只需将激活的名称添加到这里的列表中，并在即将到来的 Nucleus 类中定义它的功能和派生。只要你能定义它的导数。如果你对衍生品不太熟悉，网上有几个资源可以帮助你计算它们(想想 [Wolfram Alpha)。](https://www.wolframalpha.com/input/?i=d%2Fdx+1%2F%281%2Bexp%28-x%29%29))。

Nucleus 类负责将激活实际应用于树突的加权决策，即代理对同事的输入变得急切或沮丧。这是一个相对较小的班级，有一个简单的目标，正如我们将要看到的:

![](img/88c18b1b4e2c74a689364b8ba533cbf2.png)![](img/1921323bebd027e166ec533ee96021a0.png)![](img/a443a7778f2570fde6fad412ef4840db.png)

敏锐的眼睛可能会捕捉到类底部无用的更新方法。请原谅我，这个项目注定会变得比现在更好。另一个敏锐的观察可能会突出 EntropySigmoid 的存在，它没有列在激活列表中。同样，要编译项目，只需将 EntropySigmoid 添加到激活枚举中。EntropySigmoid 是大量空闲时间的另一个创造，它被设计用于输出层并综合对数损失的利用，同时仍然利用二次损失的公式。出于以下原因，我还没有在这篇文章中引入损失函数的概念；我不会简单地这样做，因为通过简单地从计算值中减去目标，二次损失很容易被解释为“我离目标有多远”。其他损失函数虽然更复杂，但不是这个项目的重要部分。

我要介绍的下一个类是一个更小的实用类，叫做 Axon。轴突只是激活值的存储空间，它将等待下一层的访问。也就是说，这是神经元在需要时做出决定的地方。

![](img/43b44fc2f3296ff3b510345d3f0bcf2d.png)

在预期中，你可能会注意到我们现在有了神经元三个重要部分中的两个。缺失的一环就是所谓的树突类。Dendrites 类虽然不像前面的神经组件那样简洁和令人愉快，但它为这个过程做了大量繁重的工作。在这里，输入信号被分解成多个成分，为向后传递做准备，然后在进入细胞核激活之前，通过树突的重量进行处理。我觉得在这里更详细地介绍一下向后传球本身是合适的。我们必须存储两个“导数向量”这些向量 dwrtI 和 dwrtW(分别相对于输入的导数和相对于权重的导数)在通过网络到权重的适当误差传播的反向传递期间是需要的。这里不需要密集的计算，因为权重和输入之间的乘法链的导数(例如 w1*i1 + w2*i2 + w3*i3 +..)简单来说就是*或者*权重或者输入本身。事实上，dwrtW 只是输入向量的一个副本，dwrtI 只是权重向量的一个副本。回到类比，代理人必须保留关于他或她的同事建议了什么以及代理人相信他们多少的信息(分别是输入和权重)，以便在 boss 小组用错误报告召回她后分配新的“信念”值。很简单，每个输入，在激活之前，通过分配给它的“信念”的数量来影响代理的决定。相应地，每个“信念”值通过相应输入的大小来影响决策。在基于误差的更新中，我们必须知道每个权重对误差的影响。这由 dwrtW 或输入向量给出。为了告诉他或她的同事他们对错误的影响有多大，代理人必须首先根据对他们输入的关注程度来衡量错误。由于很少注意，即很小的权重或“信念”值，对错误的影响很小，因为同事的决定大部分被丢弃，相应的同事将没有太多更新。请注意，在反向传播过程中，在计算任何传入影响之前，代理人已经应用了他或她自己激活的梯度。也就是说，一个头脑冷静或“饱和”的代理人在对同事的决定进行任何指责或表扬之前，已经将反向运行错误最小化了。

![](img/658e3a1e5f9360add498bc4e4c316075.png)![](img/0d51afc1889c76a50b8e0b570fd21e62.png)![](img/74bb68b187ca8d6557460554a53ce8e2.png)![](img/dc15e946afc74919c873243fe1d13508.png)![](img/e07e15741f0412ac80babea296c52450.png)![](img/fe53a3f65de5845d772ab2dc20c41001.png)

请原谅行距不一致，你没有错过任何东西

![](img/915dd6c84eed37161bf0ca6f2667a7b2.png)

太好了！我们现在拥有了神经元的所有部分。我现在将这些片段放在神经元类中。这是一个简单的类，负责接收输入，用树突处理它们，通过细胞核激活，在轴突中存储，并提供对轴突中存储的值的直接访问。这只是一种“直通管理器”,它将各个网段的功能组合到一个封闭的过程中，用于网络中的前向和后向传播。

![](img/607095667fb8ff5cc6de8a7f2c3f20d7.png)![](img/e19c40adcf2d3d0b0fba8dddd978433e.png)![](img/9b7f0e64301df86bbf4e8b398597554b.png)![](img/a6d1f6c1e79dea608372770c23105da4.png)

我们现在有了神经网络的神经元！太棒了。

下一步是创建层。当配备有输入层和输出层时，单层神经元可以被认为是神经网络。正如您将在后面看到的，添加层将是一项琐碎而有趣的工作。我想先从层的基本概念开始。我在这里提供在 Java 中被称为接口的东西。这是一个暗示和保证任何继承类的基本功能的概念。这里的继承类将是 InputLayer、HiddenLayer 和 OutputLayer。我们可以将它们分组为一个通用的层格式，因为每一层本质上都将执行与其他层相同的过程，尽管方式不同。要查看它的运行情况，让我们看一下代码:

![](img/526df21434f10d31e72e9417373dc057.png)

注意，接口不提供方法的解释。它们只是确保任何继承类都具有该功能。这个过程对于即将到来的网络课程来说是不透明的。事实上，网络类只知道它是由通用层组成的，并不关心它们是输入层还是隐藏层等等。我们将在后面的步骤中注意确保在构建网络时，我们自动插入输入层和输出层。用户将只构建网络的隐藏层。从这个意义上说，OutputLayer 只是一个应用于最后一个 HiddenLayer 的 cap，大小相同。

输入层将是最简单的层。它只是存放输入向量，应用任何请求的掩码(回想一下上面关于删除的部分)，并把它交给第一个 HiddenLayer。重要的是，这不需要任何神经元。参考上面的注释，这应该是一个相当简单的实现。

![](img/91c84a48b7b1a357fef436314a56b1dd.png)

请注意，InputLayer 中的 update()或 modifyBias()方法没有任何作用，因为它本身没有要更新的神经元，并且不会以任何偏差修改输入。

在 OutputLayer 中，我们将看到工作中的损失计算。为了计算误差，我使用 MSE，或均方差。顾名思义，这种计算是每个输出和每个目标之间的差的平均值，每个都是平方。这就产生了一个正值的误差，因为所有的正方形都是正的。网络的目标是最小化这个数量。这个“成本”的导数用于计算网络“损失”的方向和大小。MSE 的导数就是输出和目标值之间的差值，乘以输出数量的倒数。如果网络的猜测高于目标值，这个导数结果将是正的。同样，在目标值下产生的估计将在反向传播期间提供负的损失结果。

![](img/904e25ddbebfb3f3433b1ca0d211072c.png)

同样，OutputLayer 内部还有一些未使用的方法。这与 InputLayer 的原因相同。在这个结构中没有神经元来获得功能。

我们需要的最后一层结构是隐藏层。这一个将会涉及更多，但是请记住，它只是神经元的一个“传递管理器”,就像神经元对于它们各自的组件一样。这一层应该接受特征的输入向量，并以决策的输出向量进行响应，该输出向量又可能成为下一个隐藏层的特征向量或输出层的响应向量。在反向传播期间，它应该接收一个错误信息向量，并通过神经元传播该向量，以构建“前一个”(在本例中为“下一个”，因为我们在反向传播期间向后移动)层的影响向量。

![](img/89b0f9122b7b56dc46234ffbc9037cfa.png)![](img/02a268eed01c212f56fd2ce6ff1b5ecd.png)![](img/72b93a3b6b531d47de3588e4ba9f1432.png)

太好了！我们现在拥有了构建完整网络所需的各种类型的层。

在建立网络之前，我想先谈谈另一个概念。这似乎还不明显，但是我们将需要一些方法来保存构建这些层的指令。用户将创建指令，网络将利用它们进行物理构建。我现在想介绍这个概念，虽然它在以后使用时会变得很明显，因为它包含了我们也需要的特定枚举。这个枚举称为 MaskType。它将提供三个可用的遮罩，其中一个不起作用，称为 MaskType.NONE。另外两个提供下拉遮罩，一个用于输入图层，另一个用于隐藏图层。输入丢失，虽然仍然有用，但对我来说有点忌讳。我只是不喜欢在我要求网络做出决定时不给它所有可用信息的想法。我建议只在输入向量非常大的时候使用 dropout。输入丢失与标准版本分开的原因是，通常(我手头没有资源来证实这一点)，对输入使用比隐藏层更低的丢失率是明智的。我真的为这里缺少参考文献而道歉。尽管我有疑虑，但这是我包括在内的一个选择。调整这些辍学率，并根据您可能感觉到的任何异想天开将掩码应用于网络。探索的经验是无可替代的。

![](img/794cba4946764df9acc9bb0108123c23.png)

网络的最终组成部分可能是不言自明的。我们需要一个遮罩对象。在我介绍它之前，让我说我还没有完全涵盖辍学的话题。有一个微小的修改，而不是简单地在向量中的一些随机点赋零。这与传入向量的“响度”有关。通过给它的一些点赋值零，我们抑制了输入向量的法线，有效地使它“更安静”当我们在训练阶段结束时忽略掉的时候，我们会不经意地用所有可用的输入以最大音量使神经元超载。为了弥补这一点，Geoffrey Hinton 等人在论文“[Dropout:A Simple Way to Prevent Neural Networks over fitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)”中建议，不遭受丢失的向量应根据其不得不首先*不归零的可能性进行缩放。例如，如果我们的丢弃百分比设置为 20%，或者值为 0.2，则在没有应用丢弃时通过网络的任何向量都必须按 80%的比例缩放，或者值为 0.8***。这具有抑制向量的效果，从而不会使神经元过载。我在实践中给出了这一过程的解释:*

![](img/9b45e0e507f3c451224abdf014108f4b.png)![](img/2a339202598071d541ed72167f0b63d9.png)

完美无缺。我们现在已经准备好围绕我们到目前为止制作的组件来构建网络了！网络本身再次成为另一个“传递管理器”,为查询错误增加了一点功能。它还提供了两种无需实际训练网络就能计算误差的方法。一种称为 dryTrain()的方法将只通过 OutputLayer 反向传播。然后，您可以像往常一样查询网络的错误。另一个是静态方法 getError()，它需要输出向量和目标向量来返回错误。这些方法将是等效的。

![](img/a766edd55b7a0ca9245694fdb19735b8.png)![](img/5b60e01ba59524cc59ed84696cf3e188.png)![](img/8b1f27226d769246837a821e49f34afc.png)![](img/87406a8bab0ddb8689763b548b49f2a2.png)![](img/8c82e3c460529219cfb4796eb27109c9.png)![](img/998b1c345fab76cc0e4f2cf059fb7c27.png)

我将向您演示该网络。为了真正展示它，我将让它合成一个异或门。如果你不熟悉的话，XOR 门接受两个输入并产生一个单值输出。考虑两个输入 A 和 B 为二进制，不是 0 就是 1。XOR 门对它们进行操作，以表示“如果是其中一个或另一个，但不是两个。”这意味着，给定一个输入向量(0，0)，网络应该输出向量(0)。给定(1，0)，输出应为(1)。同样，(0，1)输出(1)。然而，XOR 门说*而不是两个*，所以输入(1，1)应该导致输出(0)。我使用这个例子的一个主要原因是:虽然可以在没有隐藏层的情况下运行网络，但是网络不可能在没有隐藏层的情况下再现 XOR 门的行为。我鼓励你尝试一下。只有通过增加至少一个隐藏层，网络才能够在这个问题上收敛。在下面的例子中，我们首先构建网络，然后创建数据和训练向量。请注意，如果您将网络的输出激活更改为 TanH，您可以也应该将输出从 0 更改为-1。TanH 是一个很好的决策工具，比 Sigmoid 的范围更广，这意味着决策的阈值也不同。对于 Sigmoid，如果网络选择高于 0.5，则判定为 1，如果网络选择低于 0.5，则判定为 0。使用 TanH，如果网络选择大于 0，则决策为+1，如果网络选择小于 0，则决策为-1。TanH 将允许比典型的 Sigmoid 更活跃的反向传播，因为它的导数更活跃。然而，在这个演示中我没有使用典型的 Sigmoid，而是利用了我的 EntropySigmoid。这个怪物在输出层上没有对手(注意:尽管我喜欢实验，我还是建议不要在任何其他层上使用这个激活*，因为它是非常特别设计的。这相当于乘坐摩托艇去看宇宙飞船比赛)。我将向您展示的示例网络将使用网络采用的所有其他方法，包括重量衰减、下降、多层、不同激活、动量和四个小批量。这是一个传统的极端过度拟合的例子******。注意，由于网络的小批量大小被设置为 4，尽管看到了 20000 个样本，但是该过程总共只会更新网络 5000 次。*

![](img/f70ff24b14b8eca1a1c5442ab24afea4.png)![](img/3d6bb98e5395c1875505d15bee508a28.png)![](img/d7c9713669487cf5bc539c61a6979110.png)![](img/5fbd0b9aa4770d8d8c467aea4c67ba5c.png)

神奇吧。你成功了！恭喜你开发了自己的生物网络！我希望听到你可能找到的任何申请。感谢您的阅读。

— — — — — — — — — — — — —

*   ***这似乎留下了一些解释，说明在退出期间如何重新调整组件。我有一些其他的想法，看起来对小规模的合成数据集很有效。
*   ******我希望增加任何类型的辍学，至少稍微阻碍我们的机会极端过度拟合。对于这个异或门来说，考虑到足够宽的层和不太极端的掉线率，这应该不成问题。综合考虑，体重下降应该还是有更大的影响。