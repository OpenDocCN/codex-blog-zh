<html>
<head>
<title>How to tune hyperparameters for better neural network performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何调整超参数以提高神经网络性能</h1>
<blockquote>原文：<a href="https://medium.com/codex/how-to-tune-hyperparameters-for-better-neural-network-performance-b8f542855d2e?source=collection_archive---------1-----------------------#2021-11-12">https://medium.com/codex/how-to-tune-hyperparameters-for-better-neural-network-performance-b8f542855d2e?source=collection_archive---------1-----------------------#2021-11-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0640" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">举个例子</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/63e8cffab5c8f1e5681a0c4a625b0dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-nm5COcN7T0D-ekc"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">马库斯·斯皮斯克在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="47b4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">到现在为止，你应该知道MLP是一种包含许多变量的灵活方法。在上一篇文章中，我们谈到了调整参数来执行不同的分析。在本文中，我们将讨论优化超参数以获得更好性能的一般方法。</p><h1 id="5d99" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated"><strong class="ak">如何选择多个隐藏图层</strong></h1><p id="f57d" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">改变神经网络基本结构的超参数之一是隐藏层的数量，我们可以将它们分为3种情况:0、1或2、许多。</p><p id="be75" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，如果数据集是线性可分的，你就不需要任何隐藏层。事实上，如果你需要的只是一个线性边界，你根本不需要使用神经网络，因为神经网络是用来解决复杂问题的。</p><p id="fd5f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">第二，如果数据集不是线性可分的，那么你需要一个隐藏层。通常情况下，一个隐藏层就足够了，因为与您需要做的额外工作相比，通过添加隐藏层来改善模型的数量并不显著。因此，在许多实际设置中，一个或两个隐藏层发挥了它们的作用。</p><p id="60c8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，如果你试图解决一个复杂的问题，比如对象分类，那么你需要多个隐藏层，它们对输入进行不同的修改。我们将在以后的帖子中更深入地讨论这一点。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lh"><img src="../Images/3e844dbd8f57a98d0bc9a78ee2e7e7d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*64PuRoia0laAoGFh8WtiXg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">隐藏层数汇总</figcaption></figure><h1 id="a5ee" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated"><strong class="ak">如何选择神经元的数量</strong></h1><p id="61d8" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">下一件事你应该选择的是你要包括在隐藏层的神经元的数量。找到一个合适的数量是至关重要的，因为太少的神经元会导致欠拟合，而太多的神经元会导致过拟合加上更长的训练时间。根据经验，最好在输入和输出大小之间使用一个数字，这个数字会根据问题的复杂程度而变化。</p><p id="4859" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果一个问题很简单，投入和产出的关系很清楚，那么关于投入大小的⅔可以是一个很好的起点。但是如果关系很复杂，这个数字可以从输入大小变化到小于输入大小的两倍。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/a96c415f0c0b071a4c088b2d8a5c5998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFWySghy6TrzV8dI8wcT5A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">隐藏层中神经元数量的汇总</figcaption></figure><p id="5da8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这似乎很模糊，但不幸的是，没有明确的答案可以遵循，因为神经网络仍然是一个活跃的研究领域，每个参数对于每个问题都是独特的。因此，您应该将这些视为起点，并需要通过反复试验来找到最适合您的特定问题的值。</p><h1 id="d18b" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated"><strong class="ak">如何选择批量、学习率、纪元</strong></h1><p id="aca2" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">最后，我们将看看与训练时间和表现相关的超参数。</p><p id="7e69" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当批次大小增加时，每个批次自然变得与完整数据集相似，因为每个批次开始包含更多的观察值。这意味着每一批不会有太大的差异。所以它的噪音会降低，所以用大的学习率来加快训练时间是顺理成章的。相反，当我们使用小批量时，噪声会增加。因此，我们使用小的学习率来抵消噪声。那么我们应该使用哪个批量呢？人们还在研究，但我们可以从别人的经历中找到答案。</p><p id="3d8f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">经验表明，批量大可能导致泛化能力差。相比之下，当我们使用小批量时，噪声有助于网络摆脱局部最小值，并导致更高的精度。它也倾向于比大批量的网络更快地收敛到合理的解决方案。因此，一般来说，32的批量可能是一个好的起点，但这个数字实际上取决于您的样本大小、问题的复杂性和您的计算环境。因此，使用网格搜索也是合适的。</p><p id="1a88" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于学习率，我们通常从0.1或0.01开始，或者我们可以使用从0.1到1e-5的网格搜索。而当学习率较小时，你需要更多的迭代才能找到一个极小点。因此，需要更多的纪元，但是需要多少呢？</p><p id="e165" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">根据问题和随机初始化，收敛所需的历元数会有所不同。因此，不存在适用于所有情况的神奇的纪元数。因此，在实践中，我们通常将历元数设置得很高，并使用早期停止，以便当通过更新其权重获得的改进没有超过阈值时，神经网络停止训练。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lj"><img src="../Images/873dfe349f970008511034611f16239c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VoqVwF3bIv9WYP3a5yvJLA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">学习率、时期、批量的摘要</figcaption></figure><h1 id="49a0" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">优化超参数示例</h1><p id="77e1" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">以此为起点，让我们<a class="ae jn" rel="noopener" href="/codex/identifying-songs-i-like-with-an-algorithm-in-r-a0c51cc44ec5">重温一下告诉我喜欢哪首歌的算法</a>。早在7月份，我将逻辑分类器与lasso结合使用，实现了47.8%的准确率和31.4%的召回率。所以f1的分数是0.379。让我们看看如何通过使用神经网络来改善这些。</p><p id="86c9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，我们需要选择隐藏层的结构。对于这个问题，一个隐藏层就足够了，因为这个问题不是线性的，也不像计算机视觉那样复杂。并且6个神经元可以是一个好的起点，因为总共有10个特征进入神经网络。然后对于输出层，我只需要一个使用逻辑(=sigmoid)激活函数的输出神经元，因为我正在尝试解决一个二进制分类。</p><div class="iy iz ja jb fd ab cb"><figure class="lk jc ll lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/d4357fe342e3ced1b200ab5b88a21739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*bNmU7JlLEJWgvpQwASCMxw.png"/></div></figure><figure class="lk jc lq lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/17c0b5a8b4d58857cfd2df18866b144b.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*tsVxjGMfY4gay_LZ1lbsSQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx lr di ls lt translated">mlp基本结构/基本结构的代码</figcaption></figure></div><p id="2d8b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我有了神经网络的基本结构，我需要调整超参数。我可以从一般的起点0.1学习率开始，但最好尝试一个网格的值，选择最好的一个。所以我实现了sklearn包中的gridsearchCV来测试0.1、0.01、1e-3、1e-4、1e-5中的学习速率和10、20、40、60中的批量大小的哪个组合最适合这个问题。对于epoch，我使用了早期停止，当10次迭代内没有改进时停止训练，并恢复最佳模型，而不是固定的数量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lu"><img src="../Images/d886ab9fe0787766e2b1368a5834ba44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pl-nVpGrrBjK46W5hAhgoQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">网格搜索CV和提前停止代码</figcaption></figure><p id="4037" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">有一点需要深入研究，那就是损失函数。因为我正在处理一个不平衡的数据集，所以使用交叉熵是不合适的。比较f1分数更有意义，因为f1分数更重视我所关心的东西:真正的积极因素。不幸的是，我不能只使用f1分数，因为它是不可微的。因此，我需要修改f1分数，使它变得可微分。但是sklearn包并没有提供这个，所以我从Kaggle那里借了一个<a class="ae jn" href="https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric" rel="noopener ugc nofollow" target="_blank"> Michal Haltuf </a>写的代码。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/f454d74dd4bfe04fe9233f5b743d6f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*gLBy_5-tXPGZYgzLDm1ncA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由<a class="ae jn" href="https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric" rel="noopener ugc nofollow" target="_blank">迈克尔·哈尔图夫</a>编写的f1_loss代码</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/627fdabb9ab2a8294aabe72620bf26aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*RB4-SiGWSIE2KHZ-Xa99bw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">最佳超参数</figcaption></figure><p id="336f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从结果中可以看出，在批量为60的情况下，最佳学习率为0.1。并且使用sklearn提供的函数，我可以用这些超参数提取模型训练的历史。</p><div class="iy iz ja jb fd ab cb"><figure class="lk jc lx lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/ef5643e23e6346c8cfefe06235174b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*db7plTYoLRBUv1zm_FF_AA.png"/></div></figure><figure class="lk jc ly lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/4a7b1457fc091cb3bf96ceef1c75762e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*CeYk483ELf2uOpbe0OokFQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx lz di ma lt translated">学习率为0.1、批量为60的培训历史记录</figcaption></figure></div><p id="4425" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如上所示，预测增加，但回忆和f1损失减少。当我用这个模型进行预测的时候，显示它可以从测试集中检测出8 TP和3 FP。所以大约73%的积极结果是真正的积极。与上次的48%相比，我认为这是一个进步。当我们查看f1分数时，该模型的f1分数比逻辑模型高0.192。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/9650c78f83c1e3b745ecdfe584982eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kmmvGaUajLGlq6XbGq_PjQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">最佳mlp网络的测试结果</figcaption></figure><h1 id="7d74" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">反射</h1><p id="b630" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">需要记住的一点是，每次运行模型时，这个结果都会发生变化。</p><div class="iy iz ja jb fd ab cb"><figure class="lk jc mc lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/3a511d65cf4e42caa32169e888ceb687.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*AOV3H1t172u7fRTEG1KTnA.png"/></div></figure><figure class="lk jc md lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/2b7867ad99333ede805cfd9a9b23b784.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*43MY4MPKjzLFghTb2ttW8g.png"/></div></figure><figure class="lk jc me lm ln lo lp paragraph-image"><img src="../Images/89e2ca6ce8d24f0594078537e2e78492.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*ejnGEm4oFYD8culbyT19GQ.png"/><figcaption class="jj jk et er es jl jm bd b be z dx mf di mg lt translated">试验1</figcaption></figure></div><div class="ab cb"><figure class="lk jc mh lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/e2853e5baa8d439a898266da1bdfbd11.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*tN2pyLVFS3yKGo6gFLIQYw.png"/></div></figure><figure class="lk jc mi lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/754ae0020f012ab5f1d2b7abc728ca3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*M1pkc_4NqN2t3-n3wNyyPw.png"/></div></figure><figure class="lk jc mj lm ln lo lp paragraph-image"><img src="../Images/64e2d1f1877660efd67295ab6072ad8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*PdWheVt3jK7iHNM4rfBtkw.png"/><figcaption class="jj jk et er es jl jm bd b be z dx mf di mg lt translated">试验2</figcaption></figure></div><p id="aa30" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是因为权重和优化包含一定程度的随机性。解决方案之一是重复预测几次，并计算这些结果的统计数据。</p><div class="iy iz ja jb fd ab cb"><figure class="lk jc mk lm ln lo lp paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/980e4f60168ca3f335b385a6e88190b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*yUeC-bj-JZpkZ-P_BJGk8w.png"/></div></figure><figure class="lk jc ml lm ln lo lp paragraph-image"><img src="../Images/3cde592cbf5b2dffcab4b1c9dd8b9884.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*STT-uB1KDaNhqmccLnsG6g.png"/><figcaption class="jj jk et er es jl jm bd b be z dx mm di mn lt translated">30次重复的代码/30次重复的平均统计</figcaption></figure></div><p id="6ae6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">于是，我又重复了一遍，统计表明，每一次的准确率和召回率分别约为52.3%和69.4%。这给了我们大约0.596的f1分数，这比使用逻辑分类器提高了大约0.217。</p><p id="0a50" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">所以神经网络确实表现出了一些改进，但是为什么我们不能获得更高的f1分数呢？</p><p id="e94c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我相信原因之一是样本量小。与其他神经网络项目相比，300是一个非常小的数据集。在这个数据集内，我必须划分训练集、验证集和测试集。因此，网络没有机会从数据集中了解更多信息。然而，我自己并不总是能够收集大量的数据。在这种情况下，我可以使用别人建立的网络，并应用迁移学习，这将是下一篇文章的主题。</p></div><div class="ab cl mo mp gp mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hb hc hd he hf"><h1 id="0d3d" class="kk kl hi bd km kn mv kp kq kr mw kt ku io mx ip kw ir my is ky iu mz iv la lb bi translated">参考</h1><p id="17f3" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">[1]本吉奥，Y. (2012年9月16日)。<em class="na">深度架构基于梯度的训练实用建议</em>。arXiv.org。检索于2021年11月12日，发自https://arxiv.org/abs/1206.5533.<a class="ae jn" href="https://arxiv.org/abs/1206.5533." rel="noopener ugc nofollow" target="_blank"/></p><p id="34f7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[2]布朗利，J. (2019年8月6日)。<em class="na">训练深度学习神经网络时如何配置学习率</em>。机器学习精通。检索于2021年11月12日，来自<a class="ae jn" href="https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/." rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/learning-rate-for-deep-learning-neural-networks/。</a></p><p id="17eb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[3]德农考特，F. (2016年9月22日)。<em class="na">训练神经网络的批量大小和迭代次数之间的权衡是什么？</em>交叉验证。2021年11月12日检索，来自<a class="ae jn" href="https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu." rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/164876/what-is-the-train-a-neu-batch-size-and-number-of-iteration-to-train-a-neu。</a></p><p id="b793" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[4]道格。(2010年8月2日)。<em class="na">前馈神经网络的隐层和节点数如何选择？</em>交叉验证。2021年11月12日检索，来自<a class="ae jn" href="https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw." rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-前馈神经网络。</a></p><p id="ecd7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[5]哈尔图夫，M. (2018年10月19日)。<em class="na">F1得分指标的最佳损失函数</em>。卡格尔。检索于2021年11月12日，来自<a class="ae jn" href="https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric." rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/rejpalcz/best-loss-function-for-f1-score-metric。</a></p><p id="28ee" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[6]j .希顿(2009年)。<em class="na">用Java介绍神经网络</em>。希顿研究。</p><p id="589d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[7]r .凯米(2020年1月31日)。<em class="na">一个神经网络需要多少个隐层和隐节点？—技术文章</em>。关于电路的一切。检索于2021年11月12日，来自<a class="ae jn" href="https://www.allaboutcircuits.com/technical-articles/how-many-hidden-layers-and-hidden-nodes-does-a-neural-network-need/." rel="noopener ugc nofollow" target="_blank">https://www . allaboutcircuits . com/technical-articles/how-many-hidden-layers-and-hidden-nodes-do-a-neural-network-needle/。</a></p><p id="3260" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[8]米乔斯，M. (2017年11月5日)。<em class="na">可视化学习速度与批量</em>。学习机器学习——我的2美分。2021年11月12日，从<a class="ae jn" href="https://miguel-data-sc.github.io/2017-11-05-first/." rel="noopener ugc nofollow" target="_blank">https://miguel-data-sc.github.io/2017-11-05-first/.</a>检索</p><p id="2dc2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[9]m .斯图尔特(2019年7月9日)。<em class="na">神经网络中超参数调整的简单指南</em>。中等。于2021年11月12日从https://towardsdatascience . com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3 Fe 03 dad 8594检索。</p></div></div>    
</body>
</html>