<html>
<head>
<title>NLP Deep Learning Training on Downstream tasks using Pytorch Lightning — NER on CONLL data — Part 3 of 7</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pytorch Lightning对下游任务进行NLP深度学习培训CONLL数据上的NER—第3部分，共7部分</h1>
<blockquote>原文：<a href="https://medium.com/codex/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-ner-on-conll-data-part-fe1512ae4183?source=collection_archive---------4-----------------------#2021-07-23">https://medium.com/codex/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-ner-on-conll-data-part-fe1512ae4183?source=collection_archive---------4-----------------------#2021-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/d116a7d11be9cb322f45316d7dce2af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*sHR3Z2_anC4x1D8OCqirWQ.png"/></div></figure><p id="4a4d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是第3部分，也是本系列的继续。请点击这里的<a class="ae jk" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-intro-part-1-of-6-c338a05f86e6" rel="noopener">介绍文章</a>了解这个系列的动机。我们将在<a class="ae jk" href="https://github.com/kswamy15/NLP_Tasks_PyLightning/blob/main/Bert_NLP_NER_Pytorch_Conll_v3.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中查看关于CoNLL公共数据的命名实体识别(NER)培训的各个部分，并对每个部分做出适当的评论。</p><ul class=""><li id="4ca1" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><strong class="io hj">下载并导入库</strong>——安装了一个名为<a class="ae jk" href="https://pypi.org/project/seqeval/" rel="noopener ugc nofollow" target="_blank"> seqeva </a> l的库——这是计算NER预测指标所需要的。除此之外，安装并导入了常规的Pytorch和Pytorch Lightning库。</li><li id="22be" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">下载数据</strong>—CoNLL数据集经过预处理，可从变压器数据集库中下载。但是出于演示的目的，笔记本从公共存储库中下载数据，并以对培训有用的格式准备数据。有14039个训练数据，3250个验证数据和3453个测试数据。每行都有一个单词标记列表以及每个单词标记的相关标签，如下所示。有关标记方案的详细信息，请参见本文<a class="ae jk" href="https://aclanthology.org/W03-0419.pdf" rel="noopener ugc nofollow" target="_blank">的第2页。有9种不同的NER标签。</a></li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="f180" class="ki kj hi ke b fi kk kl l km kn">sentences = ['Germany', "'s", 'representative', 'to', 'the', 'European', 'Union', "'s", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']</span><span id="87f7" class="ki kj hi ke b fi ko kl l km kn">tags = ['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</span></pre><ul class=""><li id="9251" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><strong class="io hj">定义预训练模型</strong> —这里使用的预训练模型是基于蒸馏的模型。它是BERT的精华版本，速度快60%，内存轻40%，但仍保留了BERT 97%的性能。一旦您成功地完成了这方面的训练，就可以通过更改model_checkpoint变量来尝试其他预训练的模型。</li><li id="3c6e" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">定义预处理函数或数据集类</strong> —此处我们定义Pytorch数据集继承类NERDataset，它将以数据加载器所需的数据集格式创建训练、val和测试数据。Pytorch使用DataLoader类将数据构建成小批。在这个类中，使用预先训练的标记器对数据进行标记。使用LabelEncoder将标记的标签转换成数字。这里需要注意的一点是，transformer tokenizer可能会将一个单词分解成多个单词，因此必须注意将标签正确地添加到被分解的单词中。此外，像CLS、SEP这样的特殊令牌被赋予-100标签，以便在损失函数的计算中可以忽略它们。</li></ul><figure class="jz ka kb kc fd ij"><div class="bz dy l di"><div class="kp kq l"/></div></figure><ul class=""><li id="bcbe" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><strong class="io hj">定义数据模块类</strong> —这是一个Pytorch Lightning定义的类，包含使用数据加载器准备小批量数据所需的所有代码。在训练开始时，训练器类将首先调用<em class="kr">准备_数据</em>和<em class="kr">设置</em>功能。这里有一个<em class="kr"> collate </em>函数来填充小批量。Bert类模型要求小批量的所有输入数据长度相同。collate函数不是将输入数据填充到整个数据集的最大长度，而是帮助将小批量的输入数据填充到该小批量中最大长度的数据。这提供了更快的训练和更少的内存使用。需要特别注意填充target_tags。tokenizer.pad函数将只填充input_ids、attention_mask列。包含每个单词标记的NER标记的target_tags必须单独填充，如下所示。</li></ul><figure class="jz ka kb kc fd ij"><div class="bz dy l di"><div class="kp kq l"/></div></figure><ul class=""><li id="8135" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><strong class="io hj">定义模型类</strong>—DL模型的正向功能在此定义。可以看出，来自最后一个隐藏层的输出(其形状为批量大小、令牌数768)取自Bert模型，并在通过具有9个输出(等于9个不同的目标标签)的线性层发送之前通过丢弃层发送。最终的输出将是这样的——批量大小，令牌数，9。</li><li id="3769" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">传统的NER训练是在前贝尔特时代使用双LSTM完成的。手套嵌入被用作单词令牌嵌入的起点，并且这些嵌入通过用于NER预测的双向LSTM发送。这里有一篇<a class="ae jk" rel="noopener" href="/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a">论文</a>展示了Tensorflow中的一个BiLSTM模型。Glover嵌入没有上下文，因此需要LSTM来构建上下文，但是Bert模型嵌入识别句子中单词的上下文。SparkNLP、Spacy等现在确实有从Bert嵌入开始并使用BiLSTM进行NER预测的模型，这可能是因为与微调大型Bert模型相比，训练BiLSTM会更快。</li></ul><figure class="jz ka kb kc fd ij"><div class="bz dy l di"><div class="kp kq l"/></div></figure><ul class=""><li id="d757" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><strong class="io hj">定义Pytorch Lightning模块类别</strong> —这是定义培训、验证和测试步骤功能的地方。在阶跃函数中计算模型损耗和精度。优化器和调度器也在这里定义。这里需要注意的一点是，CrossEntropyLoss不应该在每个数据的填充标记上计算。对于所有填充的标记，注意屏蔽将为零，因此可以用于识别填充的标记。Pytorch <a class="ae jk" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="noopener ugc nofollow" target="_blank"> CrossEntropyLoss </a>类的默认ignore_index为-100——所以从技术上讲，我们不需要使用np.where语句来用ignore索引替换填充标记，因为填充标记一开始就已经标记为-100。但是为了将来的目的仍然值得知道这一点，其中填充的令牌可能具有不同的数目。</li></ul><figure class="jz ka kb kc fd ij"><div class="bz dy l di"><div class="kp kq l"/></div></figure><ul class=""><li id="31bd" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">计算度量函数用于计算各种目标标签的精确度、召回率、f1和准确度。</li></ul><figure class="jz ka kb kc fd ij"><div class="bz dy l di"><div class="kp kq l"/></div></figure><ul class=""><li id="e3c8" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated"><strong class="io hj">定义训练器参数</strong> —所有需要的训练器参数和训练器回调都在这里定义。我们定义了3种不同的回调——提前停止、学习速率监控和检查点。最新的Pytorch Lightning更新允许使用. yaml文件定义参数，而不是使用argparse来定义参数。yaml文件可以作为一个参数提供给python。py文件。这样，教练参数可以与训练代码分开维护。因为我们使用Colab笔记本进行演示，所以我们坚持使用argparse方法。</li><li id="8556" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">训练模型</strong> —这是使用Trainer.fit()方法完成的。可以在训练器参数中定义一个分析器，以提供有关训练运行时间的更多信息。</li><li id="b7bf" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">评估模型性能</strong> —经过3个时期的训练，我们得到了以下测试数据结果:</li></ul><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="52d4" class="ki kj hi ke b fi kk kl l km kn">Overall results:<br/>{‘test_accuracy’: 0.9712, ‘test_f1’: 0.8669, ‘test_loss’: 0.1460, ‘test_precision’: 0.8618}</span><span id="1a00" class="ki kj hi ke b fi ko kl l km kn">per tag result of :</span><span id="ffd9" class="ki kj hi ke b fi ko kl l km kn">{'LOC': {'f1': 0.95455,   'number': 2618,   'precision': 0.94264,   'recall': 0.9667},  <br/>'MISC': {'f1': 0.83842,   'number': 1231,   'precision': 0.81987,   'recall': 0.8578},  <br/>'ORG': {'f1': 0.89782,   'number': 2056,   'precision': 0.90024,   'recall': 0.89542},  <br/>'PER': {'f1': 0.97329,   'number': 3034,   'precision': 0.97361,   'recall': 0.9729},</span></pre><ul class=""><li id="3093" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">正如我们所看到的，标签是不平衡的(见数字键值)，损失函数可以通过增加权重来帮助它。F1测试86.18分远低于SOTA<a class="ae jk" href="https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003" rel="noopener ugc nofollow" target="_blank">94.6分。这肯定是蒸馏模型所期望的。SOTA网站上列出的模型中很少使用BERT。在2018年，有慢性肾功能衰竭的BiLSTM有大约91分的好成绩。2018年，BERT base的F值为92.4，BERT large的F值为92.8。现在我们已经使用Pytorch Lightning对DL模型的架构有了更多的控制，可以使用其他架构进行实验。</a></li><li id="1f94" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj">对训练好的模型运行推理</strong> —向模型发送示例批处理文本，以从训练好的模型获得预测。这可以用于构建ML推理管道。</li><li id="a2e0" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated"><strong class="io hj"> TensorBoard日志数据</strong> —这将在Colab笔记本中打开TensorBoard，让您查看各种TensorBoard日志。Pytorch Lightning日志默认为TensorBoard，这可以使用Logger回调来更改。</li></ul><p id="48f0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来我们将看看本系列<a class="ae jk" href="https://knswamy.medium.com/nlp-deep-learning-training-on-downstream-tasks-using-pytorch-lightning-multiple-choice-on-swag-eb6a50498307" rel="noopener">第四部分</a>中的多项选择任务训练。</p></div></div>    
</body>
</html>