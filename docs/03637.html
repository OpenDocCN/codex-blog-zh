<html>
<head>
<title>Multi-Variate Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多元线性回归</h1>
<blockquote>原文：<a href="https://medium.com/codex/linear-regression-on-multiple-variables-1893e4d940b1?source=collection_archive---------8-----------------------#2021-09-12">https://medium.com/codex/linear-regression-on-multiple-variables-1893e4d940b1?source=collection_archive---------8-----------------------#2021-09-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3095f639872efc226aa9ce9e04fbe3a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v55w62a-dnLHhtK6"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">布莱克·惠勒在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="8d42" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">理解在sci-kit这样的流行库的幕后发生了什么来实现各种机器学习算法，是任何数据科学家旅程中最困难的部分之一。所以，我今天想到了另一个机器学习算法。在这篇文章中，我们将深入了解这种方法，对其进行编码，然后将其作为预测模型来应用。</p><h2 id="ec68" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">什么是多元线性回归？</h2><p id="36fe" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">多变量线性回归(对多个变量的线性回归)类似于简单线性回归模型或单变量线性回归模型<em class="kt"> ( </em> <a class="ae iu" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6"> <strong class="ix hj"> <em class="kt">点击此处</em> </strong> </a> <em class="kt">(如果你还没有查看我关于单变量线性回归的博客的话)</em>，但是由于有多个自变量贡献于因变量，导致要计算许多系数，并且由于额外的变量而导致更复杂的计算。</p><h2 id="17de" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">模型表示</h2><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/76c03893994f5a624054455713fea3d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*mu0KSgOhzuvDrGw49Ae4bA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">记号</figcaption></figure><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/b2cfa559af359e657f520464a9aa3e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W0D56WwIJ_uqCpMNQwkeCg.png"/></div></div></figure><h2 id="c317" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">加载数据</h2><p id="d406" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">假设你正在出售你的房子，你想知道一个好的市场价格是多少。一种方法是首先收集最近出售的房屋信息，并制作一个房价模型。考虑一个包含俄勒冈州波特兰房价的数据集。</p><p id="5743" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们导入所需的库并将数据集加载到Pandas数据框架中:</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="4db0" class="jt ju hi li b fi lm ln l lo lp">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span></pre></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="lh li lj lk aw ll bi"><span id="91a8" class="jt ju hi li b fi lq lr ls lt lu ln l lo lp">house_data = pd.read_csv('ex1data2.txt', header=None)<br/>house_data.head()</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/d195858130cbd1c93e7b7d12a77722ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*Tjr7ysJRD5mFSBrv5p4dBA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">输出:house_data.head()</figcaption></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><p id="6096" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里的<strong class="ix hj"> <em class="kt">第一栏</em> </strong>是 的<strong class="ix hj"> <em class="kt">面积(平方英尺)，第<strong class="ix hj"> <em class="kt">第二栏</em> </strong>是<strong class="ix hj"> <em class="kt">卧室数</em> </strong>，<strong class="ix hj"> <em class="kt">第三栏</em> </strong>是<strong class="ix hj"> <em class="kt">房价</em> </strong>。这里n = 2 ( <strong class="ix hj">即</strong>房间的大小和数量)</em></strong></p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="1192" class="jt ju hi li b fi lm ln l lo lp">house_data.describe()</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/f8918ef89fc8a4b2c57fd5e671ab875c.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*Rl9qn-X34djSBX7wVnritw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">输出:data.describe()</figcaption></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="796f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">特征标准化</h2><p id="87ad" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在我们着手解决一个问题之前，我们必须首先检查和分析数据。这一步乍一看似乎很简单，但如果没有正确完成，它会非常痛苦。</p><p id="d3cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据规范化(特征缩放)的目的是什么？<br/>这是因为我们的一些特性可能在0–1范围内，而其他特性可能在0–1000范围内。如果您按原样输入数据，您可能会遇到不合适的风险。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/aa1960b3770fd201b5c24a7786c1f1af.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*0HfJK5HySFDGD8BwRMp7mg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">无特征缩放</figcaption></figure><p id="cc7c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的图像中我们可以看到，如果没有特征缩放，我们的轮廓将被拉长，导致梯度下降过程在大量迭代后找到最小值，减慢了我们的方法，有时很难识别最小值。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/3dce34e3af9190d996f82e160d1810b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*99ynoa2GrxzEUrUelxSavw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用特征缩放</figcaption></figure><p id="818a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上图我们可以发现，特征缩放后，梯度下降以较少的步数找到最小值。这加快了我们的算法。</p><p id="1abc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用<strong class="ix hj">均值归一化</strong>以相似的比例缩放我们的特征，</p><p id="a6d9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将xᵢ替换为xᵢ-μᵢ，使特征的平均值约为零(不适用于x₀)</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/e6ec83b8fc0e0053652eef0710664efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*zsjD0yB_wHurbqVmWHf_Ug.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">均值归一化</figcaption></figure><p id="0545" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">示例:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/1f57e24689f9f2f15be16fbf7e02c8b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*M7_eewMv-PjLr7fAJ233QQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">特征缩放前</figcaption></figure><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/63b85aeec9b3ebf62340f558e217f1d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*MtbVSUtYRn314WJgm-q6mA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">特征缩放后</figcaption></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="93a8" class="jt ju hi li b fi lm ln l lo lp">def featureNormalize(X):<br/>    mean = np.mean(X, axis=0)<br/>    std = np.std(X, axis=0)<br/>    <br/>    X_norm = (X - mean)/std<br/>    return X_norm, mean, std</span><span id="7d88" class="jt ju hi li b fi mc ln l lo lp">mod_house_data = house_data.values<br/>m2 = len(mod_house_data[:, 2])<br/>X2 = mod_house_data[:, :2].reshape(m2, 2)<br/>y2 = mod_house_data[:, 2].reshape(m2, 1)</span><span id="71be" class="jt ju hi li b fi mc ln l lo lp">X2, mu, sigma = featureNormalize(X2)<br/>X2 = np.column_stack((np.ones((m2, 1)), X2))</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es md"><img src="../Images/4b04090b34a14d7c409b9b2d203fad55.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*bizzk-kkRzANsrdy4B4myw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">标准化后</figcaption></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="ee8b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">多元回归假设</h2><p id="b86d" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在多元回归中，我们将使用多个变量来预测输出，因此我们的假设是，</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es me"><img src="../Images/ab676e24ddbcb5032270572ed0276c2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*RYr0k3F0XBz8XpUfDzKH_g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">假设</figcaption></figure><p id="23b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，<strong class="ix hj"> θᵗ(Theta转置)</strong>是包含θᵢ(ℝⁿ⁺的大小，n =特征数/列数)所有值的行向量，<strong class="ix hj"> X </strong>(注:这里“x”代表矩阵或列向量)是包含<strong class="ix hj"> xᵢ </strong>所有值的列向量(ℝⁿ⁺的大小)(注:这里'xᵢ'代表数据集中的每一列)。</p><h2 id="d478" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">价值函数</h2><p id="14a5" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">目标是设置参数，使h(x)接近每个x的y值。例如，选择θ₀和θ₁，使h(x)接近每个x的y值。<br/>该条件可以用数字表示如下:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/beb4b30704bb600e78b4a39ba18cbc7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*uwJEHsfKMAoXfvtKLryOIQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">价值函数</figcaption></figure><p id="5d9a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">了解更多关于代价函数的知识(<a class="ae iu" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6"> <strong class="ix hj"> <em class="kt">点击此处</em></strong></a><strong class="ix hj"><em class="kt"/></strong><em class="kt">了解我上一篇文章中更多关于代价函数的知识)。</em></p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="b15e" class="jt ju hi li b fi lm ln l lo lp">def compute_cost(X, y, theta):<br/>    m = len(y)<br/>    h_theta = X.dot(theta)<br/>    J = 1/(2*m) * np.sum((h_theta-y)**2)<br/>    return J</span><span id="47fc" class="jt ju hi li b fi mc ln l lo lp">theta2 = np.zeros((3, 1))<br/>compute_cost(X2,y2,theta2)</span></pre></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="633d" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">多元回归的梯度下降</h2><p id="f5cd" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在我们之前的文章中，我们已经在单变量回归上实现了梯度下降。现在唯一的区别就是matrix X里多了一个特征(想了解更多渐变下降<a class="ae iu" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6"> <strong class="ix hj"> <em class="kt">点击这里</em> </strong> </a>)</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/88856982c0f0fa6139bd40c8652cf42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*HBTw0r1KIsFqNOPcq1FXmw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">更新规则</figcaption></figure><h2 id="7a46" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">选择学习率</h2><p id="bdb2" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">Alpha或学习率决定了算法达到最小成本值(即，给出最小成本值的参数值)所需的步长。</p><p id="7389" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将为数据集尝试不同的学习速率，并找到一个快速收敛的学习速率。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/9980aa3bf4cc55ac9e61972f604d3d4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F-ZuAyZwBOiOiswE.png"/></div></div></figure><p id="281a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当α较小时，梯度下降需要更多的时间来收敛，而当α较大时，梯度下降会过冲而无法收敛。我们可以通过尝试不同的α值来找到一个好的学习率(α)。我建议尝试对数标度上的α值，大约是前一值的3倍(即0.3、0.1、0.03、0.01等)。</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="25c4" class="jt ju hi li b fi lm ln l lo lp">lr = [0.01, 0.03, 0.09, 0.1, 0.3, 0.9]<br/>J_histories = []<br/>for x in lr:<br/>    theta2, J_history = gradientDescent(X=X2, y=y2, theta=theta2, alpha=x, n_iters=100, graph=False)<br/>    J_histories.append(J_history)</span></pre><p id="cf40" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">针对不同的学习速率可视化J(θ),</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="ae0e" class="jt ju hi li b fi lm ln l lo lp">len_J = len(J_histories[0])<br/>for x in range(len(lr)):<br/>    plt.plot(J_histories[x], label=lr[x])<br/>plt.xlabel("No. of Iterations")<br/>plt.ylabel("J(theta)")<br/>plt.title("Cost function using Gradient Descent")<br/>plt.title('Learning rates')<br/>plt.legend(loc=1)</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/f7d800fa42b4a91e50d13249d4382ada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*YAfYlWY5l6_CP0OoPAp10w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">J(θ)表示不同的学习速率(α)</figcaption></figure><p id="1637" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上图可以看出，α=0.01是不错的选择。因此，我们使用0.01作为学习率来训练我们的模型。</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="35b7" class="jt ju hi li b fi lm ln l lo lp">theta2, J_history2 = gradientDescent(X2, y2, theta2, 0.01, 400)<br/>print(f"h(x) ={round(theta2[0,0],2)}+{round(theta2[1,0],2)}x1 + {round(theta2[2,0],2)}x2")</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/2ae5e9c71e420d10b0926f6d31be2442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*cjcv6wBrW_L_pCX7vrXnpQ.png"/></div></figure><h2 id="9336" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">预言</h2><p id="1f29" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">公式保持不变，</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/0bdf4089e85cd7bc880713c38209a879.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/0*ksc-5q3Mgvj06C78.png"/></div></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="c35f" class="jt ju hi li b fi lm ln l lo lp">def predict(X, theta):<br/>    predictions = np.dot(theta.T, X)<br/>    return predictions[0]</span></pre></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><p id="f027" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在使用它进行预测之前，不要忘记对值进行归一化，</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="3ecc" class="jt ju hi li b fi lm ln l lo lp">x_sample = featureNormalize(np.array([1650, 3]))[0]<br/>x_sample = np.append(np.ones(1),x_sample)<br/>predict3 = predict(x_sample, theta2)<br/>print(f”For size of house = 1650, Number of bedroom = 3, we predict a house value of ${round(predict3,0)}”)</span></pre><p id="0a2e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:对于房屋大小= 1650，卧室数量= 3，我们预测房屋价值为430447.0美元</p><h2 id="c9c2" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">结论</h2><p id="5553" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">今天，我们看到了假设、成本函数和多变量回归的梯度下降背后的概念。然后使用python的numpy、pandas和matplotlib从头开始创建它。数据集和最终代码上传到github。</p><p id="a9bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">点击这里查看<a class="ae iu" href="https://github.com/jagajith23/Andrew-Ng-s-Machine-Learning-in-Python/tree/gh-pages/Linear%20Regression" rel="noopener ugc nofollow" target="_blank">线性回归</a>。</p><h1 id="9855" class="ml ju hi bd jv mm mn mo jz mp mq mr kd ms mt mu kg mv mw mx kj my mz na km nb bi translated">如果你喜欢这篇文章，那么看看我在这个系列中的其他文章</h1><h2 id="6a62" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">1.<a class="ae iu" rel="noopener" href="/@jagajith23/what-is-machine-learning-daeac9a2ceca">什么是机器学习？</a></h2><h2 id="be39" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">2.<a class="ae iu" rel="noopener" href="/codex/what-are-the-types-of-machine-learning-53360b7db8b4">机器学习有哪些类型？</a></h2><h2 id="dab1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">3.<a class="ae iu" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6">一元线性回归</a></h2><h2 id="c5d7" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">4.<a class="ae iu" rel="noopener" href="/@jagajith23/logistic-regression-eee2fd028ffd">逻辑回归</a></h2><h2 id="d81b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">5.<a class="ae iu" rel="noopener" href="/@jagajith23/what-are-neural-networks-3a0965e2ebfb">什么是神经网络？</a></h2><h2 id="8473" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">6.<a class="ae iu" rel="noopener" href="/@jagajith23/digit-classifier-using-neural-networks-ad17749a8f00">使用神经网络的数字分类器</a></h2><h2 id="a898" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">7.<a class="ae iu" rel="noopener" href="/@jagajith23/image-compression-with-k-means-clustering-48e989055729">利用K均值聚类进行图像压缩</a></h2><h2 id="9820" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">8.<a class="ae iu" rel="noopener" href="/@jagajith23/dimensionality-reduction-on-face-using-pca-e3fec3bb4cee">使用PCA对人脸进行降维</a></h2><h2 id="635b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">9.<a class="ae iu" href="https://jagajith23.medium.com/detect-failing-servers-on-a-network-using-anomaly-detection-1c447bc8a46a" rel="noopener">使用异常检测来检测网络上的故障服务器</a></h2><h1 id="efe6" class="ml ju hi bd jv mm mn mo jz mp mq mr kd ms mt mu kg mv mw mx kj my mz na km nb bi translated">最后做的事</h1><p id="826b" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">如果你喜欢我的文章，请鼓掌👏一个追随者会是📈回归的和媒体宣传这篇文章是有帮助的，这样其他人也可以阅读它。我是Jagajith，我会在下一个里抓住你。</p></div></div>    
</body>
</html>