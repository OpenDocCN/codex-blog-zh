<html>
<head>
<title>TensorFlow 2.0 Custom Callback in Practice:An Utility for better Data Products</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实践中的TensorFlow 2.0自定义回调:更好数据产品的实用程序</h1>
<blockquote>原文：<a href="https://medium.com/codex/tensorflow-2-0-custom-callback-in-practice-an-utility-for-data-products-be6a066f6eb1?source=collection_archive---------4-----------------------#2021-01-17">https://medium.com/codex/tensorflow-2-0-custom-callback-in-practice-an-utility-for-data-products-be6a066f6eb1?source=collection_archive---------4-----------------------#2021-01-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/f40c17ecb8a8d3d909bb6476a8c7149a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23UYevB2eZEwaoeuiEesyQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:https://en.wikipedia.org/wiki/TensorFlow</figcaption></figure><div class=""/><div class=""><h2 id="59fc" class="pw-subtitle-paragraph iv hx hy bd b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm dx translated">增加增量效益和改善神经网络训练的回调策略</h2></div><p id="4e28" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi kj translated"><span class="l kk kl km bm kn ko kp kq kr di">正如</span>我们所知，神经网络是一系列模拟人脑运作的算法，用来识别大量数据之间的关系。在神经网络的设计过程中，我们有无数的选择来使模型最适合给定的数据。在我准备<a class="ae hv" href="https://www.credential.net/a093f35c-bb93-42ec-b524-75d5e2f9265c#gs.qlqwrz" rel="noopener ugc nofollow" target="_blank"> google TensorFlow开发者认证考试期间，</a>我学到了一些很酷的技术来提高深度学习模型的质量。除了一些关键时刻艰苦的n/w设计选择，如节点数、层数、大量变量&amp;偏置初始化、激活函数等。有一些唾手可得的成果，比如控制学习速度，训练次数等等。在训练非常深的神经网络期间，平衡训练过程的一种方法是在训练过程中最优地添加早期停止。虽然TensorFlow 2.0回调主要用于类似于XGBoost/LGBM等ML算法中的早期停止方法，但它对训练具有完全的控制权。在这篇博客中，我将讨论不同的回调TensorFlow 2.0回调方法——标准的和定制的。</p><h2 id="3404" class="ks kt hy bd ku kv kw kx ky kz la lb lc jw ld le lf ka lg lh li ke lj lk ll lm bi translated"><strong class="ak">回调和收益</strong></h2><p id="b498" class="pw-post-body-paragraph jn jo hy jp b jq ln iz js jt lo jc jv jw lp jy jz ka lq kc kd ke lr kg kh ki hb bi translated">通俗地说，回调就是我们可以用来控制神经网络(NN)训练的控制器之一。具体来说，回调是在训练期间可以实现以下功能的功能块:<br/> a)基于性能监控的早期停止<br/> b)控制学习速率<br/> c)定期存档最佳可用模型权重<br/> d)训练的有条件硬停止<br/>在进入不同的回调策略之前，让我们简要讨论一下回调在模型质量和工程方面的实际好处。</p><p id="37cb" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> i)正则化</strong>:由于回调概念的中心思想是训练的早期停止，在ML机制中，这是一种简单而有效的正则化技术。由于这种技术是克服可能的<strong class="jp hz"> <em class="ls">过度拟合</em> </strong>问题的唾手可得的果实，杰弗里·辛顿称之为“美丽的免费午餐”，很多时候这些简单的技术给我们带来了显著的改善。</p><p id="2405" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> ii)资源节省:</strong>复杂的神经网络训练，涉及大量计算，基于人工智能的应用程序中的大量反向传播计算，涉及图像和视频(例如，自动驾驶汽车、<a class="ae hv" href="https://en.wikipedia.org/wiki/Virtual_reality" rel="noopener ugc nofollow" target="_blank">虚拟现实</a>等)。)具有1000次迭代(时期)的训练过程需要大规模的基础设施。下表说明了3种标准CNN架构在不同硬件平台上的典型训练运行时间，这些平台从一般的CPU到非常高端的GPU。这只是使用涉及静态图像训练的标准架构的单个训练过程。训练许多这样的网络的复杂AI系统(涉及视频),每个网络具有许多模型拟合选项(超参数调整),可以很容易地花费几天时间/相当于GPU计算的并行操作。</p><p id="4631" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了收集更多与N/W培训过程的工程方面相关的信息，我建议您阅读本文。<a class="ae hv" href="https://www.researchgate.net/publication/328458615_Evaluating_Training_Time_of_Inception-v3_and_Resnet-50101_Models_using_TensorFlow_across_CPU_and_GPU" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/publication/328458615 _ Evaluating _ Training _ Time _ of _ Inception-v3 _ and _ Resnet-50101 _ Models _ using _ tensor flow _ across _ CPU _ and _ GPU</a></p><p id="9906" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">通过回调方法的智能，训练的最佳早期停止将节省大量的GPU时间。此外，TensorFlow 2.0回调方法为我们提供了定期保存更新的模型权重的灵活性，因此在可能出现云断开问题的情况下，我们可以节省重新运行的资源。</p><p id="f5cb" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了获得大概的成本(美元)节省估计，例如使用GCP，我宁愿参考以下链接<a class="ae hv" href="https://cloud.google.com/products/calculator/" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/products/calculator/</a></p><p id="fcd1" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae hv" href="https://protect-eu.mimecast.com/s/bbiZCRllJSnzZkNH91mbR?domain=cloud.google.com" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/compute/gpus-pricing</a></p><p id="1b2b" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">业务应用</strong></p><p id="af7d" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在，我们将简要讨论这些不同的回调策略，根据一个所有行业都非常通用的预测方法-时间序列预测，它适用于任何行业。这里我将首先生成典型的模拟时间序列，然后结合不同的回调方法训练NN，以影响训练过程，最终达到我们的目标。包括数据准备代码在内的详细方法可以在这个<a class="ae hv" href="https://github.com/nitsourish/TF2_callbacks-" rel="noopener ugc nofollow" target="_blank"> github库</a>中找到。</p><p id="75ed" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">步骤1:时序准备</strong></p><p id="94cc" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里我们创建一个idea时间序列，具有趋势性、季节性和4000个时间戳的噪声。以下是该系列的组成部分。</p><pre class="lt lu lv lw fd lx ly lz ma aw mb bi"><span id="4eab" class="ks kt hy ly b fi mc md l me mf">time_period = 4000<br/>baseline = 10<br/>trend = trend(time,0.05)  <br/>baseline = 10<br/>amplitude = 35<br/>slope = 0.004<br/>noise_level = 3<br/>seasonality_period=400</span></pre><p id="43d9" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们已经合成了3350个时间戳训练数据和650个时间戳的测试/验证窗口。我们选择均方误差(MSE)作为这个问题的单一评价指标。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/3c2f76c897284a4a7f0e29f91cff3f11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*LqrFYu2H-qhibim-aU5mGQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">合成时间序列</figcaption></figure><p id="1173" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">第二步:设置基准</strong></p><p id="0294" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">由于该系列在数学上具有可预测的模式，让我们通过平滑该系列来创建一个大概的解决方案——基准移动平均线(MA)解决方案。当平滑信号时，MSE是45.5676。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es mh"><img src="../Images/3e3d87a8c721c44384942427826699e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*0OsGZ8yks9qgopfzYguz3Q.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">移动平均预测</figcaption></figure><p id="d287" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们能使用更复杂的统计建模技术来改进预测吗？让我们试着用一个标准的ARIMA(自回归综合移动平均)模型来得到更精确的解。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/38d9ddf9a8983adb7afe11c28aa73e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*qrWqChV3ugUoEPp-CpYFZg.png"/></div></figure><p id="2ef9" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在，我们有了一个优秀的模式和验证基准，MSE为21.882，几乎减少了50%。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/c532d476afa54094b35ce9ee9bbc0c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*zY3_JQCSOVMgBtvVuYbLiQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">ARIMA预测</figcaption></figure><p id="53b7" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">由于我们已经有了近乎完美的预测模式，让我们来探索一下神经网络是否能超越这个基准。</p><p id="f52f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">第三步:带回叫功能的NN训练</strong></p><blockquote class="mk ml mm"><p id="2f76" class="jn jo ls jp b jq jr iz js jt ju jc jv mn jx jy jz mo kb kc kd mp kf kg kh ki hb bi translated">数据准备</p></blockquote><p id="05e7" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">首先，我们必须准备好数据，使其适合输入标准的神经网络。与任何其他ML问题一样，我们必须将数据分为特征和标签。在这种情况下，我们的特征实际上是序列中的一些值，我们的标签是下一个值。这里的值的数量是窗口大小。我们采用一个数据窗口来训练模型，以预测下一个值。例如，如果我们取20个时间戳(窗口大小)，20个值将被用作特征，下一个值是标签，并且该模式将以滚动窗口为基础遍历整个系列。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mq"><img src="../Images/69b89ccf46eb9befbd3af9bb09d20d68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-utBtq_EgAI3KOq3LuKstg.png"/></div></div></figure><p id="e339" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">以下是N/W的数据准备和输入方法的设置</p><pre class="lt lu lv lw fd lx ly lz ma aw mb bi"><span id="1414" class="ks kt hy ly b fi mc md l me mf">window_size = 20<br/>batch_size = 32       <br/>shuffle_buffer_size = 1000 #To break Sequence bias<br/></span></pre><blockquote class="mk ml mm"><p id="5a1d" class="jn jo ls jp b jq jr iz js jt ju jc jv mn jx jy jz mo kb kc kd mp kf kg kh ki hb bi translated">模型架构</p></blockquote><p id="a2fb" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在的架构是一个2层双向LSTM(长短期存储器)，LSTM。尽管出于监控目的，我们将着眼于MSE，但对于训练优化，Huber损失将用作损失函数，这是稳健回归中的标准函数，与平方误差损失相比，对数据中的异常值不太敏感。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mr"><img src="../Images/bc352c2bd9ad87bf7ef293ea2d115d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5qwfSliTI9LhrEPGm2IFQ.png"/></div></div></figure><ul class=""><li id="b79c" class="ms mt hy jp b jq jr jt ju jw mu ka mv ke mw ki mx my mz na bi translated">*第一个Lambda层增加了处理一维数组维数扩展的灵活性。窗口数据集是一批二维数据。但是LSTM期待三维；</li><li id="c97c" class="ms mt hy jp b jq nb jt nc jw nd ka ne ke nf ki mx my mz na bi translated">批量大小、时间戳数量和序列维度。</li><li id="efa2" class="ms mt hy jp b jq nb jt nc jw nd ka ne ke nf ki mx my mz na bi translated">第二个lambda函数将输出放大100倍。由于LSTM图层中的默认激活函数是tanh(双曲正切激活),因此会输出介于负1和1之间的值。由于时间序列值的顺序为10秒，随着时间传播逐渐上升，将原始输出乘以100的系数使得N/W输出与目标大致相同，因此有助于学习过程。</li></ul><p id="21d8" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在将带有回调的NN应用于基于ML的预测时，我们可以将这些方法大致分为两类——1)标准的TF方法，2)定制的回调子类</p><p id="2970" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> 1) </strong> <strong class="jp hz">标准TF方法</strong></p><p id="0fb1" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">a)<strong class="jp hz"><em class="ls">ModelCheckpoint</em></strong>:作为一个标准的TF 2.0回调方法，model check point基于定义的评估指标的最佳值(例如，准确性、损失等)，在某些时期后定期将模型保存为检查点文件(hdf5格式)。).虽然不控制训练，但是这种方法在恢复重要的训练信息方面是有用的，特别是在非常长的训练时期的情况下，以避免在系统故障的情况下丢失训练更新。例如，在更便宜的AWS EC2 spot实例上的训练，即使m/c突然不可用，训练也可以从最后保存的重量恢复。</p><pre class="lt lu lv lw fd lx ly lz ma aw mb bi"><span id="0ad2" class="ks kt hy ly b fi mc md l me mf">checkpoint_filepath = '/callback/model.h5'<br/>model_checkpoint_callback = ModelCheckpoint(<br/>    filepath=checkpoint_filepath,<br/>    save_weights_only=True,<br/>    monitor='val_mse',<br/>    mode='min',<br/>    save_best_only=True)</span></pre><p id="7b82" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">b) <strong class="jp hz">提前停止回调</strong>:这是默认的TF2回调方法，防止模型过拟合。早期停止在训练期间监视保持验证集上每个时期的模型性能，并基于以下预定义的收敛条件终止训练。通过此功能，我们可以监控任何内部TF指标(val_accuracy，val_mse)或任何自定义指标函数(val_auc，val_r2)</p><p id="9e1c" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">I)耐心:评价指标无改善的时期数。</p><p id="9868" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">ii)min_delta:满足改进标准的被监控度量值的预定义最小变化。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ng"><img src="../Images/ed80759a7434b74c0c18254ee5a4a9e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_FiH2bI_SNXB2XP3PoQtQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">TF2回调-提前停止</figcaption></figure><p id="6210" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">尽管有一个不错的初始解决方案，但是在太低的耐心值的情况下，总是有陷入局部最小值的可能性，并且不能实现最大性能。此外，如果模型已经达到全局最小值，非常高的耐心值可能会降低性能(过度拟合)和不必要的额外时段运行。<br/>准确地说，这里训练停止，因为MSE在5个时期内没有提高到25.2782以上，但我们不知道这是否是这种NN配置下的最佳解决方案。如果我们绘制损失随时期的演变，似乎训练和验证曲线在大约75个时期后变平。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nh"><img src="../Images/5a96656e8b66a29de58acf8e93a36322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*njMWMlJY48MgkKwm8rWfCw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">训练和验证MSE图</figcaption></figure><p id="cbb3" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然而，当我们放大该区域时，我们发现训练MSE呈下降趋势，验证MSE呈波动轨迹。这意味着，如果我们再等几个时期来超越可能的局部最小值，可能还有进一步改进的余地。但是我们不知道还需要多少次训练来提高。</p><p id="1c26" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">c) <strong class="jp hz"/></p><figure class="lt lu lv lw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ni"><img src="../Images/8530cac066c6123a1ab30f18f1642ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ROKbXVV7ojQslARt2XHFLQ.png"/></div></div></figure><p id="c60b" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">虽然它没有早期停止功能，但一旦学习停滞，模型通常会受益于将学习速率动态降低2-10倍，以克服过拟合问题(新学习速率(LR) =因子* LR)。</p><p id="73a0" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">还有其他TF2实用程序回调方法，如BaseLogger、CSVLogger、CallbackList、ProgbarLogger、TensorBoard等。主要是为了更详细的培训信息(日志)。</p><p id="d46f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对于标准回调方法，没有基于评估度量的单个值的条件停止实现。到目前为止，我们得到的最佳MSE是25.2782，这与基准测试数据21.882相差甚远。我们可以使用相同的N/W架构改进模型吗？我们将在下一节探究答案。</p><p id="320b" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> 2) </strong> <strong class="jp hz">定制回调子类</strong></p><p id="b767" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这些回调方法属于TF2基类“tf.keras.callbacks”。通过对这些回调进行子类化，我们可以通过添加训练的条件停止来在训练过程中带来更大的灵活性，并且可以在训练/批次/时期开始或结束时执行某些功能。这里，可以有条件地停止训练，以达到评估度量的单个定义值。这些函数的名称解释了用途，例如“on_epoch_end”或“on_epoch_begin”等。一般来说，对于一个非常长的时间周期，如CNN应用程序和一个非常高分辨率的图像，最好等到时间周期结束，因为可能会有明显的波动。<br/>让我们将这个策略应用到我们的案例中，看看它是否能击败21.882的基准MSE。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nj"><img src="../Images/2ac130f81bdd6ca0ca2135d631186f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XiT4-ddltzskBoxO58mROA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">使用自定义回调子类进行培训</figcaption></figure><p id="33f5" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">随着等待，直到第839个时期结束，我们看到训练条件收敛，因为我们达到验证MSE 21.8755，刚好低于定义的基准MSE 21.882。我们是否可以通过运行更少的历元来实现相同的目标，从而节省一些计算资源？</p><blockquote class="mk ml mm"><p id="8090" class="jn jo ls jp b jq jr iz js jt ju jc jv mn jx jy jz mo kb kc kd mp kf kg kh ki hb bi translated"><strong class="jp hz"><em class="hy">learning rate scheduler</em></strong>:如果我们通过搜索大范围的学习率(LR)来包含调整学习率的智能，我们就可以。该功能允许在<br/>学习过程开始时大幅度改变重量，在<br/>学习过程结束时进行小幅度改变或微调。通常，训练以相对较大的值开始，并在随后的训练时段中减小该值。为了调整学习率，我们应用了一个自定义lambda函数，该函数通过将当前epoch作为参数并将其作为调度参数传递给tf.keras.callbacks类来返回所需的学习率。通过快速运行100个历元，LR探测范围从1e-8到1e-4，我们选择最佳稳定LR为1e-5。</p></blockquote><pre class="lt lu lv lw fd lx ly lz ma aw mb bi"><span id="4886" class="ks kt hy ly b fi mc md l me mf">lr_schedule = tf.keras.callbacks.LearningRateScheduler(<br/>    lambda epoch: 1e-8 * 10**(epoch / 20))</span></pre><figure class="lt lu lv lw fd hk er es paragraph-image"><div class="er es nk"><img src="../Images/ab5395e58940f0539914e40e712ac585.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*vZh6UAMiq-jxigvSueJOnQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">寻找最佳LR</figcaption></figure><p id="9fb2" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在，结合自定义回调子类和学习率调度器，我们可以将选择最佳LR 1e-5的训练时间减少到115！。最后，通过应用带有回调子类的ModelCheckpoint，我们可以定期保存最佳模型，以避免任何信息丢失。</p><p id="119a" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这样，我们通过将目标MSE 21.589提高1.35%(改进的验证MSE 21.589)，节省了大量计算资源。</p><figure class="lt lu lv lw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nl"><img src="../Images/68292ce7e2c2063db5b49155949a2c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJyLQA75uDgaD9GzDeUo1g.png"/></div></div></figure><h2 id="9932" class="ks kt hy bd ku kv kw kx ky kz la lb lc jw ld le lf ka lg lh li ke lj lk ll lm bi translated">外卖:</h2><p id="5ee8" class="pw-post-body-paragraph jn jo hy jp b jq ln iz js jt lo jc jv jw lp jy jz ka lq kc kd ke lr kg kh ki hb bi translated">虽然TensorFlow有大量的功能来设计神经网络、控制训练过程和优化计算资源，但TF2的这一特殊功能不仅帮助我们构建惊人的数据产品，还帮助我们更密切地关注模型训练。虽然TF有许多选项来减少过度拟合，但提前停止训练的回调方法是为学习、监控、记录和资源优化添加指导原则的最简单方法之一。</p></div></div>    
</body>
</html>