<html>
<head>
<title>Fantasy or Not: Reinforcement Learning and Fantasy Football? (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">幻想与否:强化学习和幻想足球？(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/codex/fantasy-or-not-reinforcement-learning-and-fantasy-football-part-2-2a54fbed40a?source=collection_archive---------6-----------------------#2021-10-09">https://medium.com/codex/fantasy-or-not-reinforcement-learning-and-fantasy-football-part-2-2a54fbed40a?source=collection_archive---------6-----------------------#2021-10-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="2652" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">RL结构和一个简单的多臂Bandit (MAB)实现</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/43faacab18b894e89f51d06bac36a98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_B01Ydpup8yvaFXln_qU6g.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">阿米尔·巴舍尔</figcaption></figure><h1 id="01f7" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">背景</h1><p id="e64f" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">这是两部分系列的第二部分，第一篇文章<a class="ae jn" rel="noopener" href="/codex/fantasy-or-not-reinforcement-learning-and-fantasy-football-part-1-9e4de960c891">在这里</a>。在那篇文章中，我提出了使用强化学习(RL)的想法，使用我自己的联盟数据(来自<a class="ae jn" href="https://github.com/cwendt94/espn-api" rel="noopener ugc nofollow" target="_blank"> ESNP api </a>，在那里可以找到任何ESPN联盟)来推荐阵容。但是现在我们有了几周的数据，我将开始使用实际的RL技术。在这篇文章中，我们将坚持一个非常基础的Multi-Armed Bandit版本，同时简要地提到更复杂的方法可能会在后面出现。这主要意味着强化学习的练习，而不是试图产生比大多数幻想应用程序内置的有用建议更有意义的结果。</p><h2 id="add8" class="lc jp hi bd jq ld le lf ju lg lh li jy kp lj lk ka kt ll lm kc kx ln lo ke lp bi translated"><strong class="ak">系列链接</strong></h2><p id="56f5" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated"><a class="ae jn" rel="noopener" href="/codex/fantasy-or-not-reinforcement-learning-and-fantasy-football-part-1-9e4de960c891"> 1。幻想与否:强化学习和幻想足球？第1部分:将强化学习应用于梦幻足球的介绍和工具</a></p><p id="ebe4" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">2.幻想与否:强化学习和幻想足球？第2部分:RL结构和多臂土匪(当前)</p><h1 id="c26d" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">方法</h1><h2 id="9a6e" class="lc jp hi bd jq ld le lf ju lg lh li jy kp lj lk ka kt ll lm kc kx ln lo ke lp bi translated"><strong class="ak"> MAB简介</strong></h2><p id="3f03" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">多臂强盗 (MAB)是一个经典的入门RL问题，其中一集只是一个动作。介绍性的例子通常是一个代理(或土匪)玩许多老虎机，它必须发现提供最高平均奖励的老虎机。代理在动作之间进行选择，为每个动作形成一个值。在这样的环境中，状态和动作空间是相同的。随着对价值的信心随着足够多的行动而增加，行动选择将收敛到为代理提供最大回报的行动。</p><p id="3500" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">我们将设置和更新花名册视为这个问题的一个版本。代理人可以选择任何位置球员与任何自由球员交换。目前，我们忽略交易。(上个星期，我一直处于一个短的结尾…我可以订阅这个！)我们不会等待每周的结果来提供奖励，而是使用预测点数来代替这个初始练习。这很容易从每个玩家的<a class="ae jn" href="https://github.com/cwendt94/espn-api" rel="noopener ugc nofollow" target="_blank"> ESPN API </a>中获得。在下一篇文章中，我们将用我们自己的点预测模型替换这个替身。</p><p id="cbcd" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">现在，你可能会注意到，我们可以很容易地对每个位置进行最大化，并迅速得出相同的答案。你可能是对的，但重点是把这当成一个RL问题。在下一篇文章中，我们将在这个框架中插入一个更有意义的概率模型。</p><h2 id="b9f3" class="lc jp hi bd jq ld le lf ju lg lh li jy kp lj lk ka kt ll lm kc kx ln lo ke lp bi translated"><strong class="ak">幻想设定</strong></h2><p id="ff2c" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">首先，我们将从我的实际球队名单中形成一个活跃的球员名单。由于我每周只能从总共16名球员(2名RBs，2名WRs，1名Flex和所有其他位置的1名)中选择9名活跃球员，我需要为每个位置选择每个位置允许数量的最大预计得分者(n)。下面的函数就是这样做的，并返回一个Pandas数据帧和结果。注意，对于每个位置,“n”被硬编码为Pandas方法“nlargest”的输入。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lv lw l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">通过最大化投影点数返回活动玩家名单</figcaption></figure><p id="4d23" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">这是传入我的完整花名册时返回的数据帧:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/177c04b0942c5ca79a9a1e170166f3a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*1bgpnMtRG2d-3--dfX5PQA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">现役花名册(基线队)</figcaption></figure><p id="6b41" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">下一个主要步骤是定义行动空间。我已经生成了每个位置所有球员的数据框架。我将只查询自由球员，并将行动总数设置为可用球员总数。现在，我们将根据<a class="ae jn" href="http://incompleteideas.net/book/RLbook2020.pdf" rel="noopener ugc nofollow" target="_blank">萨顿和巴尔托教科书</a>第二章中的内容，格式化MAB算法的关键方面。首先，我将使用一个二维矩阵来形成Q表和n表(计算每个州的访问量)。第一个维度是被换出的位置(9个空位)，第二个维度是该位置要换入的自由球员(每个自由球员的数量不同)。Q和n表被调整到最大的自由代理池，但是我们将用低负值填充不必要的槽，所以它们被我们的动作选择标准忽略。实现在下面的代码片段中。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="28fe" class="lc jp hi bd jq ld le lf ju lg lh li jy kp lj lk ka kt ll lm kc kx ln lo ke lp bi translated">该算法</h2><p id="238a" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">现在我们终于可以实现算法了。对于奖励，我们将首先使用“基线”活动花名册计算当前的预计总积分。每次交换都会产生一个新的总点数，我们从中减去基线。因此，回报是任何自由球员交换的预期积分提高。现在可能太简单了，但是当我们想要应用我们自己的投影点模型时，这是一个很好的起点。</p><p id="48da" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">我们将使用一个基本的bandit算法，它存储当前的Q值以及访问每个州或采取行动的次数(记住，对于这个MAB问题，州和行动是可以互换的)。使用下面的增量更新规则，我们可以连续地估计平均值，即使这个基本版本是确定的。<em class="ly"> Q </em>是Q表，<em class="ly"> N </em>是访问量计数器表，<em class="ly"> p </em>是位置，<em class="ly"> fa </em>是要换入的自由代理，<em class="ly"> r </em>是奖励。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/e3a957a798acbdcfee735197ab780434.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sy8X5Q5V502cKZRUjIvHVA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">增量平均或MAB更新规则</figcaption></figure><p id="1d03" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">这样，我们几乎拥有了实现完整算法的所有部分。我还有一个“get_action”函数，它将接受E-greedy exploration的epsilon值。如果随机生成的值低于ε，它将返回状态空间中的随机动作。否则，它将选择当前的最大值。由于环境目前是确定的，因为我们每次采取相同的行动都会得到相同的回报计算，所以我们只需要到达每个状态一次(采取每个行动一次)。但是我们已经建立了一个框架，来估计下一次的概率环境。下面，我有算法，运行1000集，ε值为0.3，以确保每个状态空间至少达到一次。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h1 id="989f" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">结果和结论</h1><h2 id="c5ea" class="lc jp hi bd jq ld le lf ju lg lh li jy kp lj lk ka kt ll lm kc kx ln lo ke lp bi translated">结果呢</h2><p id="a24b" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在运行了1000集之后，Q表被完全填充。如果我们在整个表中取4个最大的参数，我们可以列出4个最高值的动作。在我们的情况下，这是自由球员的替代，提供了最大的增加，或最少的减少，如果没有增加。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/3c7c52c65c6322b8414c1b101d206d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NLqjQIAzJQ8tqgkiWKJqaQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来自基本MAB算法的顶级自由球员推荐</figcaption></figure><p id="461b" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">不出所料，只有一个动作会导致奖励增加，所有前4名推荐都是踢球者。如果我还没有达到或者非常接近预计的最高分，我将不会通过简单的自由球员交换来达到目的，从而组建一个劣质的团队。此外，踢球者有较低的点预测，我不会得到很多里程数的交换。因此，从踢球员交换和大量其他自由球员踢球员的角度来看，在我的踢球员下面有一个小的增加。</p><p id="4ce8" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated"><strong class="ki hj">结论</strong></p><p id="ab08" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">所有这些仍然是微不足道的，用基本数学和许多其他方法很容易得到结果。但是更复杂的RL已经准备好了，它在更复杂的环境中会有用得多。人们可以应用神经网络来计算价值，或者形成我们自己的投影点数，作为更稳健结果的概率分布(特别是如果您使用重尾分布来说明一些玩家更可能出现的历史点数增加或减少)。在ESPN的应用程序中，这些被称为“繁荣”和“萧条”的概率，或者一个玩家得分远高于或低于他们预计得分的机会。远离简单的正态分布，希望这种模型将使模拟有趣，并阐明为什么把它作为基于模型的RL问题是有用的。同样，这可能不会击败你的应用内建议，但它可能会让你更深入地了解投影本身可能来自哪里。</p><p id="e370" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">本文代码的完整Jupyter笔记本可以在这个<a class="ae jn" href="https://github.com/SunayBhat1/Writing-Code/blob/main/Fantasy%20RL%20Series/RL_basic.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>找到。</p><p id="0bc9" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">这篇简单的关于RL和ESPN api python库的两部分介绍到此结束，我希望你从中找到一些有用的东西！</p><h1 id="5c38" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">来源:</h1><p id="1a74" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">[1]多臂土匪。维基百科。<a class="ae jn" href="https://en.wikipedia.org/wiki/Multi-armed_bandit" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Multi-armed_bandit</a></p><p id="8b89" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">[2]萨顿、理查德s .和安德鲁g .巴尔托。2018.<em class="ly">强化学习:简介</em>。第二版。自适应计算和机器学习系列。麻省剑桥:麻省理工学院出版社。</p><p id="9ae2" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated">[3]https://github.com/cwendt94/espn-api EPSN API<a class="ae jn" href="https://github.com/cwendt94/espn-api" rel="noopener ugc nofollow" target="_blank"/></p><p id="687e" class="pw-post-body-paragraph kg kh hi ki b kj lq ij kl km lr im ko kp ls kr ks kt lt kv kw kx lu kz la lb hb bi translated"><strong class="ki hj"/>【4】Choudhary，Ankit。强化学习指南:用Python从零开始解决多臂土匪问题(2018)<a class="ae jn" href="https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/09/reinforcement-Multi-Armed-Bandit-Scratch-Python</a></p></div></div>    
</body>
</html>