<html>
<head>
<title>The Way I Learned the Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我学习梯度下降的方法</h1>
<blockquote>原文：<a href="https://medium.com/codex/the-way-i-learned-the-gradient-descent-cb408e6149e4?source=collection_archive---------17-----------------------#2021-07-30">https://medium.com/codex/the-way-i-learned-the-gradient-descent-cb408e6149e4?source=collection_archive---------17-----------------------#2021-07-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0ec8284a4bb4c648d3b12cc8c2b7bf6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7YYmeyKS7z1vfgVU"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">卡伦·艾姆斯利在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="456e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大家好！我也不知道怎么写搞笑:(所以就直接跳到正题了。</p><p id="1930" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度下降是一种流行的迭代优化算法，它是机器学习中许多其他优化技术的基础。<br/>这是什么意思？简单来说，优化是一个通过调整目标函数中的权重/参数来尝试在模型中获得最小损失的过程。我知道我说过这很简单，但看起来并不像。没问题，我会为你分解我所说的，我会确保它保持简单。</p><p id="d849" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">每一个最难的话题(梯度下降没那么难)都可以很容易地用一个相关的例子来教授。对于这个主题，我使用线性回归作为例子。希望你了解线性回归。如果你不知道，不要担心，我会在这里为你输入简单的台词。无非是找一条符合给定数据的线/面来预测未知数据的类标签。</p><p id="9951" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们如何知道一条特定的线何时符合数据？当它合适的时候，损失最小。</p><p id="540b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">什么是损失？它是实际值和预测值之间的差值。</p><p id="0697" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">比如说，我们把脚长作为特征，把人的身高作为类别标签。现在我们需要找到一条线路，使其产生低损耗。对于线性回归，损失是损失的平方，即(实际值-预测值)的平方。众所周知，<em class="jt"> y= mx+c </em>是直线的方程。寻找符合数据的最佳直线就是寻找直线的斜率(m)和截距(c)的最佳值。斜率是直线相对于轴的陡度，截距是直线与轴的交点。</p><p id="1299" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们问题的损失函数是平方损失的平均值，即，</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ju"><img src="../Images/f94e77d0de3137fdbf064883b7e719e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*6t1fIG1_dRPE8j0O6SbEjQ.png"/></div></figure><blockquote class="jz"><p id="8e79" class="ka kb hi bd kc kd ke kf kg kh ki js dx translated">等等什么是损失函数？这是一个等式，它表明当我们改变该等式的权重/参数(这里是斜率和截距)时，我们获得了多少损失。请记住，如果损失巨大，这意味着实际值和预测值之间的差异很大。</p></blockquote><p id="839b" class="pw-post-body-paragraph iv iw hi ix b iy kj ja jb jc kk je jf jg kl ji jj jk km jm jn jo kn jq jr js hb bi translated">预测值取决于拟合线，损耗取决于预测值，因此损耗取决于拟合线(斜率和截距)！！不要混淆XD</p><p id="1c0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如果您找到这些权重(斜率和截距)的值，使损失(L)最小，那么我们的工作就完成了。</p><p id="909f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们如何找到这些权重的值？这就是这篇博客的全部意义！XD完整的走一遍。</p><p id="0f1d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下图举例说明了当直线的斜率和截距发生变化时，损耗是如何变化的。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/6b403b6ef05c54e9f2550b0a8d7daec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8-dZtB8_GEhjC-lr4k6yQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">[图片由作者提供]</figcaption></figure><p id="07bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">x轴代表人的脚长(以厘米为单位), y轴代表人的高度(以厘米为单位)。</p><p id="10a6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于线y=0.61x+145，(斜率= 0.61，截距=145)，均方损耗为1.52。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/9a07d6447a629351473aafa496505988.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*cJfwCZPhwQyf8MhgpNRkug.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">[图片由作者提供]</figcaption></figure><p id="7e57" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">脚长和身高是给定的数据。当x(英尺长度)值被代入y = mx+c时，预测身高就是y的值。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/a70e7d18d9563ebaed760ef6900fa325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m21Bk1AOTDicws4KEC9kjQ.png"/></div></div></figure><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/92dc5a50261865f5b75e7b6d46913bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*uSXYrMdk1HX_QTdoLYTKLw.png"/></div></figure><p id="e786" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于线y=0.29x+155.2，损耗为6.11。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/64d16b9ab04b00a81af8c6826322c3c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RlAHc97wKd2fLTdJY90Oig.png"/></div></div></figure><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/05881756b8b7ee361eb1f09f17e7474b.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*8adD_y9keLs3I34oIZKY5A.png"/></div></figure><p id="9674" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于线y=0.75x+139.5，损耗为5.78。</p><p id="3bc1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看，损失函数明显依赖于斜率和截距(这里我们称之为参数或权重)。</p><p id="894a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个损失函数的3D图(近似图)显示了当斜率和截距变化时损失是如何变化的。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/e0f1aca2a901f90e8603c8d1b271866a.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*8AmeJIGqp4zdJa8EKoVMjQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">[图片由作者提供]</figcaption></figure><p id="56ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">斜率(w1)在X轴，截距(w0)在Y轴，误差在Z轴。</p><p id="1e5e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该图显示斜率(w1)为0.6，截距(w1)为145，误差为1.58，其他点也是如此。</p><p id="637b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意:我们只有两个权重(斜率和截距),因为我们只取了一个要素，在本例中直线是2D，但在现实生活中会有许多要素，因此维数会很大，所以我们不能使用2D线来拟合高维数据，我们使用超平面。2D线只有一个斜率和截距(因此有两个权重)，但是超平面有许多斜率和截距(许多权重)。所以，我们无法想象/画出具有两个以上权重的损失函数图。</p><p id="3f86" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们需要找到权重(斜率和截距)的最佳值，以使误差/损失较低。为了找到这些最佳值，我们利用梯度。梯度只不过是3D或n-D平面的导数(2D的斜率)。</p><p id="61cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你对2D线求导，就会得到斜率</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/ab542b4bcc4c9f395a74d3ca7d49bdc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*jpww7kaaUwTKvwmrETvSvQ.png"/></div></figure><blockquote class="jz"><p id="a3ea" class="ka kb hi bd kc kd ke kf kg kh ki js dx translated">类似地，如果你对超平面求导，就会得到梯度。</p></blockquote><p id="34d9" class="pw-post-body-paragraph iv iw hi ix b iy kj ja jb jc kk je jf jg kl ji jj jk km jm jn jo kn jq jr js hb bi translated">我们知道什么是梯度，然后如何使用这个梯度找到最佳的权重是真正的问题。</p><p id="97a0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">曲线在局部或全局最小值处的梯度或斜率或导数为零。曲线的最低点称为最小值。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/c57b81119360756d0a689b29774cc6c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nhqLp5am0lbNenWr.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源</figcaption></figure><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kx"><img src="../Images/07670a165f63c052e6fc80770d7d88ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNsFHhuR6hH0V8t49vJjvg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">[图片由作者提供]</figcaption></figure><p id="e64b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是从不同角度看的损失函数的2D图。</p><p id="98fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">找到这两个单独权重的斜率等于3D损失函数曲线的梯度。我们做dL/dw0和dL/w1，并且迭代地保持改变权重，直到我们达到这些导数的最小值，这仅仅是达到曲线的局部/全局最小值，其中损失最小。损失最小的权重值是最佳权重。(现在你明白为什么我的封面照片是一个山谷了吧？XD)</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/81109c62d46ffff17894ece4d8b65b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bhR9V5ICTSQuxnSV.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" rel="noopener" href="/onfido-tech/machine-learning-101-be2e0a86c96a">来源</a></figcaption></figure><p id="a5d2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，我们已经了解了所有重要的概念，我们将把它们作为步骤。</p><p id="48ee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一步:最初，选择权重的随机值</p><p id="7767" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第二步:求损失函数的导数(即梯度),即dL/dW</p><p id="deac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中L是损失函数，W是权重。(这样做是为了知道我们离局部/全局最小值有多远)</p><p id="d89a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第三步:将权重值放入导数中，得到梯度值</p><p id="1bb7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第四步:计算步长=梯度x学习率。步长决定了我们需要在一个方向上移动多远才能更接近局部/全局最小值。学习速度决定了移动的速度。现在不要对学习率有太大的压力，尽管它在其他优化技术中有着至关重要的作用。</p><p id="a491" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">步骤5:现在用更新函数更新权重(新权重=旧权重-步长)，</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/15f04d0d08cef3340ae3001fea3781e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*lQcp_zo_Bi8HlkNXUikSxA.png"/></div></figure><p id="f803" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">重复步骤3、4、5，直到重量收敛，即，</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es la"><img src="../Images/cca657c87e9f325ce6c40881db82be7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*nPqeoZ6CzjSOE6FbNu0itg.png"/></div></figure><p id="53e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度下降就是这样！将渐变移动到较低的点。</p><p id="dc8c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这对于神经网络也是一样的，但是损失函数是不同的，就是这样。最初，我们随机化权重(有多种方法来随机初始化这些权重。如果你想知道，我会写一篇博客😊)向前传播并计算损耗。我们采用损失函数的导数，并通过反向传播迭代地调整/更新权重。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/5f6db7b677dc115049168722e616deb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z0hMS7MsixnaUAWT.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Fwhats-the-role-of-weights-and-bias-in-a-neural-network-4cf7e9888a0f&amp;psig=AOvVaw29bqs-cDZh9eUbh1fmz3Rh&amp;ust=1627659267657000&amp;source=images&amp;cd=vfe&amp;ved=0CAsQjRxqFwoTCLid0rbNiPICFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="24f8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">甚至还有梯度下降的变体:</p><p id="2cc1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1)随机梯度下降(SGD)</p><p id="2bd0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2)小批量SGD</p><p id="ab4a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了更新梯度下降中的权重，我们取损失函数的导数并将权重放入其中</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ju"><img src="../Images/f94e77d0de3137fdbf064883b7e719e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*6t1fIG1_dRPE8j0O6SbEjQ.png"/></div></figure><p id="94e3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在损失函数中，我们使用了所有的数据点(sigma(求和)中从i=1到i=n)。但这需要大量的计算能力和时间来计算，因为数据由现实生活中成千上万的点组成。<br/>新币来了。在SGD中，它只使用一个点，而不是使用所有数据点来计算损失。但是需要多次迭代才能达到最优值，因为它只有一个数据点。你知道准确性与数据成正比。</p><p id="bf96" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了克服这个问题(需要多次迭代)，我们得到了<em class="jt">小批量梯度下降</em>。它不是每次迭代都获取全部数据或一个数据点，而是批量获取数据。例如，我们有10k个数据点，并使其成为1000个点批次，因此10k个点= 10个批次(每个批次由1000个点组成),现在为每个迭代发送每个批次。如果发送完所有批次后权重仍未收敛，则再次发送，直到达到最佳值。(这里1个时期= 10次迭代。一个时期=通过算法传递全部数据)</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="ab fe cl lc"><img src="../Images/fb518c203de10ed666b6c562da98eea9.png" data-original-src="https://miro.medium.com/v2/format:webp/1*1GDN-VD4BC2EBW1jHbTdCg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">等高线图[ <a class="ae iu" href="https://laptrinhx.com/understanding-optimization-algorithms-3818430905/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="b13f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">总之，在ML和DL中使用梯度下降，通过获取损失函数的梯度(导数)在迭代过程中优化模型。梯度下降需要更多的计算资源来优化，所以我们得到了SGD，它需要更少的计算能力，因为它只需要一个数据点来优化，但它需要很多次迭代才能到达目的地。小批量梯度下降是上述方法之间的完美平衡，它比SGD收敛需要更少的迭代，比梯度下降需要更少的计算能力。</p><p id="7c07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢阅读，如果评论中有任何错误或更正，请告诉我😊。</p></div></div>    
</body>
</html>