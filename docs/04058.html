<html>
<head>
<title>Implementing R-CNN object detection on VOC2012 with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch在VOC2012上实现R-CNN目标检测</h1>
<blockquote>原文：<a href="https://medium.com/codex/implementing-r-cnn-object-detection-on-voc2012-with-pytorch-b05d3c623afe?source=collection_archive---------3-----------------------#2021-10-22">https://medium.com/codex/implementing-r-cnn-object-detection-on-voc2012-with-pytorch-b05d3c623afe?source=collection_archive---------3-----------------------#2021-10-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e291c197b91200554fb738d8d41e4eb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ARWk-GNZrQihQyYs"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">沃伦·托拜厄斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="4587" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目标检测是计算机视觉中的一个复杂问题，它涉及到从给定的图像中定位和分类多个目标。它是计算机视觉中最重要和研究最广泛的问题之一。</p><p id="aeac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有各种各样的对象检测方法，包括基于深度学习的方法。这些基于深度学习的方法又被分为单阶段模型和多阶段模型。R-CNN是最初的多级目标检测器之一。这篇文章使用PyTorch中的Pascal VOC 2012数据集讨论了R-CNN每个组件的精确实现，包括SVM类别分类器训练和边界框回归，其他教程似乎通常会忽略这些内容。此外，我指出了其他教程的数据生成过程中的一个严重错误。我按照以下顺序组织了各个部分。</p><ol class=""><li id="746d" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">R-CNN简介</li><li id="fa69" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">为特定领域的微调加载和处理数据</li><li id="4a54" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">网络</li><li id="337c" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">特定领域的精细训练(专长。包围盒回归)</li><li id="eac4" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">对象类别分类器训练</li><li id="2697" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">NMS，推论和验证结果</li></ol></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><p id="7cca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是<em class="ko">关于我实现两阶段物体检测算法系列的第二篇</em>文章。我们注意到，一些功能是在以前的职位上实现的。</p><div class="kp kq ez fb kr ks"><a rel="noopener follow" target="_blank" href="/codex/a-guide-to-two-stage-object-detection-r-cnn-fpn-mask-r-cnn-and-more-54c2e168438c"><div class="kt ab dw"><div class="ku ab kv cl cj kw"><h2 class="bd hj fi z dy kx ea eb ky ed ef hh bi translated">两阶段目标探测指南:R-CNN，FPN，掩模R-CNN等等</h2><div class="kz l"><h3 class="bd b fi z dy kx ea eb ky ed ef dx translated">多级(两级)目标检测流水线介绍</h3></div><div class="la l"><p class="bd b fp z dy kx ea eb ky ed ef dx translated">medium.com</p></div></div><div class="lb l"><div class="lc l ld le lf lb lg io ks"/></div></div></a></div><div class="kp kq ez fb kr ks"><a href="https://sieunpark77.medium.com/how-to-process-voc2012-dataset-and-implement-iou-map-for-object-detection-8ff76891a2ad" rel="noopener follow" target="_blank"><div class="kt ab dw"><div class="ku ab kv cl cj kw"><h2 class="bd hj fi z dy kx ea eb ky ed ef hh bi translated">如何处理VOC2012数据集并实现用于对象检测的IoU、mAP</h2><div class="kz l"><h3 class="bd b fi z dy kx ea eb ky ed ef dx translated">我们将实现一系列的两阶段目标检测算法，从最初的R-CNN到最新的…</h3></div><div class="la l"><p class="bd b fp z dy kx ea eb ky ed ef dx translated">sieunpark77.medium.com</p></div></div><div class="lb l"><div class="lh l ld le lf lb lg io ks"/></div></div></a></div><p id="141b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ko">完整系列的完整代码在</em> <a class="ae iu" href="https://colab.research.google.com/drive/1nCj54XryHcoMARS4cSxivn3Ci1I6OtvO?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <em class="ko">本COLAB笔记本</em> </a> <em class="ko">中提供。因为这篇文章中的代码省略了一些细节，所以请参考笔记本以获得完整的工作实现。</em></p><p id="3242" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在一个<a class="ae iu" href="https://wandb.ai/krenerd77/rcnn" rel="noopener ugc nofollow" target="_blank"> wandb </a>板上展示我们所有的结果和日志。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h1 id="38a7" class="li lj hi bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">r-CNN——概述</h1><figure class="mh mi mj mk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/d5b9661eecec0ceb258b8d08c7ea216e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAZ4z_UxVoohz2dHqjQ5Jg.png"/></div></div></figure><p id="7073" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">两阶段目标检测由两个阶段组成:生成区域建议和对它们进行分类。R-CNN使用<em class="ko">选择性搜索</em>从每幅图像中生成2000个区域建议。然后使用卷积神经网络对每个区域进行分类概率预测。在使用CNN进行推断之前，这些区域被裁剪并<em class="ko">扭曲</em>到某个固定大小。</p><figure class="mh mi mj mk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/75bb3b22c41095b279d722c11ca057ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IXY8qqXhfSAz0f8xPzRNpw.png"/></div></div></figure><p id="94c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过<em class="ko">扭曲</em>，区域提议被填充p=16个像素，并被各向异性地调整到期望的形状，如红色突出显示的。</p><p id="39d9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">R-CNN的训练就是学习CNN分类器的参数。作者建议三个训练阶段:</p><figure class="mh mi mj mk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/b9812a2f6115082cff9fd8dfec033d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEM1Uae_7mqGsIOlW29zWw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">组件1和4构建了推理中使用的最终模型。</figcaption></figure><ul class=""><li id="8baf" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js mn jz ka kb bi translated">监督预训练|组件1，2:在学习丰富的可重用特征的背景下，在更大的图像分类数据集(也称为ImageNet)上预训练CNN。</li><li id="ec40" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mn jz ka kb bi translated">特定于域的微调|组件1，3:为了处理ImageNet域的图像和扭曲区域提议之间的差异，使用扭曲区域提议上的不同全连接头微调CNN。</li><li id="ff54" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mn jz ka kb bi translated">对象类别分类器|组件4:在稍微不同的训练过程中学习一组新的SVM对象类别分类器。这集中于使模型更加了解精确定位。</li></ul><p id="a099" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们加载在ImageNet上训练的预训练CNN特征提取器，这相当于显式训练它们。我们讨论了特定领域的微调和对象分类器训练的实现。虽然这两种设置的数据都是与论文一致地实现的，但显式学习全连接层作为SVM在现代深度学习的上下文中实际上并未使用，我们重用了Softmax对象分类器，但是我们注意到，将我们的实现修改为SVM相对简单。</p><p id="7c52" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ko">有关更多信息，请参考:</em> <a class="ae iu" href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=Rich+feature+hierarchies+for+accurate+object+detection+and+semantic+segmentation&amp;ie=UTF-8&amp;oe=UTF-8" rel="noopener ugc nofollow" target="_blank"> <em class="ko">丰富的特征层次，用于精确的对象检测和语义分割</em> </a></p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">参考配置列表</figcaption></figure><h2 id="f701" class="mq lj hi bd lk mr ms mt lo mu mv mw ls jg mx my lw jk mz na ma jo nb nc me nd bi translated">选择性搜索</h2><p id="0dd3" class="pw-post-body-paragraph iv iw hi ix b iy ne ja jb jc nf je jf jg ng ji jj jk nh jm jn jo ni jq jr js hb bi translated">R-CNN使用选择性搜索实现区域提议。简而言之，该方法迭代合并分割。我们不会深入探究选择性搜索是如何工作的。让我们简短地看一下如何在Python中实现选择性搜索。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="e8a6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">OpenCV提供了一个选择性搜索的实现，可以像上面那样使用。该函数将返回一个边界框列表作为选择性搜索的结果，如下图所示。注意，像大多数OpenCV操作一样，图像应该是BGR图像，而不是RGB图像。</p><figure class="mh mi mj mk fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/b70f2ea850a0f348adfe367c43bb1782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pmX5YXNTvCSaOfUrbplfoQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">选择性搜索的结果</figcaption></figure><h1 id="a194" class="li lj hi bd lk ll nk ln lo lp nl lr ls lt nm lv lw lx nn lz ma mb no md me mf bi translated">加载和处理数据</h1><h2 id="df56" class="mq lj hi bd lk mr ms mt lo mu mv mw ls jg mx my lw jk mz na ma jo nb nc me nd bi translated">特定领域的微调</h2><p id="752c" class="pw-post-body-paragraph iv iw hi ix b iy ne ja jb jc nf je jf jg ng ji jj jk nh jm jn jo ni jq jr js hb bi translated">在CNN的特定领域微调中，我们需要一个数据集来提供扭曲区域提议的图像。作者认为与地面真实边界框重叠的with &gt; 0.5的区域为物体的正图像，否则为标记为背景(0)的负图像。因为有如此多的背景区域，作者通过为32个阳性区域选择96个阴性区域来限制阳性和阴性区域的比例。</p><p id="ab3b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为选择性搜索和循环每个样本的开销非常大，所以我们将扭曲区域的建议预先计算到一个pickle文件中。虽然我想将每个区域单独保存为. png图像，但COLAB和google drive无法处理太多的驱动器I/O。因为我们必须将所有图像保存并加载到一个文件中，所以我们只能在RAM中容纳2，000张图像。处理200，000个区域大约需要1小时。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="82a3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们首先定义变量来保存我们的结果，<code class="du np nq nr ns b">train_images</code>和<code class="du np nq nr ns b">train_labels</code>。在第13行，我们使用<code class="du np nq nr ns b">os.listdir</code>找到目录中的每个图像(由于限制，前2000个图像)。</p><p id="27ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从第19行到第21行，我们遍历每个图像并读取图像和相应的注释文件。我们在之前的文章中定义了<code class="du np nq nr ns b">dataset.read_xml</code>方法。它返回包含每个边界框信息的字典列表。</p><p id="b1ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们使用选择性搜索来寻找包围盒提议，并循环通过每个提议。</p><p id="1dd8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在第28到30行，我们通过填充16个像素来修改建议的坐标。我们还从边界框的x，y，w，h坐标计算{x1，x2，y1，y2}坐标，并将其保存在<code class="du np nq nr ns b">bbox_est</code>中。我们记录地面真实和估计的包围盒的坐标，用于训练包围盒回归器。</p><p id="82f0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从第33行到第50行，我们检查边界框提议是否与原始图像中的任何对象重叠。如果该提议与第37行中使用IoU确定的至少一个对象重叠，我们将图像和标签附加到数据集。</p><p id="c361" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果建议的边界框没有与任何对象重叠，它将被视为背景图像。只有当我们有足够的阳性样本时，我们才在第53到60行将样本添加到数据集。注意，我们将类标签设置为0，这表示背景。</p><p id="039d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，每当我们获得两幅图像的足够样本时，计数器就被重置。这种机制在第62到64行实现。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><p id="c146" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在研究这个项目时，我遇到了几个错误地实现R-CNN的数据生成过程的教程。具体来说，在<a class="ae iu" href="https://github.com/Hulkido/RCNN/blob/master/RCNN.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a>实现中，我们发现一些代码看起来像:</p><figure class="mh mi mj mk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nt"><img src="../Images/f85f728226f56c5247acda40b9b98b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9sw2q9h8U4sK95yZmQTgrA.png"/></div></div></figure><p id="60a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在代码遍历g.t .边界框列表<code class="du np nq nr ns b">gtvalues</code>的行中，根据该代码，一个提议的边界框将被分配给每个基础事实的新标签。正确的实现应该在将IoU与图像中存在的每个g.t .对象进行比较之后，将提议的边界框标记为背景。因此，<code class="du np nq nr ns b">if falsecouner&lt;30 </code>之后应该存在后循环。</p><p id="cc84" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下教程的第127行到第138行也是如此。</p><p id="a737" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2020/07/13/r-CNN-object-detection-with-keras-tensor flow-and-deep-learning/</a></p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><p id="d830" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们已经定义了生成裁剪区域建议的方法，我们可以构建一个DataLoader对象，为我们提供我们想要的微调。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="530e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们创建<code class="du np nq nr ns b">RCNN_Dataset</code>对象时，我们首先检查是否已经将处理过的图像保存在正确格式的pickle文件中。如果这样的文件存在，我们只需加载pickle文件或执行上一节讨论的<code class="du np nq nr ns b">generate_dataset</code>方法。</p><p id="399e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为实际数据已经加载到<code class="du np nq nr ns b">self.train_images</code>和<code class="du np nq nr ns b">self.train_labels</code>中，所以实现<code class="du np nq nr ns b">__getitem__</code>方法相对简单。我们将任意大小的Numpy数组形状的图像处理成固定形状的归一化张量。该整形操作完成了<em class="ko">翘曲</em>。</p><p id="e784" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第62到66行创建了一个包装器函数，该函数返回dataset对象的批量数据加载器并调用它。</p><h1 id="a29f" class="li lj hi bd lk ll nk ln lo lp nl lr ls lt nm lv lw lx nn lz ma mb no md me mf bi translated">模型</h1><p id="65a9" class="pw-post-body-paragraph iv iw hi ix b iy ne ja jb jc nf je jf jg ng ji jj jk nh jm jn jo ni jq jr js hb bi translated">虽然任何通用CNN都可以用作R-CNN管道的卷积特征提取器，但我们使用EfficientNet-B0网络。由于EfficientNet不包含在<code class="du np nq nr ns b">torchvision.models</code>中，我们使用<code class="du np nq nr ns b">efficientnet_pytorch</code> <a class="ae iu" href="https://github.com/lukemelas/EfficientNet-PyTorch" rel="noopener ugc nofollow" target="_blank">包</a>。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="c78c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的网络非常简单。它由三部分组成:</p><ul class=""><li id="fd9f" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js mn jz ka kb bi translated"><code class="du np nq nr ns b">self.convnet</code>:特征提取器CNN，定义为efficientnet模型的一部分。</li><li id="4daa" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mn jz ka kb bi translated"><code class="du np nq nr ns b">self.classifier</code> : SoftMax对象类别分类器</li><li id="925e" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mn jz ka kb bi translated"><code class="du np nq nr ns b">self.bbox_reg</code>:边界框回归器(可选)</li></ul><h1 id="bf3f" class="li lj hi bd lk ll nk ln lo lp nl lr ls lt nm lv lw lx nn lz ma mb no md me mf bi translated">特定领域的微调</h1><p id="816e" class="pw-post-body-paragraph iv iw hi ix b iy ne ja jb jc nf je jf jg ng ji jj jk nh jm jn jo ni jq jr js hb bi translated">我们现在将讨论用于特定领域微调的训练循环的实现。考虑到该任务实际上与扭曲区域提议的经典图像分类没有什么不同。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="d8aa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ko">注意:上面的函数是一个包含</em>的类的方法</p><ul class=""><li id="eb9e" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js mn jz ka kb bi translated"><code class="du np nq nr ns b">self.model</code> an _RCNN实例。</li><li id="5862" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mn jz ka kb bi translated"><code class="du np nq nr ns b">self.validator</code>用于各种验证的类，我们将在后面讨论。</li><li id="647d" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mn jz ka kb bi translated"><code class="du np nq nr ns b">self.loader</code>RCNN _ Dataset类的数据加载器。</li></ul><p id="4e2d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为_RCNN本身是<code class="du np nq nr ns b">nn.Module</code>的子类，我们可以设置优化器来计算第9行中<code class="du np nq nr ns b">self.model.parameters()</code>的梯度。</p><p id="a8e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练循环很简单，除了用于边界框回归的第38到54行之外，类似于PyTorch中用于图像分类的典型训练循环。</p><h2 id="3f45" class="mq lj hi bd lk mr ms mt lo mu mv mw ls jg mx my lw jk mz na ma jo nb nc me nd bi translated">包围盒回归</h2><div class="mh mi mj mk fd ab cb"><figure class="nu ij nv nw nx ny nz paragraph-image"><img src="../Images/d6960c6e9ffe04bc9524ff4ecc396558.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*EsUQvS8Wo6EU-65CBtaavw.png"/></figure><figure class="nu ij oa nw nx ny nz paragraph-image"><img src="../Images/dda97cf779ca05f7e92edf0e2cb3e1dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*XURWhAbALre1zvMDixmN6Q.png"/><figcaption class="iq ir et er es is it bd b be z dx ob di oc od translated">左:如何应用边界框回归，右:回归目标</figcaption></figure></div><p id="54bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">边界框回归是通过提供轻微的变换来精确细化边界框的阶段。作者提出了一种包含4个参数的变换，d_x和d_y表示要提供的翻译量，d_w和d_h表示对数空间的比例因子。这4个参数是使用神经网络预测的，该神经网络接收提取的特征并根据回归损失进行训练。</p><p id="26a7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">左边的等式示出了如何将边界框细化d应用于边界框提议P，以生成有希望类似于地面实况的细化边界框G hat。右边的等式描述了给定边界框提议P和相应的地面实况边界框g的回归目标t。在训练期间，使用带有t的MSE损失来训练网络d。</p><p id="2b0a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据论文的方程，我们在第41行和第42行读取边界框提议P和地面实况G的坐标。使用这些值，我们计算第44行的t向量。</p><p id="5f32" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为没有背景的<em class="ko">地面真相</em>边界框，所以预测分类为背景的图像的边界框细化是没有意义的。第48行计算批次中的每个图像是否是背景的掩码(class=0)，并且模型仅在掩码为真的对象样本上学习。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="6b21" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还定义了一个函数，用于将预测的边界框细化应用于区域提议。在下图中，我们展示了边界框回归确实改善了边界框的定位。该网络改进了作为选择性搜索的原始输出的红色边界框，并预测通常更准确的橙色边界框。</p><div class="mh mi mj mk fd ab cb"><figure class="nu ij oe nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/b5394260d5c45e5e3ad13545003afa7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*fp9sMlhT9QSY884spl2piQ.png"/></div></figure><figure class="nu ij of nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/cb11946e64b40dbe9627b31dfd7da9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*JpbmnNeeltptS8rOy0KhEQ.png"/></div></figure><figure class="nu ij og nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/14418c7aa6612678867234cca64e986f.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*ZIxCcvbu-02H17cUkxiHAA.png"/></div></figure></div><div class="ab cb"><figure class="nu ij oe nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/afaf2c986f8c9329a413f1bb6e647299.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*gfbDGKh8ko5_Lj6Q71-gbQ.png"/></div></figure><figure class="nu ij of nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/02702cb56507e6cb0e261dff29ca863d.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*bpdODQPA1Pa5zMWQiOalfg.png"/></div></figure><figure class="nu ij og nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/974a09c35f8a8be9cfdbfc63477776f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*As9vUo2B4NxTsvaGq9I62g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx oh di oi od translated">向上:预测，向下:g.t对象</figcaption></figure></div><h1 id="3e57" class="li lj hi bd lk ll nk ln lo lp nl lr ls lt nm lv lw lx nn lz ma mb no md me mf bi translated">对象类别分类器</h1><p id="1a78" class="pw-post-body-paragraph iv iw hi ix b iy ne ja jb jc nf je jf jg ng ji jj jk nh jm jn jo ni jq jr js hb bi translated">对象类别分类器在不同的数据集上进行微调。我们只把基本事实框作为正面例子，而把与一个类的所有实例的IoU重叠小于0.3的提议标记为该类的负面例子。因此，处理数据的逻辑略有不同。我们首先选择给定图像的所有边界框，并仅在需要时从选择性搜索区域建议中选择背景区域。</p><p id="3685" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的实现与本文中描述的方法略有不同。因为作者逐个学习每个类的SVM，而不是同时将它们训练为SoftMax分类器，所以作者建议为每个类创建一个数据集，而我们不这样做。</p><p id="71f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的方法的一个问题是，因为每个类别的分类器是同时学习的，所以我们需要在数据集中包括背景的图像。如果我们对仅由对象组成的数据集进行微调，背景的概率估计将会消失。因此，我们应用32:96的比例对物体和背景进行采样。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="052e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从第23行到第31行，我们选择所有的基本事实对象。</p><p id="5761" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们收集了一定数量的对象后，我们收集第34到67行的背景区域。我们检查所提议的边界框是否与第49到54行中的对象重叠，并对第56到64行中的背景图像进行计数。</p><h2 id="d4f3" class="mq lj hi bd lk mr ms mt lo mu mv mw ls jg mx my lw jk mz na ma jo nb nc me nd bi translated">训练对象类别分类器</h2><p id="f3b0" class="pw-post-body-paragraph iv iw hi ix b iy ne ja jb jc nf je jf jg ng ji jj jk nh jm jn jo ni jq jr js hb bi translated">除了仅学习最终SoftMax分类器的参数之外，我们用于微调对象类别分类器的训练循环与用于特定领域微调的训练循环一致。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="56e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在第9行，我们设置优化器只学习对象类别分类器的参数。我们还删除了包围盒回归的代码。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h2 id="ac4b" class="mq lj hi bd lk mr ms mt lo mu mv mw ls jg mx my lw jk mz na ma jo nb nc me nd bi translated">非最大抑制(NMS)</h2><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="4f97" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">非最大值抑制是一种贪婪的合并算法，可用于对象检测，以有效地组合同一对象的紧密定位的重复区域提议。我们重复以下内容:</p><ul class=""><li id="68ac" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js mn jz ka kb bi translated">选择具有最高预测置信度(概率)的边界框并保留。</li><li id="dfdf" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mn jz ka kb bi translated">遍历所有其他边界框，并</li><li id="a7d1" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mn jz ka kb bi translated">移除与大于特定阈值的所选边界框具有IoU重叠的所有边界框。</li></ul><p id="5689" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在上面的代码中实现了这个算法。我们在下图中说明了NMS的影响。NMS有效地结合了这些重复的边界框。</p><div class="mh mi mj mk fd ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/6b9b487d5a3c12d9b56365a14621b201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*b8cXl6Z7TbtaBClyLOEM-g.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/970d6e087a814a9438418fdb2473e8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*R_blcAcJLl4qC2iEOcScdg.png"/></figure></div><div class="ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/80a4e295267674cc6b06060bde4277a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*wsTqqazqe0XQKos_nOCgQA.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/174ce4325df2e6ba90f2d85f5abec62e.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*Tiem6I78RvzEZ3F8binnqw.png"/></figure></div><div class="ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/0828e5e0a9cd2e8b3628102e99c459b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*sgy_IWLWOmOy2njqqKZdjA.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/452cc96218758a518ea460600cd1a3c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*KbZysBdUPLUTGsTU8vwO0A.png"/></figure></div><h2 id="62a1" class="mq lj hi bd lk mr ms mt lo mu mv mw ls jg mx my lw jk mz na ma jo nb nc me nd bi translated">推理</h2><p id="c883" class="pw-post-body-paragraph iv iw hi ix b iy ne ja jb jc nf je jf jg ng ji jj jk nh jm jn jo ni jq jr js hb bi translated">R-CNN中的推理并不像将图像输入神经网络那样简单。我们来看看它是如何实现的。</p><figure class="mh mi mj mk fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="2d49" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基本上，我们创建了一个包含&lt; 2，000个区域提议的数据集，这些提议是对第16行到第32行的给定图像进行选择性搜索的结果。我们对它们进行批处理，并创建一个DataLoader对象。</p><p id="8e0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于每一批，我们使用训练好的模型对第40行到第47行的每个地区提案进行预测。在第49行，我们发现建议的索引很有可能是某种对象(而不是背景)。我们处理包围盒的信息，并在<code class="du np nq nr ns b">useful_bboxes</code>中收集所有检测到的区域提议的包围盒信息。</p><p id="0820" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我们通过将NMS应用于检测到的对象边界框的列表来减少重叠区域。</p><h2 id="0e6a" class="mq lj hi bd lk mr ms mt lo mu mv mw ls jg mx my lw jk mz na ma jo nb nc me nd bi translated">结果</h2><div class="mh mi mj mk fd ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/d3fecb59214d60da934a77cf0f9c483b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*h1z6T7w8oj1PfAD-5XAdYQ.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/8fc0a9c80f9aa326cf2a3c0f9cd85a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Q4Rug_PUrPtkQ2XWOaQyHw.png"/></figure></div><div class="ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/db3d135f0144ce7e8c01c2d5f14a9730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*sgB-45ouazMqw_yZW4dpbw.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/58443ec2472ec5fb9af43b0f3e2ae1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*X1FpzyqCpm5fYwCS1v4BGg.png"/><figcaption class="iq ir et er es is it bd b be z dx ok di ol od translated">验证集的结果，左边是预测，右边是遗传</figcaption></figure></div><div class="ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/0c6cffc533c93325f55a2001bded56c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*c22knamGmildoGcYZpzC_g.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/7c28fa18bed1cb1de7ab3642929c3e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*XgvLULLHvyNDWmm3hcdx6g.png"/></figure></div><div class="ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/c779b5bfec9f26bfeafc2b4c104510a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*trgDqJe2WX7ka2U9iOj_rw.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/7382d127d0fd9f617a52d506bc74b40a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bpdODQPA1Pa5zMWQiOalfg.png"/></figure></div><div class="ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/970d6e087a814a9438418fdb2473e8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*R_blcAcJLl4qC2iEOcScdg.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/82ef32a213e898d292d6406175f9a9a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*txbxsLNyWaZOVwlpKNOY_Q.png"/></figure></div><div class="ab cb"><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/ba1bbfc9229be519ed35a2d65e5374b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*7TEBe9T2RjkDESZJQ4ohIw.png"/></figure><figure class="nu ij oj nw nx ny nz paragraph-image"><img src="../Images/c20fb87408cf0c62ce80388c55f3dc5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*CxKqTb-yaO6aRDCHKCM-Yw.png"/><figcaption class="iq ir et er es is it bd b be z dx ok di ol od translated">训练集上的结果，左边是预测，右边是g.t</figcaption></figure></div><p id="9de2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们甚至可以观察到，不正确的区域通常具有小得多的阈值。我们可以通过找到更好的检测目标的阈值来进一步优化模型。</p><p id="ef73" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以在<a class="ae iu" href="https://colab.research.google.com/drive/1nCj54XryHcoMARS4cSxivn3Ci1I6OtvO?usp=sharing" rel="noopener ugc nofollow" target="_blank">这个</a> COLAB笔记本中获得R-CNN的完整工作源代码和更多实验的结果。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><p id="bf75" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ko">我承认我在这篇文章上拖拖拉拉，而且写得很匆忙。如果我错过了什么，或者你想知道更多关于❤️.的事情，请在评论中告诉我</em></p></div></div>    
</body>
</html>