<html>
<head>
<title>Streaming Live Network Packets Data in to Spark Streaming Using Kafka</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Kafka将实时网络数据包流传输到Spark流</h1>
<blockquote>原文：<a href="https://medium.com/codex/streaming-live-network-packets-data-in-to-spark-streaming-using-kafka-25c8b5f58181?source=collection_archive---------2-----------------------#2021-09-13">https://medium.com/codex/streaming-live-network-packets-data-in-to-spark-streaming-using-kafka-25c8b5f58181?source=collection_archive---------2-----------------------#2021-09-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7561" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Python中的循序渐进的初学者指南，使用网络流量数据作为用例，使用Spark结构化流来尝试Kafka。</p><p id="f797" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，我们将使用Apache Kafka数据馈送平台构建一个基本原型，展示如何将实时网络流量传输到Apache Spark。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="a5d9" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">介绍</h1><p id="59f2" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">在当今的大数据世界中，所有的一切都是为了实时处理连续和庞大的信息集，并即时做出决策/获取洞察力。两者——数据来源，如IOT设备、遥测设备等。以及数据库、分析/可视化引擎等数据接收器。，需要高效、快速、健壮的容错数据管道和处理平台。像Apache Kafka这样的流媒体平台和Spark这样的大数据处理平台符合这些目标。</p><p id="50cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我试图了解这些如何适用于网络领域。如何将连续产生的网络数据从其源传输到可以分析数据的地方？在我有限的研究和学习中，我从网上和YouTube上的一些资源中学到了一些概念，并可以尝试一些基本的实践，我正试图在这个博客中分享。</p><p id="3d26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，在这个练习中我们要做什么呢？我们将从pcap文件或network interface live中获取一些网络流量数据，提取一些重要的头字段，并通过Kafka服务器将它们传输到Spark应用程序。我已经提供了所有必要的步骤并分享了代码，这样你也可以在你的笔记本电脑上动手尝试。</p><p id="760e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，这将是所有的Python语言。</p><p id="b3b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们开始一步一步的练习之前，我将在这里简要介绍一些关于卡夫卡，火花等的基础知识。只是介绍一下背景。</p><h2 id="98de" class="kn jl hi bd jm ko kp kq jq kr ks kt ju iq ku kv jy iu kw kx kc iy ky kz kg la bi translated">阿帕奇卡夫卡</h2><p id="7488" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">根据<a class="ae lb" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank">官方定义</a>，Apache Kafka是一个开源的分布式事件流平台，被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p><p id="657e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lc">简单来说，Kafka是一个开源平台，可帮助将连续数据从一个或多个生产者传输到一个或多个消费者，以进行实时和批量数据处理。这里</em>  <em class="lc">可以看到卡夫卡</em> <a class="ae lb" href="https://kafka.apache.org/uses" rel="noopener ugc nofollow" target="_blank"> <em class="lc">的各种用例。</em></a></p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ld"><img src="../Images/ce01b79309a7afcfa2ab9f2bd0d32416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2sDNZcSNiKPmaeH5vOqSw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">卡夫卡概述</figcaption></figure><p id="44af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生产者将连续的数据流推送到Kafka集群。Kafka集群由许多服务器或代理组成，这些服务器或代理接收来自生产者的数据消息，并将其附加到称为主题的东西上(生产者提到它希望特定消息进入哪个主题，因此主题基本上是消息的上下文)。Kafka代理将消息保存在本地存储器中，以备消费者将来检索之需。消费者应用程序从他们感兴趣(订阅)的主题中提取数据消息作为流。Kafka集群可以包含许多代理，以便在Kafka集群中的一个或多个代理出现故障时提供数据负载共享和容错。</p><h2 id="15bb" class="kn jl hi bd jm ko kp kq jq kr ks kt ju iq ku kv jy iu kw kx kc iy ky kz kg la bi translated">阿帕奇动物园管理员</h2><p id="87de" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">ZooKeeper是一个为分布式应用程序提供的分布式开源协调服务。它公开了一组简单的原语，分布式应用程序可以在这些原语的基础上实现同步、配置维护、分组和命名等高级服务。<br/>在Kafka的上下文中，<strong class="ih hj">我们需要运行Zookeeper </strong>以便使用Kafka服务器，因为Zookeeper对于协调和管理Kafka代理集群是必不可少的。</p><h2 id="f424" class="kn jl hi bd jm ko kp kq jq kr ks kt ju iq ku kv jy iu kw kx kc iy ky kz kg la bi translated">阿帕奇火花</h2><p id="81d5" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated"><a class="ae lb" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>是用于大规模数据处理的统一分析引擎。<br/>它是一个用于大数据工作负载的开源分布式处理系统。它利用内存缓存和优化的查询执行，针对任何大小的数据进行快速分析查询。它提供Java、Scala、Python和R的开发API，并支持跨多种工作负载的代码重用——批处理、交互式查询、实时分析、机器学习和图形处理。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es lt"><img src="../Images/48d8751e145fdfc88f9e38456557974d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/0*4_AFzCcesGV0UrAT.png"/></div></figure><p id="9a40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark支持一系列库，包括<a class="ae lb" href="https://spark.apache.org/sql/" rel="noopener ugc nofollow" target="_blank"> SQL和DataFrames </a>、用于机器学习的<a class="ae lb" href="https://spark.apache.org/mllib/" rel="noopener ugc nofollow" target="_blank"> MLlib </a>、GraphX 和<a class="ae lb" href="https://spark.apache.org/streaming/" rel="noopener ugc nofollow" target="_blank"> Spark Streaming </a>。</p><p id="be9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lb" href="https://spark.apache.org/streaming/" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">Spark Streaming</strong></a>轻松构建可扩展的容错流式应用。它允许您像编写批处理作业一样编写流式作业。它支持Java、Scala和Python。</p><p id="f3fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本博客的示例中，我们将使用Spark流应用程序作为消费者，即它将订阅Kafka主题并接收来自Kafka服务器的消息。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es lu"><img src="../Images/8824a3e0c12c3bd41bcbbfa65e9f6ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*h5J6pdd4lPcajOT5FkVTqA.png"/></div></figure><p id="f3a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们从Kafka接收到Spark的数据流，我们就可以使用Spark SQL服务对数据进行任何分布式分析，或者将数据存储到数据库等。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="3d3b" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">先决条件</h1><p id="95ea" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">您需要在系统中安装Apache Zookeeper、Apache Kafka和Apache Spark。我使用了Ubuntu Linux(作为虚拟机器中的一个虚拟机),并将所有这些安装在Ubuntu虚拟机中。</p><ol class=""><li id="f753" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc ma mb mc md bi translated">为了安装虚拟盒子，Ubuntu，卡夫卡和Zookeeper，你可以观看波格丹一世的这个精彩的<a class="ae lb" href="https://www.youtube.com/watch?v=CU44hKLMg7k&amp;t=5612s" rel="noopener ugc nofollow" target="_blank">视频</a>。</li><li id="3033" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated">通过运行<strong class="ih hj"> pip install pyspark </strong>安装pyspark (Python库for Spark)</li><li id="39bb" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated">通过运行<strong class="ih hj"> pip install kafka-python </strong>为Kafka安装Python库</li><li id="96b8" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated">安装pcapy和scapy工具。这些用于从网络接口捕获实时数据包，并从数据包中提取报头字段。</li></ol><ul class=""><li id="6184" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc mj mb mc md bi translated"><strong class="ih hj"> pip安装scapy </strong></li><li id="cd15" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated"><strong class="ih hj"> sudo更新<br/> sudo安装python3-pcapy </strong></li></ul><p id="d675" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我用的版本如下。</strong></p><p id="6403" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Kafka-2 . 8 . 0(Scala 2.12—<a class="ae lb" href="https://www.apache.org/dyn/closer.cgi?path=/kafka/2.8.0/kafka_2.12-2.8.0.tgz" rel="noopener ugc nofollow" target="_blank">Kafka _ 2.12–2 . 8 . 0 . tgz</a>(<a class="ae lb" href="https://downloads.apache.org/kafka/2.8.0/kafka_2.12-2.8.0.tgz.asc" rel="noopener ugc nofollow" target="_blank">ASC</a>，<a class="ae lb" href="https://downloads.apache.org/kafka/2.8.0/kafka_2.12-2.8.0.tgz.sha512" rel="noopener ugc nofollow" target="_blank"> sha512 </a>))</p><p id="70f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">火花- pyspark 3.1.2</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="0ca2" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">练习的主要步骤</h1><ol class=""><li id="52cf" class="lv lw hi ih b ii ki im kj iq mk iu ml iy mm jc ma mb mc md bi translated">编写一个Kafka生成器，从网络数据包中提取包头字段，并将其提供给Kafka主题。</li><li id="3347" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated">使用pyspark编写一个基于Spark结构化流的程序，订阅Kafka主题，从Kafka接收数据流并消费它(使用它进行进一步分析)。</li><li id="1fc6" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated">启动Zookeeper和Kafka服务。</li><li id="1ec2" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc ma mb mc md bi translated">通过同时运行它们来测试我们的生产者和Spark消费者。</li></ol><p id="2878" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将一个接一个地完成以上所有步骤。</p><p id="23de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">你可以从我的github库</strong> <a class="ae lb" href="https://github.com/raja-surya/Network-Traffic-Kafka-Spark" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这里</strong> </a> <strong class="ih hj">下载这个练习的代码。</strong></p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="f9d8" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">应用程序编码</h1><h2 id="2853" class="kn jl hi bd jm ko kp kq jq kr ks kt ju iq ku kv jy iu kw kx kc iy ky kz kg la bi translated"><strong class="ak"> 1。生产商代码</strong></h2><p id="a143" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我写了两种类型的生产者代码和相应的消费者-</p><ul class=""><li id="0b8e" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc mj mb mc md bi translated">第一个示例通过读取pcap文件获取数据包数据，并将其传输到Kafka。</li><li id="a8bf" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated">第二个例子通过嗅探虚拟机的网络接口(pcapy工具在这方面有所帮助)来获取资源，并将其传输到Kafka。</li></ul><p id="844b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请记住运行正确的生产者和消费者程序对。文件名分别以“-pcap”和“-live”结尾。</p><p id="0d53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们分部分检查生产者代码(pkt-producer-pcap.py)的第一个例子。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mn"><img src="../Images/658f69a01d240f0248653923dea2da10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ge8VQ3bvULjFtIlKEMCi_w.png"/></div></div></figure><p id="ed66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码中，我们从Kafka导入了KafkaProducer类。我们还导入了一个支持代码(来自MyScapyExtract.py ),以便从pcap文件中读取和提取包头字段(使用scapy工具)。然后，我们创建一个producer对象，指定我们想要连接的kafka服务器- localhost和端口9092(Kafka的默认端口)。</p><p id="1843" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将进入下一组代码..</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mo"><img src="../Images/10a523800ed1e7ad22b1a6850663a5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*CKrpqZyMVROmoEjEKLx_hA.png"/></div></figure><p id="6a78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码中，我们获取了一个pcap文件，并提取了pcap文件中每个数据包的包头字段。我使用了一个小的pcap文件(可以在我的github中找到),其中包含我在笔记本电脑中使用wireshark捕获的数据包。Scapy是我用来从包中提取字段的python工具。您可以在google上搜索scapy，也可以查看MyScapyExtract.py文件中的代码来了解这一部分。返回的“datalst”将只是下面粘贴的字典元素列表。每个字典将对应于pcap文件中的一个数据包。您可以看到各个数据包的各种包头字段。</p><p id="da17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[{'id': '0 '，' len': '215 '，'时间戳':' 2021–09–12 10:06:37.000000 '，' esrc': '94:08:53:64:e0:05 '，' edst': '01:00:5e:7f:ff:fa '，' etype': '2048 '，' isrc': '192.168.1.11 '，' idst ':' 239 . 255 . 255 . 255</p><p id="7640" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">{'id': '1 '，' len': '332 '，' timestamp ':' 2021–09–12 10:06:37.000000 '，' esrc': '30:a9:de:a5:93:50 '，' edst': '94:08:53:64:e0:05 '，' etype': '2048 '，' isrc': '192.168.1.7 '，' idst': '192.168.1.11</p><p id="5552" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们转到pkt-producer-pcap.py中的下一部分代码</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ld"><img src="../Images/c47d97ac071c08c82464357de7e395fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F6ZRaV28BMuLm5DNExLqqg.png"/></div></div></figure><p id="0b00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面部分代码只是遍历每个包，只取IP包(以太类型十进制2048对应0x0800)并提取5元组(源IP地址，目的IP地址，UDP的话协议- 17，TCP的话协议-6，源端口，目的端口)。然后，它将这个5元组数据与数据包的计数一起打包到一个消息中，并使用KafkaProducer的send()方法将其发送给Kafka。<strong class="ih hj">请注意，第一个参数‘PKT test _ pcap’是生产者选择在Kafka中附加消息的主题。</strong></p><p id="1f89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将看到生产者的第二个例子(pkt-producer-live.py ),其中我们将从网络接口捕获实时数据包并将其发送到Kafka。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mp"><img src="../Images/e209d82cb2589c3cfb7b1dfabd573bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6qPWwdSiTJF6JkBBEzXJQ.png"/></div></div></figure><p id="4af4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用pcapy工具从Ubuntu VM的网络接口“enp0s3”中嗅探数据包。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mq"><img src="../Images/ea2385d35dd0df1985f60048abdd4740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*76noZj1jiGCQYKwibZTZeA.png"/></div></div></figure><p id="a983" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">cap.next()将返回下一个探测到的数据包。我们在一个无限循环中嗅探数据包，从L2报头中提取源mac和目的mac，并将其发送给Kafka到一个名为“pkttest”的主题。就这么简单！</p><h2 id="7701" class="kn jl hi bd jm ko kp kq jq kr ks kt ju iq ku kv jy iu kw kx kc iy ky kz kg la bi translated">2.Spark结构化流消费者代码</h2><p id="873a" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated"><strong class="ih hj"> <em class="lc">结构化流</em> </strong> <em class="lc">是一款基于Spark SQL引擎构建的可扩展、容错的流处理引擎。您可以像表达静态数据上的批处理计算一样表达您的流计算。Spark SQL引擎将负责增量地、持续地运行它，并随着流数据不断到达而更新最终结果。(摘自</em> <a class="ae lb" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#overview" rel="noopener ugc nofollow" target="_blank"> <em class="lc">结构化流媒体编程指南</em> </a> <em class="lc"> ) </em></p><p id="c14b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从我有限的研究中了解到，“结构化流”是Spark的一个新功能，在其中，您可以对流数据执行分析计算，就像它是一个数据帧一样。使用<em class="lc">微批处理</em>引擎处理结构化流查询，该引擎将数据流作为一系列小批量作业进行处理。</p><p id="b15c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据流被视为一个无限制的数据帧，新的行被连续地附加到该数据帧中(当新的流数据到达时)。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mr"><img src="../Images/18a147a35e41872bf8da03e6afe98fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0LirGIIpPUdTWvVy.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">结构化流</figcaption></figure><p id="8cdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以如上所述，<em class="lc">Spark SQL引擎将负责增量地、持续地运行它，并随着流数据不断到达而更新最终结果。</em></p><p id="c77c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将进入我们的消费者代码(spark-consumer-pcap.py ),它用python实现了spark结构化流，并与Kafka集成(您可以参考<a class="ae lb" href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html" rel="noopener ugc nofollow" target="_blank">结构化流+ Kafka集成指南</a>)</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ms"><img src="../Images/38ff8fc0c1c7b89ed6539204a2376dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4cH_L-TSYiIGnQg-l9bwA.png"/></div></div></figure><p id="6298" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码部分中，我们从pyspark SQL导入SparkSession和函数。我们设置Kafka主题的名称，消费者需要从这个主题接收消息——这个主题就是生产者向其提供消息的主题。我们还设置了kafka服务器和端口，并启动了Spark会话。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mt"><img src="../Images/0cb59c9f6600653824c56dea6a29949e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RtV0HLXt3RkPkBpZAYvKTg.png"/></div></div></figure><p id="a687" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们使用spark会话，并通过SparkSession.readStream()返回的DataStreamReader接口获得一个流数据帧。我们通过Kafka服务器，主题细节在这里。可以使用不同的源，如文件、kafka、socket等。这里我们读卡夫卡；所以我们提格式(“卡夫卡”)。<br/>我们得到的数据帧就像Pandas数据帧一样，有行(流数据记录)和列。这里，我们感兴趣的是提取“值”列和“时间戳”。“值”包含由生产者发送的消息。当它以字节格式(序列化)到达时，我们将它类型转换为字符串。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mu"><img src="../Images/d6d2652a9b89eae72f0fa9959f912bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*68VYwGNBR7kpgKq-Xr38UQ.png"/></div></div></figure><p id="b4bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们定义一个模式字符串来分割和解释流记录的“值”部分，我们在前面的步骤中将其转换为字符串。记住从生产者发送的消息(来自pcap源的第一个流示例)在格式计数和包含源IP地址、目的IP地址等的5元组中。因此，在上面的代码中，我们定义了相同的模式，并将“值”列拆分为这些单独的列。pyspark中的这些操作语法与我们处理Pandas数据帧的方式不同。</p><p id="3eeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上述代码之后，pkt_df3包含一个列的数据帧-count、src_ip、dst_ip、proto、sport、dport和“timestamp”。</p><p id="8da8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将进入消费者守则的最后一部分。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mv"><img src="../Images/95dbf10ba11f14f8e43412740f7dd1b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*RhaiAHRuNcH95RksoMO9yA.png"/></div></figure><p id="245f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们从数据流中获得数据帧，我们就可以做进一步的分析或将数据帧写入内存或控制台等。为此，我们必须使用通过Dataset.writeStream()返回的DataStreamWriter。我们开始流式计算。我们将查询触发间隔设为5秒，这是检查新数据可用性的时间间隔。有不同的输出模式，如“附加”，“更新”等。我们在这里使用“update”(只有结果表中自上一次触发后更新的行才会输出到接收器)。这里我们提到的接收器是“控制台”，意思是我们希望数据帧输出到控制台。这些的各种选项可以在<a class="ae lb" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#overview" rel="noopener ugc nofollow" target="_blank"> <em class="lc">结构化流编程指南</em> </a> <em class="lc">中找到。</em></p><p id="ca1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您看到上面代码中被注释掉的部分，我们也可以选择做一些分析，比如聚合。在那里，我按“proto”字段分组，并对属于特定协议的数据包进行计数(有多少TCP数据包和UDP数据包)。</p><p id="23c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">执行此代码后，流计算将在后台开始。查询对象是活动流式查询的句柄。我们使用awaitTermination()等待查询终止，以防止流程在查询活动时退出。</p><p id="75ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们完成了生产者和消费者的编码。是时候看看如何测试我们的数据包流了。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="ca30" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">测试我们的应用</h1><p id="6589" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated"><strong class="ih hj">请注意，我们需要四台Ubuntu终端来做这个练习。<br/> </strong>一个对应<strong class="ih hj"> </strong>这些中的每一个——启动Zookeeper服务器，启动Kafka服务器，运行生产者代码，运行消费者代码。</p><h2 id="4c19" class="kn jl hi bd jm ko kp kq jq kr ks kt ju iq ku kv jy iu kw kx kc iy ky kz kg la bi translated">3.启动Zookeeper和Kafka服务器</h2><p id="bd77" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">正如我之前在先决条件部分提到的，你可以观看波格丹一世的视频<a class="ae lb" href="https://www.youtube.com/watch?v=CU44hKLMg7k&amp;t=5612s" rel="noopener ugc nofollow" target="_blank">来学习如何在Ubuntu中安装Apache Kafka，启动Zookeeper和Kafka服务器。</a></p><p id="6cff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们必须启动Zookeeper，这是启动和运行Kafka所必需的。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mw"><img src="../Images/b67fb391eea85d360dab6265ed5edc87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y5ujhRQIUekHbGuILDB25A.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">启动动物园管理员</figcaption></figure><p id="b5cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让Zookeeper保持运行，打开另一个终端标签并运行Kafka服务器，如下所示。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mx"><img src="../Images/c6aa050242d45f5e888005251bb06fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jwEszfWwSN-JYrC8YWxvwQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">启动Kafka-服务器</figcaption></figure><p id="d0e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">启动Kafka服务器后，不要干扰窗口，打开新的终端标签来测试我们的应用程序。</p><h2 id="bd3e" class="kn jl hi bd jm ko kp kq jq kr ks kt ju iq ku kv jy iu kw kx kc iy ky kz kg la bi translated">4.测试我们的生产者和火花消费者</h2><p id="ea54" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated"><strong class="ih hj">首先，我们将运行Spark消费者程序</strong>来启动它，并让它等待来自生产者的新消息。</p><p id="e387" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在新窗口中，转到存放消费者代码文件的文件夹。</p><p id="4a29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须调用一个名为<strong class="ih hj">的spark脚本spark-submit并向其传递整合卡夫卡和Spark所需的包路径。</strong></p><p id="0e01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">~/.local/bin/spark-submit-packages org . Apache . spark:spark-SQL-Kafka-0–10 _ 2.12:3 . 1 . 2 spark-consumer-pcap . py</p><p id="124b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的CLI中，2.12是Kafka包的Scala版本，3.1.2是我们使用的Spark版本(参考<a class="ae lb" href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html" rel="noopener ugc nofollow" target="_blank">结构化流+ Kafka集成指南</a>的“部署”部分)。</p><p id="fd8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以添加<strong class="ih hj"> PATH=$PATH:~/。local/bin/ </strong>在你的~/里。巴沙尔调用火花-提交从任何地方。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es my"><img src="../Images/482e072b4227997c502cc3a33b20500c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*07z3pAuBFV3X1CXq-cNEvA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">启动Spark消费者代码</figcaption></figure><p id="b3e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，不要干扰消费者程序窗口，打开一个新的选项卡来运行生产者代码。</p><p id="738a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">sudo-E python 3 PKT-producer-pcap . py</strong></p><p id="9c6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我使用sudo是因为如果您想运行第二个示例(从网络接口捕获实时数据包)，管理员权限是必不可少的。这里我们运行第一个例子(从pcap文件获取数据包数据)。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mz"><img src="../Images/78c8b16dc17b7ddf445c583b5fc5ac5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eYwEttylrxzhQsTEV4p04w.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">生产者将数据包5元组流式传输到Kafka</figcaption></figure><p id="a1c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在上面看到，生产者将包数据(5元组)一个接一个地流式传输到Kafka。在我的例子中，总共有27个包。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es na"><img src="../Images/d3f0bede814fc0abd651af38cbeda626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZreNiYzDPZJCA8pRrLGGzA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">生产者将数据包5元组流式传输到Kafka</figcaption></figure><p id="495f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，生产者完成了从pcap文件中提取包数据并将其流式传输到Kafka服务器。</p><p id="1b39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以通过运行带有- list选项的kafka-topics.sh来检查主题是否在Kafka中成功创建。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nb"><img src="../Images/86e47579893a6fa35fd2f91a05fc791e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPDdsO63js17HppXpSBaiw.png"/></div></div></figure><p id="f369" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在列表中看到我们的主题“pkttest_pcap”。</p><p id="ba3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将切换到消费者窗口，检查消费者是否能够接收到来自Kafka的消息流。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nc"><img src="../Images/3a939b1a3ee2bb77167033da2e8985c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NKoyPzngsjw5OV6q-g-FQ.png"/></div></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nd"><img src="../Images/7afa204cda55ad53d85bf8d40575d6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SXg4wqfvwXqifDZDff5wuQ.png"/></div></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ne"><img src="../Images/410617dac30bd0f74d2d77c643ff1ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H_X1o-ZA0nPxmHKcR1DPnw.png"/></div></div></figure><p id="689b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">太棒了！我们看到Spark消费者代码能够接收不同批次的传入流，并在控制台中显示所有27个数据包的数据帧。</p><p id="0f8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以通过消费者代码中的groupBy on协议字段在pyspark中尝试对数据帧进行聚合操作，如下所示。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nf"><img src="../Images/6a02a72aae0f6f29f3edbd9681799db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52B7Ie1WZWUqIWLZD6_8UQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">聚合操作示例</figcaption></figure><p id="1765" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出如下所示。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ng"><img src="../Images/b8525018f2a13a7b0c0ad02aafe79eec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PzrdmCAxfhxBavWGanBPhw.png"/></div></div></figure><p id="1ec9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，当新数据到达时，它会不断更新两个协议TCP (6)和UDP (17)的计数。最后，我们看到5个TCP数据包和22个UDP数据包，总共27个数据包。</p><p id="7831" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您想将接收到的数据帧批量保存在文件中，您可以尝试以下代码。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nh"><img src="../Images/d326edf8d5e6c2764e5a925c05e69c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rej6mhq7doF9GPOHQChRRA.png"/></div></div></figure><p id="57c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以开始学习更多关于pyspark SQL和DataFrame操作的知识，并做更多的分析，因为我们现在知道如何在spark中从源接收流数据。</p><p id="89cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，您可以尝试第二个示例，即从网络接口实时嗅探数据包并进行流式传输(pkt-producer-live.py和spark-consumer-live.py)。<strong class="ih hj">请注意，生产者从接口开始捕获数据包可能需要一点时间，所以你必须耐心尝试。</strong></p><p id="d70a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面给出了实时捕获示例的输出。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ni"><img src="../Images/8117e2f0c37e6b188f51fc36dc5b1786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yrejq8CrRTtVqRN7Sqg18g.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">实时嗅探和流式传输的生产者输出</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nj"><img src="../Images/a6d27508b9f4af17bdebadaa65bb63a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xqml1HFQfLBvVJC5oTmjnw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">Spark消费者收到活嗅包</figcaption></figure></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="5389" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结论</h1><p id="6749" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">到目前为止，我们已经用</p><ul class=""><li id="d1fd" class="lv lw hi ih b ii ij im in iq lx iu ly iy lz jc mj mb mc md bi translated">如何使用Kafka流式传输网络流量数据？</li><li id="d60f" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated">如何写一个卡夫卡式的制片人？</li><li id="ec04" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated">如何写一个Spark结构化的流消费者？</li></ul><p id="69aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">毫无疑问，还有很多东西需要深入研究和探索，当然，在接收到的数据的Spark处理/分析方面也有更多的工作要做。</p><p id="c126" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管如此，我希望这个博客能够帮助初学者(尤其是网络领域的爱好者)开始他们在Kafka和Spark streaming的学习之路。感谢您读到这里！如果你喜欢这个博客，请鼓掌，因为它会鼓励我写更多类似的学习！也请<a class="ae lb" href="https://rajarams.medium.com/subscribe" rel="noopener">订阅</a>来接收我发来的更多此类博客的通知！</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h2 id="90fa" class="kn jl hi bd jm ko kp kq jq kr ks kt ju iq ku kv jy iu kw kx kc iy ky kz kg la bi translated">参考材料:</h2><ul class=""><li id="ef05" class="lv lw hi ih b ii ki im kj iq mk iu ml iy mm jc mj mb mc md bi translated"><a class="ae lb" href="https://www.youtube.com/playlist?list=PL7_h0bRfL52qWoCcS18nXcT1s-5rSa1yp" rel="noopener ugc nofollow" target="_blank">主数据块和阿帕奇火花</a></li><li id="5451" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated"><a class="ae lb" href="https://www.youtube.com/watch?v=CU44hKLMg7k&amp;t=5612s" rel="noopener ugc nofollow" target="_blank">适合初学者的阿帕奇卡夫卡</a></li><li id="736d" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated"><a class="ae lb" href="https://www.ciscopress.com/store/data-analytics-for-it-networks-developing-innovative-9780135183465" rel="noopener ugc nofollow" target="_blank">IT网络数据分析</a> -作者John Garret</li><li id="1413" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated"><a class="ae lb" href="https://upcommons.upc.edu/bitstream/handle/2117/110827/125003.pdf?sequence=1&amp;isAllowed=y" rel="noopener ugc nofollow" target="_blank">使用Apache Spark进行网络流量分类</a></li><li id="951c" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated"><a class="ae lb" href="https://github.com/oberljn/pcap_streaming#pcap-stream-processing" rel="noopener ugc nofollow" target="_blank"> PCAP流处理</a></li><li id="c45f" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated"><a class="ae lb" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8268735" rel="noopener ugc nofollow" target="_blank">使用Spark流的在线互联网流量监控系统</a></li><li id="288f" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated"><a class="ae lb" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">结构化流媒体节目指南</a></li><li id="3f53" class="lv lw hi ih b ii me im mf iq mg iu mh iy mi jc mj mb mc md bi translated"><a class="ae lb" href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html" rel="noopener ugc nofollow" target="_blank">结构化流媒体+ Kafka集成指南</a></li></ul></div></div>    
</body>
</html>