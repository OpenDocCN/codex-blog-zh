<html>
<head>
<title>Integrate LibTorch library to QT for GPU Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将LibTorch库集成到QT中用于GPU推理</h1>
<blockquote>原文：<a href="https://medium.com/codex/integrate-libtorch-library-to-qt-for-gpu-inference-8c523d682e51?source=collection_archive---------2-----------------------#2021-07-04">https://medium.com/codex/integrate-libtorch-library-to-qt-for-gpu-inference-8c523d682e51?source=collection_archive---------2-----------------------#2021-07-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f8cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Qt是构建跨平台GUI应用程序的强大框架。对于生产部署场景，您需要将在PyTorch机器学习框架中开发的模型集成到您的c++ QT中。为了在c++中加载PyTorch模型，您需要将您的模型转换为Torch脚本格式。你可以在这里找到如何将Pytorch模型转换成torch脚本<a class="ae jd" href="https://pytorch.org/tutorials/advanced/cpp_export.html" rel="noopener ugc nofollow" target="_blank">的详细步骤。</a></p><h1 id="5f06" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">下载libtorch库</h1><p id="f616" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">进入PyTorch官网<a class="ae jd" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">链接</a>，根据安装的OS和Cuda版本选择合适的libtorch版本</p><p id="a6d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Windows操作系统</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="f9e8" class="kq jf hi km b fi kr ks l kt ku"><a class="ae jd" href="https://download.pytorch.org/libtorch/cu111/libtorch-win-shared-with-deps-1.8.1%2Bcu111.zip" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/libtorch/cu111/libtorch-win-shared-with-deps-1.8.1%2Bcu111.zip</a></span></pre><p id="276d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Linux操作系统</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="9cbd" class="kq jf hi km b fi kr ks l kt ku">Download here (Pre-cxx11 ABI):<br/><a class="ae jd" href="https://download.pytorch.org/libtorch/cu111/libtorch-shared-with-deps-1.8.1%2Bcu111.zip" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/libtorch/cu111/libtorch-shared-with-deps-1.8.1%2Bcu111.zip</a><br/><br/>Download here (cxx11 ABI):<br/><a class="ae jd" href="https://download.pytorch.org/libtorch/cu111/libtorch-cxx11-abi-shared-with-deps-1.8.1%2Bcu111.zip" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/libtorch/cu111/libtorch-cxx11-abi-shared-with-deps-1.8.1%2Bcu111.zip</a></span></pre><h1 id="f4f6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">将libtorch添加到qt项目中</h1><p id="b4d1" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">这是最重要的，也是比较麻烦的。首先，打开QT项目”。pro "文件，并添加libtorch头文件目录(include)和库文件目录(lib)。需要添加基于OS平台的编译器和链接标志，否则libtorch不会针对torch_cuda进行链接，最终以“torch::cuda::is_available()”返回0。Libtorch建议使用CMAKE作为构建工具，但是您也可以使用QMAKE for QT实现同样的功能，但是应该小心使用标志。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="cee3" class="kq jf hi km b fi kr ks l kt ku">CONFIG += c++11<br/>#Include the headers<br/>INCLUDEPATH += $$PYTORCH_DIR/include/torch/csrc/api/include \<br/>               $$quote($$CUDA_DIR/include)<br/>#compiler and linker flags<br/>unix {<br/><strong class="km hj">QMAKE_LFLAGS += -Wl,--no-as-needed</strong><br/>}<br/>win32 {<br/><strong class="km hj">QMAKE_LFLAGS += -INCLUDE:?warp_size@cuda@at@@YAHXZ<br/>QMAKE_LFLAGS += -INCLUDE:?searchsorted_cuda@native@at@@YA?AVTensor@2@AEBV32@0_N1@Z<br/>QMAKE_LFLAGS += /machine:x64</strong><br/>}</span><span id="477b" class="kq jf hi km b fi kv ks l kt ku"># linking with libs<br/>win32 {<br/>LIBS += $$quote($$PYTORCH_DIR\lib\*.lib)<br/>LIBS +=  -L$$quote($$PYTORCH_DIR\lib)</span><span id="485d" class="kq jf hi km b fi kv ks l kt ku">LIBS += $$quote($$CUDA_DIR\lib\x64\*.lib)<br/>LIBS += -L$$quote($$CUDA_DIR\v11.1\bin)<br/>}<br/>unix {<br/>LIBS += -L$$PYTORCH_DIR/lib  -ltorch_cpu -ltorch  -lc10 -ltorch_cuda  -lc10_cuda -lcaffe2_observers  \<br/>-lcaffe2_nvrtc -lcaffe2_detectron_ops_gpu  \<br/>-lnvrtc-builtins -lprocess_group_agent -lshm  \<br/>-ltensorpipe_agent -ltorch -ltorch_cuda_cpp -ltorch_cuda_cu -ltorch_global_deps</span><span id="b1ea" class="kq jf hi km b fi kv ks l kt ku">}</span></pre><h1 id="7465" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">用LibTorch构建QT应用程序</h1><p id="e986" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">下一步是构建QT项目源代码，如果您的应用程序基于QT小部件，您将会得到关于插槽冲突编译错误的错误。这是由于QT和libtorch库之间的slots关键字冲突造成的，您可以通过在源代码开头添加libtorch头文件来避免这种情况，如下所示</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="9f3b" class="kq jf hi km b fi kr ks l kt ku">#undef slots<br/>#include "torch/torch.h"<br/>#include "torch/jit.h"<br/>#include "torch/nn.h"<br/>#include "torch/script.h"<br/>#define slots Q_SLOTS</span><span id="44d9" class="kq jf hi km b fi kv ks l kt ku">#include "mainwindow.h"<br/>#include "ui_mainwindow.h"<br/>#include &lt;QList&gt;</span></pre><p id="037e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查加载了Cuda功能的Libtorch及其配置</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="0a42" class="kq jf hi km b fi kr ks l kt ku">qDebug() &lt;&lt; "Cuda Device Count" &lt;&lt; torch::cuda::device_count();</span><span id="9796" class="kq jf hi km b fi kv ks l kt ku">qDebug() &lt;&lt; "cudnn_is_available" &lt;&lt; torch::cuda::cudnn_is_available();</span><span id="923e" class="kq jf hi km b fi kv ks l kt ku">qDebug() &lt;&lt; "cuda::is_available" &lt;&lt; torch::cuda::is_available();</span><span id="4c68" class="kq jf hi km b fi kv ks l kt ku">qDebug() &lt;&lt; "cuda::show_config" &lt;&lt; torch::show_config().c_str();</span></pre><p id="983a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦应用程序执行，你会得到如下日志，如果有任何GPU存在</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="8a83" class="kq jf hi km b fi kr ks l kt ku">Cuda Device Count 1<br/>cudnn_is_available true<br/>cuda::is_available true<br/>cuda::show_config PyTorch built with:<br/> - C++ Version: 199711<br/>  - MSVC 192829913<br/>  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications<br/>  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)<br/>  - OpenMP 2019<br/>  - CPU capability usage: AVX2<br/>  - CUDA Runtime 11.1<br/>  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37<br/>  - CuDNN 8.0.5<br/>  - Magma 2.5.4<br/>  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=C:/w/b/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -DNDEBUG -DUSE_FBGEMM -DUSE_XNNPACK, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON,</span></pre><p id="b555" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:需要用MSVC构建QT应用程序，因为libtorch不支持windows上的MinGW。</p><h1 id="6d56" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">加载torchscript模型并运行推理</h1><p id="53d8" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在lib torch模块中加载torch脚本模型</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="64b7" class="kq jf hi km b fi kr ks l kt ku">torch::jit::script::Module aLibTorchModule = torch::jit::load("torch script model path");</span></pre><p id="a1c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择GPU设备(如果可用),否则选择CPU设备</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="2f5a" class="kq jf hi km b fi kr ks l kt ku">torch::DeviceType aDeviceType;<br/>if (torch::cuda::is_available())<br/>{<br/>aDeviceType= torch::kCUDA;<br/>}<br/>else <br/>{<br/>aDeviceType= torch::kCPU;<br/>}<br/>torch::Device aTorchDevice = torch::Device(aDeviceType);</span></pre><p id="5f5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将加载的模块移动到选择的设备，在我们的例子中是GPU</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="c97a" class="kq jf hi km b fi kr ks l kt ku">aLibTorchModule.to(aTorchDevice);</span></pre><p id="51a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，您可以在libtorch模块中执行推理，绕过模型的正确输入。如果您有一个打开的cv::Mat图像对象，Lib torch提供了一个转换为输入张量的函数。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="1c74" class="kq jf hi km b fi kr ks l kt ku">auto input_tensor = torch::from_blob(</span><span id="451f" class="kq jf hi km b fi kv ks l kt ku"><em class="kw">data</em>, {1, Height, Width, 3}); // here data should opencv CV::MAT</span></pre><p id="f86f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有时，您可能需要根据模型预期对输入进行预处理，遵循lib torch函数允许您更改输入期限维度的顺序，并在进行实际推理之前对输入进行规范化。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="399c" class="kq jf hi km b fi kr ks l kt ku">input_tensor = input_tensor.permute({0, 3, 1, 2});<br/>std::vector&lt;double&gt; norm_mean = {0.485, 0.456, 0.406};<br/>std::vector&lt;double&gt; norm_std = {0.229, 0.224, 0.225};<br/>input_tensor = torch::data::transforms::Normalize&lt;&gt;(norm_mean, norm_std)(input_tensor);</span></pre><p id="0c20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，在libtorch模块中执行推理，该模块是在开始时加载模型的。Libtorch Forward方法将结果作为张量对象返回，您可以根据需要检索结果并操作结果。如果您的模型加载在GPU中，请确保您应该在处理推理之前将您的输入张量移动到GPU，否则将会出现错误。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="da25" class="kq jf hi km b fi kr ks l kt ku">input_tensor = input_tensor.to(aTorchDevice);<br/>torch::Tensor out_tensor = aLibTorchModule.forward({input_tensor}).toTensor();</span></pre></div></div>    
</body>
</html>