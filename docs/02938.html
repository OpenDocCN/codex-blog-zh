<html>
<head>
<title>EM algorithm and Gaussian Mixture Model (GMM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EM算法和高斯混合模型(GMM)</h1>
<blockquote>原文：<a href="https://medium.com/codex/em-algorithm-and-gaussian-mixture-model-gmm-6ea5e0cf9d6e?source=collection_archive---------1-----------------------#2021-08-12">https://medium.com/codex/em-algorithm-and-gaussian-mixture-model-gmm-6ea5e0cf9d6e?source=collection_archive---------1-----------------------#2021-08-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="dcda" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">Python中的示例实现</h2></div></div><div class="ab cl ix iy gp iz" role="separator"><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc"/></div><div class="hb hc hd he hf"><p id="97a1" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">前言:本文旨在提供潜在主题的综合信息，不应被视为原创作品。这些信息和代码通过一些在线文章、研究论文、书籍和开源代码被重新利用。</p></div><div class="ab cl ix iy gp iz" role="separator"><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc"/></div><div class="hb hc hd he hf"><h1 id="8702" class="ka kb hi bd kc kd ke kf kg kh ki kj kk io kl ip km ir kn is ko iu kp iv kq kr bi translated"><strong class="ak">简介</strong></h1><p id="272d" class="pw-post-body-paragraph je jf hi jg b jh ks ij jj jk kt im jm jn ku jp jq jr kv jt ju jv kw jx jy jz hb bi kx translated"><span class="l ky kz la bm lb lc ld le lf di">期望最大化算法，简称EM算法，是一种在存在潜在变量的情况下进行最大似然估计的方法。EM算法在1977年由A. Dempster和D. Rubin在《皇家统计学会杂志》上发表的一篇经典论文中被解释并命名。</span></p><p id="0207" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><strong class="jg hj">首先，让我们回忆一下聚类方法的类型:</strong></p><ul class=""><li id="5f76" class="lg lh hi jg b jh ji jk jl jn li jr lj jv lk jz ll lm ln lo bi translated">硬聚类:聚类不重叠(元素属于或不属于聚类)，例如K-means、K-Medoid。</li><li id="5a20" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">软聚类:聚类可能重叠(聚类和实例之间的关联强度)，例如使用期望最大化算法的混合模型。</li></ul><p id="2841" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">GMM聚类更灵活，但不一定比K-means更准确，因为您可以将其视为一种模糊或软聚类方法。软聚类方法为每个聚类的数据点分配一个分数。分值指示数据点与聚类的关联强度。与硬聚类方法相反，软聚类方法是灵活的，因为它们可以将一个数据点分配给多个聚类。当使用GMMs进行聚类时，得分是后验概率。</p><p id="9ac9" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><strong class="jg hj">混合车型:</strong></p><ul class=""><li id="97ff" class="lg lh hi jg b jh ji jk jl jn li jr lj jv lk jz ll lm ln lo bi translated">基于概率的软聚类方法</li><li id="4b9b" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">每个聚类:一个生成模型(高斯或多项式)</li><li id="ff39" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">参数(例如，均值/协方差未知)</li></ul><h1 id="4f1a" class="ka kb hi bd kc kd lu kf kg kh lv kj kk io lw ip km ir lx is ko iu ly iv kq kr bi translated">GMM在Python中的实现</h1><p id="b2d1" class="pw-post-body-paragraph je jf hi jg b jh ks ij jj jk kt im jm jn ku jp jq jr kv jt ju jv kw jx jy jz hb bi translated">完整的代码可以从GitHub 上的<a class="ae lz" href="https://github.com/OksanaKalytenko/mediumposts/blob/main/EM_algorithm.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本中获得。</a></p><p id="2538" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">让我们创建一个样本数据集，其中的点是从两个高斯过程之一生成的。第一个分布的平均值为100，第二个分布的平均值为90；我们分布的标准差分别是5和2。</p><p id="cc62" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">第一个流程我们会有6万分；第二个过程50000个点混合在一起。</p><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es ma"><img src="../Images/88ef494b467b0c91e417c249b1cb2d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*-AAFpLHEKHEI3qelQK_skg.png"/></div></figure><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mi"><img src="../Images/5fb7916faae367dc093c97f65a71a940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j6graaopQ0B39gpUt3pL2A.png"/></div></div></figure><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mn"><img src="../Images/998ee1dc3c77b4a22e83ef48708219ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tehHg9ZYaIYRDLcEKHDhZQ.png"/></div></div></figure><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mo"><img src="../Images/619a4aa29846d6b93e1f09b7ac7bb652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nC7gs4HtTEyPhR2NOHnBBw.jpeg"/></div></div></figure><p id="4978" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">将这些过程混合在一起后，我们就有了我们在图上看到的数据集。我们可以注意到两个峰值:大约90°和100 °,但是对于峰值中间的许多点来说，它们来自哪个分布是不明确的。那么我们应该如何处理这个问题呢？</p><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mp"><img src="../Images/2fcfc7bc930114623c4c8e35937c130c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6VXeEQ3W5kPjHvy2LAMsRg.png"/></div></div></figure><p id="b3d9" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">所以在这里，我们必须估计总共5个参数:</p><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es mq"><img src="../Images/7485701cc37b12a35d02f58fb339fe86.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*RgrV2ec_Hn9rvqazZ29xGA.png"/></div></figure><p id="5917" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">其中p是数据来自第一个高斯分布的概率，1-p是数据来自第二个高斯分布的概率。</p><p id="41eb" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">我们可以使用高斯混合模型，该模型将使用期望最大化算法(EM)来估计分布的参数。</p><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mr"><img src="../Images/8d8e0491ee0c805d141acad967f1f752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7WRLpynqSI8amb1S3IjTA.png"/></div></div></figure><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es ms"><img src="../Images/504fd015bba43779415c92ec01f9d316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*AY7YUAk_TpCZKojtnQSB1w.png"/></div></figure><p id="3d23" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">EM在执行期望E步骤和最大化M步骤之间交替，期望E步骤通过包括潜在变量来计算可能性的期望，就像它们被观察到一样，最大化M步骤通过最大化在E步骤上找到的期望可能性来计算参数的最大可能性估计。在M-step上找到的参数然后被用于开始另一个E-step，并且该过程被重复直到收敛。</p><p id="6d9f" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">在sklearn中，我们使用了<a class="ae lz" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html" rel="noopener ugc nofollow" target="_blank"> GaussianMixture类</a>，它实现了EM算法来拟合混合高斯模型。</p><p id="2e7a" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">该类允许我们在定义模型时通过<em class="mt"> n_components </em>参数来指定用于生成数据的潜在进程的可疑数量。我们将它设置为2，因为我们有两个进程(或分布)。</p><p id="50ad" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">如果流程的数量未知，可以测试一系列不同数量的组件，并选择最适合的模型，其中可以使用分数(如阿凯克或贝叶斯信息标准(AIC或BIC))来评估模型。</p><p id="e873" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">我们还可以通过许多方式来配置模型，以纳入我们可能知道的关于数据的其他信息，例如如何估计分布的初始值。在这种情况下，我们将通过将<em class="mt"> init_params </em>参数设置为‘random’来随机猜测初始参数。</p><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mu"><img src="../Images/3b65b2797b84b33b2ac2cebbf2903efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yu-et7p4A0UF2dcDRR2kvw.png"/></div></div></figure><p id="1eeb" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">运行该示例将使用EM算法在准备好的数据集上拟合高斯混合模型。拟合后，该模型用于预测训练数据集中示例的潜在变量值。</p><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mv"><img src="../Images/b1c7833d35313f1a5aa792cd8e7ab7dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mkY0-tX_4qU1kTt2NippZw.png"/></div></div></figure><p id="81e7" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">我们可以看到，至少对于数据集中的前几个和后几个示例，模型大多预测了潜在变量的正确值。方法predict_proba()预测给定数据的每个分量的后验概率。在我们的例子中，点105.0属于每个高斯过程的概率是0.501和0.499。</p><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mw"><img src="../Images/b5b9db3e4e7048b610e4657897e3e966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NmEzE4Sxj1GHX6cveKlbfA.png"/></div></div></figure><p id="6993" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">一般来说，k-means和EM可能表现得更好或更差，这取决于我们想要聚类的数据的性质，以及我们定义好的聚类结果的标准。</p></div><div class="ab cl ix iy gp iz" role="separator"><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc"/></div><div class="hb hc hd he hf"><p id="2cd2" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">对于多变量高斯分布，情况稍微复杂一些，如下所示:</p><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="er es mx"><img src="../Images/bee56e31863b4c463583d7ebc74b31dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wiTEVsyUZhucoBx-GCY77A.jpeg"/></div></div></figure><ul class=""><li id="9eb3" class="lg lh hi jg b jh ji jk jl jn li jr lj jv lk jz ll lm ln lo bi translated">具有<em class="mt"> d </em>属性的数据，来自<em class="mt"> k </em>源</li><li id="1501" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">每个源<em class="mt"> c </em>是一个高斯</li><li id="67f5" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">迭代估计参数:</li></ul><p id="59b5" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">-之前来自来源<em class="mt"> c </em>的实例百分比是多少？</p><p id="8e54" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">- mean:来自源<em class="mt"> c </em>的属性<em class="mt"> j </em>的期望值</p><p id="2f1d" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">-协方差:源<em class="mt"> c </em>中属性<em class="mt"> j </em>和<em class="mt"> k </em>的相关程度如何？</p><p id="7979" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">-基于:我们对每个实例来源的猜测</p></div><div class="ab cl ix iy gp iz" role="separator"><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc"/></div><div class="hb hc hd he hf"><h1 id="2946" class="ka kb hi bd kc kd ke kf kg kh ki kj kk io kl ip km ir kn is ko iu kp iv kq kr bi translated">EM算法的应用</h1><p id="ed4c" class="pw-post-body-paragraph je jf hi jg b jh ks ij jj jk kt im jm jn ku jp jq jr kv jt ju jv kw jx jy jz hb bi translated">潜在变量模型在机器学习中有几个实际应用:</p><ul class=""><li id="0aa5" class="lg lh hi jg b jh ji jk jl jn li jr lj jv lk jz ll lm ln lo bi translated">用于计算函数的<strong class="jg hj">高斯密度</strong>。</li><li id="9153" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">有助于在取样期间填写<strong class="jg hj">缺失数据</strong>。</li><li id="9a03" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">用于寻找<strong class="jg hj">潜变量</strong>的值。</li><li id="e65e" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">用于<strong class="jg hj">医学和结构工程</strong>领域的图像重建。</li><li id="94b3" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">它在不同的领域有着广泛的用途，如自然语言处理、计算机视觉、计算机视觉等。</li><li id="aa95" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">用于估计<strong class="jg hj">隐马尔可夫模型</strong>的参数，也用于其他一些混合模型，如<strong class="jg hj">高斯混合模型</strong>等。</li></ul><h1 id="ba35" class="ka kb hi bd kc kd lu kf kg kh lv kj kk io lw ip km ir lx is ko iu ly iv kq kr bi translated">EM算法的优缺点</h1><p id="10eb" class="pw-post-body-paragraph je jf hi jg b jh ks ij jj jk kt im jm jn ku jp jq jr kv jt ju jv kw jx jy jz hb bi translated"><strong class="jg hj">优点</strong></p><ul class=""><li id="f4c9" class="lg lh hi jg b jh ji jk jl jn li jr lj jv lk jz ll lm ln lo bi translated">就实现而言，EM算法的两个基本步骤，即E步骤和M步骤，对于许多机器学习问题来说通常是非常容易的。</li><li id="3964" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">M步的解通常以封闭形式存在。</li><li id="48cc" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">每次迭代后，可能性的值总是会增加。</li></ul><p id="9bfd" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><strong class="jg hj">缺点</strong></p><ul class=""><li id="fbdc" class="lg lh hi jg b jh ji jk jl jn li jr lj jv lk jz ll lm ln lo bi translated">它有<strong class="jg hj">慢收敛</strong>。</li><li id="7d72" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">它对起点敏感，仅收敛于<strong class="jg hj">局部最优</strong>。</li><li id="f36a" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">它不能发现K(可能性随着聚类的数量而不断增长)</li><li id="b65c" class="lg lh hi jg b jh lp jk lq jn lr jr ls jv lt jz ll lm ln lo bi translated">它同时考虑了向前和向后的概率。这与数值优化相反，数值优化只考虑<strong class="jg hj">前向概率</strong>。</li></ul><h1 id="3411" class="ka kb hi bd kc kd lu kf kg kh lv kj kk io lw ip km ir lx is ko iu ly iv kq kr bi translated">参考资料:</h1><p id="1f65" class="pw-post-body-paragraph je jf hi jg b jh ks ij jj jk kt im jm jn ku jp jq jr kv jt ju jv kw jx jy jz hb bi translated">期望最大化:工作原理——<a class="ae lz" href="https://youtu.be/iQoXFmbXRJA" rel="noopener ugc nofollow" target="_blank">https://youtu.be/iQoXFmbXRJA</a></p><p id="0e57" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">Python中的计算统计学:期望最大化(EM)算法—<a class="ae lz" href="https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html" rel="noopener ugc nofollow" target="_blank">https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html</a></p><p id="5224" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">无监督学习:聚类:高斯混合模型(GMM)——<a class="ae lz" href="https://www.python-course.eu/expectation_maximization_and_gaussian_mixture_models.php" rel="noopener ugc nofollow" target="_blank">https://www . python-course . eu/expectation _ maximization _ and _ Gaussian _ Mixture _ Models . PHP</a></p><p id="be2d" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">隐马尔可夫模型和贝叶斯网络简介—<a class="ae lz" href="http://mlg.eng.cam.ac.uk/zoubin/papers/ijprai.pdf" rel="noopener ugc nofollow" target="_blank">http://mlg.eng.cam.ac.uk/zoubin/papers/ijprai.pdf</a></p><p id="39f2" class="pw-post-body-paragraph je jf hi jg b jh ji ij jj jk jl im jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">感谢阅读。<em class="mt">如果您有任何反馈，请随时对本文发表评论，在</em> <a class="ae lz" href="https://www.linkedin.com/in/oksana-kalytenko-546515119/" rel="noopener ugc nofollow" target="_blank"> <em class="mt"> LinkedIn </em> </a> <em class="mt">上给我发消息，或者给我发电子邮件【(oksanakalytenko@gmail.com】</em></p></div></div>    
</body>
</html>