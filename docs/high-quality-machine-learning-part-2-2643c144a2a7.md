# 高质量 ML —第 2 部分—公平性

> 原文：<https://medium.com/codex/high-quality-machine-learning-part-2-2643c144a2a7?source=collection_archive---------13----------------------->

## *请查看* [*第 1 部分*](https://mukund-kannan.medium.com/high-quality-machine-learning-part-1-d45259ee7613) *如果还没有完成*

![](img/8d994fed653982849f9755f5a82d7264.png)

马库斯·斯皮斯克在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

# ML 公平性

尽管 ML 公平性已经成为一个感兴趣的领域有一段时间了，但它只是在最近才获得更多的关注。这种兴趣从较小的利基群体到更广泛的企业的变化实际上可以被视为衡量 ML 模型获得工业牵引力的速度。

遵循我们本系列的主题，让我们从数据科学生命周期的角度来看公平性，在每个阶段可以做些什么(如[第 1 部分](https://mukund-kannan.medium.com/high-quality-machine-learning-part-1-d45259ee7613)所述)来确定&增加 ML 模型的公平性。

公平性定义分为个体和群体。人口统计均等、均等优势和预测率均等属于群体公平范畴，而无意识、个体均等和反事实公平属于个体。我们选择衡量和平衡的是与业务专家协商。

为了量化 ML 模型中的偏差，有相当多的指标可供选择。但鉴于对 ML 模型公平性的判断最终是由人类来完成的，选择应该根据验证机构的舒适程度来做出——商业或统计术语。如果目标受众来自商界，则应该直观地呈现更简单的报告，显示 ML 模型在优势和劣势群体(被怀疑偏向和反对的群体)上的表现。

由于偏倚缩减对 ML 模型施加了额外的约束，普遍的观点是公平性和准确性具有反比关系。在架构阶段，必须仔细考虑潜在的公平性-准确性权衡。

# 数据管理阶段

数据代理市场非常大——有人估计这是一个 2000 亿美元的行业。大型商业数据提供商，如 Acxiom、Equifax、Bureau 等。一直在收集数据并向企业提供数据。我不知道任何标准的公平标准以及获得的数据。但是使用数据的团队应该识别并解决任何问题。

在这个早期阶段寻找数据中的偏差似乎是一个很大的问题。更简单的预处理技术，如重新加权、不同影响消除器、学习公平表示等。有助于在模型构建阶段之前识别和剔除数据中的部分信息。快速浏览这些技术，让读者了解在 DSLC 的这个阶段可以做些什么。

*   完全不同的影响消除器在某种程度上类似于功能缩放，我们试图在训练之前“正常化”优势和劣势群体的受保护属性。这降低了有偏特征与地面实况标签的直接相关性，同时保留了两个组的分布轮廓。显然，这主要适用于连续特征。
*   重新加权是一种基于受保护属性有利于弱势群体的概率来计算一组权重，然后用这些权重训练分类器的方法。
*   公平代表是指在代表候选人数据时没有(或减少)性别、种族等方面的偏见。在语言模型中，由于来自原始语料库的(人为引入的)偏差，在词向量表示中发现了这些偏差。学习公平表征的重点是通过减少偏向轴方向上偏向词之间的语义潜在距离来减少偏向。

虽然这些技术对于简单和直接的偏差很有效，但是它们在处理分布在高度相关的特征上的偏差时效果不佳。但在现实世界中，人类引入的无意识偏差是最简单的偏差，不涉及太多属性。我们在这个阶段可以消除的仍然具有很高的价值，并且减少了对下游处理的不必要的压力。

# 模型构建阶段

在模型构建阶段，偏差消除通常作为构建过程本身的一部分来处理。最直接的方法之一是将公平性惩罚集成到损失函数中。另一种是使用生成对抗网络(GAN)方法来减少偏差。这将需要与预测器网络一起训练鉴别器网络，以减少受保护属性对预测的影响。当鉴别器试图识别受保护的变量时，预测器试图调整权重以模糊受保护的特征。据称，这种方法也更好地最小化了公平性-准确性的折衷。

# 部署管理(AIS)阶段

测试 ML 模型中的偏倚是一个功能性的方面，而不是非功能性的。虽然在第 1 部分的[验收测试部分中描述的一些标准实践将对公平性测试有用，但建议不要将此视为 DSLC 中的一个独立阶段，并将其与功能测试策略完全集成。应在此阶段根据 ML 模型中的潜在偏差和适当的公平性指标选择正确的数据采样策略，以获得最有效的结果。](https://mukund-kannan.medium.com/high-quality-machine-learning-part-1-d45259ee7613)

# 部署管理(ILS)阶段

在一个活动的 ML 模型的生命周期内，必须持续监控其最佳行为。这是实时模型维护的标准操作流程的一部分。实时模型性能质量是预测准确性指标和公平性指标的组合。对于在高度管制的环境中执行的 ML 模型，仅仅监控总体指标是不够的。应进行个体预测的群组级抽样，评估 ML 模型的公平性，并且必须将案例发送给看起来不公平/极端的案例进行人工干预。在随后的训练周期中，手动操作必须反馈到 ML 模型中。这个小周期的治理需要是最高级别的，因为 ML 模型的行为可以在一段时间内随着新偏差的引入而被修改。

# 关键要点

*   由于公平性的判断最终是由人来完成的，所以强烈建议根据审查涉众的期望来选择这些指标
*   在所有阶段都要明智地应用偏差消除技术。必须牢记公平-性能权衡
*   选择正确的数据采样策略和合适的公平性度量对于度量 ML 模型的公平性非常重要
*   ML Ops 应包括持续检查偏差，以及在怀疑预测不公平的情况下采取的措施。
*   有效的治理应该到位，以避免反馈-再培训循环将 ML 模型行为推向负面方向

*我们将在* [*第三部*](https://mukund-kannan.medium.com/high-quality-machine-learning-part-3-c660b885233c) *中覆盖 DSLC 的安全方面。第 3 部分结尾的致谢和参考。*