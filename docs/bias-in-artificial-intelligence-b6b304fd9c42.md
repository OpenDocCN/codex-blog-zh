# 有偏见的人工智能

> 原文：<https://medium.com/codex/bias-in-artificial-intelligence-b6b304fd9c42?source=collection_archive---------9----------------------->

## 数字时代的后果和机遇

![](img/0516959f13eea19890cd37d418729fd4.png)

[附身摄影](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

我写代码。

我还意识到，这一准则可能会对整个社会阶层的就业前景、贷款审批和健康状况产生意想不到的影响。

这种认识促使我更深入地研究人工智能中的偏见概念及其在现实世界场景中的意外后果。

然而，这并不都是厄运和黑暗！有可能建立对偏见和歧视更加鲁棒的人工智能系统。此外，人类和机器之间的伙伴关系实际上可以提高人类决策的公平性。我在这篇博客中的意图是明确地关注一个有偏见的系统如何直接影响一个少数群体，以及我们可以采取什么步骤来纠正它。(当然，机器影响少数群体还有很多其他方式。例如，通过机器创造的财富和经济剩余的不平均分配。然而，那是另一天的谈话……)

# **歧视制度**

![](img/fc41217ded9271061490ea9638a82e96.png)

照片由[附身摄影](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

人工智能系统的功能，从根本上来说，是一种鉴别系统。就其核心而言，它们是“区分、排列和分类”的分类技术(西，2019)。决策树、强化学习、神经网络和预测分析等工具通过处理历史数据来预测未来的结果。因此，这些系统的输出反映了当前社会中歧视性的权力平衡，这有什么好奇怪的？有偏见的人工智能的成本由历史上遭受损失的人承担。即:性别少数群体、有色人种和少数民族。

# **意想不到的后果**

有偏系统的真实世界后果是有问题的，近年来已经出现了各种系统事故的例子。苹果联合创始人史蒂夫·沃兹尼亚克的妻子珍妮特·希尔在一张苹果卡上获得了一笔“仅为她丈夫 10%的信用额度”。杰出的软件工程师大卫·海涅梅尔·汉森在苹果卡上收到了“比他妻子高 20 倍”的信用额度，他将此归咎于“性别歧视的黑盒算法”。(克劳福德，2019 年)

即使是图像识别软件也无法通过通用镜头看世界。图像识别系统继续对黑人面孔进行错误分类——尤其是黑人女性面孔。谷歌照片臭名昭著的标签错误将黑人贴上了大猩猩的标签。同时，“判决算法歧视黑人被告，聊天机器人在接受在线话语训练时很容易采用种族主义和厌恶女性的语言，优步的面部识别对跨性别司机不起作用。”(韦斯特，2019 年)

2018 年，路透社报道了亚马逊一项歧视女性的实验性招聘工具；降级任何包含“女性”一词的简历或所有女子大学的候选人的简历。在尝试修正该工具失败后，它最终被亚马逊的软件工程师放弃了。看起来“性别歧视在系统中根深蒂固——在亚马逊过去的招聘实践中也是如此——无法用纯粹的技术手段根除。”(韦斯特，2019 年)

# **走向人工智能公平的步骤**

那么我们如何解决这些问题呢？人工智能开发过程是一个很好的起点。然而，我们也需要考虑批判性推理和软技能在计算机科学课程中的重要性。

![](img/dfbbe7cba03e168ee03028268423ec28.png)

[附身摄影](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

**AI 开发流程:**

在软件开发层面，需要考虑整个算法过程的偏差。偏差可以在几个点引入人工智能系统:建模之前、期间和之后:

1.  **建模偏差。**

开发人员需要仔细调查在初始建模过程中所做的任何假设。应仔细检查所有一般假设，以确保它们包含有限的偏差。

2.**训练中的偏差。**

在测试数据的训练过程中很容易引入偏差。用于训练和测试的数据可能包含有偏见的人类决策，或者反映反映当时和当地社会权力结构的社会不平等。例如，“在新闻文章上训练的单词嵌入(一套自然语言处理技术)可能会表现出社会中存在的性别刻板印象。”(西尔伯和马尼伊卡，2019 年)。测试数据应该被清理，以确保模型接收到无偏见的输入。

3.**用法偏差。**

开发人员还需要考虑他们的模型在特定业务环境或任务中的决策效力。该模型的使用情况如何？会和人类决策结合使用吗？它会完全取代人类决策吗？在人工智能系统开发的这一点上引入偏差被认为是“使用偏差”。(西尔伯和马尼伊卡，2019 年)

# **计算机科学教育者的角色**

![](img/0c0e5f567e8730c1464f9560d884edd7.png)

[瓦西里·科洛达](https://unsplash.com/@napr0tiv?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

教育者在塑造未来程序员的实践中扮演着重要的角色。哈佛大学已经认识到伦理推理的紧迫性，认为它是当今计算机科学家工具箱中的一项关键技能。为了应对对有道德意识的科学家的迫切需求，它开发了一个名为嵌入式伦理的课程，该课程教导学生识别、推理、沟通和设计道德和社会责任体系。(Embedded EthiCSTM @ Harvard:，2020)课程模块嵌入到整个计算机科学课程中，涵盖人工智能、道德黑客和嵌入式偏见等主题。

课程校友不仅了解如何构建人工智能系统，还知道他们构建的系统的潜在风险和后果。他们可以平衡性能和正确性之间的潜在权衡，并可以在业务环境中交流这种推理，以在现实生活中的系统构建过程中影响决策。他们被教导去建立系统，这些系统不仅在算法上是最优的，而且从公平和道德的角度来看也是最优的。

# **有偏见的人类还是有偏见的计算机？**

到目前为止，这些方法通过批判性地评估技术来解决人工智能偏见。然而，这是假设科技是罪魁祸首。相反，人类必须承认自己决策中的根本缺陷和偏见，人工智能已经帮助暴露了这些缺陷和偏见。

AI 算法经常使用历史数据来预测未来事件。因此，我们能否分析有偏差的人工智能输出，以提高人类系统和人类决策的公平性？这样做可以提高偏见和公平的社会标准。

> 因此，我们能否分析有偏差的人工智能输出，以提高人类系统和人类决策的公平性？

目前，社会接受从被认为“公平”的过程中产生的结果(如评价标准)。合成公平，一种人类用来减少决策系统中的偏见的代理，就是这样一个例子。这个代理假设，如果一个群体在做决策时包含了不同的观点，那么这个群体的决策必须是公平的。(西尔伯和马尼伊卡，2019 年)。但是程序公平和结果公平真的是一回事吗？我们是否也应该让人类承担更多的责任？

# **结论**

如上所述，有几种方法可以解决人工智能中偏见的潜在有害后果。从开发的角度来看，对人工智能模型进行批判性评估是一个很好的起点。开发人员需要意识到在人工智能开发过程中可能引入偏见的临界点(建模、培训或使用偏见的环境)。他们还需要坚实的伦理基础和批判性推理，以质疑和理解其系统设计的内在现实风险。衡量性能与正确性的利弊的能力是另一项基本技能，在真实的业务场景中沟通和影响道德决策的能力也是如此。教育工作者需要考虑这些技能差距，并相应地调整他们的课程。

在根据人类决策训练的算法被证明有偏差的情况下，我们可以利用这种洞察力来纠正我们自己的决策。我们可以对数据提出质疑，对有偏见的潜在人类行为得出结论，并实施政策以确保未来的决策更少歧视。

如果我们利用这种潜力，人工智能可以彻底改变我们人类的平等和公平标准。

**来源:**

*人工智能和机器学习:伦理、治理和合规资源。(2020 年 12 月 1 日)。检索自隐私未来论坛:https://FPF . org/artificial-intelligence-and machine-learning-ethics-governance-and-compliance-resources/Crawford，K. R. (2019)。*

*AI Now 2019 报告。纽约:NYU。检索自*[*https://ainowinstitute.org/AI_Now_2019_Report.pdf*](https://ainowinstitute.org/AI_Now_2019_Report.pdf)

*嵌入式 EthiCSTM @ Harvard:。(2020 年 12 月 3 日)。检索自*[*【https://embeddedethics.seas.harvard.edu/:】*](https://embeddedethics.seas.harvard.edu/:)*[【https://embeddedethics.seas.harvard.edu/module.html】](https://embeddedethics.seas.harvard.edu/module.html)*

*人工智能的 9 大伦理问题。检索自世界经济论坛:https://www . weforum . org/agenda/2016/10/top-10-ethical-issues-in-artificial intelligence/Silberh，j .，& Manyika，J. (2019，6 月)。*

**人工智能前沿笔记。解决偏见和人类。检索自*[https://www . McKinsey . com/~/media/McKinsey/featured % 20 insights/artificial % 20 intelligenc](https://www.mckinsey.com/~/media/mckinsey/featured%20insights/artificial%20intelligenc)*e/攻坚% 20 bias % 20 in % 20 artificial % 20 intelligence % 20 and % 20 in % 20 humans/mgi-tackling bias-in-ai-June-2019 . pdf 6a 4347 a 50249 C4 B9 West，S. W. (2019)。**

**歧视系统:人工智能中的性别、种族和权力。纽约:艾现在研究所。泽维尔，f .，范努宁，t .，&这样，J. M. (2020)。人工智能中的偏见和歧视。eprint arXiv:2008.07309，1。**