<html>
<head>
<title>Fast Oriented Text Spotting with a Unified Network (FOTS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用统一网络快速定位文本(FOTS)</h1>
<blockquote>原文：<a href="https://medium.com/codex/fast-oriented-text-spotting-with-a-unified-network-fots-ac5626c81f33?source=collection_archive---------5-----------------------#2021-06-28">https://medium.com/codex/fast-oriented-text-spotting-with-a-unified-network-fots-ac5626c81f33?source=collection_archive---------5-----------------------#2021-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/9067a19fd8e41213e4f428a128693494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*aC8Z_dxp0Xw357dZQPqviw.png"/></div></figure><p id="2708" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于在文档扫描、机器人导航和图像检索等领域的实际应用，从图像中检测和识别文本(也称为文本定位)是深度学习研究人员多年来一直在研究的一个非常有用和具有挑战性的问题。迄今为止，几乎所有的方法都包括两个独立的阶段:1)文本检测2)文本识别。文本检测只是找出文本在给定图像中的位置，根据这些结果，文本识别实际上从文本中识别字符。由于这两个阶段，需要训练两个独立的模型，因此预测时间有点长。由于测试时间较长，这些模型不适合实时应用。与此相反，FOTS通过同时检测和识别文本，使用统一的端到端可训练模型/网络来解决这个两阶段问题。它在文本检测和识别任务之间使用共享的卷积特征，学习更多的通用特征并改进测试时间，使得它可以在实时应用中有用，例如从较高FPS的视频流进行OCR。FOTS还改进了对具有对齐/旋转文本的场景的文本检测，因为它有一个名为“RoIRotate”(感兴趣区域旋转)的特殊组件，该组件通过保持纵横比不变来旋转对齐的文本，然后应用文本识别。</p><h1 id="27b0" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">内容</h1><ol class=""><li id="665d" class="ki kj hi io b ip kk it kl ix km jb kn jf ko jj kp kq kr ks bi translated">商业问题</li><li id="d95f" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">ML问题公式化</li><li id="0f5a" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">用于问题的损失</li><li id="fa43" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">数据来源和概述</li><li id="3bc9" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">电子设计自动化(Electronic Design Automation)</li><li id="684f" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">数据生成</li><li id="3e08" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">模型</li><li id="bda3" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">模特培训</li><li id="89ae" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">部署</li><li id="8d18" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">结论</li><li id="a9ba" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">未来的工作</li><li id="3f05" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">参考</li></ol><h1 id="2658" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">1.商业问题</h1><p id="5c43" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">在计算机视觉领域，阅读自然图像中的文本已经引起了越来越多的关注。由于这个问题是计算机视觉和自然语言处理的结合，它在文档分析、场景理解、机器人导航、图像检索、自动驾驶汽车等方面有着广泛的实际应用。。这也是最具挑战性的任务之一，因为现实生活中的图像文本有不同的字体、大小/比例和对齐方式。此外，它必须在有限的时间限制。这个问题试图在几秒钟内找到文本。</p><h1 id="39db" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">2.ML问题公式化</h1><p id="0d52" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">从自然图像中提取文本的问题可以表述为两个阶段的过程:1)文本检测/定位2)文本识别。文本检测可以进一步公式化为边界框回归和文本的每像素分类(无论该像素是否是文本的一部分)。FOTS结合了这两个阶段，并允许训练一个端到端的模型，以最小的损失进行准确的文本检测和识别。</p><h1 id="72d6" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">3.用于问题的损失</h1><p id="d2d8" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">文本检测有两个损失:骰子损失和IoU损失</p><h2 id="a4e9" class="lb jl hi bd jm lc ld le jq lf lg lh ju ix li lj jy jb lk ll kc jf lm ln kg lo bi translated">骰子损失</h2><p id="af31" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">了解清晰边界检测的骰子损失。为什么骰子损失是比交叉熵损失好得多的选择？当使用交叉熵损失时，标签的统计分布在训练准确性中起着很大的作用。标签分布越不平衡，训练就越困难。虽然加权交叉熵损失可以缓解这一困难，但改善并不显著，也没有解决交叉熵损失的本质问题。在交叉熵损失中，损失被计算为每像素损失的平均值，并且每像素损失被谨慎地计算，而不知道其相邻像素是否是边界。因此，交叉熵损失只考虑微观意义上的损失，而不是全局考虑，这对于图像级预测是不够的。现在骰子损失考虑像素是否是边界。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/1e1f92e522862d5683d14bde40b4a3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*B0VtjJN8I_fh-N61V-5Swg.png"/></div></figure><p id="05ed" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里，pi表示预测边界框，g_i是地面实况。pi和g_i的值将是1或0，这意味着像素是否是边界。因此，分母是预测的和地面真实的总边界像素，分子是正确预测的像素的总和，因为它仅在pi和g_i与1匹配时增加。</p><h2 id="ee19" class="lb jl hi bd jm lc ld le jq lf lg lh ju ix li lj jy jb lk ll kc jf lm ln kg lo bi translated">欠条损失</h2><p id="ced6" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">在目标检测任务中，我们试图引导计算机在给定的图像数据中预测目标及其位置。为了实现这一点，我们试图制定一种机制，通过用“矩形”符号(通常称为边界框)包围一个对象来模仿“定位”该对象的行为。这些边界框，通常标注为4点值，表示边界框坐标的特定角点/中心点或其宽度/高度。注释的常用格式是(左、上、右、下)、(左、上、宽、高)或(center_x、center_y、宽、高)。因此，用于对象检测任务的深度学习被专门设计成通过对定位部分进行回归来预测与这些点相关的值。早期基于Ln范数的损失被用作成本函数。因此，Rezatofighi等人在2019年提出了第一种基于IoU的包围盒回归损失方法。公式是:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/bb59f5bfeeeee651d6850bfc7dfa3383.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*dSR3OmrmjqG1LZ1CSaTZPA.png"/></div></figure><h2 id="d0df" class="lb jl hi bd jm lc ld le jq lf lg lh ju ix li lj jy jb lk ll kc jf lm ln kg lo bi translated">文本识别丢失— CTC丢失</h2><p id="b6b3" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">CTC损失</p><p id="1cc4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">CTC丢失:CTC代表连接主义时间分类。模型如何在输入和输出之间对齐以定位图像中的每个字符并将它们转换成文本？这就是反恐委员会发挥作用的地方。CTC cost函数允许RNN生成如下输出:CTC在字符之间引入空白标记来分隔它们，它这样做是为了分隔单个字符，以便没有空白标记的重复字符将折叠成一个字符。为了计算CTC损失，在每个时间戳，网络输出可能标签值的概率分布。如果我们独立地随机选择任何一个标签，这将给出一个坍塌成地面真理的输出序列的可能性有多大。CTC损失是这个概率的负对数</p><h1 id="251c" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">4.数据来源和概述</h1><p id="47a8" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">为了训练端到端FOTS模型，如原始论文所建议的，应使用以下数据集:</p><p id="7628" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lv" href="https://www.robots.ox.ac.uk/~vgg/data/scenetext/" rel="noopener ugc nofollow" target="_blank"> SynthText数据集:</a>这是一个合成生成的数据集，其中word实例被放置在自然场景图像中，同时考虑到场景布局。这是一个非常大的数据集，包含80万张不同文本的图片。</p><p id="44d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lv" href="https://iapr.org/archives/icdar2015/index.html?p=254.html" rel="noopener ugc nofollow" target="_blank"> ICDAR-2015 </a>数据集:这是真实世界的数据集，包含来自可穿戴相机的图像。与SynthText数据集相比，该数据集相对较小(只有1000个训练图像)。</p><p id="fb9c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于SynthText数据集足够大，该论文建议在其上训练整个模型，然后适应真实世界的图像，该模型可以在ICDAR-2015数据集上进行微调。</p><h1 id="3555" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">5.探索性数据分析</h1><p id="e919" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated"><em class="lw">让我们先来看看SynthText数据集:</em></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es lx"><img src="../Images/aa0c8e8270b14886edb08265e9b1f12c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q2lT8WlXzOB3gyL1Cm58lA.png"/></div></div></figure><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mc"><img src="../Images/4f3eee5cacce77b0f1de93111e4bbb9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vjj-y0a7atWjKuXPgQq9_Q.png"/></div></div></figure><p id="a06b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="c3ae" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">随机文本写在图像的随机位置。</li></ol><p id="ec5a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.每个图像的像素大小都不同。</p><p id="1652" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">先说图像的宽度</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/541f7bc67b1ac7fcef981b247adf39ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*kM81auswgy_48kppmdw2CQ.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">宽度直方图</figcaption></figure><p id="7f59" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="8bc9" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数图像的宽度在300到500之间。</li><li id="7ffb" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">很少有图像的宽度达到或超过600。</li><li id="ee83" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">宽度小于315的图像大约不到20%。</li></ol><p id="afc4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">现在身高特征</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/4f8257950222b3fcbec0703e9c6333fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*4FeNRdMlI1q13NeGPAAixg.png"/></div></figure><p id="42f6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="7584" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数图像的高度为600或更高。</li><li id="ede8" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">很少有身高低于400的图片。</li></ol><p id="3d06" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">现尺寸特征:</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/1fc340c53df02df63cd1c2054a3ab311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*RUVZ9gcSJZAnXYRFDzwlAg.png"/></div></figure><p id="8013" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="2cb0" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数图像的大小在20到80之间。</li><li id="ad90" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">所有图像的平均大小是40字节</li><li id="3725" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">很少有图像的尺寸大于80。</li></ol><p id="619b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">可视化图像上的边界框</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mn"><img src="../Images/b7f1d6aebad779fafd19de7ebcef2311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8j_8pMc7jQBevEvf6vkbg.png"/></div></div></figure><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mo"><img src="../Images/411e754b795f6245b0da8d4f6f881fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1tyZ1YXtIs7wozmxIhSXA.png"/></div></div></figure><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mp"><img src="../Images/55bc39a89f0b51b9164bf9125f2c44c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gB0nthZSBs9y-dp5W8DjwA.png"/></div></div></figure><p id="6e8d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">包围盒上的EDA</em></strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/7a5656e48c3375d6796a2f22df72c1cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*cbFAUlMPDLa9-o61y1DGLg.png"/></div></figure><p id="1c56" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="5f2e" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数图像的边界框数量少于15个。</li><li id="0c93" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">大约95%的图像具有不到20个边界框。</li><li id="0642" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">每个图像的边界框的平均数量大约是9。</li></ol><p id="f233" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">边框宽度:</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/3999aa17260c467fb46c79cb39f0c795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*pZFCPEFN0jD8aHLakaFxDg.png"/></div></figure><p id="a7ce" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="8d8a" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数边界框的宽度在50到200之间。</li><li id="2748" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">大约90%的图像的边界框宽度小于180。</li><li id="d84c" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">边界框的平均宽度是大约。80</li></ol><p id="f0cc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">边框高度:</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/bb7ceb9efbf325db1e50369145488a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*-4oYEs0sKXshQ1_6ngKmlw.png"/></div></figure><p id="6a92" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="b85f" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数边界框的高度在20到50之间。</li><li id="7fca" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">高度大于50的边界框很少。</li><li id="467a" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">边界框的平均宽度是大约。30</li></ol><p id="394f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">包围盒大小:</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/80b429c9c1510741476e2ee1e826989e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*V24aoFEg0Fz0mp0bTlxG9g.png"/></div></figure><p id="dd88" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="9f87" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数边界框的面积小于20000。</li><li id="a40a" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">很少有边界框的面积大于20000。</li><li id="3586" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">边界框的平均面积约为。5000</li></ol><p id="1092" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="lw">IC Dar 15数据集上的EDA</em></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mu"><img src="../Images/eae23122d6875eb05075eb619689822d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVEN_MEyE6SO9SG5D1YZvQ.png"/></div></div></figure><p id="c6fa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><p id="33dd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该icdar数据集包含宽度为720、高度为1280的所有图像。这些也是真实世界的图像，与SynthText数据集相比，背景有些模糊，不太清晰。</p><p id="1562" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">用边界框可视化图像</strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mv"><img src="../Images/45bb16b099991c8f6bfd72ac98b14fc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5tV-bNzI1Gr4fkUkko2NA.png"/></div></div></figure><p id="3c05" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">每幅图像的包围盒数量:</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/6252717b3cb013d09ce44e7ff69b1200.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*eUoagm6EWqlxgW-6XpZ2GQ.png"/></div></figure><p id="3ca0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="e4f5" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">每幅图像的边界框大多在1到25之间。</li><li id="2f2d" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">大约90%的焊接盒低于30。</li><li id="980d" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">每幅图像的平均边界框数量约为..8.</li></ol><p id="9d88" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">边框宽度</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/cf811f40fc993902c019bb6fee845c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*ylmabmCGNB-cIjFkMpcp2g.png"/></div></figure><p id="e692" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="251a" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数边界框的宽度在30到100之间。</li><li id="1411" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">大约95%的焊接盒低于150。</li><li id="e593" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">所有图像的平均宽度约为。60</li></ol><p id="d1c7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">边框高度:</strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es my"><img src="../Images/a187bbb1abd241f48745af4acc325d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*xieoQ5l7SDGYnsCzvjLwew.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">Bbox的高度</figcaption></figure><p id="1de4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="4891" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数边界框的高度在10到60之间。</li><li id="e8e4" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">很少有宽度超过60的。</li><li id="998c" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">所有边界框的平均高度约为。40</li><li id="b541" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">大多数边界框的高度几乎相同。</li></ol><p id="b2a3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lw">边框尺寸:</em> </strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/737abe8499ce1841f7c541fa50f83d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*E5nzhtAWPBRqyC4aXxYd9Q.png"/></div></figure><p id="5cad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">观察:</strong></p><ol class=""><li id="f0d1" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">大多数边界框的大小为5000。</li><li id="221e" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">很少有宽度超过15000的。</li><li id="1c7f" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">几乎所有的边界框都有相同的大小。</li></ol><h1 id="a0ae" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">6.数据预处理/地面实况生成</h1><p id="71d4" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated"><strong class="io hj">用于文本检测的数据发生器</strong></p><p id="fa7f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了训练FOTS模型的文本检测组件，需要为每个基本事实图像生成以下基本事实遮罩/图像。</p><ol class=""><li id="520a" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated">分数图:这是一个图像通道，表示给定图像中每个像素的像素是文本的一部分还是背景的一部分。以下是地面实况得分图和相应图像的示例:</li><li id="e137" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">地理图:地理图包含5个遮罩/通道:对于作为文本一部分的每个像素，前4个通道预测其到包含该像素的边界框的顶部、底部、左侧、右侧的距离，最后一个通道预测相应边界框的方向。以下是这5个地理地图通道的可视化，以及相应的地面实况:</li><li id="17b2" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">训练遮罩:这是单通道图像，用于忽略非常小的边界框和没有来自训练和损失计算过程的抄本的边界框。</li></ol><p id="5c05" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，在训练模型之前，必须生成以下基本事实:1)得分图，2)地理图，3)训练掩码，4)文本抄本，5)边界框列表</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="na nb l"/></div></figure><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es nc"><img src="../Images/c40ca7a3ac65841b8b2b42725345be35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*G808tlO4kmUICpn9VP8Qjw.png"/></div></div></figure><p id="0601" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">数据生成器FOT文本识别</strong></p><p id="7795" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在文本识别模型中，我们创建了两个生成器(训练和测试)。我们已经将图像转换为大小(64，128，3)，并将特定单词图像的单词转换为大小为23的矢量。在这里，我们为这些向量创建了词汇表，其中包含了构成一个单词的所有可能的字符。</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="na nb l"/></div></figure><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="na nb l"/></div></figure><h1 id="1674" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">7.建模</h1><h2 id="f66a" class="lb jl hi bd jm lc ld le jq lf lg lh ju ix li lj jy jb lk ll kc jf lm ln kg lo bi translated">FOTS的整体建筑</h2><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es nd"><img src="../Images/3bbfad6676732256b0503fd60256dd4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yn2uflvZOofRbUsVYxr83g.png"/></div></div></figure><p id="9942" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="lw">共享卷积:</em></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es ne"><img src="../Images/934697f63210fcb48accc7d1ba4c0579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3iK_f_YSx-UG4Y_hqrwuA.png"/></div></div></figure><p id="b4b1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了从图像中提取高级特征，使用共享卷积，这些卷积在图像网上预先训练，并使用<a class="ae lv" href="https://iq.opengenus.org/resnet50-architecture/#:~:text=ResNet50%20is%20a%20variant%20of,explored%20ResNet50%20architecture%20in%20depth." rel="noopener ugc nofollow" target="_blank"> Resnet50 </a>作为其主干。共享卷积是其中共享相同权重的卷积层。上图显示了共享卷积的架构，其中第一个橙色块包含一个卷积层，随后是max pool层，最后是resnet50层。其他橙色块包含唯一的resnet50层架构。浅绿色块是相应块的输出要素。蓝色块是解卷积块，对相应的输出特征执行解卷积。由于自然场景图像中有许多小文本框，我们在共享卷积中将特征图从原始输入图像的1/32放大(解卷积)到1/4。即它对输入特征进行上采样并增加图像或特征的大小。整个体系结构中使用的激活函数是ReLu。另请注意，低级功能与高级功能地图直接相连，在上图中显示为黑色箭头。</p><p id="eae7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="lw">文本检测分支:</em></p><p id="212c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于文本检测分支，采用了高效且<a class="ae lv" href="https://arxiv.org/abs/1704.03155" rel="noopener ugc nofollow" target="_blank">准确的场景文本检测器(EAST ) </a>研究论文中的思想，其中文本检测分支使用全卷积网络作为文本检测器。这些卷积层具有用于分数图和地理图5个通道。一旦边界框由文本检测器分支提出，位置感知NMS(非最大抑制)将被用于获得具有高于地面真实边界框的最高IoU的边界框。</p><p id="446b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面是实现检测层的代码片段。</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="na nb l"/></div></figure><p id="1014" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">检测支路损耗代码:</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="na nb l"/></div></figure><p id="0802" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="lw"> RoIRotate(感兴趣区域旋转):</em></p><p id="c2ae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">RoIRotate对定向/对齐的特征区域应用变换，以获得轴对齐的特征图。ROI使用双线性插值来计算输出值。该操作避免了RoI和提取的特征之间的不对准，此外，它使得输出特征的长度可变，这更适合于文本识别。这个过程可以分为两步。首先，仿射变换参数通过文本提议的预测或地面真实坐标来计算。然后，对每个区域的共享特征图分别进行仿射变换，得到文本区域的标准水平特征图。</p><p id="6452" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是RoIRotate过程的直观视图</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es nf"><img src="../Images/57fc91272a98c0359a95712fd17a50f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JrxbO-BkfZJL_sT8.png"/></div></div></figure><p id="104d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">注意，上图只是为了形象化。RoIRotate的实际实现对通过共享卷积而不是原始图像提取的特征图进行操作。</p><p id="5c6b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="lw">文本识别分支:</em></p><p id="a050" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">文本识别分支旨在使用由共享卷积提取并由RoIRotate变换的区域特征来预测文本标签。考虑到文本区域中标签序列的长度，LSTM的输入特征仅通过原始图像沿宽度轴的共享卷积而减少两倍。否则，紧凑文本区域中的可辨别特征，尤其是窄字符的特征将被消除。我们的文本识别分支包括一个类似VGG的顺序卷积，一个只沿高度轴递减的集合，一个双向LSTM，一个完全连接的最终CTC解码器。这部分主要类似于CRNN，结构如下图所示</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/bbb5d8d056c76a12a61903670d6abe4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*X47KPrZgbJMdUgxt.jpeg"/></div></figure><p id="4616" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面是识别分支的代码:</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="na nb l"/></div></figure><p id="ff55" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">CTC损失代码:</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="na nb l"/></div></figure><p id="9da9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上面的FOTS架构工作如下，第一图像被馈送到共享卷积中，从共享卷积中提取共享特征。这些共享特征被馈送到文本检测分支，在那里我们预测和检测文本到图像中的边界框。共享卷积(共享特征)和文本检测分支(图像中文本的预测框)的输出被馈送到RoI旋转算子，该算子提取文本提议特征。这些特征然后被馈送到文本识别分支，该文本识别分支由递归神经网络(RNN)编码器和连接主义时间分类(CTC)解码器组成，用于识别和预测文本。最后，文本检测分支和文本识别分支的输出被合并到图像中，以预测边界框和边界框的预测文本。因为在这个网络中用于模型的所有looses都是可微分的，所以整个网络可以被端到端地训练。</p><p id="edba" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们已经了解了FOTS，让我们问一下为什么使用FOTS来实现文本定位？</p><p id="9221" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先，与其他方法相比，FOTS速度更快，因为FOTS同时实现了检测和识别。此外，文本识别监督迫使模型考虑字符的细节，FOTS学习一个单词中具有不同模式的不同字符之间的语义信息。它还增强了具有相似模式的字符和背景之间的差异。</p><h1 id="a057" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">8.模特培训</h1><p id="5baf" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">由于SynthText数据集非常大，即800k图像(41GB ),由于计算能力较低，在所有数据上训练我们的模型是不可能的。因此，作为一个解决方案，我们从该数据集中随机选择15k张图像，并使用它们来训练我们的模型。</p><p id="325d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于模型训练，基本上我们将整个训练过程分为两个部分:检测和识别。我们将分别训练这两个模型，即检测和识别，在推断结束时，我们将把这两个模型与ROI旋转结合起来。</p><p id="f7e2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在培训过程中，我们使用了各种Tensorflow回调，例如:</p><ol class=""><li id="9076" class="ki kj hi io b ip iq it iu ix md jb me jf mf jj kp kq kr ks bi translated"><strong class="io hj">减少平台期回调</strong> :-当我们的模型权重陷入局部最小值时，这用于减少学习率。</li><li id="2e56" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated"><strong class="io hj">模型检查指向回调</strong> :-用于训练时保存模型重量。</li><li id="f5e3" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated"><strong class="io hj">张量板回调</strong> :-用于可视化张量板中层的损失和权重。</li></ol><p id="79d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">以下是该模型的一些预测:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/29a660a7844c4e9b58c20a66e5b5a564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*yvK6dUj9QDzsEYyp_2-vcQ.jpeg"/></div></figure><h1 id="cbbd" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">9.部署</h1><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="ni nb l"/></div></figure><h1 id="e646" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak"> 10。结论:</strong></h1><ol class=""><li id="4510" class="ki kj hi io b ip kk it kl ix km jb kn jf ko jj kp kq kr ks bi translated">我们可以看到，检测模型仍然做得很好，但识别模型的表现不是那么好。</li><li id="c408" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">这是因为我们只从SynthText的800k图像中提取了15k图像用于检测，10k图像用于识别，我们没有在SynthText的全部数据上训练我们的模型，因为它是41GB大的，需要高计算能力，并且因为较少的计算能力和资源使用那么多数据。</li><li id="01c0" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">检测和识别模型都可以在更多的数据上训练，以提供更好的结果。</li></ol><h1 id="fece" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">11.未来工作:</h1><ol class=""><li id="2a35" class="ki kj hi io b ip kk it kl ix km jb kn jf ko jj kp kq kr ks bi translated">将利用完整的SynthText数据来训练检测和识别部分。</li><li id="358c" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">处理检测部分以产生更精确的边界框。</li><li id="4557" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">以更易读的形式组织代码。</li><li id="1b9d" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated">可以使用NLP方法层在识别层正确解码单词。</li></ol><h1 id="f856" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">12:参考文献:</h1><p id="8ad6" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated"><a class="ae lv" href="https://arxiv.org/pdf/1801.01671.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1801.01671.pdf</a></p><div class="nj nk ez fb nl nm"><a href="https://github.com/jiangxiluning/FOTS.PyTorch" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">江西路宁/FOTS。PyTorch</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">conda create-name fots-file spec-file . txt conda activate fots pip install-r reqs . txt #非常简单，对于单个gpu…</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">github.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ik nm"/></div></div></a></div><div class="nj nk ez fb nl nm"><a rel="noopener follow" target="_blank" href="/ai-salon/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">了解清晰边界检测的骰子损失</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">计算机视觉中边界检测任务的交叉熵损失的更好替代方案</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">medium.com</p></div></div><div class="nv l"><div class="ob l nx ny nz nv oa ik nm"/></div></div></a></div><div class="nj nk ez fb nl nm"><a href="https://www.appliedaicourse.com/" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab dw"><div class="no ab np cl cj nq"><h2 class="bd hj fi z dy nr ea eb ns ed ef hh bi translated">应用课程</h2><div class="nt l"><h3 class="bd b fi z dy nr ea eb ns ed ef dx translated">我们知道转行是多么具有挑战性。我们的应用人工智能/机器学习课程被设计为整体学习…</h3></div><div class="nu l"><p class="bd b fp z dy nr ea eb ns ed ef dx translated">www.appliedaicourse.com</p></div></div><div class="nv l"><div class="oc l nx ny nz nv oa ik nm"/></div></div></a></div><p id="2cd6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你可以在我的Github <a class="ae lv" href="https://github.com/iaayushgupta/FOTS" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">这里</strong> </a> <strong class="io hj">找到完整的代码。</strong></p><p id="f195" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">很高兴在<a class="ae lv" href="https://www.linkedin.com/in/aayush-gupta-925442190/" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> linkedin </strong> </a>上与您连线😃</p></div></div>    
</body>
</html>