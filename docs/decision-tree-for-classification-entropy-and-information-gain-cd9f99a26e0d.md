# 分类、熵和信息增益的决策树

> 原文：<https://medium.com/codex/decision-tree-for-classification-entropy-and-information-gain-cd9f99a26e0d?source=collection_archive---------1----------------------->

![](img/32cac769e73b20c063aaf0487a9e34ad.png)

决策树学习是一种预测建模方法。它用于解决统计学、数据挖掘和机器学习中的分类问题。它有一个倒置的树状结构，代表决策或用于决策。它可以处理高维数据，具有很好的准确性。决策树算法在医疗生产、制造业、金融分析等领域有着广泛的应用。例如，决策树可以根据房间的大小和数量来预测房子的价格，或者根据身高和体重来预测一个人是男是女，等等…

决策树可以表示如下。最顶端的节点称为根节点，它没有传入的边。内部节点代表一个测试或一个属性，每个分支代表一个测试的结果，每个终端节点或叶子持有一个类。它有一条输入边和两条或多条输出边。终端节点或叶节点代表一个类节点，并且只有一个传入节点，没有传出节点。

![](img/01cc923b90d0ba6e41bd791ae6665074.png)

决策树中的每个节点都是特征的一个条件。旨在将数据集分割成相似的响应值，但最终会出现在同一个数据集中。

# CART 分类和回归树

树类比一般用 CART 表示，称为分类和回归树。CART 易于理解、解释和可视化，并且不需要太多的数据准备工作。此外，它还执行特征选择。回归树主要用于目标变量为数字的情况。这里，由终端节点获得的值总是落在该区域中的响应的平均值。因此，如果有任何看不见的数据或观察值将会用平均值来预测。当目标变量是分类变量时，使用分类。这里，由终端节点获得的值是落在该区域中的响应模式，并且该区域中的任何看不见的数据或观察将基于该模式值进行预测。

尽管 CART 很简单并且有很大的优势，但是如果数据处理不当，它会导致过度拟合。此外，如果数据中有很小的变化，就会导致不稳定。

在下面种植一棵树时，需要考虑以下几点:

1.  可供选择的功能
2.  拆分的条件
3.  知道在哪里停下来
4.  修剪

进行战略分割的决策会严重影响树的准确性，回归树和分类树的决策标准也会不同。熵/信息增益或基尼指数可用于选择最佳分割。熵和信息增益是密切相关的。

对于具有不同特征的给定数据集，为了决定哪个特征被认为是根节点，哪个特征应该是下一个决定节点等等，应该知道每个特征的信息增益。具有最大信息增益的特征将被认为是根节点。为了计算信息增益，我们首先应该计算熵。

# 熵:

熵是给定数据集中无序或不纯的度量。

在决策树中，基于与每个数据点相关联的特征向量的值来分割杂乱数据。随着每次拆分，数据变得更加同质，这将降低熵。然而，一些节点中的一些数据不会是同质的，其中熵值不会小。熵越高，就越难得出任何结论。当树最终到达终端或叶节点时，添加最大纯度。

对于有 C 个类和从类中随机选择数据的概率的数据集，I 是 Pi。那么熵 E(S)在数学上可以表示为

![](img/8a089e0192bbd6830e77a3baf73dacff.png)

如果我们有一个数据集，其中 10 个观察值属于两个类“是”和“否”。如果 6 个观察值属于“是”类，4 个观察值属于“否”类，那么熵可以写成如下形式。

![](img/f295598b1d82d0ce8fe113fb51884a12.png)

*Pyes* 是选择 yes 的概率， *Pno* 是选择 no 的概率，这里 *Pyes* 是 6/10， *Pno* 是 4/10。

![](img/02b4501faf9bc618e09dc66180e9038a.png)

如果所有 10 个观察值都属于一个类别，那么熵将等于零。这意味着该节点是纯节点。

![](img/dcc3203d8825b67423323ce461b57abd.png)

如果“是”和“否”两类都有相同数量的观察值，那么熵将等于 1。

![](img/4e7c18796d06f8acaef9a20f981fa0ed.png)

# 信息增益

信息增益衡量熵的预期减少。熵测量数据中的杂质，信息增益测量数据中杂质的减少。具有最小杂质的特征将被认为是根节点。

信息增益用于决定在构建树的每一步中分割哪个特征。子节点的创建增加了同质性，即减少了这些节点的熵。子节点越相似，每次分割后方差减少得越多。因此，信息增益是方差减少，并可以通过每次分割后方差减少多少来计算。

父节点的信息增益可以被计算为父节点的熵减去子节点的加权平均值的熵。

根据上述示例，数据集有 10 个属于两个类“是”和“否”的观察值，其中 6 个观察值属于类“是”，4 个观察值属于类“否”

![](img/b720a9598a2f5ed1f4da223404b09f09.png)

红色有 3 个是结果和 3 个否结果，而黄色有 3 个是结果和 1 个否结果。

E(S)，我们已经计算过了，它大约等于 0.971

![](img/d16dfcf6b6067252ea6f877b3eff4d07.png)![](img/2d3240777c18edebc91c2331e8847c3c.png)![](img/8c3ed61ec37fb36d3cc050a5e8754598.png)![](img/2aa5a1653f58f68e82c6ae24b1e7d5b8.png)

对于具有许多特征的数据集，计算每个特征的信息增益。具有最大信息增益的特征将是最重要的特征，其将是决策树的根节点。

# 基尼指数

基尼指数也可以用于特征选择。该树选择最小化基尼系数的特征。基尼系数越高，表明杂质越多。基尼系数和基尼系数可以互换使用。基尼指数或基尼系数有利于大分区，并且实现起来非常简单。它只执行二进制分割。对于分类变量，它以“成功”或“失败”的形式给出结果。

基尼指数可以通过下面的数学公式计算，其中 c 是类的数量，pi 是与第 I 个类相关的概率。

![](img/cfc305b0ea25fcf8aca9ba038cce0014.png)

基尼指数或基尼系数有利于大分区，并且实现起来非常简单。它只执行二进制分割。对于分类变量，它以“成功”或“失败”的形式给出结果。

# 修剪

当树完全长大时，它喜欢过度拟合数据，因为噪声或离群值会导致决策树中的异常。这反过来导致精确度差。这可以通过修剪来解决。

修剪是删除冗余比较或删除子树的过程。修剪减少了不必要的比较并获得了更好的性能。修剪过的树不那么复杂，更小，也更容易理解。有两种修剪方法，预修剪方法，其中树的分裂或划分在特定节点处停止，而后修剪方法从整个树中移除子树。在节点处修剪子树。这是通过删除一个节点上的分支并用一个叶节点替换它来完成的。

# 用 Python 实现决策树分类器

对于决策树分类算法，特征值需要是分类的。最重要的要素构成了根节点，对于数据集中的每个要素，算法都构成了一个节点。叶节点包含决策树的决策或结果。如果一个节点的熵为零，则称它为纯节点。

这里我们使用一个数据集来分析蘑菇是有毒的还是可食用的。对于数据分析，需要导入库和数据集。

![](img/c538560d427c09830bb3d842b2c8d034.png)![](img/3712b286420f1810ef6e0781108e8235.png)

要查看数据集的前五行，我们可以使用 dataset.head()，要知道数据集中的行数和列数，可以使用 dataset.shape。

![](img/4da78978c2cc2732735eddc61574d0ce.png)![](img/c6d3415fc01cc8086b5be781aac67a0b.png)

可以使用 **dataset.info()** 检查数据集的信息，如列名、非空计数和空计数的编号(如果有)以及数据类型。

![](img/fd6d41f5d078f123d7ac47312d440e95.png)

Dtype 对象指示所有值都是分类的。可以使用 pandas 虚拟变量对特征进行编码，使用 LabelEncoder 对目标进行编码。

Dummy 变量为列的每个唯一值创建一个单独的列，而 LabelEncoder 用 0 和 n_classes-1 之间的值对目标标签进行编码。LabelEncoder 应该用于编码目标值，即 y，而不是输入(特征)。

在编码之前，特征被分配给 X，目标被分配给 y。

![](img/28ea5a63612046e4081f2491cc951b7c.png)![](img/f9614797781b63a736776b9be84d7112.png)

数据集必须分成训练集和测试集。这可以通过 Scikit-learn 库的 Model_selection 模块中的函数 train_test_split 函数来完成。

![](img/6b51c6baea681df283f14b9d9deeb669.png)

可以使用熵和基尼指数来创建决策树分类器。首先，我们可以使用熵创建一个决策树。

![](img/cbdc1169c869c34fb23190344a2567d4.png)![](img/5cc160f0fea819c7c3950199b3ff1488.png)![](img/11501827f71f42879c158a249ea45c87.png)

过度拟合是决策树的一个主要关注点，如果训练集的准确性高而测试集的准确性低，则可以识别出过度拟合。因此，目标是预测训练和测试数据，并比较它们的准确性，以检查是否存在过拟合。

![](img/4dffae3e1605203ef809ec5b4bf76948.png)

这里，模型和训练集的准确性表明没有过度拟合的迹象。

现在，我们可以使用基尼指数创建一个决策树。

![](img/37edff59159ca8474a637bfc7b71d901.png)![](img/6363a8ab29def5e07698fffd9b86eae0.png)![](img/14dca35886b9c6d0aab42a50e30a6a70.png)![](img/2cde83b77356e1f3cd51b3ca3afd5ee3.png)

这里，模型和训练集的准确性也表明没有过度拟合的迹象。

**感谢阅读！！！！如果这篇文章对你有帮助，欢迎鼓掌，分享和回复。**