<html>
<head>
<title>Scaling Vision Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">缩放视觉变压器</h1>
<blockquote>原文：<a href="https://medium.com/codex/scaling-vision-transformers-ca51034246df?source=collection_archive---------8-----------------------#2021-08-05">https://medium.com/codex/scaling-vision-transformers-ca51034246df?source=collection_archive---------8-----------------------#2021-08-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/dca7af7f3112d1b28b8888a787481195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-MDlB6bBU60dgJ-Ngt4gCQ.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">杰克·威瑞克在<a class="ae iq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="88c5" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">现代深度学习系统相信<em class="jp">规模</em>。具有数十亿甚至数万亿参数的大型神经网络似乎表现得令人惊讶，因此神经网络的缩放特性很重要。在CNN和变压器的有效缩放方法方面有突出的工作。Vision transformer(ViT)是一个完全转换的架构，在图像分类方面表现出与最先进的CNN相当的训练性能。我们应该如何扩展视觉变形金刚？如果我们在训练ViT时缩放数据和模型会发生什么？</p><p id="fc26" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这篇最近(2021年6月)的论文通过对不同数据和模型大小进行实验，研究了缩放视觉变压器的属性。作为结论，本文提出了一个用于视觉变形器的<em class="jp">缩放法则</em>，一个缩放视觉变形器的准则。本文还建议对ViT管道进行架构上的更改。截至8月4日，提议的网络在ImageNet上达到了最先进的水平，最高准确率为90.45%。</p><p id="cc54" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们在<a class="ae iq" rel="noopener" href="/codex/how-do-vision-transformers-work-an-image-is-worth-16x16-words-df47aed1b634">之前的文章</a>中探讨了ViT的概念。这个帖子将是论文的总结:“<a class="ae iq" href="https://arxiv.org/pdf/2106.04560.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jp">缩放视觉变形金刚</em></a><em class="jp">”</em></p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es jq"><img src="../Images/87688a9752a17c5c4a4597d8fca1303d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lqkP0uhbd6iEnNtDgHVwig.png"/></div></div></figure><h2 id="8425" class="jz ka hi bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt bi translated">建筑变化</h2><p id="f26a" class="pw-post-body-paragraph ir is hi it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hb bi translated">本文对ViT框架进行了更改，并对超参数空间做出了如下结论。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/00da117e8c3b0951430046f51677c5af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*rNoguwu8ogGscbur7rNMpg.png"/></div></figure><ul class=""><li id="a0ed" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated">“头部”的解耦权重衰减:本文发现，对于最终的线性层(头部)和主干，少击学习中优选的权重衰减强度是不同的。上图表明，身体的小重量衰减和头部的大重量衰减对模型非常有益。</li><li id="c1b4" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">通过删除[class]标记来节省内存:在原来的256个补丁编码标记中添加一个[class]标记会产生257个标记，在TPU硬件中，从256个标记增加到257个标记会产生50%的内存开销。不使用类标记，而是使用全局平均池或多头注意力池来聚合补丁编码。通过去除最后的非线性投影来简化头部。</li><li id="ca11" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">根据右图，修改似乎不会对性能产生显著影响。</li></ul><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/ff22e5121f43a3d8c349e891c35d8ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*vWFzf6Ypv22TqmUB5HaCHA.png"/></div></figure><ul class=""><li id="84d3" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated">将300米的数据放大到3B影像可以提高小型和大型模型的性能。</li><li id="6446" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">内存高效优化器:因为训练了数十亿个参数，adam优化器所需的存储空间是一个很大的瓶颈(根据论文，需要额外的16GiB)。该论文提出使用具有半精度动量的Adam或修改的Adafactor优化器，这将优化器开销从权重的2倍减少到0.5倍。</li><li id="531c" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">附加训练技术:本文实验了用于提高模型性能的常用训练技术的效果，例如学习率计划。</li></ul><h2 id="c193" class="jz ka hi bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt bi translated">缩放ViT</h2><p id="e252" class="pw-post-body-paragraph ir is hi it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hb bi translated">CNN通常由以下三个因素决定:</p><ul class=""><li id="fffb" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated">宽度:每层的通道数</li><li id="09cb" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">深度:层数</li><li id="4dc0" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">分辨率:输入图像尺寸</li></ul><p id="53fc" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">由于与注意的密切关系，有许多参数控制着ViT的规模。这些包括面片大小、编码器块的数量(深度)、面片嵌入和自我注意的维度(宽度)、注意头的数量和MLP块的隐藏维度(MLP宽度)。</p><p id="780f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">本文使用Tensorflow <a class="ae iq" href="https://www.tensorflow.org/xla" rel="noopener ugc nofollow" target="_blank"> XLA编译器</a>优化运行速度和内存。XLA以最佳方式权衡了内存和速度，并输出了下表中列出的许多模型体系结构配置。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/5297820a2c0e739ca181edc1b3c10a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*uxTrK8IC_ZdyunpR58lzgQ.png"/></div></figure><p id="aae4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">然而，并不是每个网络都能在一台设备上运行。作者执行了下图中描述的模拟，以测量每种配置是否可以实际实现。本文中与记忆相关的修改允许在绿色和蓝色区域中训练模型。</p><p id="328a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">ViT论文包含了一项关于权衡不同成分的有效规则的研究。规则是以相似的量同时缩放所有深度、宽度、MLP宽度和面片大小。最终型号以这种方式选择，如下图所示的对角线模式所示。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es lp"><img src="../Images/679c8753e99e98f5f60547f7d48058bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_qv0DPVKIPN6zAdG4tNh5A.png"/></div></div></figure><h2 id="4f96" class="jz ka hi bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt bi translated">关于扩展ViT的见解</h2><p id="fbac" class="pw-post-body-paragraph ir is hi it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hb bi translated">最重要的是，本文研究了修改网络规模、数据和持续时间的影响模式。在实验中，一个<em class="jp">表示质量</em>度量标准测量了学习特征的有用性。准确地说，它是通过<em class="jp"> (i)通过在冻结重量上训练线性分类器的少量转移，(ii)通过在所有数据上微调整个模型的转移</em>来测量的。该文件建议</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/a532fb584f5d13793c62af135b7378e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*WJsk6kFhTQ8tYPSGCFOMVQ.png"/></div></figure><ul class=""><li id="ff57" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated">上图描绘了作为计算函数的错误率。较大模型的性能似乎在某个程度后饱和(ImageNet中约10%的误差)。</li><li id="99eb" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">当单独针对模型大小(右上图)或数据集大小(右下图)绘制时，最佳训练设置的比例也会增加。</li><li id="fb33" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">将计算、模型和数据一起放大可提高制图表达质量。如左图和中图所示，使用最大模型、datatset和compute的训练在右下角达到最佳性能。</li><li id="c862" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">较小的模型(蓝色)或在较少图像上训练的模型(小)在训练时间较长时会脱离曲线。</li><li id="6a81" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">较小的模型不会从增加数据集大小/计算资源中受益。而大型模型似乎明显受益于甚至超过1B的图像。</li></ul><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/ee0767913f7974f100411629a6264d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*sNL_9PKiWSdJRibN1LTniA.png"/></div></figure><ul class=""><li id="d96e" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated">上图研究了不同尺寸模型相对于步数的误差率。</li><li id="e463" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">大模型更有效。例如，在10次拍摄的学习中，Ti/16模型需要看到100倍以上的图像才能达到L/16模型的性能，在微调中需要看到20倍。当在足够的数据上训练时，用更少的步骤学习更大的模型更好。</li></ul><p id="3517" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">然而，少量的学习结果似乎是不公平的。竞争对手使用未标记但在域内的数据进行预训练，而ViT-G模型能够在不同任务的更大数据上进行自我监督学习。</p><p id="4ee8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我可能遗漏了一些东西，但这篇论文似乎提出了关于伸缩ViTs属性的矛盾陈述。第3.3节中的句子“<em class="jp">小模型和大模型都受益于这种变化，受益系数约为……</em>”和“<em class="jp">”此外，当增加数据集大小时，我们观察到大模型的性能提高，而小模型的性能没有提高。</em>2.1节中的“是矛盾的。</p><h2 id="43ce" class="jz ka hi bd kb kc kd ke kf kg kh ki kj jc kk kl km jg kn ko kp jk kq kr ks kt bi translated">摘要</h2><p id="397e" class="pw-post-body-paragraph ir is hi it b iu ku iw ix iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo hb bi translated">我们将把这项研究中所观察到的情况总结出来。</p><ul class=""><li id="ae38" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated">同时缩放总计算和模型大小是有效的。特别是，当额外的计算变得可用时，不增加模型的大小是次优的。</li><li id="7263" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">具有足够训练数据的视觉转换器模型大致遵循(饱和)幂定律。</li><li id="2111" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">较大的模型在少数镜头学习中表现更好。</li><li id="2bca" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">提出新的训练技术，提高性能并减少计算瓶颈。</li></ul><p id="053b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">视觉变形器是计算机视觉中一个有效但尚未研究的分支。讨论维生素t各种特性的后续论文越来越引起人们的兴趣。特别是，谷歌大脑的研究人员似乎有很大的兴趣。要进一步了解vit的各种特性，请阅读:</p><ul class=""><li id="4db1" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated"><a class="ae iq" href="https://arxiv.org/pdf/2106.10270.pdf" rel="noopener ugc nofollow" target="_blank">如何训练你的ViT？视觉转换器中的数据、增强和规则化</a></li><li id="6ec2" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><a class="ae iq" href="https://arxiv.org/pdf/2106.01548.pdf" rel="noopener ugc nofollow" target="_blank">当视觉转换器在没有预训练或强大数据增强的情况下优于ResNets时</a></li><li id="e83a" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><a class="ae iq" href="https://arxiv.org/pdf/2106.13700" rel="noopener ugc nofollow" target="_blank"> Vision Transformer架构搜索</a></li><li id="5845" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><a class="ae iq" href="https://arxiv.org/pdf/2107.02174.pdf" rel="noopener ugc nofollow" target="_blank">什么造就了分层视觉转换器？</a></li></ul></div></div>    
</body>
</html>