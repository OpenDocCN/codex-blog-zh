<html>
<head>
<title>Easy explanation of Relational Deep Reinforcement Learning with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用张量流简单解释关系深度强化学习</h1>
<blockquote>原文：<a href="https://medium.com/codex/easy-explanation-of-relational-deep-reinforcement-learning-with-real-code-4e2455217853?source=collection_archive---------5-----------------------#2021-06-26">https://medium.com/codex/easy-explanation-of-relational-deep-reinforcement-learning-with-real-code-4e2455217853?source=collection_archive---------5-----------------------#2021-06-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fec9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于LSTMs的内存限制，目前的深度学习模型大多使用了注意机制。DeepMind已经在2019年ICLR会议上发表了一篇题为“具有关系归纳偏差的深度强化学习”的论文。</p><h1 id="a7b7" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">关系环境</h1><p id="dec3" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">本文使用网格型环境来验证该模型的性能。然而，可以肯定的是，由于移动滑车的方式，与训练的质量相比，这种环境花费了太多的时间。这就是为什么我决定使用<a class="ae jd" href="https://drive.google.com/file/d/1mfZ1O0DKWPlBQYl6UNQUL0IMgfJ4_FyX/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">另一个环境</a>的原因，它具有关系特性，但具有简单的移动方式。在确认论文的算法是否可行之后，我们就可以在星际争霸2的环境下使用了。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/b8799989b91cdfc5bd80f4c5c344b2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/1*E9rUvJobOZ6BvMo3k89TmQ.gif"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">盒子世界环境的可视化</figcaption></figure><p id="8518" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个非常简单的环境，代理应该按顺序堆叠箱子。难度取决于积木的数量。因此，这是一个非常理想的环境来检查关系强化学习的性能。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/902f157b0282c7be98dcd8b14517d7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ksW1jcA4OXtoxx7EkKMQA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">网络体系结构</figcaption></figure><p id="41d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们来看一下使用Tensorflow 的<a class="ae jd" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank">官方变形金刚教程的多头注意力方法解决一个有三个盒子的盒子世界环境的代码。</a></p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">Tensorflow的变压器教程的多头关注功能</figcaption></figure><p id="3b39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了训练速度快，使用了Tensorflow 2.0的<a class="ae jd" href="https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic" rel="noopener ugc nofollow" target="_blank">演员-评论家教程代码</a>。您只需要添加一个关系模块来建模部分代码。</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">A2C与关系政策</figcaption></figure><p id="aab1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果没有，下载<a class="ae jd" href="https://drive.google.com/file/d/1QLttW9Za_Tzk0P50q25Wx6xzZiEieecT/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> my code </a>并用BoxWorldEnv.py文件运行它。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es la"><img src="../Images/c7ccd8ebe3d5ed1bb225e1f17b2ed182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2A3rbQ7-h2fTGT6bYA2zg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">A2C培训结果</figcaption></figure><p id="f86e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在注意力策略下，训练将在大约11k集内完成。与同等条件下的CNN政策相比较是合理的。和注意力政策一样，CNN的政策可以达到最高奖励。然而，它需要更多的插曲。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lb"><img src="../Images/06019896786592207dc995ce3cbacb08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAK0SkS7e01doUB49ghVCA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">DQN培训结果</figcaption></figure><p id="6e82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有趣的是，DQN不能用A2C的注意力网络训练好。要自己检查，下载<a class="ae jd" href="https://drive.google.com/file/d/19dz18dZ9wzS7g2U47K9HRc_gjELBzXwl/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">我的代码</a>并在BoxWorld环境中使用它。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lc"><img src="../Images/2d14061b2e5faf34e81168131863d620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2IjbOqvgrRLkfuQdR6bBTw.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">注意网络的数量效应</figcaption></figure><p id="9f36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后需要检查的是根据关系块数训练性能。在我的情况下，我看不出使用相同环境和算法的注意力网络的数量有任何性能差异。</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">嵌套注意网络</figcaption></figure><p id="49e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意力网络的嵌套方式类似于上面的代码。</p><h1 id="5df7" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">关系块如何工作？</h1><p id="4724" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">查看模型的结构。首先，一个大小为(batch，64，64，3)的图像通过两个CNN。这样就得到一个大小为(batch，W，H，C)的矩阵。如果将其转换为size (batch，W x H，C)，则获得具有多个C特征的多个W x H实体。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ld"><img src="../Images/06e04fdf5708f168667396cfbd16cc93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*I-jfooRsAwMH-nuo3ii6kQ.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">论文CNN输出中的实体抽取</figcaption></figure><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">从CNN输出的代码中提取实体</figcaption></figure><p id="a945" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是将之前获得的(batch，W x H，C) size实体作为查询、键、值放入MultiHeadAttention模型中。这也将产生一个关注值size (batch，W x H，C)。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es le"><img src="../Images/6c5c88e3611de8e6ba51f5ea13dad89d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DReYBMIerb7QS-ixScSJUg.png"/></div></div></figure><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="6482" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，注意添加大小为(batch，W x H，C)的实体。然后，选取具有最大C值的图元。它将给出一个矩阵的大小(批，C)。在残差操作之前对关注值应用下降，然后在操作之后应用层归一化。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lf"><img src="../Images/41ee0b3a211a8392ea69c440dff91055.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*Ryjrqef1mFb9M5crWUeVsg.png"/></div></figure><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="9087" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，得到的值可以作为深度强化学习中的共同特征。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lg"><img src="../Images/ccaacc4e80846f5767b60a7eb2e704bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-uVZGTX0bbozyUZ-q6S1Jw.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">经由可微分神经逻辑的归纳逻辑编程</figcaption></figure><p id="85ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，在BoxWorld中，当指定背景知识时，可以使用谓词above(a，b)来表示框a在框b之上。这样的谓词然后可以在盒c和d的学习期间使用。</p><h1 id="e796" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">Dr-Derk环境</h1><p id="9223" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">上面解释的网络可以应用于需要关系功能的其他环境。在我的例子中，我将其应用于一个名为Derk's Gym的3: 3多人在线战斗竞技场类型的游戏。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lh"><img src="../Images/e4ae210183676b48ff154be79afde16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*XKZnQOAfCJhhLyogElwzXQ.gif"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">德克健身房</figcaption></figure><p id="cdbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的理论是，分享同一个团队成员的信息比只使用自己的信息玩游戏更有利于获得更高的分数。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es li"><img src="../Images/cb59619272d0facc5c18ed8ef1916689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wAFe7X3HLGe_TQ9exHJ_cQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">德克健身房的观察与行动</figcaption></figure><p id="ab2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对德克健身房的观察包括地形、物品以及敌人和盟友的个人信息，大小为64。在动作的情况下，它简单地由向前移动的速度、旋转的速度和使用项目1/2/3组成，大小为5。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lj"><img src="../Images/76a2cb08e003d91aea44ed50cf7dcae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cWBJjlZnKNy8Qy57kIFKsA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">德克健身房的奖励</figcaption></figure><p id="de04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">德克的健身房奖励包括给予和接受单位的伤害，给予建筑物的伤害，给予单位的治疗量，摔倒死亡和合作分数。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lk"><img src="../Images/b267bb024513b9203c22b6478d9256ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GXOzwDKcZAwhJ9f4gvNZBQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">竞技场和平行</figcaption></figure><p id="c7e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Derk's Gym的一个特点就是可以自己提供多种环境功能。用户无需额外编码即可轻松确定名为Arena的游戏场数。最近，有各种分布式强化学习方法用于在多种环境下快速训练。我为A2C修改了谷歌研究代码的Seed-RL。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ll"><img src="../Images/5b18af30d8c8c787e41bda58670c02cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PfPHcREFMPhGffvLWRTUtA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">德克体育馆一个场馆的网络结构</figcaption></figure><p id="89e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个竞技场中每个代理的信息可以看作一个实体。如果我们把它们放在一起，网络就能知道每个实体之间的关系信息，从而为团队游戏选择行动。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lm"><img src="../Images/01e27e453049d4f078e5cf52f650a1f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCknUFLSeYJLGYAQMIjbvA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">德克健身房注意阻滞的效果</figcaption></figure><p id="d99b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我用与第一个实验相同的方式比较了CNN政策、关系政策的案例。如果注意力集中，您可以看到代理获得更多奖励，如上图所示。</p><h1 id="00b6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结论</h1><p id="efbe" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">德克健身房的代码是一个正在进行的比赛。因此，与公众分享它看起来不是一个好主意。然而，如果你完全理解关系推理的注意，你可以很容易地把它应用到你自己的项目中。</p></div></div>    
</body>
</html>