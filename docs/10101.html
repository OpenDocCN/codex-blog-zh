<html>
<head>
<title>Natural Language Processing: Cleaning up Tweets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理:清理推文</h1>
<blockquote>原文：<a href="https://medium.com/codex/natural-language-processing-cleaning-up-tweets-2b71793e91b5?source=collection_archive---------2-----------------------#2022-12-04">https://medium.com/codex/natural-language-processing-cleaning-up-tweets-2b71793e91b5?source=collection_archive---------2-----------------------#2022-12-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c4c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Twitter最近一直是新闻的焦点，所以我想借此机会写一下我在自然语言处理项目上的第一次尝试。作为我的跳板数据科学训练营的一部分，我最后的顶点项目是建立一个分类模型，以区分灾难和非灾难相关的推文。当我开始从事我的顶点项目时，我已经花了几个小时在Data Camp和LinkedIn课程上学习了无数关于如何处理文本数据的策略。所以，就我个人而言，我想通过这个项目来真正检验我的数据处理技能。</p><h1 id="6683" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据</h1><p id="6d97" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我使用了来自Kaggle 的数据集<strong class="ih hj"> </strong> <a class="ae kg" href="https://www.kaggle.com/competitions/nlp-getting-started/overview" rel="noopener ugc nofollow" target="_blank">，该数据集包含7613条推文，其中4342条推文被标记为与任何灾难无关，3271条被标记为与灾难有关。大约三分之二的推文标记了它们的位置，并且大多数推文(99%)在推文旁边有相关的关键词。在我深入研究这些推文之前，我看了一下丢失数据的分布情况:我想看看某个特定类别的数据是否丢失了特定的部分。例如，与灾难性推文相比，更多的非灾难性推文缺少位置标签吗？然而，这并没有产生有意义的结果:丢失的数据似乎是随机的。我总是发现可视化这些问题是最容易的，所以我创建了一个数据可用性的条形图(见下文),它说明了缺失数据的随机分布。同样令人欣慰的是，数据集中的每条推文都被贴上了标签，所以我不必进一步缩减数据集的大小。</a></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/a364f95a54703baf839ef56ecf93cc0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*XHcmVHY-5LHdw4hlw-Z45Q.png"/></div></figure><h1 id="fed7" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">关键词</h1><p id="9563" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">7552条推文在数据集中标记了关键词。有趣的是，在7552个关键词中，只有221个是唯一的！因此，对于数据争论过程的第一步，我专注于清理关键字，因为我想更详细地探索关键字。</p><p id="dc32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如何清理文本数据？欢迎来到正则表达式的奇妙世界。</p><p id="b171" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kg" href="https://www.dataquest.io/wp-content/uploads/2019/03/python-regular-expressions-cheat-sheet.pdf" rel="noopener ugc nofollow" target="_blank">正则表达式</a>是字符序列，可以是字母或标点符号，这有助于设计搜索模式。因此，我们可以使用正则表达式来搜索推文，看看它们是否包含类似的模式。Python有一个名为re的内置包，在这里非常有用。我还推荐unidecode，查看文档，因为这是一种在Unicode和ASCII之间转换的简单方法。</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="b3b1" class="ku je hi kq b be kv kw l kx ky">import re<br/>import unidecode</span></pre><p id="4983" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我首先删除了不需要的标点符号，比如感叹号、重音符号、括号、逗号、分号等等。</p><p id="08af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有一个我用来清理关键词的正则表达式的例子，如果你想了解更多的细节，可以看看我的笔记本<a class="ae kg" href="https://github.com/iban121/Springboard/blob/main/Capstone%203/notebooks/.ipynb_checkpoints/Data%20Wrangling%20and%20EDA-checkpoint.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"/></a>。</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="8932" class="ku je hi kq b be kv kw l kx ky">def clean_keywords(keyword):<br/>    cleaned = re.sub(r'%20', ' ', keyword)<br/>    return cleaned<br/>def remove_accents(keyword):<br/>    cleaned = unidecode.unidecode(keyword)<br/>    return cleaned<br/>def remove_punctuation(keyword):<br/>    cleaned = re.sub(r"[!\"#$%&amp;()*+-./:;&lt;=&gt;?@[\]^_`{|}~\n -' ]"," ",keyword)<br/>    return cleaned</span></pre><p id="bbe5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我做了一个不同类使用的所有关键字的计数图(看看下面的计数图)。</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="d1b2" class="ku je hi kq b be kv kw l kx ky">keywords['keyword'] = keywords.groupby('keyword')['target'].transform('mean')<br/>fig = plt.figure(figsize=(8, 40), dpi=100)<br/>sns.countplot(y=keywords.sort_values(by='keyword', ascending = False)['keyword'], <br/>              hue = keywords.sort_values(by='keyword', ascending =False)['target'])<br/>plt.savefig('figures/keywords_distributions_before_stemming.png', bbox_inches = 'tight',facecolor='white', transparent =None)<br/>plt.show()</span></pre><p id="45e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你仔细看看右边地块上的关键词，很快就会发现有一些重复，比如野火和野火。解决这个问题的一个可能的方法是阻止单词。词干化是指我们将单词简化为词干或词根。这样做的好处是相似的词可以组合成一个词，如自杀炸弹，自杀炸弹，自杀炸弹手都可以组合成自杀炸弹。</p><p id="f774" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，我用NLTK的波特·斯特梅尔来做关键词。</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="78ed" class="ku je hi kq b be kv kw l kx ky">from nltk.stem import PorterStemmer<br/>stemmer = PorterStemmer()<br/>keywords['stems'] = keywords['keyword'].apply(lambda x: stemmer.stem(x))</span></pre><div class="ki kj kk kl fd ab cb"><figure class="kz km la lb lc ld le paragraph-image"><img src="../Images/1ecd8bbf518b660072e403f56561cf8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*7HU8UgpKkZsFZmFd7k8EDQ.png"/></figure><figure class="kz km lf lb lc ld le paragraph-image"><img src="../Images/bc383d2395904231cfc5d2495f45d5aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*YkxrTKRlk2KUoQdiO_eTXg.png"/><figcaption class="lg lh et er es li lj bd b be z dx lk di ll lm translated">左:词干提取之前的关键字。右图:词干后的关键字</figcaption></figure></div><p id="5184" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你看看词干化后的两个图，我们可以看到关键词的数量减少了，但我们仍然有一些不太固定的关键词，例如wildfir和wild fir仍然被分为两类。这里的解决方案是返回并删除单词之间的空格，或者手动合并这两组。由于我的关键词数量如此之少，我快速浏览了一下这些词，并决定继续前进，因为没有多少其他组可以组合。我决定现在看看两组中100个最常见关键词的视觉表现。一个有趣而简单的形象化方法是使用单词云！</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="11f0" class="ku je hi kq b be kv kw l kx ky">from collections import Counter<br/>from wordcloud import WordCloud<br/><br/>def make_dict(tup, dictionary):<br/>    for x, y in tup:<br/>        dictionary.setdefault(x, []). append(y)<br/>    return dictionary<br/>count = Counter(list(keywords[keywords['target']==1]['keyword']))<br/>top_words = {}<br/>make_dict(count.most_common(100), top_words)<br/>df = pd.DataFrame.from_dict(top_words, orient = 'index').reset_index()<br/>df.columns=['word', 'count']<br/>text = df['word'].values<br/>wordcloud_keywords = WordCloud(background_color = 'white', width = 500, height = 300, collocations = True).generate(str(text))<br/>plt.figure(figsize=(8, 8), dpi=100)<br/>plt.imshow(wordcloud_keywords)<br/>plt.axis('off')<br/>plt.title('Disaster Keywords', fontsize = 22)<br/>plt.savefig('figures/wordcloud_disaster_keywords.png', bbox_inches = 'tight',facecolor='white', transparent =None)<br/>plt.show()</span></pre><div class="ki kj kk kl fd ab cb"><figure class="kz km ln lb lc ld le paragraph-image"><img src="../Images/bc4527b241e0c6363606cef5a12404e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*N-wxwcEPpY3msQblWVFiJA.png"/></figure><figure class="kz km lo lb lc ld le paragraph-image"><img src="../Images/084a6e9771048b2bd2260867eaf4ed29.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*Ck2aCiTDPv71B7NckFfu_Q.png"/></figure></div><p id="7903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这两个类别在关键词的选择上肯定有一些共同点，但我们也看到一些关键词在一个类别中比其他类别出现得更频繁:例如，裹尸袋作为非灾难推文的关键词出现得比灾难推文更频繁。几个词(碎片和残骸)只作为灾难推特的关键词出现，而余震只作为非灾难推特的关键词出现。最终，我认为关键字之间没有足够的差异来尝试使用它们开发一个分类模型，所以我在项目的其余部分删除了关键字列。我想在这里做的一个可能的改变是不删除这个列，而是将keywords列与tweet列合并。</p><h1 id="b315" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">位置</h1><p id="a703" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">大约三分之二的推文有位置标记，大约34%的推文在两个类别中都没有。在5080个位置标签中，大约有3341个唯一条目。仔细查看这些位置可以发现，它们可以用国家、城市、缩写、日期来输入，甚至还有破折号、逗号和标签等标点符号。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lp"><img src="../Images/9e5d47ce8874645da0a4cdcc3e949227.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*SxVtZGoIkookTT-SpUGE1g.png"/></div></figure><p id="9d75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，使用正则表达式和lambda函数，我删除了数字和空格。</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="de5e" class="ku je hi kq b be kv kw l kx ky">location['location'] = location['location'].apply(lambda x: remove_accents(x))<br/>location['location'] = location['location'].apply(lambda x: remove_punctuation(x))<br/>def remove_nums(location):<br/>    cleaned = re.sub(r'\d+', '', location)<br/>    return cleaned<br/>location['location'] = location['location'].apply(lambda x: remove_nums(x))<br/>def remove_extra_w_space(location):<br/>    cleaned_text = re.sub(r"\s+"," ",location).strip()<br/>    return cleaned_text<br/>location['location'] = location['location'].apply(lambda x: remove_extra_w_space(x))</span></pre><p id="8fed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将5080个条目中唯一位置的数量从3341个减少到3106个。其中，只有82个位置是两个班级共有的。这可能只是因为5080并不是一个足够大的样本来代表twitter用户的人口统计数据。还有，使用两个班级之间的共同词的数量有一些明显的问题；如果我们有一个更大的样本量，常用词的数量会增加，但是不一定是因为有更多的相似性，而只是因为数据集中有更多的词。所以，我决定用余弦相似度来代替。</p><p id="01a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">余弦相似度是什么？如果你喜欢向量的点积，可以把余弦相似度看作是计算两个向量之间夹角的余弦。考虑y=cosx的图表(如下)，我们可以看到，随着x值增加到π/2，或90度，y值或cos(x)从1减少到0。这意味着如果两个向量之间的角度更接近于零，cos(x)将接近于1，这意味着两个向量彼此相似。然而，如果cos(x)的值更接近于0，那么这两个向量几乎相互垂直，这意味着它们是不同的。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lq"><img src="../Images/20324aa026f8678f5375861edf4b5d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ix3pjurHqeLpgv4K28Et0Q.png"/></div></div></figure><p id="f84f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，向量中的相似性如何与位置条目相关联呢？查看<a class="ae kg" href="https://www.machinelearningplus.com/nlp/cosine-similarity/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>，它对余弦相似性进行了非常深入的解释。想象一下，对于每个单词，我们都有一个“轴”。因此，我们可以画出每个类别中每个单词的使用频率——这被称为单词的矢量化。但是我们不是看线条的长度，或者矢量的大小，而是测量矢量之间的角度。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lv"><img src="../Images/357e277ad6efc805c246442bcff3e53e.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*f7dQfsRxODtUObzNDajJ8g.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated"><a class="ae kg" href="https://www.machinelearningplus.com/nlp/cosine-similarity/" rel="noopener ugc nofollow" target="_blank">自然语言处理中余弦相似度的一个例子</a></figcaption></figure><p id="33f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看<a class="ae kg" href="https://github.com/iban121/Springboard/blob/main/Capstone%203/notebooks/.ipynb_checkpoints/Data%20Wrangling%20and%20EDA-checkpoint.ipynb" rel="noopener ugc nofollow" target="_blank">我的笔记本</a>了解更多细节，但这里是我用来矢量化位置信息和计算余弦相似性的代码片段:</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="8ee3" class="ku je hi kq b be kv kw l kx ky">from sklearn.preprocessing import MultiLabelBinarizer<br/>vectorizer= MultiLabelBinarizer()<br/>mlb = vectorizer.fit(corpus)<br/>df = pd.DataFrame(mlb.transform(corpus), columns=[mlb.classes_], index=['Disaster', 'Non Disaster'])<br/>from sklearn.metrics.pairwise import cosine_similarity<br/>cosine_similarity(df)</span></pre><p id="9b12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这给了我们:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lw"><img src="../Images/f5b4efa1f9912d534fe710062856962a.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*3UJZziu8-MhsVU3_EZR7RQ.png"/></div></figure><p id="3bdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是什么意思？0.2897更接近于0而不是1，这告诉我们两组词，灾难推文的位置和非灾难推文的位置并不是很相似。</p><h1 id="b6a1" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">最后是推文！</h1><p id="bb12" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">所以，坦白地说，我本想在这里做更多的事情，但出于期限和全职工作的考虑，我决定暂时不去追求这些领域。希望有一天我能回到这里。</p><p id="56c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我想说的是，推文中包含了大量的信息。因此，我们必须选择在一个近乎循环的过程中找到提取信息和清理数据的平衡点。例如，标签后面是文本。如果我们像对待位置和关键词那样去掉所有标点符号，我们就会失去' # '符号。</p><h1 id="7bf4" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">#、@和😊！</h1><p id="c7d9" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我编写了函数，然后统计了推文中使用的提及次数、标签和表情符号，然后按类别分组，看看它们的使用是否有明显的模式。</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="5a6d" class="ku je hi kq b be kv kw l kx ky">def hash_count(string):<br/>    words = string.split()<br/>    hashtags = [word for word in words if word.startswith('#')]<br/>    return len(hashtags)<br/>tweets['num_hashtags'] = tweets['text'].apply(hash_count)<br/>tweets['hashtags'] = tweets['text'].apply(lambda x: [x for x in x.split(" ") if x.startswith("#")])</span></pre><p id="89d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于表情符号，我使用了表情符号包:</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="7f61" class="ku je hi kq b be kv kw l kx ky">import emoji<br/>def emoji_count(tweet):<br/>    tweet = emoji.demojize(tweet, delimiters=('__','__'))<br/>    pattern = r'_+[a-z_&amp;]+_+'<br/>    return len(re.findall(pattern, tweet))<br/>tweets['emojis'] = tweets['text'].apply(emoji_count)</span></pre><p id="e74f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个快速的可视化(如下)显示，非灾难性的推文有更多的标签、提及和表情符号的使用。所有这三个都显示了两个类中标签和提及次数非常相似的分布。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lx"><img src="../Images/7ccd4a3fe01b4dbba0a3a74455374a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*LrDgr-gFHlL7rcJLvpDVbQ.png"/></div></figure><h1 id="3130" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">口语</h1><p id="1bd3" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">然后，网络语言的挑战来了！我使用了他的缩写字典，这是一个救生工具，来扩展常见的缩写！看看这本字典，它相当完整，我强烈推荐它。此外，我使用了宫缩软件包，它有助于扩展常见的宫缩。</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="6779" class="ku je hi kq b be kv kw l kx ky">import contractions<br/>def expand_contractions(tweet):<br/>    return contractions.fix(tweet)<br/>tweets['text'] = tweets['text'].apply(lambda x: expand_contractions(x))</span></pre><p id="551e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我用<a class="ae kg" href="https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b" rel="noopener ugc nofollow" target="_blank">这个</a>来帮助删除推文中的表情符号、符号、象形文字、交通和地图图标以及旗帜。</p><pre class="ki kj kk kl fd kp kq kr bn ks kt bi"><span id="9aad" class="ku je hi kq b be kv kw l kx ky">def demojize(tweet):<br/>    emojis = re.compile("["<br/>        u"\U0001F600-\U0001F64F"  # emoticons<br/>        u"\U0001F300-\U0001F5FF"  # symbols &amp; pictographs<br/>        u"\U0001F680-\U0001F6FF"  # transport &amp; map symbols<br/>        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)<br/>        u"\U00002702-\U000027B0"<br/>        u"\U000024C2-\U0001F251"<br/>        "]+", flags = re.UNICODE)<br/>    cleaned_text = emojis.sub(r'', tweet)<br/>    return cleaned_text<br/>tweets['text'] = tweets['text'].apply(lambda x: demojize(x))</span></pre><p id="b09a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快到了！现在，所有的缩写和缩写都扩展了，我们可以预测的符号都被清理了，我们删除了所有的标点、重音、提及、标签、html文本、URL、数字和额外的空格，其中一些是因为清理过程而留下的。最后我把推文都改成小写了。下一个合乎逻辑的步骤是对单词进行标记，但在此之前，我花了一些时间进行了另一轮探索性数据分析，以查看推文中经过清理的单词是否可以揭示两个类别之间不同的语言选择。</p><p id="86d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我查看了两个不同班级使用的单词数量、不同类型的名词以及单词长度，得出的结论是这两个班级之间仍然没有太大的差异。因此，我研究这两个特征不是为了帮助开发分类模型，而是为了帮助形成一个定制的<a class="ae kg" href="https://kavita-ganesan.com/what-are-stop-words/#.Y4yccMtBzb0" rel="noopener ugc nofollow" target="_blank">停用词列表。</a>停用词是在语言中使用的不太重要的词，例如“a”、“the”、“then”等等。我想创建一个自定义停用词的列表，因为这个数据集的大小(大约7000条推文)相当小。因此，通过创建一个自定义的停用词列表，我可以开发一个更小、更具体的不重要词列表。这个想法是识别停用词，从整个词汇、语料库中删除它们，以允许机器学习算法在开发分类模型时只关注重要的词。</p><p id="ca7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的EDA强调了两个或更少字符的单词似乎不会在两个类之间产生差异，所以我首先删除了两个或更少字符的单词以及它们删除后留下的任何空白。</p><p id="e980" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我使用计数矢量器，查看语料库上的文档。由于这是我第一次尝试NLP，我决定也尝试使用TFIDF矢量器来了解更多关于它们的区别。我的目标是找出我们可以用来创建停用词列表的最常用的词。这花了一些时间，主要是因为我想看看这些矢量器的范围。查看<a class="ae kg" href="https://github.com/iban121/Springboard/blob/main/Capstone%203/notebooks/.ipynb_checkpoints/Data%20Wrangling%20and%20EDA-checkpoint.ipynb" rel="noopener ugc nofollow" target="_blank">我的笔记本</a>。从本质上说，清理过程的最后一步是删除已识别的停用词。稍后，我将更详细地介绍对推文进行符号化和词条化，以及我如何创建我的二元分类模型，但现在，我希望这能给你一些好主意，让你知道如何处理推文等社交媒体文本！</p></div></div>    
</body>
</html>