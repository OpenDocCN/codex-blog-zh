<html>
<head>
<title>Connecting to AWS Athena databases using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python连接到AWS Athena数据库</h1>
<blockquote>原文：<a href="https://medium.com/codex/connecting-to-aws-athena-databases-using-python-4a9194427638?source=collection_archive---------0-----------------------#2021-06-18">https://medium.com/codex/connecting-to-aws-athena-databases-using-python-4a9194427638?source=collection_archive---------0-----------------------#2021-06-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ba57" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这里有两种方法</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/4a27914af2b442c6c0a7769c188533c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*lBHVxW6apsoTePK8IjC_0Q.png"/></div></figure><p id="6ff8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">注</strong>:与本文相关的完整代码可以在<a class="ae kb" href="https://github.com/ramdesh/athena-python-examples" rel="noopener ugc nofollow" target="_blank">这个Github repo </a>上找到。</p><p id="236a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae kb" href="https://aws.amazon.com/athena" rel="noopener ugc nofollow" target="_blank"> AWS Athena </a>是一项服务，允许您在AWS S3存储桶中存储的数据文件上构建数据库，并从中查询数据。如果您有一个存储为CSV或parquet文件的大规模数据集，并且您不想花几天时间编写ETL作业并将其加载到标准SQL数据库中，那么这将非常有用。它基本上允许您编写标准的SQL查询，从存储在S3上的平面数据文件中检索数据。<br/>我刚刚开始使用Athena数据库，面临着让我们的团队能够通过Python，更具体地说是Jupyterlab访问Athena的问题，我想出了两种不同的方法来开发Python包装器，以便轻松有效地访问Athena数据库。</p><h1 id="37f9" class="kc kd hi bd ke kf kg kh ki kj kk kl km io kn ip ko ir kp is kq iu kr iv ks kt bi translated">方法1: PyAthena + SQLAlchemy</h1><p id="a7bc" class="pw-post-body-paragraph jf jg hi jh b ji ku ij jk jl kv im jn jo kw jq jr js kx ju jv jw ky jy jz ka hb bi translated">如果你像我一样是SQLAlchemy的粉丝，你会知道它是一个很棒的库，可以抽象出到不同数据库的数据库连接，维护一个ORM层，还有许多其他很酷的好处。每当我遇到一个新的数据库，需要编写一个包装器来连接它时，我都会想SQLAlchemy能做到吗？’在雅典娜的例子中，事实证明是可以的。大多数Python开发人员非常习惯使用SQLAlchemy来连接和查询数据库，因此检查是否有方法使用它来连接任何新的数据库总是好的。</p><p id="c007" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae kb" href="https://pypi.org/project/pyathena/" rel="noopener ugc nofollow" target="_blank"> PyAthena </a>是一个使用<a class="ae kb" href="https://docs.aws.amazon.com/athena/latest/APIReference/Welcome.html" rel="noopener ugc nofollow" target="_blank"> Athena的REST API </a>连接Athena并获取查询结果的库。它可以直接包装在SQLAlchemy中，并且您可以创建一个SQLAlchemy连接对象，该对象可以非常直接地绑定到您通常使用的任何查询方法，例如将数据加载到一个<code class="du kz la lb lc b">pandas</code> DataFrame中:</p><pre class="iy iz ja jb fd ld lc le lf aw lg bi"><span id="489b" class="lh kd hi lc b fi li lj l lk ll">from urllib.parse import quote_plus<br/>from sqlalchemy.engine import create_engine</span><span id="ea04" class="lh kd hi lc b fi lm lj l lk ll">AWS_ACCESS_KEY = "AWS_ACCESS_KEY"<br/>AWS_SECRET_KEY = "AWS_SECRET_KEY"<br/>SCHEMA_NAME = "schema_name"<br/>S3_STAGING_DIR = "s3://s3-results-bucket/output/"<br/>AWS_REGION = "us-east-1"<br/><br/><br/>conn_str = (<br/>    "awsathena+rest://{aws_access_key_id}:{aws_secret_access_key}@"<br/>    "athena.{region_name}.amazonaws.com:443/"<br/>    "{schema_name}s3_staging_dir{s3_staging_dir}&amp;work_group=primary"<br/>)<br/><br/><br/># Create the SQLAlchemy connection. Note that you need to have pyathena installed for this.<br/>engine = create_engine(<br/>    conn_str.format(<br/>        aws_access_key_id=quote_plus(AWS_ACCESS_KEY),<br/>        aws_secret_access_key=quote_plus(AWS_SECRET_KEY),<br/>        region_name=AWS_REGION,<br/>        schema_name=SCHEMA_NAME,<br/>        s3_staging_dir=quote_plus(S3_STAGING_DIR),<br/>    )<br/>)</span><span id="0d1c" class="lh kd hi lc b fi lm lj l lk ll">athena_connection = engine.connect()</span></pre><p id="0871" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，您需要计算出AWS IAM帐户的访问密钥和秘密密钥，该帐户有权查询您正在连接的Athena数据库，还需要计算出您为Athena实例配置的结果S3存储桶，因为Athena需要一个结果S3存储桶来写入物理结果文件，也就是它从中提取结果的位置。您会注意到，我们甚至没有导入PyAthena，但是当我们对连接字符串使用<code class="du kz la lb lc b">awsathena+rest</code>语法时，它需要作为SQLAlchemy的底层数据库驱动程序安装。</p><p id="d869" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在我们已经连接到您的数据库，您需要做的就是使用您构建的连接对象查询它。我更喜欢用熊猫:</p><pre class="iy iz ja jb fd ld lc le lf aw lg bi"><span id="ab0c" class="lh kd hi lc b fi li lj l lk ll">import pandas as pd</span><span id="a259" class="lh kd hi lc b fi lm lj l lk ll">df_data = pd.read_sql_query("select * from table", conn)</span></pre><p id="3384" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">使用非常简单，但可能不是最好的方法，因为它在查询大型数据集时非常慢。Athena REST API的<code class="du kz la lb lc b">GetQueryResults</code>方法一次最多只能返回1000条记录，这意味着PyAthena必须维护一个游标并请求下一个1000条结果，以此类推。当我们继续查询数百万行数据时，这会花费很多时间。</p><p id="a9e2" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这让我们想到:</p><h1 id="4667" class="kc kd hi bd ke kf kg kh ki kj kk kl km io kn ip ko ir kp is kq iu kr iv ks kt bi translated">方法2:使用Boto3并下载结果文件</h1><p id="62e3" class="pw-post-body-paragraph jf jg hi jh b ji ku ij jk jl kv im jn jo kw jq jr js kx ju jv jw ky jy jz ka hb bi translated">在我们开始讨论下一个方法之前，让我们看看Athena是如何工作的(在非常高的层次上):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/b94b6336eee5f89d585ddbff0cdc9895.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*NRQu6qx1aYaQw5GCUi7R2w.png"/></div></figure><p id="cd9c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当用户在Athena数据库上进行查询时，Athena将从源S3存储桶中获取源数据，将结果返回给用户，并将结果(大多数情况下作为CSV文件)写入输出S3存储桶，这也是Athena实例配置的一部分。</p><p id="047a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将使用这种工作方式，并利用Python的AWS库<a class="ae kb" href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" rel="noopener ugc nofollow" target="_blank"> boto3 </a>来运行我们的查询，获取刚刚运行的查询的ID，并使用该ID获取相关的CSV文件。</p><p id="abc1" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">首先，我们使用boto3构建一个Athena客户端:</p><pre class="iy iz ja jb fd ld lc le lf aw lg bi"><span id="6e7a" class="lh kd hi lc b fi li lj l lk ll">import boto3</span><span id="08cd" class="lh kd hi lc b fi lm lj l lk ll">AWS_ACCESS_KEY = "AWS_ACCESS_KEY"<br/>AWS_SECRET_KEY = "AWS_SECRET_KEY"<br/>AWS_REGION = "us-east-1"</span><span id="3d3b" class="lh kd hi lc b fi lm lj l lk ll">athena_client = boto3.client(<br/>    "athena",<br/>    aws_access_key_id=AWS_ACCESS_KEY,<br/>    aws_secret_access_key=AWS_SECRET_KEY,<br/>    region_name=AWS_REGION,<br/>)</span></pre><p id="214d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">然后，我们使用客户端运行一个查询:</p><pre class="iy iz ja jb fd ld lc le lf aw lg bi"><span id="ce43" class="lh kd hi lc b fi li lj l lk ll">query_response = athena_client.start_query_execution(<br/>    QueryString="SELECT * FROM table",<br/>    QueryExecutionContext={"Database": SCHEMA_NAME},<br/>    ResultConfiguration={<br/>        "OutputLocation": S3_STAGING_DIR,<br/>        "EncryptionConfiguration": {"EncryptionOption": "SSE_S3"},<br/>    },<br/>)</span><span id="8203" class="lh kd hi lc b fi lm lj l lk ll">while True:<br/>    try:<br/>        # This function only loads the first 1000 rows<br/>        client.get_query_results(<br/>            QueryExecutionId=query_response["QueryExecutionId"]<br/>        )<br/>        break<br/>    except Exception as err:<br/>        if "not yet finished" in str(err):<br/>            time.sleep(0.001)<br/>        else:<br/>            raise err</span></pre><p id="0fa1" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">注意这里的<code class="du kz la lb lc b">get_query_results</code>函数并没有回调函数让我们知道查询已经完成；我们必须在while循环中不断检查。我们完全忽略了由<code class="du kz la lb lc b">get_query_results</code>函数生成的返回值，因为我们想要做的只是计算出查询何时停止执行，然后下载结果文件。</p><p id="1118" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">然后，我们使用boto3创建一个S3客户端，并使用它直接从S3下载查询结果文件。</p><pre class="iy iz ja jb fd ld lc le lf aw lg bi"><span id="180a" class="lh kd hi lc b fi li lj l lk ll">import pandas as pd</span><span id="3d48" class="lh kd hi lc b fi lm lj l lk ll">S3_BUCKET_NAME = "s3-results-bucket"<br/>S3_OUTPUT_DIRECTORY = "output"</span><span id="4af1" class="lh kd hi lc b fi lm lj l lk ll">temp_file_location: str = "athena_query_results.csv"<br/>s3_client = boto3.client(<br/>    "s3",<br/>    aws_access_key_id=AWS_ACCESS_KEY,<br/>    aws_secret_access_key=AWS_SECRET_KEY,<br/>    region_name=AWS_REGION,<br/>)<br/>s3_client.download_file(<br/>    S3_BUCKET_NAME,<br/>    f"{S3_OUTPUT_DIRECTORY}/{query_response['QueryExecutionId']}.csv",<br/>    temp_file_location,<br/>)<br/>df = pd.read_csv(temp_file_location)</span></pre><p id="1a1e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这会将结果下载到您的工作区中一个名为<code class="du kz la lb lc b">athena_query_results.csv</code>的文件中，然后您可以将它加载到熊猫数据框架中。</p><p id="887c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Github repo 上有一个更有条理的版本。</p><h1 id="6de7" class="kc kd hi bd ke kf kg kh ki kj kk kl km io kn ip ko ir kp is kq iu kr iv ks kt bi translated">比较两者</h1><p id="dd8a" class="pw-post-body-paragraph jf jg hi jh b ji ku ij jk jl kv im jn jo kw jq jr js kx ju jv jw ky jy jz ka hb bi translated">下面是这两种方法的比较，从包含200多万条记录的表中提取一列，分别提取100、1000、10，000、100，000和100万条记录:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lo"><img src="../Images/09a3b7bede06834efc71e9e545086968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9In6JlhA7wSdyJKibqv2WA.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">结果文件下载方法胜出！</figcaption></figure><p id="68d7" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如您所见，随着我们试图检索的数据量增加并超过10，000条记录，Boto3显然是更好的选择。PyAthena必须使用游标不断返回Athena，这一事实似乎真的阻碍了它对大型查询的处理。</p><p id="8a71" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">看来我们要从S3下载结果文件了。</p></div></div>    
</body>
</html>