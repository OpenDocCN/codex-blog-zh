<html>
<head>
<title>Web Scraping Paginated Webpages With Python, Selenium, and BeautifulSoup4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python、Selenium和BeautifulSoup4进行网页抓取分页网页</h1>
<blockquote>原文：<a href="https://medium.com/codex/web-scraping-paginated-webpages-with-python-selenium-and-beautifulsoup4-8b415f833132?source=collection_archive---------1-----------------------#2022-03-29">https://medium.com/codex/web-scraping-paginated-webpages-with-python-selenium-and-beautifulsoup4-8b415f833132?source=collection_archive---------1-----------------------#2022-03-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b572f4613a3cf62a9f26081c0bfb7191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oINHs75Ju4oL6W9HsnxdIQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来自<a class="ae iu" href="https://unsplash.com/photos/OqtafYT5kTw?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLink" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/OqtafYT5kTw?utm_source=unsplash&amp;UTM _ medium = referral&amp;UTM _ content = creditShareLink</a></figcaption></figure><h1 id="c713" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">在这篇文章中，我介绍了从多个不同的网页中一次性抓取数据的一般过程和技术。我将从g2g.com<a class="ae iu" href="http://g2g.com" rel="noopener ugc nofollow" target="_blank">为MMORPG失落的方舟</a>刮出所有地区和服务器的黄金价格，这是一个受玩家欢迎的在线市场，为各种游戏购买游戏内物品或货币。</h1><h1 id="e315" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">先决条件</h1><p id="7edc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在我们开始之前，您需要做一些事情:</p><ol class=""><li id="1194" class="kr ks hi jv b jw kt ka ku ke kv ki kw km kx kq ky kz la lb bi translated">Chrome网络驱动</li><li id="d690" class="kr ks hi jv b jw lc ka ld ke le ki lf km lg kq ky kz la lb bi translated">安装了必要的Python库</li></ol><p id="a6b7" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">幸运的是，这两个步骤非常容易实现。Chrome webriver是用于测试web应用程序的自动化工具；但是，我们将使用它来自动浏览网页，以到达我们想要的位置。Chromedriver下载可以在<a class="ae iu" href="https://chromedriver.chromium.org/downloads" rel="noopener ugc nofollow" target="_blank">这里</a>找到。你会想要下载与你系统上安装的Chrome版本相匹配的Chrome驱动，你可以在浏览器的“帮助”部分点击“关于谷歌Chrome”找到。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/0eae27030bfa3c6f369fa4c11e9719c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*VPSm14tlEoENNYzCpX9BHw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/ab13279035597aa5750891feb77664d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*5vgUzaPMAWeSq9hzMVb1tg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="b327" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">在撰写本文时，我有Chrome版本99.0.4844.82，所以我想下载Chrome版本99的Chrome驱动程序。将它存储在一个您以后能够找到的地方，因为我们需要在代码中指定它的路径。</p><p id="c534" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">接下来，如果您的系统上还没有安装它们，那么在您的控制台或anaconda navigator中使用<code class="du lq lr ls lt b">pip install [library name]</code>或<code class="du lq lr ls lt b">conda install [library name]</code>安装下面的python库。您将需要以下库:</p><ol class=""><li id="9580" class="kr ks hi jv b jw kt ka ku ke kv ki kw km kx kq ky kz la lb bi translated"><code class="du lq lr ls lt b"><strong class="jv hj">bs4</strong></code>解析html并使其可读</li><li id="7293" class="kr ks hi jv b jw lc ka ld ke le ki lf km lg kq ky kz la lb bi translated"><code class="du lq lr ls lt b"><strong class="jv hj">selenium</strong></code>使用Chromedriver</li><li id="fafa" class="kr ks hi jv b jw lc ka ld ke le ki lf km lg kq ky kz la lb bi translated"><code class="du lq lr ls lt b"><strong class="jv hj">pandas</strong></code>对于数据帧</li><li id="88e0" class="kr ks hi jv b jw lc ka ld ke le ki lf km lg kq ky kz la lb bi translated"><code class="du lq lr ls lt b"><strong class="jv hj">datetime</strong></code>获取数据被抓取的时间</li><li id="bc8a" class="kr ks hi jv b jw lc ka ld ke le ki lf km lg kq ky kz la lb bi translated"><code class="du lq lr ls lt b"><strong class="jv hj">time</strong></code>用于暂停某些网页</li><li id="45fb" class="kr ks hi jv b jw lc ka ld ke le ki lf km lg kq ky kz la lb bi translated"><code class="du lq lr ls lt b"><strong class="jv hj">os</strong></code>写文件</li><li id="9787" class="kr ks hi jv b jw lc ka ld ke le ki lf km lg kq ky kz la lb bi translated"><code class="du lq lr ls lt b"><strong class="jv hj">re</strong></code>对表达式进行格式化</li></ol><p id="a3f6" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">安装好这些之后，我们就可以开始了！</p><h1 id="7d4c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">入门指南</h1><p id="0705" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">首先，我们打开一个Jupyter笔记本或Python文件，并导入所有必要的库:</p><pre class="ll lm ln lo fd lu lt lv lw aw lx bi"><span id="c1ac" class="ly iw hi lt b fi lz ma l mb mc">from bs4 import BeautifulSoup as bs<br/>from selenium import webdriver<br/>import pandas as pd<br/>from datetime import datetime<br/>import time<br/>import os<br/>import re<br/>from selenium.webdriver.chrome.options import Options<br/>from selenium.webdriver.common.by import By<br/>from selenium.webdriver.chrome.service import Service<br/>from selenium.common.exceptions import NoSuchElementException<br/>from selenium.common.exceptions import ElementNotInteractableException</span></pre><p id="c0dd" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">最后五行帮助我们使用Chromedriver，并且更有效地在g2g.com的HTML代码中搜索项目。我会根据需要介绍这些内容。现在，行<code class="du lq lr ls lt b">from selenium.webdriver.chrome.options import Options</code>使我们能够指定我们希望如何打开Chrome。就我个人而言，我喜欢我的Chrome实例被最大化(这样我就不必手动最大化浏览器)和隐姓埋名模式(这样我的个人历史就不会受到抓取很多很多页面的影响)。这是通过代码实现的:</p><pre class="ll lm ln lo fd lu lt lv lw aw lx bi"><span id="65c5" class="ly iw hi lt b fi lz ma l mb mc">service = Service('insert path to your Chromedriver download')</span><span id="e2f1" class="ly iw hi lt b fi md ma l mb mc">options = Options()<br/>options.add_argument('--incognito')<br/>options.add_argument('start-maximized')</span><span id="df20" class="ly iw hi lt b fi md ma l mb mc">driver = webdriver.Chrome(service=service, options=options)</span></pre><p id="13fe" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">在这里，我全局设置了一个Chromedriver，供我们稍后将编写的几个函数使用。我创建了一个<code class="du lq lr ls lt b">Service</code>对象，它指定了Chromedriver的路径，以及一个<code class="du lq lr ls lt b">Options</code>的实例，它使我们能够在匿名模式下最大化地打开Chrome。接下来，我打开Chrome，将我们的<code class="du lq lr ls lt b">Service</code>和<code class="du lq lr ls lt b">Options</code>实例传递给<code class="du lq lr ls lt b">webdriver.Chrome()</code>。这将打开Chrome，如果你按原样运行代码的话，什么也不做。无论如何，这完成了我们的初始设置。现在我们将继续刮削。</p><h1 id="fc6e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">最初的刮擦步骤</h1><p id="503e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在我们开始降低黄金价格之前，我们必须克服几个障碍。其中一个障碍是，有62个独立的服务器可以购买黄金，并且它们都有不同的URL。见下图来自<a class="ae iu" href="https://www.g2g.com/categories/lost-ark-gold" rel="noopener ugc nofollow" target="_blank">https://www.g2g.com/categories/lost-ark-gold</a>。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/308231e6f80a6e82ead83e908bb024e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4t5qAeiEmZ1FCQobCrBZww.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">这些服务器中的每一个都有我们需要访问的不同的URL！作者图片</figcaption></figure><p id="22ac" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">让我们获取所有这些服务器的URL，因为我们想要从所有这些服务器的<strong class="jv hj"><em class="mf"/></strong>中抓取数据。我们想查看页面的HTML代码，看看在哪里可以找到它们。在Chrome中，我们只需在<a class="ae iu" href="https://www.g2g.com/categories/lost-ark-gold" rel="noopener ugc nofollow" target="_blank">https://www.g2g.com/categories/lost-ark-gold</a>导航到合适的URL，右键点击页面并点击inspect。接下来，我们希望查看页面上某个元素的HTML代码，特别是offer框，这样我们就可以知道在哪里可以找到指向该服务器黄金价格的URL。在Chrome中，我们可以通过点击侧边栏左上角的按钮来检查特定的元素，只需将鼠标悬停在我们想要查看其HTML代码的元素上。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/a3151b434d3e78e8403c4ac425188bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*uU1Ul4SuvPB0GdApw811dg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">选择按钮以选择元素。作者图片</figcaption></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/4f95383dc28216d5a5f5f79451b9ef25.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*urViPOjr-dTBhpLoRjJShA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">将鼠标悬停在我们想要的项目上。作者图片</figcaption></figure><p id="6b4d" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">当点击上图中的项目时，该元素的HTML代码会在侧边栏中显示出来。在检查代码时，我们可以看到，在我们正在查看的HTML代码部分中只有一个URL，服务器的所有元素都有相似的代码，如下图所示。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/90661dff62077977ea1bbabaff4b1b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*6nxazIosld1BmZdQw7LziA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">这些服务器由一个类为“col-12 col-md-3”的“div”元素分隔。作者图片</figcaption></figure><p id="6b61" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">现在我们已经准备好抓取URL了。在初始化了之前的Chromedriver之后，我执行了以下代码:</p><pre class="ll lm ln lo fd lu lt lv lw aw lx bi"><span id="4f2d" class="ly iw hi lt b fi lz ma l mb mc">URL = '<a class="ae iu" href="https://www.g2g.com/categories/lost-ark-gold" rel="noopener ugc nofollow" target="_blank">https://www.g2g.com/categories/lost-ark-gold</a>'</span><span id="135f" class="ly iw hi lt b fi md ma l mb mc"># initialize an empty list to store the URLs in<br/>links = []</span><span id="b032" class="ly iw hi lt b fi md ma l mb mc">driver.get(URL)<br/>time.sleep(5) # my internet is slow, so this pauses to load it in<br/>html = driver.page_source<br/>soup = bs(html, features='html.parser')</span><span id="9971" class="ly iw hi lt b fi md ma l mb mc">boxes = soup.find_all('div', class_='col-12 col-md-3')<br/>for box in boxes:<br/>    links.append(box.find('a', href=True)['href'])</span></pre><p id="c9e2" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">上面，我们告诉我们的驱动程序转到我们一直在检查的URL，并允许它使用<code class="du lq lr ls lt b">time.sleep(5)</code>行加载(由于我的网速很慢，这对你来说可能是不必要的)。接下来，我们使用<code class="du lq lr ls lt b">html = driver.page_source</code>获取网页的HTML然而，如果我们照原样打印HTML，它将是一堆混乱的、不可理解的文本。因此，我们使用BeautifulSoup4来解析HTML代码，使它对我们来说更具可读性。对于“features”参数，您可以使用您喜欢的HTML解析器，但是为了简单起见，我喜欢使用html.parser。在这之后，<code class="du lq lr ls lt b">boxes = soup.find_all(...)</code>行将返回一个所有“div”元素的列表，这些元素的类等于“col-12 col-md-3”，我们将它标识为“失落的方舟”黄金销售页面上每个服务器的HTML代码。然后，我们遍历每个“box”元素，提取我们遇到的每个服务器的URL。</p><p id="e9c2" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">向下滚动到页面底部，我们看到服务器列在两个不同的页面上:</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/2946858ef6040c7ac57faedeb040a84c.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*mzj9376KneX4QL_ZUl8mww.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="e1a3" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">为了解决这个问题，我使用了强力方法，因为合理的假设是页面数量在不久的将来不会增加。我只是对游戏服务器的第二页重复上述过程:</p><pre class="ll lm ln lo fd lu lt lv lw aw lx bi"><span id="7d12" class="ly iw hi lt b fi lz ma l mb mc">URL2 = URL + '?page=2'<br/>driver.get(URL2)<br/>time.sleep(5)<br/>html = driver.page_source<br/>...</span></pre><p id="787b" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">这种强力方法可以满足本文的目的。现在我们有了每个服务器的URL，我们将继续讨论如何收集数据。</p><h1 id="519a" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">导航多个分页页面</h1><p id="e65e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了简单起见，我将演示如何获取一台服务器的所有黄金价格。如果你对我如何浏览每一个服务器并处理一些发生的异常感兴趣，你可以在这里查看这个项目<a class="ae iu" href="https://github.com/cbarger233/Lost-Ark-g2g-Scraping-and-Dashboard-App/blob/main/gold_scraping.py" rel="noopener ugc nofollow" target="_blank">的原始代码。</a></p><p id="b09f" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">我们将查看Azena的黄金价格，它应该是“链接”列表中的第一个(Python中的第零个)链接，而不是链表。通过设置<code class="du lq lr ls lt b">azena_link = links[0]</code>可以轻松获得链接。首先，让我们看看页面是如何设置的，以及我们将如何计划收集数据。导航到黄金列表页面，我们看到它们都列在一个表格中，由四个不同的页面分隔。就像我们对链接所做的那样，我们想要识别每个列表的HTML代码。这一次，我们还希望我们的Chromedriver为我们在页面之间导航，这样我们就不必手动操作了。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/fefb6141b6a582eab1483f7491b11a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cHeZ6jsJo6BPapnYxNETRA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="6107" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi">…</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/85a55437a11b8c24e6170fa35d74031e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxDnbfutfyAQ6nT1fQHcvA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">底部表示我们有四页要浏览。作者图片</figcaption></figure><p id="545c" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">上面我们看到，这个特定的服务器，至少在撰写本文时，有四页的黄金价格。为了浏览所有四个页面，我们需要收集初始页面上的数据，单击下一页按钮，收集第二页上的数据，单击按钮，等等。因此，我们需要单击按钮进入下一页，直到它不再出现，同时收集我们想要的数据。为此，我通常使用下面的助手函数:</p><pre class="ll lm ln lo fd lu lt lv lw aw lx bi"><span id="8485" class="ly iw hi lt b fi lz ma l mb mc">def check_exists_by_xpath(driver, xpath):<br/>    try:<br/>        driver.find_element(By.XPATH, xpath)<br/>    except NoSuchElementException:<br/>        return False<br/>    return True</span></pre><p id="46d7" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">在这个函数中，我通过使用元素的XPATH来检查元素是否存在，这可以通过检查HTML代码来找到，类似于我们前面所做的。只需右键单击元素的HTML代码，就会出现复制其XPATH的选项，如下所示:</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/fb5c5a53de358b1ddb4eef0dc5f2ab1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*nlVUFEzcG3jQdoBBvxhkOg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="a03b" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">在我们的例子中，我们特别寻找包含文本'&gt;'的按钮。因此，我们希望使用我们的函数来检查该按钮是否存在:</p><pre class="ll lm ln lo fd lu lt lv lw aw lx bi"><span id="de13" class="ly iw hi lt b fi lz ma l mb mc">if check_exists_by_xpath(driver, "//a[contains(text(), '&gt;')]"):<br/>    element = driver.find_element(By.XPATH, "//a[contains(text(), '&gt;')]")<br/>    driver.execute_script('arguments[0].scrollIntoView();', element)<br/>    driver.execute_script('window.scrollBy(0, -200);')<br/>    element.click()</span><span id="ad6b" class="ly iw hi lt b fi md ma l mb mc">else:<br/>    print('No next page!')</span></pre><p id="3e7f" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">在上面的代码块中，我们使用helper函数检查按钮是否存在，如果存在则返回<code class="du lq lr ls lt b">True</code>，如果不存在则返回<code class="du lq lr ls lt b">False</code>。如果按钮确实存在，我们通过执行几行脚本滚动到它的位置，然后单击它进入下一页。现在，我们可以简单地执行类似这样的代码块，直到按钮不存在，在这种情况下，我们会看到消息“没有下一页！”当然，我们会希望一路刮走黄金价格(以及你想要的任何其他信息)。</p><p id="748d" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">我们基本上完成了！现在我们要做的就是收集数据。与我们之前所做的类似，检查任何gold列表以查看其HTML代码的结构。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/8699b5c08c51e93d83f488465bd2a29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65u0HB1nIr-JYITYHy6wkg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="3452" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">通过检查，我们看到这些“报价箱”中的每一个都包含在一个“div”中，其“class”等于“other _ offer-desk-main-box other _ offer-div-box”。我们将希望使用BeautifulSoup4来专门搜索这些内容，并将它们存储在一个列表中。</p><pre class="ll lm ln lo fd lu lt lv lw aw lx bi"><span id="c43d" class="ly iw hi lt b fi lz ma l mb mc">offer_boxes = soup.find_all('div', class_='other_offer-desk-main-box other_offer-div-box')</span></pre><p id="2553" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">现在，我们浏览每个报价框，提取我们想要的数据。</p><pre class="ll lm ln lo fd lu lt lv lw aw lx bi"><span id="0a28" class="ly iw hi lt b fi lz ma l mb mc">for box in offer_boxes:<br/>    name = box.find('div', class_='seller__name-detail').text.strip()<br/>    price = box.find('span', class_='offer-price-amount').text.strip()<br/>    price = re.sub(',', '', price)<br/>    price = float(price)</span></pre><p id="ff75" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">在上面的代码块中，我取出了卖家的名字和他们出售黄金的价格。我使用regex来确保价格没有不必要的字符，并将其转换为浮点值。就是这样！在我的原始代码中，我将值存储在列表中，并将其转换为pandas数据帧，以便以后转换为。csv文件。如何组织数据以及想要收集什么取决于你自己。同样，如果你想更详细地查看我的代码，你可以在这里查看原始代码<a class="ae iu" href="https://github.com/cbarger233/Lost-Ark-g2g-Scraping-and-Dashboard-App/blob/main/gold_scraping.py" rel="noopener ugc nofollow" target="_blank"/>。你可以在这里查看我为这个项目<a class="ae iu" href="https://lost-ark-gold-prices.herokuapp.com/" rel="noopener ugc nofollow" target="_blank">制作的仪表盘。</a></p></div></div>    
</body>
</html>