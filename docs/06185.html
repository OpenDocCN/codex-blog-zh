<html>
<head>
<title>Setting up a Multi-Node Apache Spark Cluster on Apache Hadoop and Apache Hive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Apache Hadoop和Apache Hive上设置多节点Apache Spark集群</h1>
<blockquote>原文：<a href="https://medium.com/codex/setting-up-a-multi-node-apache-spark-cluster-on-apache-hadoop-and-apache-hive-412764ab6881?source=collection_archive---------4-----------------------#2022-04-12">https://medium.com/codex/setting-up-a-multi-node-apache-spark-cluster-on-apache-hadoop-and-apache-hive-412764ab6881?source=collection_archive---------4-----------------------#2022-04-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f9d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Spark是一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习。在这篇文章中，我将在Apache Hadoop 3.3.0上安装Apache Spark 3.0.1。由于应该有一个正在运行的Apache Hadoop集群来设置Apache Spark集群，我建议使用我以前的博客文章<a class="ae jd" rel="noopener" href="/codex/running-a-multi-node-hadoop-cluster-257068e5f276">运行多节点Hadoop集群</a>来设置您的Apache Hadoop集群。</p><div class="je jf ez fb jg jh"><a rel="noopener follow" target="_blank" href="/codex/running-a-multi-node-hadoop-cluster-257068e5f276"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">运行多节点Hadoop集群</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">Apache Hadoop软件库是一个框架，允许跨…分布式处理大型数据集</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">medium.com</p></div></div><div class="jq l"><div class="jr l js jt ju jq jv jw jh"/></div></div></a></div><figure class="jy jz ka kb fd kc er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es jx"><img src="../Images/77bfcf3609332118c17be3500c66cc41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rq98phowGgaUatq-Hx90PQ.png"/></div></div><figcaption class="ki kj et er es kk kl bd b be z dx translated"><a class="ae jd" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇火花</a></figcaption></figure><p id="f35b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我可能会使用一些Scala代码和CentOS (RHEL)命令。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="af9f" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">设置Apache Spark环境</h1><p id="8c80" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi lw translated">运行Hadoop框架后，我们可以开始设置Apache Spark。我将假设您已经使用我的帖子<a class="ae jd" rel="noopener" href="/codex/running-a-multi-node-hadoop-cluster-257068e5f276">运行多节点Hadoop集群</a>设置了Hadoop框架，并在同一页面上。现在，下载并复制文件给所有工人。</p><pre class="jy jz ka kb fd mf mg mh mi aw mj bi"><span id="5061" class="mk ku hi mg b fi ml mm l mn mo">[hduser@master ~]# wget {Apache Spark LINK (I downloaded spark-3.0.1-bin-hadoop3.2.tgz)}<br/>[hduser@master ~]# scp spark-3.0.1-bin-hadoop3.2.tgz hduser@worker1:/home/hduser/spark-3.0.1-bin-hadoop3.2.tgz<br/>[hduser@master ~]# scp spark-3.0.1-bin-hadoop3.2.tgz hduser@worker2:/home/hduser/spark-3.0.1-bin-hadoop3.2.tgz</span></pre><p id="f684" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在所有服务器</strong>上，提取Apache Spark，并创建一个软链接，以便将来能够简单地更改其版本。我将解压<code class="du mp mq mr mg b">/opt</code>目录中的所有文件。</p><pre class="jy jz ka kb fd mf mg mh mi aw mj bi"><span id="5c27" class="mk ku hi mg b fi ml mm l mn mo">[hduser@{server} ~]# cd /opt<br/>[hduser@{server} /opt]# sudo tar xzf /home/hduser/spark-3.0.1-bin-hadoop3.2.tgz<br/>[hduser@{server} /opt]# sudo ln -s spark-3.0.1-bin-hadoop3.2/ spark<br/>[hduser@{server} /opt]# sudo chown -R hduser:hadoop spark<br/>[hduser@{server} /opt]# sudo chown -R hduser:hadoop spark-3.0.1-bin-hadoop3.2</span></pre><h1 id="f052" class="kt ku hi bd kv kw ms ky kz la mt lc ld le mu lg lh li mv lk ll lm mw lo lp lq bi translated">配置Apache Spark</h1><p id="4ca6" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi lw translated"><span class="l lx ly lz bm ma mb mc md me di"> B </span>在开始配置过程之前，我应该提到在Apache Hadoop集群上配置Apache Spark集群是一个两步过程。首先，Spark应该知道Hadoop是如何工作的，然后它应该知道它应该如何工作！让我给你看看代码。将Hadoop的配置文件复制到所有服务器上Spark的配置目录中。所有的Spark配置文件都在目录<code class="du mp mq mr mg b">/pot/spark/conf</code>下。</p><pre class="jy jz ka kb fd mf mg mh mi aw mj bi"><span id="bae0" class="mk ku hi mg b fi ml mm l mn mo">[hduser@{server} ~]# cd <!-- -->/pot/spark/conf<br/>[hduser@{server} <!-- -->/opt/spark/conf<!-- -->]# cp /opt/hadoop/etc/hadoop/core-site.xml .<br/>[hduser@{server} <!-- -->/opt/spark/conf<!-- -->]# cp /opt/hadoop/etc/hadoop/hdfs-site.xml .<br/>[hduser@{server} <!-- -->/opt/spark/conf<!-- -->]# cp /opt/hadoop/etc/hadoop/yarn-site.xml .</span></pre><p id="8285" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在主服务器上，编辑Spark的配置文件，并将它们复制到workers。</p><pre class="jy jz ka kb fd mf mg mh mi aw mj bi"><span id="ac37" class="mk ku hi mg b fi ml mm l mn mo">[hduser@master ~]# cd <!-- -->/pot/spark/conf<br/>[hduser@master <!-- -->/opt/spark/conf<!-- -->]# cp slaves.template slaves<br/>[hduser@master <!-- -->/opt/spark/conf<!-- -->]# cp spark-env.sh.template spark-env.sh<br/>[hduser@master <!-- -->/opt/spark/conf<!-- -->]# cp log4j.properties.template log4j.properties<br/>[hduser@master <!-- -->/opt/spark/conf<!-- -->]# vi slaves</span><span id="ef9a" class="mk ku hi mg b fi mx mm l mn mo"># Add the names of all workers<br/>master<br/>worker1<br/>worker2</span><span id="b7d8" class="mk ku hi mg b fi mx mm l mn mo">[hduser@master <!-- -->/opt/spark/conf<!-- -->]# vi spark-env.sh</span><span id="f5e4" class="mk ku hi mg b fi mx mm l mn mo"># Add these lines<br/>export JAVA_HOME=/opt/jdk</span><span id="4571" class="mk ku hi mg b fi mx mm l mn mo">export SPARK_HOME=/opt/spark<br/>export SPARK_CONF_DIR=$SPARK_HOME/conf<br/>export HADOOP_HOME=/opt/hadoop<br/>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop<br/>export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><span id="4e06" class="mk ku hi mg b fi mx mm l mn mo">export SPARK_WORKER_DIR=/tmp/spark/work</span><span id="fa80" class="mk ku hi mg b fi mx mm l mn mo">[hduser@master <!-- -->/opt/spark/conf<!-- -->]# vi log4j.properties</span><span id="5984" class="mk ku hi mg b fi mx mm l mn mo"># Add this line<br/>log4j.logger.org=OFF</span><span id="fbd0" class="mk ku hi mg b fi mx mm l mn mo">[hduser@master <!-- -->/opt/spark/conf<!-- -->]# scp log4j.properties spark-env.sh hduser@{slaves}:/opt/spark/conf/</span></pre><p id="6b5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，编辑<code class="du mp mq mr mg b">.bashrc</code>文件，并添加这几行。</p><pre class="jy jz ka kb fd mf mg mh mi aw mj bi"><span id="f4df" class="mk ku hi mg b fi ml mm l mn mo"># Spark<br/>export SPARK_HOME=/opt/spark<br/>export SPARK_CONF_DIR=$SPARK_HOME/conf<br/>export PATH=$SPARK_HOME/bin:$PATH</span></pre><h1 id="2b64" class="kt ku hi bd kv kw ms ky kz la mt lc ld le mu lg lh li mv lk ll lm mw lo lp lq bi translated">提供Apache火花罐文件</h1><p id="e39e" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi lw translated">Apache Spark的配置已经完成，但是在提交任何Spark应用程序之前，我们应该提供Spark JAR文件。简单地说，在任何服务器上运行这些命令！</p><pre class="jy jz ka kb fd mf mg mh mi aw mj bi"><span id="7030" class="mk ku hi mg b fi ml mm l mn mo">[hduser@{server} ~]# hdfs dfs -mkdir -p /user/spark/share/lib<br/>[hduser@{server} ~]# hadoop fs -put /opt/spark/jars/* /user/spark/share/lib/</span></pre><p id="b06b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，您可以提交Spark应用程序，并在任何服务器上使用类似这样的命令来使用Apache HDFS和Apache Yarn。</p><pre class="jy jz ka kb fd mf mg mh mi aw mj bi"><span id="ad7b" class="mk ku hi mg b fi ml mm l mn mo">[hduser@{server} ~]# /opt/spark/bin/spark-submit \<br/>--master yarn --deploy-mode cluster \<br/>--class com.github.saeiddadkhah.hadoop.spark.Application \<br/>--driver-memory 2G --driver-cores 2 \<br/>--executor-memory 1G --executor-cores 1\<br/>--name MyApp \<br/>--num-executors 10 \<br/>MyApp.jar</span></pre><p id="5b78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您应该在应用程序中为Apache Spark提供一些其他配置。下面的代码是Scala代码，但是Apache Spark为其他语言提供了类似的API。</p><figure class="jy jz ka kb fd kc"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated">火花配置</figcaption></figure><h1 id="2eec" class="kt ku hi bd kv kw ms ky kz la mt lc ld le mu lg lh li mv lk ll lm mw lo lp lq bi translated">添加Apache配置单元支持</h1><p id="cc21" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi lw translated">决定是否在Spark应用程序中使用Apache Hive是您的责任，这完全取决于用例以及您的偏好。在这里，我将只配置Apache Spark来使用Apache Hive 3.1.2。自然，我们需要一个运行中的Apache Hive，好消息是您可以使用我以前的帖子<a class="ae jd" rel="noopener" href="/codex/setting-up-an-apache-hive-data-warehouse-6074775cf66">设置Apache Hive数据仓库</a>来设置它。</p><div class="je jf ez fb jg jh"><a rel="noopener follow" target="_blank" href="/codex/setting-up-an-apache-hive-data-warehouse-6074775cf66"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">设置Apache Hive数据仓库</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">Apache Hive数据仓库软件有助于在Hadoop中使用SQL读取、写入和管理大型数据集…</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">medium.com</p></div></div><div class="jq l"><div class="na l js jt ju jq jv jw jh"/></div></div></a></div><p id="a815" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，将Hive的配置文件复制到所有服务器上Spark的配置目录中。</p><pre class="jy jz ka kb fd mf mg mh mi aw mj bi"><span id="3936" class="mk ku hi mg b fi ml mm l mn mo">[hduser@{server} ~]# cd <!-- -->/pot/spark/conf<br/>[hduser@{server} <!-- -->/opt/spark/conf<!-- -->]# cp /opt/hive/conf/hive-site.xml .</span></pre><p id="854e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您还应该在应用程序中添加其他配置来启用配置单元支持。</p><figure class="jy jz ka kb fd kc"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated">带配置单元的火花配置</figcaption></figure></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><p id="e9ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请继续关注下一篇文章来设置更多的工具。我希望你喜欢这个教程。谢谢你。</p></div></div>    
</body>
</html>