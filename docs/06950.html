<html>
<head>
<title>An Anatomy of the Support Vector Machines.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机剖析。</h1>
<blockquote>原文：<a href="https://medium.com/codex/an-anatomy-of-the-support-vector-machines-19e1cdcbe4c5?source=collection_archive---------9-----------------------#2022-05-21">https://medium.com/codex/an-anatomy-of-the-support-vector-machines-19e1cdcbe4c5?source=collection_archive---------9-----------------------#2022-05-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/c59f86f91128d5dc307e412752673ce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*n6l5sl6PwhdLbDav"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">照片由<a class="ae hv" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> JJ英</a>在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><div class=""/><h1 id="d095" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="fa9e" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">支持向量机(SVM)是一种监督机器学习算法，同样强大和通用。它能够执行线性或非线性分类和回归，还可以执行异常值检测等任务。这是机器学习中最广为人知的模型之一，任何热衷于机器学习的人都必须学习这个模型。它非常适合于复杂但小型或中型数据集的分类。</p><h1 id="b93b" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">说明</h1><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es kr"><img src="../Images/7785c34400f5548b7f45b53e32528ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*uwUjTCm-rB3wGtu2V-jRxg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">从iconspng.com检索的图像</figcaption></figure><p id="dcad" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">让我们更生动地讨论线性SVM分类。借助一张图来解释任何概念都很方便。看一下上面的图片，它显示了一些绘制在图表上的数据点。数据点由一条红线分隔，这条红线称为“决策边界”。这条直线清楚地将数据点分成两组或两类，这表明数据点是线性可分的。这仅仅意味着通过这些点的一条直线可以将它们分成两个不同的类，从而成为一个很好的线性分类器。该直线不仅将两个类别分开，而且尽可能远离最近的训练数据点。</p><p id="59cf" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">您可以将SVM分类器视为适合最宽的街道，其中街道由类之间中间的红线表示。这就是所谓的大幅度分类。当我们在“街道之外”添加更多的训练实例时，它根本不会影响决策边界，因为它完全由位于街道边缘的数据点的示例来确定或“支持”。这些实例被称为支持向量(用红色圈出)。</p><p id="a461" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">如果我们严格规定所有示例数据点必须远离红色决策边界线并位于右侧，这就是所谓的硬边界分类。硬边界分类产生了问题，因为它仅对线性可分的数据点有效，并且它对异常值非常敏感，在某些情况下可能不能很好地概括。</p><p id="10c0" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">因此，使用不太严格的模型更为可取。我们这里的主要目标是通过保持决策边界尽可能大来找到一个好的平衡，同时也限制边界违规。限制边界违规是指数据点位于决策边界的中间或位于错误的一边。该分类遵循上述条件，允许一些固定数量的边界违规，并且具有多一点的灵活性，被称为软边界分类。</p><p id="ad12" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">在scikit-learn的SVM类中，我们可以使用C超参数来控制这种平衡。较小的C导致较宽的决策边界，但更多的边际违规，另一方面，较高的C值导致较少的边际违规，但最终得到较小的决策边界或边际。有时，如果边际违规在边际的正确一侧，边际违规可以导致更少的预测错误。</p><h1 id="b5f9" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">用Python实现</h1><p id="b59a" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">以下scikit-learn代码加载iris数据集，缩放要素，并使用C=1的LinearSVC类和铰链损失函数来训练线性SVM模型以检测Iris-Virginica花朵。</p><pre class="ks kt ku kv fd lb lc ld le aw lf bi"><span id="b3af" class="lg iw hy lc b fi lh li l lj lk">import numpy as np<br/>from sklearn import datasets<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.svm import LinearSVC<br/>iris = datasets.load_iris()<br/>x = iris[“iris_data”][:, (2, 3)] # petal length, petal width<br/>y = (iris[“iris_target”] == 2).astype(np.float64) # Iris-Virginica<br/>svm_clf = Pipeline([<br/>(“scaler”, StandardScaler()),<br/>(“linear_svc”, LinearSVC(C=1, loss=”hinge”)),<br/>])<br/>svm_clf.fit(x, y)<br/>svm_clf.predict([[5.5, 1.7]])<br/>output — array([1.])</span></pre><p id="3004" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">SVM分类器不像逻辑回归分类器那样输出每个类别的概率。我们也可以使用SVC(kernel=" linear "，C=1)来使用SVC类，但是由于它非常慢，特别是对于大型训练集，所以不建议这样做。另一个替代方案是使用SGDClassifier类，其中SGDClassifier(loss="hinge "，alpha=1/(m*C))。该技术使用规则随机梯度下降来训练线性SVM分类器。它的收敛速度不如LinearSVC类快，但在处理不适合内存的大型数据集或处理在线分类任务时，它会很有用。</p><p id="905e" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">LinearSVC类调整偏差，因此我们必须首先通过减去其平均值来确定训练数据的中心。如果使用StandardScaler缩放数据，这将自动完成。此外，我们应该确保将损耗超参数设置为“铰链”，因为它不是默认值。</p><p id="0176" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">因此，为了获得更好的性能，您应该将对偶超参数设置为False，除非有比训练示例更多的特征。也有可能进行非线性和多项式SVM分类，我希望在另一篇博客中讨论。</p></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="4f53" class="iv iw hy bd ix iy ls ja jb jc lt je jf jg lu ji jj jk lv jm jn jo lw jq jr js bi translated">结束语</h1><p id="f513" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">今天到此为止。非常感谢你阅读我的文章并支持我。:)</p><p id="b00c" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">想了解我更多，想获得更多类似的内容，请在我的LinkedIn、Twitter或脸书页面上关注我</p><p id="25b3" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">领英—<a class="ae hv" href="https://www.linkedin.com/in/salman-ibne-eunus-09255a144/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/salman-ibne-eunus-09255a144/</a></p><p id="8b36" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">推特——https://twitter.com/ibne_eunus<a class="ae hv" href="https://twitter.com/ibne_eunus" rel="noopener ugc nofollow" target="_blank"/></p><p id="797e" class="pw-post-body-paragraph jt ju hy jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">https://www.facebook.com/salmaneunus27脸书—<a class="ae hv" href="https://www.facebook.com/salmaneunus27" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>