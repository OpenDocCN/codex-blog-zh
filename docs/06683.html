<html>
<head>
<title>Feature Selection Algorithms for Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的特征选择算法</h1>
<blockquote>原文：<a href="https://medium.com/codex/feature-selection-algorithms-for-machine-learning-35507679eab?source=collection_archive---------2-----------------------#2022-05-07">https://medium.com/codex/feature-selection-algorithms-for-machine-learning-35507679eab?source=collection_archive---------2-----------------------#2022-05-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a4b4" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">选择正确的</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/4f7e7c75bf9ae34f3fe96184ec06228a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MKMLi7DqtYxM87Fz5Qrhbw.png"/></div></div></figure><p id="6992" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对于机器学习模型，特征选择是一个可选但重要的预处理步骤。一种常见的做法是在收到机器学习模型数据时就输入这些数据。这是一个非常常见的新手错误，你从任何来源得到的数据总是包含错误。在继续训练模型之前，您必须对这些数据执行两个重要步骤。</p><ol class=""><li id="8713" class="kf kg hi jl b jm jn jp jq js kh jw ki ka kj ke kk kl km kn bi translated">数据清理</li><li id="b9bd" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">特征过滤/选择</li></ol><p id="2a5f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我在其他文章中多次谈到数据清理，如果你想了解更多，那么你可以查看下面的<a class="ae kt" href="https://writersbyte.com/featured-post/applied-data-science-with-python-and-pandas/?swcfpc=1" rel="noopener ugc nofollow" target="_blank">博客文章</a>。</p><div class="ku kv ez fb kw kx"><a href="https://writersbyte.com/featured-post/applied-data-science-with-python-and-pandas/?swcfpc=1" rel="noopener  ugc nofollow" target="_blank"><div class="ky ab dw"><div class="kz ab la cl cj lb"><h2 class="bd hj fi z dy lc ea eb ld ed ef hh bi translated">Python和熊猫的应用数据科学</h2><div class="le l"><h3 class="bd b fi z dy lc ea eb ld ed ef dx translated">数据科学是一项非常重要的技能，已经成为21世纪的必备技能。随着数据的增加…</h3></div><div class="lf l"><p class="bd b fp z dy lc ea eb ld ed ef dx translated">writersbyte.com</p></div></div><div class="lg l"><div class="lh l li lj lk lg ll jh kx"/></div></div></a></div><h2 id="54d6" class="lm ln hi bd lo lp lq lr ls lt lu lv lw js lx ly lz jw ma mb mc ka md me mf mg bi translated">特征选择</h2><p id="69ae" class="pw-post-body-paragraph jj jk hi jl b jm mh ij jo jp mi im jr js mj ju jv jw mk jy jz ka ml kc kd ke hb bi translated">然而，在本文中，我们将讨论如何从数据集中选择重要的输入要素，从而使其对输出的影响最小并降低数据的复杂性。</p><p id="930f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">首先，让我们谈谈输入变量之间的相关性如何影响机器学习模型。</p><h2 id="1ce1" class="lm ln hi bd lo lp lq lr ls lt lu lv lw js lx ly lz jw ma mb mc ka md me mf mg bi translated">特征之间的相关性</h2><p id="e530" class="pw-post-body-paragraph jj jk hi jl b jm mh ij jo jp mi im jr js mj ju jv jw mk jy jz ka ml kc kd ke hb bi translated">假设您有一个包含以下2个要素的数据集，输出标记为Y，如下图所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mm"><img src="../Images/fbe85ea2978ae249efd83296f743010f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_YCOX3yHxNA---EAZUp1A.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">具有冗余特征的数据集</figcaption></figure><p id="c00d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">变量x_2和x_3在整个数据集中具有完全相同的值。机器学习模型学习对应于每个输出变量的输入变量中的给定模式。现在，对于具有高度相关性的变量(上图中的x2和x3)，其中一个变量没有为我们的模型提供任何有价值的信息。这是因为它们具有相同的模式，就好像它们是相同的变量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mm"><img src="../Images/8c5fbe22bf0d49e9b0dbf3ff3191ee54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qm80lts4S4E2MGJFYjwodg.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">仅包含重要特征的精简数据集</figcaption></figure><p id="c8c0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于这些变量中的一个没有给模型增加额外的信息，我们可以完全删除该特征。你可能会问，保留这样一个变量有什么坏处？</p><p id="4512" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">嗯…</p><ol class=""><li id="0f7a" class="kf kg hi jl b jm jn jp jq js kh jw ki ka kj ke kk kl km kn bi translated">它不必要地增加了数据的维度。这增加了训练时间，我们可能会遇到<a class="ae kt" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维数灾难</a>的问题。</li><li id="ef0d" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">虽然在大多数模型中，保留该特征可能不会使模型变得“更糟”,但是这种高度相关的变量对不同模型的影响是不同的。这些可能会给模型带来混乱，从而降低性能。</li></ol><p id="b90d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果你觉得我的工作有帮助，考虑在科菲上支持我。点击下面的图片。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><a href="http://ko-fi.com/moosaali9906"><div class="er es mr"><img src="../Images/a9b75b9a2d9b6bf73b23151d0a95f04f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAVsw7Mcvsh8aJL3LuG6bg.png"/></div></a><figcaption class="mn mo et er es mp mq bd b be z dx translated">点击图片以示支持</figcaption></figure><h2 id="8b8d" class="lm ln hi bd lo lp lq lr ls lt lu lv lw js lx ly lz jw ma mb mc ka md me mf mg bi translated">算法</h2><p id="389c" class="pw-post-body-paragraph jj jk hi jl b jm mh ij jo jp mi im jr js mj ju jv jw mk jy jz ka ml kc kd ke hb bi translated">在这一点上，它可能看起来很简单，只是从2个特性中随机删除一个特性。然而，在现实生活中，您很少会发现在整个数据集中显示完全相同的模式/值的变量，因此在移除一个变量或决定两个变量是否高度相关之前，还有一些事情要考虑。</p><p id="1425" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这就是为什么有专门的算法为你决定，哪些变量要保留，哪些要删除。我们将讨论两个这样的算法。</p><h2 id="2a3a" class="lm ln hi bd lo lp lq lr ls lt lu lv lw js lx ly lz jw ma mb mc ka md me mf mg bi translated">Boruta特征选择</h2><p id="2855" class="pw-post-body-paragraph jj jk hi jl b jm mh ij jo jp mi im jr js mj ju jv jw mk jy jz ka ml kc kd ke hb bi translated">Boruta特征选择算法最初是作为r的一个包引入的。它是一个非常有用的算法，可以定义自己的阈值，并从提供的数据集中为您提供最准确的特征。</p><p id="da38" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">关于Boruta的完整解释和实现可以在<a class="ae kt" href="https://writersbyte.com/programming/boruta-feature-selection-explained-in-python/?swcfpc=1" rel="noopener ugc nofollow" target="_blank">这里找到</a>:</p><div class="ku kv ez fb kw kx"><a href="https://writersbyte.com/programming/boruta-feature-selection-explained-in-python/?swcfpc=1" rel="noopener  ugc nofollow" target="_blank"><div class="ky ab dw"><div class="kz ab la cl cj lb"><h2 class="bd hj fi z dy lc ea eb ld ed ef hh bi translated">用Python - WritersByte解释Boruta特征选择</h2><div class="le l"><h3 class="bd b fi z dy lc ea eb ld ed ef dx translated">本文旨在解释，非常流行的，Boruta特征选择算法。博鲁塔自动化的过程…</h3></div><div class="lf l"><p class="bd b fp z dy lc ea eb ld ed ef dx translated">writersbyte.com</p></div></div><div class="lg l"><div class="ms l li lj lk lg ll jh kx"/></div></div></a></div><p id="7433" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Boruta对提供的输入特征(每个特征列单独)进行混洗，然后将这些特征(称为阴影特征)与原始数据连接起来。此后，使用随机森林分类器训练完整的数据集。该分类器返回整个输入的特征重要性。然后，博鲁塔将阈值设置为<strong class="jl hj">最强的混洗(阴影)特征。</strong></p><p id="4bf3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">重要性级别低于最重要的混洗特征的任何真实特征被丢弃。Boruta有一个python包可以帮助你计算特征。下面演示了它是如何工作的。</p><pre class="iy iz ja jb fd mt mu mv mw aw mx bi"><span id="00b9" class="lm ln hi mu b fi my mz l na nb"># install the package<br/>!pip install boruta</span><span id="e350" class="lm ln hi mu b fi nc mz l na nb"># import important libraries<br/>import pandas as pd<br/>from boruta import BorutaPy<br/>from sklearn.ensemble import RandomForestRegressor<br/>import numpy as np</span></pre><p id="dcbc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，我们加载数据集并清理它，就像删除NaN值并将分类变量转换为数字表示一样。</p><pre class="iy iz ja jb fd mt mu mv mw aw mx bi"><span id="baff" class="lm ln hi mu b fi my mz l na nb">#load data<br/>heart_data = pd.read_csv("healthcare-dataset-stroke-data.csv")</span><span id="7d63" class="lm ln hi mu b fi nc mz l na nb"># converting to numeric</span><span id="30d1" class="lm ln hi mu b fi nc mz l na nb">heart_data["gender"] = pd.factorize(heart_data["gender"])[0]<br/>heart_data["ever_married"] = pd.factorize(heart_data["ever_married"])[0]<br/>heart_data["work_type"] = pd.factorize(heart_data["work_type"])[0]<br/>heart_data["Residence_type"] = pd.factorize(heart_data["Residence_type"])[0]<br/>heart_data["smoking_status"] = pd.factorize(heart_data["smoking_status"])[0]</span><span id="bed8" class="lm ln hi mu b fi nc mz l na nb"># additional cleaning<br/>heart_data.dropna(inplace =True)<br/>heart_data.drop("id", axis =1, inplace = True)</span><span id="95c5" class="lm ln hi mu b fi nc mz l na nb">heart_data.head()</span></pre><p id="f94b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最终数据集如下图所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/14bc826f8ef3cc9e4bb1c9779834e23c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W7me8CikqhphBlJmRJF9aA.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">心脏中风数据集</figcaption></figure><p id="7928" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在让我们运行博鲁塔算法。</p><pre class="iy iz ja jb fd mt mu mv mw aw mx bi"><span id="e0a3" class="lm ln hi mu b fi my mz l na nb">X = heart_data.drop("stroke", axis = 1)<br/>y = heart_data["stroke"]</span><span id="3792" class="lm ln hi mu b fi nc mz l na nb"># we will use the randomforest algorithm<br/>forest = RandomForestRegressor(n_jobs = -1,max_depth = 10)<br/># initialize boruta<br/>boruta = BorutaPy(estimator = forest, n_estimators = 'auto',max_iter = 50,)</span><span id="4ab4" class="lm ln hi mu b fi nc mz l na nb"># Boruta accepts np.array <br/>boruta.fit(np.array(X), np.array(y))</span><span id="f74b" class="lm ln hi mu b fi nc mz l na nb"># get results<br/>green_area = X.columns[boruta.support_].to_list()<br/>blue_area = X.columns[boruta.support_weak_].to_list()<br/>print('Selected Features:', green_area)<br/>print('Blue area features:', blue_area)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ne"><img src="../Images/34fcf0900519bf2f01f45b01da1bdbeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*EVtVgiNLxzkn7oMfVh0LOQ.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">博鲁塔算法的结果</figcaption></figure><p id="2c90" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，在10个原始特性中，Boruta认为只有返回的2个特性是做出任何合理决策的最重要的特性。</p><h2 id="44c0" class="lm ln hi bd lo lp lq lr ls lt lu lv lw js lx ly lz jw ma mb mc ka md me mf mg bi translated">mRMR特征选择</h2><p id="1001" class="pw-post-body-paragraph jj jk hi jl b jm mh ij jo jp mi im jr js mj ju jv jw mk jy jz ka ml kc kd ke hb bi translated">MRMR代表<em class="nf">最大关联最小冗余。</em>当博鲁塔在这些特征中寻找最重要的特征时，<strong class="jl hj"> MRMR确保选择的特征不仅在输入特征之间提供最小的相关性，而且与输出变量有很高的相关性</strong>。</p><p id="b4f9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这个算法在下面的<a class="ae kt" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1423-9" rel="noopener ugc nofollow" target="_blank">论文</a>中首次介绍。</p><p id="a2a5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">MRMR以迭代方式工作，它首先会询问您想要保留多少个要素，然后在每次迭代中，它会计算1个与输出变量最相关且与数据集中的任何要素最不相关的要素。一旦选择了一个要素，就会将其从原始数据集中移除，并开始下一次迭代，直到K(我们需要的要素数)次迭代完成。</p><p id="869b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我将在另一篇文章中解释算法的细节。现在，让我们看看它的python实现。</p><p id="80e3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">使用以下命令安装python包</p><pre class="iy iz ja jb fd mt mu mv mw aw mx bi"><span id="04e7" class="lm ln hi mu b fi my mz l na nb">!pip install mrmr_selection</span></pre><p id="bb63" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你可以在官方的<a class="ae kt" href="https://github.com/smazzanti/mrmr" rel="noopener ugc nofollow" target="_blank"> Github库这里</a>找到这个包的完整文档。</p><p id="bcb0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">用法非常简单。</p><pre class="iy iz ja jb fd mt mu mv mw aw mx bi"><span id="5784" class="lm ln hi mu b fi my mz l na nb">from mrmr import mrmr_classif<br/>selected_features = mrmr_classif(X=X, y=y, K=2)</span></pre><p id="8d11" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我将K设置为2，只是为了看看所选择的特性是否与Boruta返回的匹配。</p><pre class="iy iz ja jb fd mt mu mv mw aw mx bi"><span id="951d" class="lm ln hi mu b fi my mz l na nb">print(selected_features)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/637bd25ea83b9852a615403b4eaefee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*Py-fpLS5z1-SPiIZO3FuBg.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">K=2时由MRMR返回的特征</figcaption></figure><p id="2dd0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">是的，我们有和上面的Boruta算法完全相同的特征。然而，使MRMR灵活的是，如果你认为2个功能可能不足以让你得到更好的结果，那么你可以选择使用你想要的。</p><p id="f50f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们再进行几次测试。</p><pre class="iy iz ja jb fd mt mu mv mw aw mx bi"><span id="ecf0" class="lm ln hi mu b fi my mz l na nb"># top 4 features<br/>top_4 = mrmr_classif(X=X, y=y, K=4)<br/># top 6 features<br/>top_6 = mrmr_classif(X=X, y=y, K=6)</span><span id="b43a" class="lm ln hi mu b fi nc mz l na nb">print("Best 4 features:", top_4)<br/>print("Best 6 features:", top_6)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nh"><img src="../Images/830a741d2db94687e4f05957503117c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxyY0M-PwqPDMC448G-j7w.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">MRMR为k = 4和k = 6返回的特征</figcaption></figure><h2 id="e606" class="lm ln hi bd lo lp lq lr ls lt lu lv lw js lx ly lz jw ma mb mc ka md me mf mg bi translated">结论</h2><p id="c9e9" class="pw-post-body-paragraph jj jk hi jl b jm mh ij jo jp mi im jr js mj ju jv jw mk jy jz ka ml kc kd ke hb bi translated">当内存资源不足时，特征选择是一个实时的保护程序，有时甚至可以帮助提高模型的性能。这是建立机器学习模型过程中必不可少的一步。</p><p id="0c74" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">考虑在科菲问题上支持我。点击下面的图片。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><a href="http://ko-fi.com/moosaali9906"><div class="er es mr"><img src="../Images/a9b75b9a2d9b6bf73b23151d0a95f04f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAVsw7Mcvsh8aJL3LuG6bg.png"/></div></a><figcaption class="mn mo et er es mp mq bd b be z dx translated">点击图片以示支持</figcaption></figure><p id="7f79" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">要了解更多关于机器学习的知识，你可以看看下面的文章。</p><div class="ku kv ez fb kw kx"><a href="https://writersbyte.com/featured-post/naive-bayes-python-implementation-and-understanding/" rel="noopener  ugc nofollow" target="_blank"><div class="ky ab dw"><div class="kz ab la cl cj lb"><h2 class="bd hj fi z dy lc ea eb ld ed ef hh bi translated">朴素贝叶斯Python实现和理解</h2><div class="le l"><h3 class="bd b fi z dy lc ea eb ld ed ef dx translated">朴素贝叶斯是基于条件概率的贝叶斯定理的机器学习分类器。在这个…</h3></div><div class="lf l"><p class="bd b fp z dy lc ea eb ld ed ef dx translated">writersbyte.com</p></div></div><div class="lg l"><div class="ni l li lj lk lg ll jh kx"/></div></div></a></div><div class="ku kv ez fb kw kx"><a href="https://writersbyte.com/datascience/implementing-multi-variable-linear-regression-algorithm-in-python/?swcfpc=1" rel="noopener  ugc nofollow" target="_blank"><div class="ky ab dw"><div class="kz ab la cl cj lb"><h2 class="bd hj fi z dy lc ea eb ld ed ef hh bi translated">多元线性回归Python实现。</h2><div class="le l"><h3 class="bd b fi z dy lc ea eb ld ed ef dx translated">机器学习算法在过去十年中获得了巨大的普及。今天，这些算法被用于…</h3></div><div class="lf l"><p class="bd b fp z dy lc ea eb ld ed ef dx translated">writersbyte.com</p></div></div><div class="lg l"><div class="nj l li lj lk lg ll jh kx"/></div></div></a></div></div></div>    
</body>
</html>