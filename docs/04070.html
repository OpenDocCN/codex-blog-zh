<html>
<head>
<title>Dimensionality Reduction on Face using PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于主成分分析的人脸降维</h1>
<blockquote>原文：<a href="https://medium.com/codex/dimensionality-reduction-on-face-using-pca-e3fec3bb4cee?source=collection_archive---------1-----------------------#2021-10-23">https://medium.com/codex/dimensionality-reduction-on-face-using-pca-e3fec3bb4cee?source=collection_archive---------1-----------------------#2021-10-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/06713df5a98fccbe5eb48871668284cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gUnKr_K1nywg-Js9"/></div></div></figure><p id="a078" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">机器学习有各种各样的降维技术。它是数据科学领域中最重要的方面之一。因此，在本文中，我将介绍当今使用的最重要的降维技术之一，即主成分分析(PCA)。</p><p id="a53b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但首先，我们需要了解什么是降维，为什么它如此重要。</p><h2 id="6e1f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">降维</h2><p id="5129" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">降维，也称为降维，是将数据从高维空间转换到低维空间，使得低维表示保留原始数据的一些有意义的属性，最好接近其底层维度。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/9892070ad578b6d0693985d0493cb577.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*sVGIAnkWlJUgeCWgiNvwXw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">还原前</figcaption></figure><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/4d40f9ecfadb4726315e815688848b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*d9EHnxUcdFbjY9Z6X1cWGg.png"/></div></figure><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/e22752b107fbabbf51ecfa8205023e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*a2Yq_525Jtog-SqNuY0AWw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">还原后</figcaption></figure><h2 id="5659" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">为什么有用？</h2><p id="7648" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">由于各种原因，使用高维空间可能不方便，包括原始数据由于维数灾难而经常是模糊的，并且处理数据通常在计算上是昂贵的。降维在诸如信号处理、语音识别、神经信息学和生物信息学等处理大量观察值和/或变量的领域中很流行。</p><h2 id="dd9b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">主成分分析</h2><p id="47d2" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">二维、三维或更高维度中的“最佳拟合”线可以定义为最小化从点到该线的平均垂直距离平方的线。第二条最佳拟合线可以用同样的方法从垂直于第一条线的方向选取。如果这对你来说似乎是胡言乱语，不要担心，我们将在这篇文章中看到这些事情的细节。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/98fb2d3ab1ed2fba24800d5e6e4de8ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*ypoiwT5CPoDRgApOu2-7Ow.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">PCA示例</figcaption></figure><p id="5d53" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">绿点是实际点，而蓝点是投影点。<br/>首先，PCA确定投影数据的方向(矢量u⁽ ⁾ ∈ ℝⁿ),以最小化投影误差。在上图中，距离用粉色线条表示。PCA在最小平方误差的方向上选择一条线，该方向由所选择的线和一个点之间的距离决定。从上图我们还可以得出结论，橙色线是最差的线，因为点和线之间的距离很大。这里，负方向也可以选择，但是这里正负方向都在同一条线上。</p><p id="d38f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一般来说，从n维减少到k维:找到k个向量u⁽ ⁾，u⁽ ⁾,….将数据投影到其上的,u⁽ᵐ⁾，以便最小化投影误差。</p><p id="2f4c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了帮助您理解，我们将首先使用一个示例2D数据集进行实验，以直观地了解PCA是如何工作的，然后在更大的数据集上使用它。</p><p id="7ea4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们加载我们的样本数据集，并以超快的速度将其可视化:</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kp kq kr ks fd lh li lj lk aw ll bi"><span id="03c3" class="jo jp hi li b fi lm ln l lo lp">mat3 = loadmat('ex7data1.mat')<br/>X3 = mat3['X']</span><span id="b881" class="jo jp hi li b fi lq ln l lo lp">plt.scatter(X3[:, 0], X3[:, 1], alpha=0.5)</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/7277657004cb0e8e50e9abe6c41c6f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*nK3i0B7lzhKTtI9qAd0dzQ.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">数据可视化</figcaption></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><p id="0c3e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在使用我们的数据之前，我们需要将其标准化。在这篇文章中，我不会深入探讨正常化。了解更多关于正常化<a class="ae ls" rel="noopener" href="/codex/linear-regression-on-multiple-variables-1893e4d940b1"> <strong class="is hj"> <em class="lt">点击这里</em> </strong> </a>。</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kp kq kr ks fd lh li lj lk aw ll bi"><span id="01cd" class="jo jp hi li b fi lm ln l lo lp">def featureNormalize(X):<br/>    <br/>    mu = np.mean(X, axis=0)<br/>    sigma = np.std(X, axis=0)<br/>    X_norm = (X - mu)/sigma<br/>    <br/>    return X_norm, mu, sigma</span></pre></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="6fc9" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">主成分分析算法</h2><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/57947dc160a2337e47c978ce80a0cc38.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*8duhRFkyyHDyFCmD350yug.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">主成分分析算法</figcaption></figure><h2 id="5f67" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">1.计算协方差矩阵</h2><p id="b00e" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">两个变量之间关系的度量是协方差。它量化了两个变量与其平均值的差异程度。它可以帮助我们理解这两个变量之间的关系。</p><p id="1885" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">协方差矩阵在上式中用希腊字母(sigma)表示，与求和不同。通过乘以x⁽ⁱ⁾和x⁽ⁱ⁾转置，我们可以获得协方差矩阵。</p><h2 id="8b5a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">2.计算特征向量</h2><p id="d738" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">解释更多的特征值和特征向量会使这篇文章太长，我将只使用numpy的svd()方法。然而，为了方便起见，我将把计算特征值和特征向量的过程留在文章的最后。简单来说，特征值和特征向量将帮助我们找到PCA线的投影方向。<a class="ae ls" href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> <em class="lt">点击这里</em> </strong> </a>了解更多特征值和特征向量。这里u是u⁽⁾,….u⁽⁾的向量集合要将数据投影到的,u⁽ᵐ⁾。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/38e7e7169d5f24b586c6a8bc17b195bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*PMq5K46jNDEBMOnI8KBy_Q.png"/></div></figure><p id="dbbe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用svd()后，我们得到U作为上面的矩阵。返回的U矩阵包含n维的方向，我们需要选择k将n维矩阵减少到k维。k维矩阵称为U_reduce。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/a1624aac55c614b54c3f96c38c0d2852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*jyXsdCh7-6Y8v8yWBNtmzQ.png"/></div></figure><p id="3ede" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以通过将U_reduce转置与x相乘得到降维的z，这将大小为(n×1)的x⁽ⁱ⁾减少到(k×1)。</p><p id="8662" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过将U_reduce乘以z，我们可以近似地得到原始数据(X≈Xₐₚₚᵣₒₓ = U_reduce ⋅ Z)。原始数据和近似数据如下图所示:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/4b121d8e58f23d4ad4a1bd2f3fd28e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*nzywaoeP0V5W0QYDpWYPPA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">原始与近似</figcaption></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kp kq kr ks fd lh li lj lk aw ll bi"><span id="46ee" class="jo jp hi li b fi lm ln l lo lp">def PCA(X):<br/>    m, n = X.shape<br/>    cov_matrix = 1/m * X.T @ X<br/>    U, S, V = np.linalg.svd(cov_matrix)<br/>    return U, S, V</span><span id="54a1" class="jo jp hi li b fi lq ln l lo lp">X_norm, mu, sigma = featureNormalize(X3)<br/>U, S, V = PCA(X_norm)<br/><br/>plt.scatter(X3[:, 0], X3[:, 1], alpha=0.5)<br/>plt.plot([mu[0], (mu + 1.5 * S[0] * U[:, 0].T)[0]], [mu[1], (mu + 1.5 * S[0] * U[:, 0].T)[1]], color="black", linewidth=3)<br/>plt.plot([mu[0], (mu + 1.5 * S[1] * U[:, 1].T)[0]], [mu[1], (mu + 1.5 * S[1] * U[:, 1].T)[1]], color="black", linewidth=3)<br/>plt.xlim(-1,7)<br/>plt.ylim(2,8)</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/b2b42153d0570e5c6b1f9ba42df96682.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*u-HUamulXrhi4vnXBfuutg.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">PCA方向</figcaption></figure><pre class="kp kq kr ks fd lh li lj lk aw ll bi"><span id="77a5" class="jo jp hi li b fi lm ln l lo lp">def projectData(X, U, K):<br/>    <br/>    m = X.shape[0]<br/>    Z = np.zeros((m, K))<br/>    for i in range(m):<br/>        for j in range(K):<br/>            projection_k = X[i, :] @ U[:, j] #x @ U_reduce<br/>            Z[i, j] = projection_k<br/>    return Z</span><span id="369b" class="jo jp hi li b fi lq ln l lo lp">K=1<br/>Z = projectData(X_norm, U, K)</span><span id="b9e4" class="jo jp hi li b fi lq ln l lo lp">def recoverData(Z, U, K):<br/>    z = Z.shape[0]<br/>    u = U.shape[0]<br/>    X_rec = np.zeros((z, u))<br/>    <br/>    for i in range(z):<br/>        for j in range(u):<br/>            X_rec[i, j] = Z[i, :] @ U[j, :K]<br/>    return X_rec</span><span id="f21a" class="jo jp hi li b fi lq ln l lo lp">X_rec  = recoverData(Z, U, K)</span><span id="5d91" class="jo jp hi li b fi lq ln l lo lp">plt.scatter(X_norm[:,0],X_norm[:,1],marker="o",label="Original",facecolors="none",edgecolors="b",s=15)<br/>plt.scatter(X_rec[:,0],X_rec[:,1],marker="o",label="Approximation",facecolors="none",edgecolors="r",s=15)<br/>plt.title("The Normalized and Projected Data after PCA")<br/>plt.legend()</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/bf80f7ac2dc6aadffa7ff7a5e61349fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*HskxA49FdeTpKQfDBz1CPA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">原始值与近似值</figcaption></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="d1df" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">选择k(主成分数)</h2><p id="7d7a" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">我们可以通过下式选择k:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/0ae89933fcac648a5323dd21d37f0a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*NvG6XEAKMeS5lyXwFwPUFg.png"/></div></figure><p id="5178" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">保留95%到99 %的差异是好的和可接受的。</p><h2 id="3eeb" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">应用PCA</h2><ol class=""><li id="ed90" class="mb mc hi is b it kj ix kk jb md jf me jj mf jn mg mh mi mj bi translated">x⁽ⁱ⁾ →z⁽ⁱ⁾的映射应该通过仅在训练集上运行PCA <strong class="is hj"> <em class="lt">来定义。</em>T9】</strong></li><li id="9bf5" class="mb mc hi is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj bi translated">不要使用PCA将特征的数量减少到k。这可能行得通，但不是解决过度拟合的好方法。</li><li id="4897" class="mb mc hi is b it mk ix ml jb mm jf mn jj mo jn mg mh mi mj bi translated">在实现PCA之前，首先尝试运行您想对原始数据做的任何事情。只有当它不做你想要的，然后实施PCA。</li></ol><p id="a1f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这需要大量的代数和数学运算，让我们深入到使用主成分分析的人脸降维中。</p><p id="36df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们快速加载和可视化数据集:</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><pre class="kp kq kr ks fd lh li lj lk aw ll bi"><span id="b18e" class="jo jp hi li b fi lm ln l lo lp">mat4 = loadmat('ex7faces.mat')<br/>X4 = mat4['X']</span><span id="6512" class="jo jp hi li b fi lq ln l lo lp">fig, ax = plt.subplots(nrows=10,ncols=10,figsize=(8,8))<br/>for i in range(0,100,10):<br/>    for j in range(10):<br/>        ax[int(i/10),j].imshow(X4[i+j,:].reshape(32,32,order="F"),cmap="gray")<br/>        ax[int(i/10),j].axis("off")</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/12bd4985d5f3c3f871445e8216a2bfa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*vNtxQhcMtnG7kczrtcTPtg.png"/></div></figure><p id="1ced" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们对该数据集应用主成分分析并将其可视化:</p><pre class="kp kq kr ks fd lh li lj lk aw ll bi"><span id="4e46" class="jo jp hi li b fi lm ln l lo lp">X_norm2, mu2, sigma2 = featureNormalize(X4)<br/>U2, S2, V2 = PCA(X_norm2)<br/>U_reduced = U2[:,:36].T<br/>fig2, ax2 = plt.subplots(6,6,figsize=(8,8))<br/>for i in range(0,36,6):<br/>    for j in range(6):<br/>        ax2[int(i/6),j].imshow(U_reduced[i+j,:].reshape(32,32,order="F"),cmap="gray")<br/>        ax2[int(i/6),j].axis("off")</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/d1d8c1282df7002a4f025be1917f5d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*aw-81oMe2qXI7aMq_eaACg.png"/></div></figure><p id="1eec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们将面的尺寸从1024减少到100:</p><pre class="kp kq kr ks fd lh li lj lk aw ll bi"><span id="40b8" class="jo jp hi li b fi lm ln l lo lp">K2 = 100<br/>Z2 = projectData(X_norm2, U2, K2)<br/>print("The projected data Z has a size of:",Z2.shape)<br/>X_rec2  = recoverData(Z2, U2, K2)<br/>fig3, ax3 = plt.subplots(10,10,figsize=(8,8))<br/>for i in range(0,100,10):<br/>    for j in range(10):<br/>        ax3[int(i/10),j].imshow(X_rec2[i+j,:].reshape(32,32,order="F"),cmap="gray")<br/>        ax3[int(i/10),j].axis("off")</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/3be858309b5f2e6e9e8592d20e416582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*HhIi1iJzB5HAj-fO-KPAww.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">预计数据</figcaption></figure></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="23db" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">关于特征值和特征向量的补充说明</h2><p id="95d7" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">求特征值和特征向量的程序:<br/> 1。求特征方程。<br/> 2。求解特征方程得到特征根。它们也被称为特征值或潜在根。<br/> 3。为了找到特征向量，对于不同的λ值求解(A-λI)X= 0。</p><h1 id="cc9a" class="ms jp hi bd jq mt mu mv ju mw mx my jy mz na nb kb nc nd ne ke nf ng nh kh ni bi translated">结论</h1><p id="9042" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">今天，我们看到了PCA的内幕以及它实际上是如何工作的。然后用python的numpy，pandas和matplotlib从头开始创建。它在许多应用中更有用，有助于以非常低的数据损失来降低数据的维度。数据集和最终代码上传到Github。</p><p id="a88a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">点击这里查看<a class="ae ls" href="https://github.com/jagajith23/Andrew-Ng-s-Machine-Learning-in-Python/tree/gh-pages/Unsupervised%20Machine%20Learning" rel="noopener ugc nofollow" target="_blank"> PCA </a>。</p><h1 id="bdeb" class="ms jp hi bd jq mt mu mv ju mw mx my jy mz na nb kb nc nd ne ke nf ng nh kh ni bi translated">如果你喜欢这篇文章，那么看看我以前在这个系列中关于</h1><h2 id="b506" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">1.<a class="ae ls" rel="noopener" href="/@jagajith23/what-is-machine-learning-daeac9a2ceca">什么是机器学习？</a></h2><h2 id="01ca" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">2.<a class="ae ls" rel="noopener" href="/codex/what-are-the-types-of-machine-learning-53360b7db8b4">机器学习有哪些类型？</a></h2><h2 id="1fa7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">3.<a class="ae ls" rel="noopener" href="/codex/linear-regression-on-single-variable-f35e6a73dab6">一元线性回归</a></h2><h2 id="00a1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">4.<a class="ae ls" rel="noopener" href="/codex/linear-regression-on-multiple-variables-1893e4d940b1">多元线性回归</a></h2><h2 id="0c3c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">5.<a class="ae ls" rel="noopener" href="/codex/logistic-regression-eee2fd028ffd">逻辑回归</a></h2><h2 id="a934" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">6.<a class="ae ls" rel="noopener" href="/@jagajith23/what-are-neural-networks-3a0965e2ebfb">什么是神经网络？</a></h2><h2 id="b309" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">7.<a class="ae ls" rel="noopener" href="/codex/digit-classifier-using-neural-networks-ad17749a8f00">使用神经网络的数字分类器</a></h2><h2 id="64d0" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">8.<a class="ae ls" rel="noopener" href="/codex/image-compression-with-k-means-clustering-48e989055729">利用K均值聚类的图像压缩</a></h2><h2 id="bd9e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">9.<a class="ae ls" href="https://jagajith23.medium.com/detect-failing-servers-on-a-network-using-anomaly-detection-1c447bc8a46a" rel="noopener">使用异常检测来检测网络上的故障服务器</a></h2><h1 id="519d" class="ms jp hi bd jq mt mu mv ju mw mx my jy mz na nb kb nc nd ne ke nf ng nh kh ni bi translated">最后做的事</h1><p id="544b" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">如果你喜欢我的文章，请鼓掌👏一个追随者会是🤘统一🤘而且有助于媒体推广这篇文章，让其他人也能阅读。<em class="lt">我是Jagajith，下一集再来抓你。</em></p></div></div>    
</body>
</html>