<html>
<head>
<title>ConvMixer: Patches Are All You Need? Overview and thoughts 🤷</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ConvMixer:你只需要补丁吗？概述和想法🤷</h1>
<blockquote>原文：<a href="https://medium.com/codex/an-overview-on-convmixer-patches-are-all-you-need-8502a8d87011?source=collection_archive---------1-----------------------#2021-11-01">https://medium.com/codex/an-overview-on-convmixer-patches-are-all-you-need-8502a8d87011?source=collection_archive---------1-----------------------#2021-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="dc5e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">CNN并不总是需要逐渐降低分辨率。一个革命性的想法可能会塑造计算机视觉的下一代架构。</h2></div><figure class="ix iy iz ja fd jb"><div class="bz dy l di"><div class="jc jd l"/></div><figcaption class="je jf et er es jg jh bd b be z dx">🤷</figcaption></figure><p id="b74a" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hj"><em class="ke">TL；【T2博士】</em></strong></p><ul class=""><li id="806b" class="kf kg hi jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">变形金刚(NLP): <strong class="jk hj">各向同性建筑</strong>+自我关注</li><li id="6f84" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">ViT:变形金刚(NLP)+ <strong class="jk hj">补丁表示</strong></li><li id="c5d7" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">CNN:<em class="ke">金字塔架构(分辨率递减)</em> + <strong class="jk hj">卷积</strong></li><li id="32fa" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">ConvMixer: <em class="ke">各向同性架构+面片表示+卷积</em></li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="33fa" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">变形金刚的代表是注意力的广泛使用及其各向同性的架构，这种架构多次重复同一块。他们在许多问题上表现出惊人的性能，尤其是在自然语言处理方面。然而，在图像分辨率非常高的情况下，自我注意的二次计算复杂度是视觉的主要瓶颈。因此，多年来计算机视觉的主流架构是CNN。</p><p id="4669" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这种情况已经改变，因为最近关于视觉变形金刚的工作提出了一种方法，将图像分成小块，并将每个图像小块嵌入到一个可以输入变形金刚的令牌中。ViT表现出了良好的性能，并在后来的作品如DeiT和Swin transformers中进一步改进，在许多视觉任务中超过了CNN。ConvMixers的作者提出了一种基于以下想法的架构:虽然视觉变压器的一些增益是由于强大的变压器架构，但面片表示可能是一个重要因素。</p><p id="7c7d" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">作者完全破坏了CNN架构的惯例，即自AlexNet以来一直没有改变的增加特征尺寸和降低分辨率的<em class="ke">金字塔</em>设计。ConvMixers无疑是计算机视觉和各向同性视觉架构中最具革命性的想法之一。他们并没有提出任何最先进的网络，而是提出了一个重要的讨论:补丁在卷积架构中工作得非常好，值得我们关注。</p><p id="033c" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">作者:</p><ul class=""><li id="67da" class="kf kg hi jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">描述一个极其简单而有效的模型类别，该类别可以容纳280个字符，无需大量实验即可达到80%的分类准确率。</li><li id="3644" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">与其他vit和CNN相比，比较如何解释新架构。</li></ul><p id="5594" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><em class="ke">原文:</em> <a class="ae la" href="https://openreview.net/forum?id=TVHS5Y4dNvM" rel="noopener ugc nofollow" target="_blank"> <em class="ke">补丁都是你需要的？</em> </a> <em class="ke">(在审)</em></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lb"><img src="../Images/f7f22702815f52424beb0e03fbbe9896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UmFxBnaGkrZKwx9Db3Gd3Q.png"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx translated">一篇研究论文的非常规标题🤷</figcaption></figure></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="95de" class="lj lk hi bd ll lm ln lo lp lq lr ls lt jr lu lv lw jv lx ly lz jz ma mb mc md bi translated">ConvMixer架构</h2><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es me"><img src="../Images/539f74b6c1fe91ac6d27616f24e3f6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4j4pv_8cHcMNGUExUE2Kw.png"/></div></div></figure><p id="9e3a" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">提议的架构非常简单。它有一个<em class="ke">补丁嵌入阶段</em>，后面是各向同性重复的<em class="ke">卷积块</em>。</p><p id="8419" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">补丁嵌入将一个p×p补丁总结为一个嵌入的e维向量。作者通过与内核大小p、步长p和h输出通道进行单卷积，然后进行非线性运算来实现这一点。这个惊人的技巧将把n×n图像转换成h × n/p × n/p形状的特征。</p><p id="893a" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">连续卷积和合并的CNN和vit变压器由<em class="ke">一致</em>重复的ConvMixer模块取代。单个ConvMixer模块是一种稍加修改的深度方向可分离卷积，广泛用于现代CNN架构。在典型的CNN中，特征尺寸通过汇集或步进卷积逐步减小，并且频道的数量增加。相反，ConvMixer的每个中间特性都具有一致的尺寸。</p><p id="fc5a" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">类似于典型的CNN，特征通过全局平均池变平，并且在最后阶段使用softmax分类器进行推断。</p><h2 id="0053" class="lj lk hi bd ll lm ln lo lp lq lr ls lt jr lu lv lw jv lx ly lz jz ma mb mc md bi translated">将ConvMixer解释为其他架构的变体</h2><p id="7214" class="pw-post-body-paragraph ji jj hi jk b jl mf ij jn jo mg im jq jr mh jt ju jv mi jx jy jz mj kb kc kd hb bi translated">看完论文后，我脑海中浮现出许多类似的架构。作者也提出了类似的概念。我觉得类似架构的特点对理解ConvMixer架构是有帮助的。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mk"><img src="../Images/664eb5a97f105ff9867ad3aa7b66db71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4C6fZoCgUXfPUT8ee9d64Q.png"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx translated">MLP密炼机公司</figcaption></figure><p id="bd0d" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">最重要的是，所提出的ConvMixer架构在很大程度上是一个带卷积的MLP混频器。它直接作用于嵌入的补丁，分辨率和大小在所有层中都是一致的。此外，深度方向可分离卷积类似于MLP混合器(甚至跳跃连接是相同的)分离信息的信道方向混合和空间混合。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es ml"><img src="../Images/f20b6c3907ad5afb25b4318bae486ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4872PqOfecs7Oazg34MsHA.png"/></div></div></figure><p id="f1af" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">同时，该网络实际上是一个完全卷积的神经网络。ConvMixer的所有操作都可以仅使用激活、BN和卷积来实现。因此，它实际上只是一个CNN与一些特定的建筑超参数。具体来说，</p><ul class=""><li id="732f" class="kf kg hi jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">初始层中的大量下采样，但在主要瓶颈中不再进行。</li><li id="d911" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">具有相同分辨率的各向同性架构，每层中有#个通道。</li><li id="b214" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">非常规的大内核大小，我们将很快讨论这个问题(注意在上面的实现中内核大小默认为9)。</li></ul><p id="47e0" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我认为这尤其具有革命性，因为这些选择导致CNN完全破坏了CNN架构的惯例，即自AlexNet以来一直没有改变的增加特征尺寸和降低分辨率的<em class="ke">三角形</em>设计。</p><p id="8171" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">然而，我们必须注意到，重复的各向同性CNN网络运行在224/7=32的图像大小上，当与大多数CNN相比时，这并不小，因为大多数CNN具有更多的层来处理更小分辨率的特征。</p><p id="aac2" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">各向同性架构实际上类似于变形金刚(NLP和vision)，而主要计算是通过卷积而不是自我关注来执行的。因此，我个人对架构的理解是:</p><blockquote class="mm mn mo"><p id="435f" class="ji jj ke jk b jl jm ij jn jo jp im jq mp js jt ju mq jw jx jy mr ka kb kc kd hb bi translated">设计像变形金刚一样的CNN，使用面片表示。</p></blockquote></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="9586" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><em class="ke">好吧，革命就是酷。但是，像变形金刚一样设计CNN有什么好处呢？他们真的表现好吗？</em></p><p id="caef" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">TL；大卫:是的，他们展示了<em class="ke">有前途的</em>性能。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es ms"><img src="../Images/83948ccef868c698f08c9fabec60d509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gL1zrTZBgD2bCEuIUDSVFw.png"/></div></div></figure></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="ae97" class="lj lk hi bd ll lm ln lo lp lq lr ls lt jr lu lv lw jv lx ly lz jz ma mb mc md bi translated">动机和优势:大感受野</h2><p id="21af" class="pw-post-body-paragraph ji jj hi jk b jl mf ij jn jo mg im jq jr mh jt ju jv mi jx jy jz mj kb kc kd hb bi translated">我们将讨论面片表示和ConvMixer架构的一些理论优势和动机。</p><p id="9cac" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">作者认为这项工作的动机是用卷积代替MLP混合器的混合操作。深度方向卷积可以混合空间位置，点方向卷积可以混合通道方向位置。</p><p id="6df4" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">MLP和变压器可以在空间位置混合过程中模拟相距较远的信息，但卷积只能在内核大小内混合信息。然而，作者认为卷积的这种<em class="ke">感应偏差</em>非常适合视觉任务，并导致高数据效率。</p><p id="be5f" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">映射空间上相距较远的信息的能力，也称为感受野，可以由深度方向卷积的核大小来控制。MLP混合器是指我们使用的内核大小等于输入分辨率。作者在非常规的大核尺寸(例如9)中找到了深度方向卷积的平衡，以更快地增加感受野，并发现这样做是有益的。</p><p id="61ff" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">此外，小块嵌入产生了更大的感受野，因为下采样同时发生，并且所有层都以小分辨率操作。</p><h2 id="459d" class="lj lk hi bd ll lm ln lo lp lq lr ls lt jr lu lv lw jv lx ly lz jz ma mb mc md bi translated">实验</h2><p id="31d9" class="pw-post-body-paragraph ji jj hi jk b jl mf ij jn jo mg im jq jr mh jt ju jv mi jx jy jz mj kb kc kd hb bi translated">ConvMixer的设计参数包括:</p><ul class=""><li id="347b" class="kf kg hi jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">补丁大小:补丁的尺寸</li><li id="66c0" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">内核大小:深度方向卷积的内核大小</li><li id="6e23" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">宽度:面片嵌入的维数e，在整个网络中保持一致，等于输出特征的维数。</li><li id="1cd8" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">深度:要使用的ConvMixer块(层)的数量。</li></ul><p id="761e" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">网络配置命名为<em class="ke"> ConvMixer-h/d </em>，其中h指网络宽度，d指网络宽度。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mt"><img src="../Images/64ba290d9f0ac846f69f7eef141c9e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*woZHPFAijDWbZ-uIGgbuAg.png"/></div></div></figure><p id="5458" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">所有这些实验都是在受控环境中完成的，在每个实验中，潜在的混淆配置(如数据扩充策略或学习率计划)都是固定的。</p><p id="d94c" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">ConvMixer-1536/20具有51.6M的参数，是最大且工作最佳的ConvMixer设置，其ImageNet精度为81.6%，优于其他类似尺寸的ResNets和vit。实验设置意味着在验证ConvMixers的性能方面有更多有趣的见解。</p><p id="4ce8" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">训练配置类似于DeiT，因为DeiT在网络架构方面类似于原始ViT，所以评估在ViT设计中加入卷积的效果应该是有意义的。我要说的是，这种比较是有争议的，至少当ConvMixer在有限的计算和次优超参数下接受训练时是如此。</p><p id="52fe" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">当比较类似大小的DeiT和conv mixer时，conv mixer在模型大小(参数计数)和准确性方面确实表现稍好，尽管没有模型能够赢得最大的DeiT-B模型。然而，吞吐量明显更差，尤其是对于具有大内核大小的网络。虽然这是因为conv mixer使用小得多的小块大小7，因为作者在使用小块大小7的DeiT时观察到类似的吞吐量，但缓慢的推断是一个显著的缺点，因为当conv mixer使用较大的小块大小时，准确性会显著下降。</p><p id="af39" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">三个ResNets在相同的训练配置上进行训练，并且优于ConvMixers。各向同性MobileNetv3基准测试是在重复各向同性MobileNet块的前期工作的基础上进行的。积木非常复杂，作者认为动机与这项工作非常不同。我不能对此发表太多评论，因为我不知道那项工作，但它似乎绝对值得一试。</p><p id="31e4" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">然而，由于计算能力有限，与竞争对手相比，这些模型在更少的时期上进行训练，根据作者的说法，没有超参数调整，它们是根据常识从一个模型中选择的。因此，模型可能是过度正则化或欠正则化的，并且报告的精度可能低估了我们的模型的真实能力。</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="e1a0" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">关于超参数的选择，我们观察到两个重要趋势:</p><ul class=""><li id="2baf" class="kf kg hi jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">小的补丁大小对于性能来说是至关重要的<strong class="jk hj"/>，但是对计算的要求要高得多。</li><li id="09de" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">增加内核大小具有显著的优势，同时需要相对较小的计算量。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="f9d3" class="lj lk hi bd ll lm ln lo lp lq lr ls lt jr lu lv lw jv lx ly lz jz ma mb mc md bi translated">简单</h2><p id="9058" class="pw-post-body-paragraph ji jj hi jk b jl mf ij jn jo mg im jq jr mh jt ju jv mi jx jy jz mj kb kc kd hb bi translated">ConvMixer的一个特殊优势是各向同性和基本架构的简单性。我们可以在下面描述的实际实现中看到这一点。在PyTorch中，CNN和ViT都不可能只用280个字符来实现。除了能够“适合一条tweet”之外，实现简单也是一个优势，因为它有可能实现复杂架构改进的应用，可能来自CNN和transformer领域。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mu"><img src="../Images/21adfe7bc61b370cce432bcff9fb3a27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpvpYWoNgjfFPM295PGFxg.png"/></div></div><figcaption class="je jf et er es jg jh bd b be z dx translated">def ConvMixr '..？281个字符应该合适…🤔</figcaption></figure></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="e5a7" class="lj lk hi bd ll lm ln lo lp lq lr ls lt jr lu lv lw jv lx ly lz jz ma mb mc md bi translated">摘要</h2><ul class=""><li id="2bf5" class="kf kg hi jk b jl mf jo mg jr mv jv mw jz mx kd kk kl km kn bi translated">变形金刚(NLP): <strong class="jk hj">各向同性建筑</strong>+自我关注</li><li id="5293" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">ViT:变形金刚(NLP)+ <strong class="jk hj">补丁表示</strong></li><li id="3092" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">CNN:<em class="ke">金字塔</em> <em class="ke">架构</em> + <strong class="jk hj">卷积</strong></li><li id="d99f" class="kf kg hi jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">ConvMixer: <strong class="jk hj"> <em class="ke">各向同性架构+面片表示+卷积</em> </strong></li></ul><p id="c163" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">ConvMixer是一种非常简单的网络架构，结合了ViTs和卷积中的贴片思想。我很惊讶自己是如何无意中接受了CNN的分辨率必须逐渐降低的观点，却不知道为什么。</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="0836" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">结果很有希望，但在ConvMixers成为下一代视觉模型的基准之前，肯定还有改进的空间。虽然大的感受野肯定是重要的，但我觉得ConvMixers的有效性和效率还没有足够的理论支持。“为什么你只需要补丁？”这个问题似乎有更多的答案。</p><p id="81b6" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">作者们也承认，他们提出了一种可能性，并证明了一个革命性想法的概念。尽管如此，还是要对作者们令人惊叹的工作表示祝贺！我将期待后续的论文进一步改进这种方法，并可能在许多基准测试中实现最先进的性能。我个人认为可能性很大。</p><p id="6ef2" class="pw-post-body-paragraph ji jj hi jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">此外，我认为研究基于ConvMixer的模型的特征会很有趣，因为该架构是两种非常不同的架构的混合。例如，我们如何回答这样的问题:我们应该像EfficientNets或ViTs那样扩展ConvMixers吗？<a class="ae la" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">他们看到的更像ViTs还是CNN</a>？我们如何有效地将它们应用于像语义分割这样的像素级任务？</p></div></div>    
</body>
</html>