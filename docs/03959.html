<html>
<head>
<title>RNNs for Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的RNNs</h1>
<blockquote>原文：<a href="https://medium.com/codex/rnns-for-natural-language-processing-d7242e6e0842?source=collection_archive---------6-----------------------#2021-10-11">https://medium.com/codex/rnns-for-natural-language-processing-d7242e6e0842?source=collection_archive---------6-----------------------#2021-10-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/53f21bde7711a59dafa5b57901592ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*6sDG60RTIZaSSAUd8sISRg.jpeg"/></div></figure><p id="3822" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在过去的十年中，用于解决自然语言问题的深度学习模型的开发取得了显著进展。目前的技术水平使用复杂的大规模神经网络，具有数亿个参数。然而，为了理解最先进的体系结构，本文将提供一个更小的神经网络的背景，以便理解前沿技术。</p><h1 id="c917" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">递归神经网络(RNN)</h1><p id="f1f5" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><em class="kn">什么事？</em></p><p id="a6d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">递归神经网络对于处理顺序输入非常有用，几乎所有的NLP任务都使用顺序输入。顺序输入是一系列输入，其中输入的顺序包含重要性或意义。检查任何语言的文本，很明显，改变单词的顺序可以彻底改变句子的意思。与前馈神经网络不同，递归神经网络保持一个隐藏状态，并在处理序列中的每个新输入时更新这一内部状态——这使rnn能够了解输入排序的重要性。</p><p id="f61b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kn">它有什么用处？</em></p><p id="e569" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">自然语言处理中的递归神经网络有许多实际应用，例如实体识别、情感分析或机器翻译等等。在本文的上下文中，为了理解更复杂的RNNs，将介绍最基本形式的RNNs，如长短期记忆网络(LSTMs)，门控循环单元(gru)，注意模型和变压器。</p><p id="fde6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kn">它是如何工作的？</em></p><p id="f125" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们考虑实体识别的一个任务:确定句子中哪些单词是名称。作为一个例子，考虑句子“文森特和儒勒采取的地方”。对于这个句子中的每个单词，输出应该是这个单词是否是一个名称。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ko"><img src="../Images/9f5be86519b4870ddf9157a976ec8b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3zeAi7hjchnhabiy2qkkwQ.png"/></div></div></figure><p id="eb99" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于RNNs只能接收数字或向量作为输入，而不是文本的字符表示，因此每个单词都需要转换为唯一的向量。在这个例子中，我们将使用一个热点编码向量。要复习单词嵌入，请查看这篇文章:</p><div class="kx ky ez fb kz la"><a rel="noopener follow" target="_blank" href="/@matthewkramer_20144/word-embeddings-173d67c7a295"><div class="lb ab dw"><div class="lc ab ld cl cj le"><h2 class="bd hj fi z dy lf ea eb lg ed ef hh bi translated">单词嵌入</h2><div class="lh l"><h3 class="bd b fi z dy lf ea eb lg ed ef dx translated">将单词编码为向量，以便计算机理解和推理</h3></div><div class="li l"><p class="bd b fp z dy lf ea eb lg ed ef dx translated">medium.com</p></div></div><div class="lj l"><div class="lk l ll lm ln lj lo ik la"/></div></div></a></div><p id="2616" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，每个输入都是一个0的独热向量，只有一个1条目:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lp"><img src="../Images/e4900ee197663fa273ce78f838cd2c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxO9G018Ni3Ej0yKtg-8Qw.png"/></div></div></figure><p id="5c03" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们来看看一个递归网络如何寻找这个用例</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lq"><img src="../Images/722640a913abfc6622e50dd8a380dc51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9HU31OFd_hrcEnnLeyEvg.png"/></div></div></figure><p id="6717" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">每个输入都有自己的小神经网络——在图中标记为循环单元。递归单元有两个输入:输入单词的一键编码向量，以及一个隐藏状态——在图中显示为h^t。隐藏状态是嵌入到网络中的另一个向量，它对以前的单词和预测的上下文进行编码，以便以后的递归单元可以使用该上下文来学习单词的排序如何影响输入的含义。例如，考虑句子“文森特和儒勒采取的地方。”当神经网络试图预测“Jules”是否是一个名字时，隐藏状态会对前面的两个词“Vincent”和“and”进行编码，以便意识到“Jules”这个词更有可能是一个名字。</p><p id="ce23" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">每个递归单元也有两个输出:一个预测和下一个递归单元的隐藏状态。</p><p id="4c31" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">递归单元是一个独立的神经网络，其结构可以根据使用情况而变化。递归单元的内部可能变得有点数学密集，理解所有细节并不太重要。简而言之:</p><ol class=""><li id="efd0" class="lr ls hi io b ip iq it iu ix lt jb lu jf lv jj lw lx ly lz bi translated">输入x向量和隐藏状态h向量连接在一起。</li><li id="ed18" class="lr ls hi io b ip ma it mb ix mc jb md jf me jj lw lx ly lz bi translated">为了产生当前预测ŷ，级联向量乘以一组学习到的权重并添加到偏差，然后通过激活函数，通常是sigmoid函数。</li><li id="f09a" class="lr ls hi io b ip ma it mb ix mc jb md jf me jj lw lx ly lz bi translated">为了产生下一个隐藏状态h，级联向量乘以一组单独学习的权重，并添加到单独的偏差，然后通过激活函数，通常是双曲正切函数。</li></ol><p id="ccbf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下图显示了上图中第一个循环单元的情况。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mf"><img src="../Images/15304cc1719dd446daec8eea5fc0d763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wMEYt7GfQjp-KhCtkIZ-Q.png"/></div></div></figure><p id="64bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该图中的隐藏状态向量具有100的维度，但是这可以是任何值，例如200，或者甚至1000。输入向量的维数取决于词汇量，在这种情况下，假设词汇量为10，000。</p><p id="d97f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在训练期间，第一隐藏状态，h⁰可以被初始化为0的向量或随机数。</p><p id="ddb8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kn">有哪些局限性？</em></p><p id="e02a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这种简单的递归神经网络方法有许多缺点，这就是为什么当前最先进的模型使用更先进的体系结构来试图减轻这些限制中的一些。</p><ol class=""><li id="b73f" class="lr ls hi io b ip iq it iu ix lt jb lu jf lv jj lw lx ly lz bi translated">一个通常被称为“消失梯度”的问题。对于较长的输入序列，隐藏状态向量开始失去序列中较早单词的上下文。这是由于训练算法和激活函数的性质导致梯度接近0。关于这个问题的更深入的数学分析，请看https://youtu.be/qhXZsFVxGKo的<a class="ae mg" href="https://youtu.be/qhXZsFVxGKo" rel="noopener ugc nofollow" target="_blank"/></li><li id="5b3d" class="lr ls hi io b ip ma it mb ix mc jb md jf me jj lw lx ly lz bi translated">并行是很难的。对于新的预测，网络中的每个递归单元要求前一个递归单元在它之前计算隐藏状态。在训练期间，梯度和损失必须以与隐藏状态如何通过网络传播相反的方式通过网络反向传播，从而使得训练也难以并行化。</li></ol><p id="ebcf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">门控循环单元(GRU)和长短期记忆(LSTM)试图弥补消失梯度的限制，而变压器试图通过使网络高度并行化来解决第二个限制。</p><h1 id="38cb" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">有用的资源</h1><ul class=""><li id="6608" class="lr ls hi io b ip ki it kj ix mh jb mi jf mj jj mk lx ly lz bi translated">https://www.coursera.org/learn/nlp-sequence-models吴恩达关于序列模型的课程(点击‘审计课程’可免费观看所有视频)</li></ul></div></div>    
</body>
</html>