<html>
<head>
<title>Making a Thai byte-level language model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">制作泰语字节级语言模型</h1>
<blockquote>原文：<a href="https://medium.com/codex/making-a-thai-byte-level-language-model-5d27d2fd57dd?source=collection_archive---------5-----------------------#2021-07-04">https://medium.com/codex/making-a-thai-byte-level-language-model-5d27d2fd57dd?source=collection_archive---------5-----------------------#2021-07-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="bd27" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">当单词和子单词不起作用时</h2></div><p id="e3ab" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我采用的一个泰国伯特模型最近出现在一篇关于生物医学中跨语言学习的论文中。同样的模型在空间文件中。人们在GitHub和Kaggle上使用它。正如在大多数NLP模型中一样，我使用了一个标记器，它将文本分成一串单词和子单词前缀和后缀，单词在分成前缀和后缀之前用空格分隔。但这一假设在许多全球语言中并不成立。例如，泰语在[大多数]单词之间不加空格。</p><p id="a246" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae jt" href="https://huggingface.co/airesearch/wangchanberta-base-att-spm-uncased" rel="noopener ugc nofollow" target="_blank">泰国AI研究院2021年发布的WangchanBERTa </a>就是一个比较好的模型。<a class="ae jt" href="https://huggingface.co/airesearch/wangchanberta-base-att-spm-uncased#preprocessing" rel="noopener ugc nofollow" target="_blank">模型卡</a>显示了使用PyThaiNLP库进行预处理所需的步骤。我推荐那个而不是我自己的。但通常，编码人员会寻找可以放入现有管道中的模型。我们可以在HuggingFace中添加文档或支持更多的标记器，但后来有两种新方法引起了我的注意…</p><h1 id="4a7e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke io kf ip kg ir kh is ki iu kj iv kk kl bi translated">输入Charformer和ByT5</h1><p id="1b96" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">2021年6月，谷歌研究院的不同小组发布了关于字符和字节级变压器模型的论文。在计算语言学家如此努力地将文本解析为有意义的语素和嵌入之后，应该将“波霸”和“狒狒”拆分为单独的字母或字节，以便模型推导出自己的规则……这是对传统的重大突破。第一个网络层的架构也必须改变(从数万个单词嵌入到少数几个)。但是在深入研究了其他模型上的记号化器问题之后，这可能是一个改进？</p><p id="74a2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ByT5论文的作者科林·拉斐尔:</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="1cab" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Charformer论文的作者Yi Tay说:</p><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="9439" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的观点是，谷歌参与了这些模型，因为规定性的令牌词汇对于像谷歌搜索这样的超级通用任务来说是一个障碍。单是英语的单词列表就已经非常庞大了，语言的多样性也非常具有挑战性，以至于他们想让模型找出一个神经解决方案。</p><p id="f808" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我应该提到，字节级标记化并不新鲜，<a class="ae jt" href="https://arxiv.org/abs/1909.03341" rel="noopener ugc nofollow" target="_blank">脸书研究公司在2019年底探索了它的翻译功能</a>，wav2vec 在语音到文本方面做了类似的事情，谷歌在2021年早些时候发表了另一篇论文<a class="ae jt" href="https://arxiv.org/abs/2103.06874" rel="noopener ugc nofollow" target="_blank">犬类</a>(我喜欢这篇论文对包括阿拉伯语、格陵兰语和泰语在内的语言进行分组，讨论它们如何受益)。</p><h1 id="aaf7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke io kf ip kg ir kh is ki iu kj iv kk kl bi translated">预训练和微调ByT5</h1><p id="f16c" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">幸运的是，ByT5在mC4上进行了预训练，mC4是一个包括泰语的多语言数据集。如果你只想预先训练泰语，不幸的是，阅读和从mC4中分离出一种语言需要<a class="ae jt" href="https://github.com/tensorflow/datasets/issues/2762" rel="noopener ugc nofollow" target="_blank">大量资源</a>(这里是<a class="ae jt" href="https://github.com/chrisjihee/tensorflow-datasets-ko/blob/master/tensorflow_datasets_ko/text/c4ko.py" rel="noopener ugc nofollow" target="_blank">韩语示例</a>)。<br/>如果你想对你自己的数据集或mC4之外的语言进行预训练，我在这里开始<a class="ae jt" href="https://colab.research.google.com/drive/1PEBsdTwNlgigLLoL7wfUPpsMluvNM6Wv?usp=sharing" rel="noopener ugc nofollow" target="_blank">一个笔记本。<em class="ky">我真的希望在不久的将来为迪维希尝试一下。</em></a></p><p id="7741" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大多数用户可能希望将5-large放入他们的微调管道，然后就到此为止。不幸的是，大多数T5代码样本是为文本到文本(例如HuggingFace的<code class="du kz la lb lc b">T5ForConditionalGeneration</code>)设计的，制作一个分类器需要编写一些PyTorch。我大量借用了Suraj Patil的T5笔记本，并在这里更新了我的更新:</p><ul class=""><li id="8c36" class="ld le hi iz b ja jb jd je jg lf jk lg jo lh js li lj lk ll bi translated">适应T5车型的变化</li><li id="43e1" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">较新版本的依赖项</li><li id="afde" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">使用HuggingFace的数据集库代替原来的CSV。</li></ul><p id="d367" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae jt" href="https://colab.research.google.com/drive/1H8jbvjnBdd0AoIuwxBf79dR8oanmyyIb?usp=sharing" rel="noopener ugc nofollow" target="_blank">最终笔记本</a> —在对IMDB和Wongnai数据集进行问题训练后，我对Wisesight情感数据集进行了微调<code class="du kz la lb lc b">byt5-small</code>。在谷歌云A100上，你可以升级到<code class="du kz la lb lc b">byt5-base</code>，但不能升级到<code class="du kz la lb lc b">byt5-large</code>或更大的版本。</p><p id="eda5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有人看了我的笔记本，推荐了一个不太DIY的T5训练库——它将很快支持by T5:<a class="ae jt" href="https://pypi.org/project/simplet5/" rel="noopener ugc nofollow" target="_blank">pypi.org/project/simplet5/</a></p><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es lr"><img src="../Images/caaa29995264bab6e5ae87deb7b54d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*tc-y_s7DxevCGIaJkaU4ag.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">图来自ByT5论文</figcaption></figure><p id="a909" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在应该使用字节模型吗？ByT5论文承认<em class="ky"> mT5 </em>(一个多语种模型，带有一个更标准的标记器)在泰语XNLI上仍然表现更好。</p><blockquote class="ly lz ma"><p id="7fde" class="ix iy ky iz b ja jb ij jc jd je im jf mb jh ji jj mc jl jm jn md jp jq jr js hb bi translated">句子块标记压缩率较高的语言(例如泰语和泰卢固语)倾向于使用mT5，而压缩率较低的语言则倾向于使用ByT5</p></blockquote><p id="3474" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我想知道一个泰国独有的或训练时间更长的模型在这里是否会显示出一些更好的结果，或者每个模型在不同的任务上是否表现得更好。</p><h1 id="b219" class="ju jv hi bd jw jx jy jz ka kb kc kd ke io kf ip kg ir kh is ki iu kj iv kk kl bi translated">Charformer</h1><p id="8f06" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">Charformer 的代码6月29日才在GitHub上发布，所以还没有HuggingFace模型(<a class="ae jt" href="https://github.com/huggingface/transformers/issues/12410" rel="noopener ugc nofollow" target="_blank">问题</a>)。<br/>主要开发的是一个<em class="ky">基于软梯度的子词标记化</em> (GBST)模块，它被训练并添加到一个更大的变压器神经网络的开始。Letitia Parcalabescu的这段视频有助于解释这个概念。</p><p id="d3a0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在GitHub上搜索一些非官方的实现，我找到了牟的<a class="ae jt" href="https://github.com/ChenghaoMou/embeddings" rel="noopener ugc nofollow" target="_blank">这个文本嵌入库</a>。一切都还在开发中，每晚都要用到PyTorch，但是这些例子给出了如何将Charformer嵌入到PyTorch Lightning模块中的想法。</p><h1 id="80cb" class="ju jv hi bd jw jx jy jz ka kb kc kd ke io kf ip kg ir kh is ki iu kj iv kk kl bi translated">更新？</h1><p id="0cd8" class="pw-post-body-paragraph ix iy hi iz b ja km ij jc jd kn im jf jg ko ji jj jk kp jm jn jo kq jq jr js hb bi translated">这篇文章发表于2021年7月。关于我的最新建议，请查看GitHub自述文件的泰语NLP部分。</p></div></div>    
</body>
</html>