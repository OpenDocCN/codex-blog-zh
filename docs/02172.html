<html>
<head>
<title>Important Clustering Algorithms — with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">重要的聚类算法-使用Python</h1>
<blockquote>原文：<a href="https://medium.com/codex/important-clustering-algorithms-with-python-c81a170f3a3f?source=collection_archive---------9-----------------------#2021-07-05">https://medium.com/codex/important-clustering-algorithms-with-python-c81a170f3a3f?source=collection_archive---------9-----------------------#2021-07-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="47dd" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">聚类可用于知识发现和扩充预测模型。</h2></div><p id="bbe5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di">在</span>机器学习(ML)和人工智能(AI)领域，有两种处理数据的基本方法:有监督的和无监督的机器学习。与监督学习相反，在监督学习中，我们用数据来训练模型，而在非监督学习中，我们不用。因此，监督技术主要是为预测而设计的。对于无监督学习，由于没有地面真值/标签，我们主要用它来进行描述性分析。换句话说，无监督学习用于知识发现。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/dccaee7849133144fc942c224dea7353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTKQiwdFXa8bCVeXLmsxYQ.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated"><a class="ae ks" href="https://www.nationalgeographic.com/science/article/factors-allow-viruses-infect-humans-coronavirus" rel="noopener ugc nofollow" target="_blank">病毒</a></figcaption></figure><p id="89ca" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">聚类作为一种无监督的技术，是一种快速有效的知识发现方法。它们还可以用于在某些特征丢失的情况下扩充预测模型。集群对于“冷启动问题”是一个特别重要的工具在这种类型的问题中，例如发起新的营销活动或识别潜在的新型欺诈或垃圾邮件，我们最初可能没有任何响应来训练模型。随着时间的推移，随着数据的收集，我们可以更多地了解系统，并建立一个传统的预测模型。但是聚类通过识别人群来帮助我们更快地开始学习过程[1]。</p><p id="fb6a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇短文中，我们将介绍一些在不同情况下使用的著名聚类算法。我们将使用Python作为实现语言来演示它们在真实场景中的应用。我们将讨论K-means、分层和基于模型的聚类，以及给定数据集上相应的Python实现。</p><h1 id="a4f0" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">资料组</h1><p id="d397" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">为了展示主成分分析在现实世界问题中的应用，我们将主成分分析应用于一个股票市场数据集。完整的数据集可以从<a class="ae ks" href="https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231" rel="noopener ugc nofollow" target="_blank"> Kaggle网站</a>下载。它由30家DJIA公司中的29家的多种股票价格组成(不包括“V ”,因为它没有完整的12年数据)。我们将利用<em class="lq"> Sci-kit Learn </em>包来实现股票回报的PCA。完整的代码可以在<a class="ae ks" href="https://www.kaggle.com/vahidnaghshin/clusterstock" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="d02a" class="lw ku hi ls b fi lx ly l lz ma">dic_stock_return = defaultdict(int)<br/>for dirname, _, filenames in os.walk('/kaggle/input'):<br/>    for filename in filenames:<br/>        print(os.path.join(dirname, filename))<br/>        df = pd.read_csv(os.path.join(dirname, filename))<br/>        df['return'] = (df.Close - df.Open) / (df.Open)<br/>        dic_stock_return[df.Name[0]] = df['return']</span><span id="e466" class="lw ku hi ls b fi mb ly l lz ma">df_stock_return = pd.DataFrame(dic_stock_return)<br/>df_stock_return.head()</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mc"><img src="../Images/899bd703a10641d67d9bd3ebafa5a607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Xft9uPtvUTsxClFrGJfpg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">数据集的前五条记录</figcaption></figure><h1 id="0ec5" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">k均值聚类</h1><p id="8adf" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">聚类是一种将记录分成不同组的技术，其中在每个组中，不同组的成员的相似性最小，同时在同一组的成员之间保持高相似性。K-means作为最有效的聚类算法之一，通过最小化每个记录到其分配的聚类的<em class="lq">均值</em>的平方距离之和，将数据划分为<em class="lq"> K </em>个聚类。这被称为<em class="lq">类内平方和</em>或<em class="lq">类内平方和</em>。<em class="lq"> K </em> -means不确保聚类具有相同的大小，但会找到最佳分离的聚类[1]。聚类内的平方和由下式给出:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es md"><img src="../Images/adebbbfcb3fa0ab59a06f3ce0ca59c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QvkZC-ve5AfNfvGoZApbDQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">一个分类成员的平方和</figcaption></figure><p id="6f7e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lq"> K </em> -means查找所有四个分类的分类内平方和最小的记录分配。</p><p id="ef5a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们挑选了两只股票，谷歌和苹果的股票，并尝试使用K-means算法对每日收益进行聚类。K-means需要预先设置聚类数。我们将把集群的数量设置为4。使用K-means算法下面的colde将数据分组为4个簇。</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="aa26" class="lw ku hi ls b fi lx ly l lz ma">tech_px = df_stock_return[['GOOGL', 'AAPL']]<br/>#drop last row since it is NaN<br/>tech_px = tech_px.drop(3019)<br/>kmeans = KMeans(n_clusters=4).fit(tech_px)<br/>tech_px['cluster'] = kmeans.labels_<br/>centers = pd.DataFrame(kmeans.cluster_centers_, columns=['GOOGL', 'AAPL'])<br/>print(centers)</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es me"><img src="../Images/949c81b15ee99957f800c41c217a358f.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*6DeC4lF2_l43V_OGnxYZBA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">集群中心</figcaption></figure><p id="2db7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以将这些点和它们相应的聚类中心可视化如下</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="f21c" class="lw ku hi ls b fi lx ly l lz ma">fig, ax = plt.subplots(figsize=(14, 10))<br/>ax = sns.scatterplot(x='GOOGL', y='AAPL', hue='cluster', style='cluster', <br/>                     ax=ax, data=tech_px)<br/>ax.set_xlim(-0.1, 0.1)<br/>ax.set_ylim(-0.1, 0.1)<br/>centers.plot.scatter(x='GOOGL', y='AAPL', ax=ax, s=50, color='black')</span><span id="388c" class="lw ku hi ls b fi mb ly l lz ma">plt.tight_layout()<br/>plt.show()</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mf"><img src="../Images/8861256878909ac593cddc9cbb886229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oY0mgsN8pE9jj3wMkZ4Jpg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">四个不同的集群和点</figcaption></figure><p id="074b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们对包含所有公司的完整数据集应用K-means，并将聚类数设置为4，则每个聚类中的成员数可以通过以下方式获得</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="9850" class="lw ku hi ls b fi lx ly l lz ma">#remove rows with at least one NaN values<br/>nan_rows = df_stock_return[df_stock_return.isnull().T.any()]</span><span id="4ece" class="lw ku hi ls b fi mb ly l lz ma">df_stock_return = df_stock_return.drop(list(nan_rows.index))<br/>syms = sorted(['MMM', 'AXP', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DIS', 'XOM', 'GE',<br/>               'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM', 'MCD', 'MRK', 'MSFT', 'NKE', 'PFE',<br/>               'PG', 'TRV', 'UTX', 'UNH', 'VZ', 'WMT', 'GOOGL', 'AMZN', 'AABA'])<br/>top_df = df_stock_return[syms]<br/>kmeans = KMeans(n_clusters=4).fit(top_df)<br/>from collections import Counter<br/>print(Counter(kmeans.labels_))</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mg"><img src="../Images/96e3129a56ff2b57f35be8b616372f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*Zum4hQnKaeB3UW0GL6K0Cg.png"/></div></div></figure><p id="445e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">可以看出，每个聚类中的点数非常不平衡。不平衡聚类可能是由远处的异常值或与其余数据截然不同的记录组造成的，这两种情况都可能需要进一步检查[1]。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mh"><img src="../Images/c4dd56dbd9f0d4e1aa67854160601bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*cWqMDHYIDVsAF2ed9X_Aog.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">聚类均值</figcaption></figure><p id="0078" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过观察聚类中心，我们可以看到聚类1和3对应于下行市场，而聚类2和4对应于上行市场。</p><h2 id="8959" class="lw ku hi bd kv mi mj mk kz ml mm mn ld jg mo mp lf jk mq mr lh jo ms mt lj mu bi translated">选择群集的数量</h2><p id="7c06" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">因为K-means算法需要设置聚类的数量，所以我们应该能够选择适当数量的聚类。不幸的是，没有一种放之四海而皆准的方法来设置集群的数量。一种方法是调查不同数量的聚类的平均聚类内平方距离。</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="3e43" class="lw ku hi ls b fi lx ly l lz ma">inertia = []<br/>for n_clusters in range(2, 15):<br/>    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(top_df)<br/>    inertia.append(kmeans.inertia_ / n_clusters)<br/>inertias = pd.DataFrame({'n_clusters': range(2, 15), 'inertia': inertia})<br/>ax = inertias.plot(x='n_clusters', y='inertia')<br/>plt.xlabel('Number of clusters(k)')<br/>plt.ylabel('Average Within-Cluster Squared Distances')<br/>plt.ylim((0, 1.1 * inertias.inertia.max()))<br/>ax.legend().set_visible(False)</span><span id="4e87" class="lw ku hi ls b fi mb ly l lz ma">plt.tight_layout()<br/>plt.show()</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mv"><img src="../Images/6d3346434c06dbb51fd71d19092f9bf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*cX2QiHw1hba18M47wlH0CQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">不同集群数量的集群内平均sqr距离</figcaption></figure><p id="36b8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在评估要保留多少集群时，最重要的测试可能是:这些集群在新数据上复制的可能性有多大？聚类是可解释的吗？它们与数据的一般特征相关吗？或者它们只是反映了一个特定的实例？你可以通过交叉验证来评估这一点。</p><h1 id="5947" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">分层聚类</h1><p id="85e3" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">分层聚类允许用户可视化指定不同数量的聚类的效果。它在发现边缘或异常群体或记录方面更敏感。分层聚类也有助于直观的图形显示，从而更容易解释聚类。</p><p id="b9fd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">层次聚类从将每个点视为一个单独的聚类开始，然后将相似的聚类合并成一个更大的聚类，直到只有一个单独的聚类。两个聚类的相似性包括不同的距离定义，该距离定义基于两个不同聚类的成员的成对比较。分层聚类的灵活性是有代价的，而且分层聚类不能很好地扩展到包含数百万条记录的大型数据集。</p><p id="271f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了在股票市场上应用层次聚类，我们的目标是基于它们的每日回报对相似的公司进行分组。因此，我们将有30行，对应于每个公司，每天有3018列。通过应用层次聚类，我们有</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="64ce" class="lw ku hi ls b fi lx ly l lz ma">top_df = top_df.transpose()</span><span id="de47" class="lw ku hi ls b fi mb ly l lz ma">Z = linkage(top_df, method='complete')</span><span id="96ad" class="lw ku hi ls b fi mb ly l lz ma">fig, ax = plt.subplots(figsize=(8, 5))<br/>dendrogram(Z, labels=list(top_df.index), color_threshold=0)<br/>plt.xticks(rotation=90)<br/>ax.set_ylabel('distance')</span><span id="11e1" class="lw ku hi ls b fi mb ly l lz ma">plt.tight_layout()<br/>plt.show()</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mw"><img src="../Images/3490f5fa0f9590c7135b502bc51c1791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*_sXuR5sLCXTFPBvuEI4MjA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">系统树图</figcaption></figure><p id="ea18" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上图可以看出，亚马逊和UNH与其他股票的差异最大，因为它们在更高的距离合并。我们可以通过以下方式访问具有给定数量的集群的成员</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="5afe" class="lw ku hi ls b fi lx ly l lz ma">memb = fcluster(Z, 4, criterion='maxclust')</span><span id="8ea6" class="lw ku hi ls b fi mb ly l lz ma">memb = pd.Series(memb, index=top_df.index)<br/>for key, item in memb.groupby(memb):<br/>    print(f"{key} : {', '.join(item.index)}")</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mx"><img src="../Images/b77ed3bf050abaea2fa31eb2047a8a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TfvF6XqIBdIZnswX_Xr5Hg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">4个集群的集群成员</figcaption></figure><p id="5c74" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">常见的相异度度量有四种:<em class="lq">完全连锁</em>、<em class="lq">单连锁</em>、<em class="lq">平均连锁</em>、<em class="lq">最小方差</em>。前面定义的完全链接方法倾向于产生具有相似成员的聚类。单一链接方法是两个分类中的记录之间的最小距离。这是一种“贪婪”的方法，产生的集群可能包含完全不同的元素。平均连锁法是所有距离对的平均值，代表单一连锁法和完全连锁法之间的折衷。最后，最小方差法，也称为<em class="lq"> Ward的</em>法，类似于<em class="lq"> K </em> -means，因为它最小化了组内平方和[1]。使用最小方差最类似于K均值聚类。使用<code class="du my mz na ls b">linkage</code>中的<code class="du my mz na ls b">method</code>属性，我们可以选择我们将要使用的。</p><h1 id="06c9" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated"><strong class="ak">基于模型的聚类</strong></h1><p id="a21e" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">基于模型的聚类方法最近获得了关注。该技术基于统计理论，并提供了更严格的方法来确定集群的性质和数量。例如，在可能有一组记录彼此相似但不一定彼此接近的情况下，可以使用它们[1]。</p><p id="8a66" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最广泛使用的基于模型的聚类方法依赖于多元正态分布。多元正态分布是正态分布对一组<em class="lq"> p </em>变量<em class="lq"> X </em> 1、<em class="lq"> X </em> 2、...，<em class="lq"> X p </em>。协方差矩阵是变量之间相互关系的度量。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es nb"><img src="../Images/4f3dc7126a9b72cc2cf2b44542570710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*wp-gu_DqcCU1ffQhHQ3_HA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated"><em class="nc"> p变量的协方差矩阵</em></figcaption></figure><p id="249e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，协方差矩阵是围绕从左上到右下的对角线对称的。基于模型的聚类背后的关键思想是，每个记录都被假设为分布为<em class="lq"> K </em>多元正态分布之一，其中<em class="lq"> K </em>是聚类的数量[1]。</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="9678" class="lw ku hi ls b fi lx ly l lz ma">mclust = GaussianMixture(n_components=2).fit(tech_px)<br/>fig, ax = plt.subplots(figsize=(8, 8))<br/>colors = [f'C{c}' for c in mclust.predict(tech_px)]</span><span id="36c6" class="lw ku hi ls b fi mb ly l lz ma">tech_px.plot.scatter(x='GOOGL', y='AAPL', c=colors, alpha=0.5, ax=ax)<br/>ax.set_xlim(-0.1, 0.1)<br/>ax.set_ylim(-0.1, 0.1)</span><span id="ab25" class="lw ku hi ls b fi mb ly l lz ma">plt.tight_layout()<br/>plt.show()</span></pre><p id="1194" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出将是</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es nd"><img src="../Images/21dcb3d4de85806e23188db701d2ce42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*qbczANcsxOT3uS2qv61TJA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">通过基于模型的聚类获得的聚类</figcaption></figure><p id="a719" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如您所看到的，基于模型获得的聚类不同于K-means聚类获得的聚类。</p><p id="5d4b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样，我们可以改变集群的数量，看看BIC是如何变化的。</p><pre class="kd ke kf kg fd lr ls lt lu aw lv bi"><span id="1cd3" class="lw ku hi ls b fi lx ly l lz ma">results = []<br/>covariance_types = ['full', 'tied', 'diag', 'spherical']<br/>for n_components in range(1, 9):<br/>    for covariance_type in covariance_types:<br/>        mclust = GaussianMixture(n_components = n_components, warm_start=True,<br/>                                 covariance_type = covariance_type)<br/>        mclust.fit(tech_px)<br/>        results.append({<br/>            'bic': mclust.bic(tech_px),<br/>            'n_components': n_components,<br/>            'covariance_type': covariance_type,<br/>        })<br/>        <br/>results = pd.DataFrame(results)</span><span id="f03f" class="lw ku hi ls b fi mb ly l lz ma">colors = ['C0', 'C1', 'C2', 'C3']<br/>styles = ['C0-','C1:','C0-.', 'C1--']</span><span id="f130" class="lw ku hi ls b fi mb ly l lz ma">fig, ax = plt.subplots(figsize=(8, 8))<br/>for i, covariance_type in enumerate(covariance_types):<br/>    subset = results.loc[results.covariance_type == covariance_type, :]<br/>    subset.plot(x='n_components', y='bic', ax=ax, label=covariance_type, <br/>                kind='line', style=styles[i]) # , color=colors[i])</span><span id="126b" class="lw ku hi ls b fi mb ly l lz ma">plt.tight_layout()<br/>plt.show()</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ne"><img src="../Images/42b8bcde00cd31d17a3a91f192a06426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzLz_TX35TuYZNmNNPTufA.png"/></div></div></figure><p id="8faa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用<code class="du my mz na ls b">warm_start</code>参数，计算将重用来自先前拟合的信息。这将加快后续计算的收敛速度。</p><h1 id="caeb" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">参考</h1><p id="5d4c" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">[1]布鲁斯、彼得、安德鲁·布鲁斯和彼得·格德克。<em class="lq">数据科学家实用统计:使用R和Python的50多个基本概念</em>。奥莱利媒体，2020。</p></div></div>    
</body>
</html>