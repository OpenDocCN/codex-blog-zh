<html>
<head>
<title>Onikle Paper Summary: An Attention—Free Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Onikle论文摘要:无人注意的变压器</h1>
<blockquote>原文：<a href="https://medium.com/codex/onikle-paper-summary-an-attention-free-transformer-22fc65b4490e?source=collection_archive---------8-----------------------#2022-02-15">https://medium.com/codex/onikle-paper-summary-an-attention-free-transformer-22fc65b4490e?source=collection_archive---------8-----------------------#2022-02-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/343e319c66a1ddc4d32d2abd4663a333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UZ7-DorTmFLFWv_XZHoNA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片作者:Atsutaka Odaira</figcaption></figure><p id="867e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="js">原文:</em>【https://onikle.com/articles/435319】T2</p><h2 id="812f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jf kf kg kh jj ki kj kk jn kl km kn ko bi translated"><strong class="ak">简介</strong></h2><p id="d304" class="pw-post-body-paragraph iu iv hi iw b ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn kt jp jq jr hb bi translated">Transformer自论文《注意力就是你所需要的》提出以来，已经为NLP领域做出了重大贡献。此外，在NLP之外，它也开始广泛应用于计算机视觉。转换器的核心是自我注意力对注意力的矩阵映射。这次我要介绍一篇论文，叫《变形金刚》，注意力自由变形金刚，几乎不需要注意力。</p><p id="054b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">变换器的自我关注部分在时间和空间上都是非常计算密集的，例如，使用变换器的典型模型BERT需要超过3天来学习。注意力自由变压器是搜索注意力架构的结果，该架构以高速学习和预测变压器，同时将变压器的精度保持在相同水平或更高水平。</p><h2 id="0c06" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jf kf kg kh jj ki kj kk jn kl km kn ko bi translated"><strong class="ak">建议型号</strong></h2><p id="627f" class="pw-post-body-paragraph iu iv hi iw b ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn kt jp jq jr hb bi translated">注意力自由转换器用另一种计算方法代替点积计算，因为点积计算在注意力中花费的时间最长。原来的注意力算法如下:</p><h2 id="c9c2" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jf kf kg kh jj ki kj kk jn kl km kn ko bi translated"><strong class="ak">原创关注</strong></h2><p id="f486" class="pw-post-body-paragraph iu iv hi iw b ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn kt jp jq jr hb bi translated">注意力由三个向量组成:查询、键和值。它从查询和关键字中计算关注分数，并根据该值对分数进行加权，您将从关注中获得单词嵌入。</p><p id="8d30" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">查询和键的维度相等，值有不同的维度。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ku"><img src="../Images/7638d6aa03262340e2f227693cfc7341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVKrm8ysLW2RkABPEdo1bA.png"/></div></div></figure><p id="6442" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">相关度由查询和键的点积计算，降维，应用softmax函数。之后再按价值加权。</p><p id="ee20" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">原始注意力有以下时间复杂度和空间复杂度。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/6a89732cfc540d0e2bf440d595a5bfe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*tmem77P7Bts6y0Jwl8FcRA.png"/></div></figure><h2 id="d5fb" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jf kf kg kh jj ki kj kk jn kl km kn ko bi translated"><strong class="ak">免关注变压器</strong></h2><p id="378c" class="pw-post-body-paragraph iu iv hi iw b ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn kt jp jq jr hb bi translated">注意力自由转换器通过用逐元素乘积计算代替点积计算来减少计算量。下面是算法:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/d32d79f3e92eb4d7d7f68324922fcdb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11wbQZTgmIDhBnlmIVLvbA.png"/></div></div></figure><p id="0e75" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对每个目标位置的值执行加权平均，并将结果作为查询和基于元素的乘积。该算法的时间复杂度和空间复杂度如下:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/7dcb987e5109d2c7f7f3a08a9b365fe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*Q4a5w-3Ngqq5U02qUqNYSA.png"/></div></figure><p id="b89d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">元素式乘积消除了平方计算。一个很简单的改变，就可以减少1 / T倍的计算量。</p><h2 id="a7a7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jf kf kg kh jj ki kj kk jn kl km kn ko bi translated"><strong class="ak">结果</strong></h2><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/7730ef549bcea5f07e5938165e8c2842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vh__lXONI5YV1S9QLmnK0w.png"/></div></div></figure><p id="cee0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">表中与典型方法进行了比较。该表指的是纸无注意变压器(AFT)。可以观察到，AFT可以更快地迭代，同时保持低测试损失。此外，通过降低空间复杂度，GPU使用率也显著降低。</p><h2 id="12b8" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jf kf kg kh jj ki kj kk jn kl km kn ko bi translated"><strong class="ak">测试实施</strong></h2><p id="a74f" class="pw-post-body-paragraph iu iv hi iw b ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn kt jp jq jr hb bi translated">虽然它是计算机视觉领域的一篇论文，但注意力自由转换器，可以应用于NLP的算法。我在NLP的情感分析任务中实现了这段代码。结果，学习速度和收敛速度增加了一倍，总速度提高了4倍。此外，精度等同于原始变压器。稍后我会在google colab网址上分享代码</p><h2 id="1f03" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jf kf kg kh jj ki kj kk jn kl km kn ko bi translated"><strong class="ak">结论</strong></h2><p id="b77f" class="pw-post-body-paragraph iu iv hi iw b ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn kt jp jq jr hb bi translated">查看Onikle，了解你接下来应该阅读的其他论文的摘要。你也可以在这里看到会议论文和其他研究人员汇编的论文:</p><div class="lc ld ez fb le lf"><a href="https://onikle.com/chronicle/910" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab dw"><div class="lh ab li cl cj lj"><h2 class="bd hj fi z dy lk ea eb ll ed ef hh bi translated">引起高度重视</h2><div class="lm l"><h3 class="bd b fi z dy lk ea eb ll ed ef dx translated">这个编年史是一个论文集，旨在减少计算，提高准确性，更稳定的学习…</h3></div><div class="ln l"><p class="bd b fp z dy lk ea eb ll ed ef dx translated">onikle.com</p></div></div><div class="lo l"><div class="lp l lq lr ls lo lt io lf"/></div></div></a></div><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/9c1a1306152d73f02dda9003dbf825a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dbuulk0Z-31kYW0lpKkRtg.png"/></div></div></figure><p id="326e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="js">如果您对我们的服务感兴趣，请在以下链接中注册您的电子邮件地址，以便尽早访问和测试我们全新的预印本平台，该平台通过人工智能引擎提供无压力的搜索体验。</em></p><div class="lc ld ez fb le lf"><a href="https://onikle.com/" rel="noopener  ugc nofollow" target="_blank"><div class="lg ab dw"><div class="lh ab li cl cj lj"><h2 class="bd hj fi z dy lk ea eb ll ed ef hh bi translated">奥尼克尔</h2><div class="lm l"><h3 class="bd b fi z dy lk ea eb ll ed ef dx translated">预印本搜索平台为有志于计算机科学的研究人员提供了一个简单的方法来找到论文与…</h3></div><div class="ln l"><p class="bd b fp z dy lk ea eb ll ed ef dx translated">onikle.com</p></div></div><div class="lo l"><div class="lv l lq lr ls lo lt io lf"/></div></div></a></div><p id="f909" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">摘要由Kengo Shikama <br/>撰写，翻译:Wanonno Iqtyider</p></div></div>    
</body>
</html>