<html>
<head>
<title>Making VGG-style convnets great again with RepVGG</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用RepVGG让VGG风格的convnets再次变得伟大</h1>
<blockquote>原文：<a href="https://medium.com/codex/making-vgg-style-convnets-great-again-with-repvgg-df8ed7205318?source=collection_archive---------2-----------------------#2021-10-26">https://medium.com/codex/making-vgg-style-convnets-great-again-with-repvgg-df8ed7205318?source=collection_archive---------2-----------------------#2021-10-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f8582e1e04d10d9294a3fa0e343d8929.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dM73wxlvpAS8lohb"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">克拉克·蒂布斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="4b1a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">VGG被认为是深度CNN的基石之一。VGG的作者为设计CNN提供了广泛接受的原则。特别是，他们认为多重3 × 3卷积比使用更大的内核更有效。</p><p id="4b1f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，与复杂的现代最先进的CNN架构相比，仅由3 × 3卷积和ReLU的连续堆栈组成的VGG架构可能显得粗糙和脆弱，其中大多数架构涉及复杂的卷积变体和神经架构搜索(NAS)。最近的一篇论文表明，使用一些技术，平面设计可以显示相当的图像识别推理性能。</p><p id="1da5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RepVGG是一种设计类似多分支模型(例如ResNet、Inception)的架构，但可以通过<em class="jt">结构重新参数化</em>转换为类似VGG的模型，该模型具有连续的3 × 3卷积和ReLU堆栈，在推理时产生相同的结果。在本帖中，我们将讨论建议的RepVGG架构的细节，以及它为什么有用。</p><p id="7fd6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者建议</p><ul class=""><li id="0005" class="ju jv hi ix b iy iz jc jd jg jw jk jx jo jy js jz ka kb kc bi translated">与现代CNN架构中使用的某些组件相比，简单架构在推理方面的优势。</li><li id="6e48" class="ju jv hi ix b iy kd jc ke jg kf jk kg jo kh js jz ka kb kc bi translated">设计多分支模型的一个聪明的技巧</li><li id="7423" class="ju jv hi ix b iy kd jc ke jg kf jk kg jo kh js jz ka kb kc bi translated">RepVGG，一个多分支架构，可以转换成VGG风格的架构，优于许多复杂的模型。</li></ul><blockquote class="ki kj kk"><p id="05bd" class="iv iw jt ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">本文不仅仅是证明简单模型可以相当好地收敛，也不打算训练像ResNets这样的非常深的网络。相反，我们的目标是建立一个简单的模型，具有合理的深度和良好的精度-速度平衡，可以用最常见的组件(<em class="hi">例如</em>)简单地实现。正则conv和BN)和简单代数。</p></blockquote><p id="6183" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">原文:</em> <a class="ae iu" href="https://arxiv.org/pdf/2101.03697.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jt"> RepVGG:让VGG式的康文网再次伟大</em> </a></p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h2 id="98fd" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">VGG的可能优势:快速、节省内存和灵活</h2><p id="28e5" class="pw-post-body-paragraph iv iw hi ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">虽然以前的方法已经提出了复杂而有效的网络结构，但它们在真实设备中的表现通常不如预期。具体来说，作者认为:</p><ol class=""><li id="61d0" class="ju jv hi ix b iy iz jc jd jg jw jk jx jo jy js lv ka kb kc bi translated">多分支设计，如ResNets中的跳过连接和Inception网络中的分支级联，是内存低效的，因为每个分支的结果都需要保留，直到相加或级联。</li><li id="7774" class="ju jv hi ix b iy kd jc ke jg kf jk kg jo kh js lv ka kb kc bi translated">一些组件，通常是许多高效架构(Xception、MobileNets、EfficientNetV2)中使用的深度方向卷积和ShuffleNets中的通道混洗，会大幅增加内存成本，并且没有在硬件加速器(也称为GPU)上完全优化。如下表所述，许多最新的多分支架构的理论FLOPs低于VGG，但运行速度可能不会更快。</li></ol><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/b832613af1833eee931b5fdf63170381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*anTrF94yPwBAnj76jZE39g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在1080ti上，速度是以每秒采样数来衡量的，越高越快</figcaption></figure><p id="83eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.多分支网络不太灵活，而且很难修改，因为在应用某些技术时，某些体系结构规范会造成限制。例如，多分支拓扑限制了通道修剪的应用。相比之下，简单的架构允许我们根据自己的需求自由配置每个conv层。</p><p id="4f32" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些可能是为什么VGG和ResNets的原始版本仍然被大量用于学术界和工业界的真实世界应用程序的一些原因，尽管它们缺乏性能。</p><p id="bb3c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第二个问题与特定架构设计和操作的GPU利用率低有关，这是一个众所周知的问题。此外，VGG的主要运算3 × 3卷积通过现代计算库(如NVIDIA cuDNN和英特尔MKL)在GPU和CPU上使用Winograd算法进行了高度优化。下表说明了使用3 × 3卷积时计算时间的实际优势。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/6500dbabfe32220be06d9fe2396d31b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GwsSW_3ZwJ-TsOgS3TkOSw.png"/></div></div></figure></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h2 id="e88c" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">概述和直觉</h2><p id="64a2" class="pw-post-body-paragraph iv iw hi ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">RepVGG背后的直觉是，多分支网络的好处在训练时间上是有限的，而与推理速度相关的弊端在测试时间上是不可取的。</p><p id="226a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，声称</p><blockquote class="ki kj kk"><p id="9421" class="iv iw jt ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">一种解释是多分支拓扑，<em class="hi">，例如</em>。ResNet使模型成为众多较浅模型的隐式集合[36]，因此训练多分支模型避免了梯度消失问题。</p><p id="0cf5" class="iv iw jt ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">由于多分支架构<strong class="ix hj">的好处都是为了训练</strong>而缺点是不希望用于推理的，</p></blockquote><p id="bb20" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对我来说似乎有问题，尤其是作者认为多分支模型的好处都是为了培训，尽管只提出了一个对培训有用的案例。尽管如此，考虑到本文中描述的多分支体系结构在转换为普通体系结构后可能会执行得更快，作者确实有一些令人信服的观点。</p><p id="935a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RepVGG是一种设计类似多分支模型(例如ResNet、Inception)的架构，但可以通过<em class="jt">结构重新参数化</em>转换为类似VGG的模型，该模型具有连续的3 × 3卷积和ReLU堆栈，在推理时产生相同的结果，但通过现代计算库进行了高度优化。这种设计受益于多分支模型在训练中的优势和简单模型在推理中的优势。</p><p id="2a35" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[36]安德烈亚斯·维特、迈克尔·J·威尔伯和塞尔日·贝隆吉。剩余网络的行为类似于相对较浅的网络的集合。神经信息处理系统进展，第550–558页，2016年</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h2 id="5ee8" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">RepVGG架构</h2><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/92a5123b1aaff2d9887078c1323346c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5WkOlBGSLBJS28PF2WNA6Q.png"/></div></div></figure><p id="4451" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ResNets构造了一个快捷方式，并将信息流建模为y = x + f(x)，其中f是学习到的剩余块。当x和f(x)的维数因为步幅不匹配时，建模为y = g(x)+f (x)其中g是1 × 1卷积。使用相似的捷径和1 × 1卷积来构造训练时间RepVGG网络的构建块，这导致模型y = x + g(x) + f(x)。该架构如上图所示。</p><p id="d6cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个区别是ResNets中的残差块通常由2个conv和ReLU层组成，而RepVGG每层仅使用一个层。在每个分支的操作之后应用批量归一化，因此构造块被精确地定义为y = ReLU( BN(x) + BN(g(x)) + BN(f(x))。</p><h2 id="446e" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">结构重新参数化</h2><p id="cee0" class="pw-post-body-paragraph iv iw hi ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">结构重新参数化是一系列步骤，通过利用RepVGG的构造块的设计特征，将训练块转换成单个3 × 3 conv层用于推理。</p><p id="35ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结构重新参数化的第一步是将批量标准化融合到卷积参数中。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/58122ce45316bf01e899589241a0c049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAJkRi3xpRzkpP40qgUnqg.png"/></div></div></figure><p id="72b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在推断时对输入M应用批量归一化可以描述为上面的等式。请注意，μ、σ、γ、β分别是BN层的累积平均值、标准偏差、学习比例因子和偏差。</p><p id="054b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">跟随有批量归一化BN(M∫W)的卷积层可以表示为具有偏置向量的conv，如下式所示。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/e47d642af2a4661db719bf82940323be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5kNbQhkYCczoWB5RdPERmQ.png"/></div></div></figure><p id="c719" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">具体地说，w’和b’定义如下式。给定这些参数，我们可以很容易地证明上面等式的两边是相等的。</p><p id="bde8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">bn(M∫W，μ，σ，γ，β)=(M∫W—μ)×(γ/σ)+β和</p><p id="ce36" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(M∫W `)+b ` =(M∫W)×(γ/σ)—μ×(γ/σ)+β=(M∫W—μ)×(γ/σ)+β</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/63128c12103440e71c159d9975c8730b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rd5xmoQ-f8DCpNGglXB14g.png"/></div></div></figure><p id="d6b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种变换也适用于identity分支，它可以看作是一个具有固定内核的1×1 conv。因此，我们使用w’和b’的定义来转换每个分支的参数，以获得一个3×3核、两个1×1核和三个偏置向量。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/dda0a413d22963eccc6a6714b8d1aa98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-CmpUlPoVYLAvH2x0dgMXA.png"/></div></div></figure><p id="f8ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在通过变换核来融合批量归一化操作之后，我们将这些核添加到单个3×3 conv核中。这是通过首先将两个1×1内核零填充为3×3，然后将三个内核相加来实现的，如上图中的(B)所示。结果是一个单一的3×3 conv层，这意味着三个分支和批量归一化。酷！</p><h2 id="c525" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">细节和比例</h2><p id="9899" class="pw-post-body-paragraph iv iw hi ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">RepVGG是VGG风格的，它采用简单的拓扑结构，大量使用3×3 conv，但它不像最初的VGG那样使用最大池。事实上，架构和训练配置非常不同，因为作者利用了更现代的改进和设计原则。RepVGG的设计遵循以下原则:</p><ul class=""><li id="e128" class="ju jv hi ix b iy iz jc jd jg jw jk jx jo jy js jz ka kb kc bi translated">第一级以高分辨率运行，因此我们仅使用一层来降低延迟。</li><li id="3b7e" class="ju jv hi ix b iy kd jc ke jg kf jk kg jo kh js jz ka kb kc bi translated">最后一级应该有更多的通道，所以我们只使用一层来保存参数。</li><li id="7a50" class="ju jv hi ix b iy kd jc ke jg kf jk kg jo kh js jz ka kb kc bi translated">我们将大多数层放入倒数第二个阶段，跟随ResNet及其最近的变体。</li></ul><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/29ff44e9de02dc3dc57b0a8de3460e7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JeuTgXckAizGL4OPW0QbMg.png"/></div></div></figure><p id="0a5e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果是一个类似上表的架构。虽然这些超参数有改进的空间，但它们是类似于许多深度CNN的合理设计。此外，在奇数层(第3层、第5层、第7层、…第21层、…)使用g=1、2、4的<em class="jt">分组卷积</em>来加速训练和推理。</p><p id="d4a2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者通过增加或减少A和b的值来缩放模型的宽度，并使用两种配置RepVGG-A和RepVGG-B来缩放模型的深度。本文描述了用于进一步提高性能的技巧和配置的规格(例如，学习速率衰减、余弦退火、数据扩充)。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/2fe7afd83c89e95d4590cb408afec4ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UQZKvvoHaf57rgM2DX2_aQ.png"/></div></div></figure><h2 id="cb40" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">实验</h2><p id="ff6e" class="pw-post-body-paragraph iv iw hi ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">作者提供了在各种设置下对RepVGG模型的性能的实验。结果是“如预期”的，因为作者基本上训练了一个ResNet变体，只是它被优化以在推理时间上利用简单架构的好处。因此，它在速度-精度权衡方面明显更好。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/bf1cb2b102a459c01021409fa407b9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kgrHZb8TiV0BLsIOx6BweQ.png"/></div></div></figure><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/265cd712474369c7d36ed90eda0bf6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GMvOgQmKdIdBtiRUGHAYrw.png"/></div></div></figure><p id="2a9d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在一项消融研究中，作者表明，RepVGG-B0的简单训练时间模型仅产生72.39%的准确性，而两个额外的分支将其提高到75.14%。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/f50c8aca4bff3e5c69d0e3d9087f8120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eVHNzfsduBMi_dga4BrPQ.png"/></div></div></figure><p id="7469" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者还比较了他们的结构重新参数化，这不会对最终性能造成损害，与通过重新参数化融合分支的类似变体。没有详细描述变体，但是与提议的结构重新参数化(全功能重新参数化)的比较证明了RepVGG的有效性。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/66d53da5218a7fc5b3cf6c566f2b3b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*RxdlYfH1rNc03VYFAvgWkg.png"/></div></div></figure><h2 id="f730" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">讨论</h2><p id="b3ff" class="pw-post-body-paragraph iv iw hi ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">作者展示了简单架构的好处，如快速、灵活和节省内存。为了结合两种设计的优点，作者提出了一种网络，该网络可以被训练为多分支网络，但可以被转换为用于推理的平面网络。结合现代训练配置，作者能够用一个简单的VGG式网络展示与流行的现代CNN相当的速度-准确度曲线。</p><p id="2e2c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我认为RepVGG架构是一种压缩方法，通过将ResNets转换为普通架构来加速ResNets，这在许多方面都是有益的。这实际上与许多以前的经常在初始化或训练时进行的重新参数化技术非常不同，因为该方法对最终性能没有负面影响。</p></div></div>    
</body>
</html>