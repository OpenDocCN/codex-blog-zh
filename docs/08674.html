<html>
<head>
<title>Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/codex/logistic-regression-577585504e3c?source=collection_archive---------10-----------------------#2022-08-25">https://medium.com/codex/logistic-regression-577585504e3c?source=collection_archive---------10-----------------------#2022-08-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c1ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归是一种常用的分类模型。在这个模型中，因变量或目标值是一个离散的二进制值，即1或0表示通过或失败，赢或输，真或假。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/681c44df0ba97e1a02b60d614d35572c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*303pG8IfT4SHDWeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">照片由<a class="ae jt" href="https://unsplash.com/@ffstop?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">福蒂斯·福托普洛斯</a>在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="a3ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然它是一个分类模型，但术语回归在其名称中表明该模型的工作方式类似于预测建模的回归。我们不是拟合回归线(就像线性回归一样)，而是拟合一条“S”曲线，称为<strong class="ih hj"><em class="ju">S形曲线</em> </strong>，它预测两个值0或1。这条“S”曲线表示事件的<strong class="ih hj"> <em class="ju">最大可能性</em> </strong>。</p><h1 id="e298" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">逻辑回归方程</h1><p id="f482" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">我们将从直线方程推导出逻辑斯谛方程。假设有两个特征x1和x2，那么特征和目标值之间的线性关系将是<code class="du ky kz la lb b">y = Ax1 + Bx2 + C</code>，但是这里y的范围是负无穷大到无穷大。</p><p id="dc48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于逻辑回归，我们需要y为0或1，因此我们将方程操作为<code class="du ky kz la lb b">y/(1-y)</code>，现在范围为0到无穷大。为了使范围为0到1，我们取对数，所需的逻辑方程由<code class="du ky kz la lb b">log(y/(1-y))</code>给出</p><h1 id="f126" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">SIGMOID概率</h1><p id="6d55" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">目标y的概率被限制为0或1，这被称为sigmoid概率。数学上，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lc"><img src="../Images/335ff439c5bb5981bb1f9d75e3c892a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*eDDWSf1jWDT9kaEX7HHsCA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">sigmoid概率</figcaption></figure><ul class=""><li id="9bd7" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">这里的“t”是数据值，即特征“X”的值</li><li id="af1e" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">S(t)代表为真或为假的概率，即因变量“Y”的值</li></ul><p id="f895" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个数学函数给出了一个“S”曲线，其有限极限为0到1，当“t”接近<strong class="ih hj"><em class="ju">-无穷大</em> </strong>时为0，当“t”接近<strong class="ih hj"><em class="ju">+无穷大</em> </strong>时为1</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="ab fe cl lr"><img src="../Images/f97e3220af5ee1f40712bd487ac7a56b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*n4pP5E2XXiJVpLxvx702Zg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">给出值0或1的sigmoid函数</figcaption></figure><h1 id="e196" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">混淆矩阵</h1><p id="8680" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">混淆矩阵是用于评估分类模型性能的矩阵，该矩阵将目标变量的预测值与其实际值进行比较。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/d2ce0ce483ad744a73409048c8b972ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-3DyNjMMOPaSFM6Z_i9yw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">混淆矩阵</figcaption></figure><ul class=""><li id="de1c" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">TN:真阴性(预测正确的实际错误数)</li><li id="4ad2" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">TP:真阳性(预测正确的实际真值的数量)</li><li id="72c5" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">FN:假阴性(实际为真的预测假的数量)</li><li id="f5fa" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">FP:假阳性(预测为真，实际为假的数量)</li></ul><h1 id="a7cb" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">混淆矩阵的重要性</h1><p id="ae23" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">混淆矩阵用于确定一些重要的衡量标准，如准确度、精确度、召回率/敏感度和f-1分数。</p><blockquote class="lt lu lv"><p id="24e5" class="if ig ju ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated"><strong class="ih hj">精度</strong></p></blockquote><p id="cd95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它只是表示模型预测目标值的准确程度，由以下公式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lz"><img src="../Images/5f01aa93edf7a197aefdbb6265438a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYVl_tQKraedzSGq2bqewA.png"/></div></div></figure><blockquote class="lt lu lv"><p id="002c" class="if ig ju ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated"><strong class="ih hj">精度</strong></p></blockquote><p id="f26b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它指的是预测中达到的正确性，它只是告诉我们总预测阳性中的实际阳性，它由以下公式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ma"><img src="../Images/74846b46b5977c07aec3f45e9f377a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJ6NdMYkQLPciS9mTE515g.png"/></div></div></figure><blockquote class="lt lu lv"><p id="7ca3" class="if ig ju ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated"><strong class="ih hj">回忆</strong></p></blockquote><p id="03cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它通过确定有多少实际阳性被正确预测来告诉我们模型的敏感性，其计算方法如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mb"><img src="../Images/7425464cb93d99ca61e0d0b27b5e88a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2MdVrxOjxCritxXdS8wm6w.png"/></div></div></figure><blockquote class="lt lu lv"><p id="3268" class="if ig ju ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated"><strong class="ih hj"> F-1得分</strong></p></blockquote><p id="a364" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它有助于我们同时评估召回率和精确度来比较两个模型，它的计算方法是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/57fe673af96d48d8c851bb20eff010a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*BSGVb7MTctFB9QRC_MOAZA.png"/></div></figure><h1 id="9c0f" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">PYTHON中的逻辑回归</h1><p id="1eb1" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">我们将使用Scikit Learn库在取自<a class="ae jt" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank">kaggle.com</a>的<a class="ae jt" href="https://www.kaggle.com/competitions/titanic/data?select=train.csv" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集</a>上实现逻辑回归和混淆矩阵。在本例中，我们使用了train.csv数据集。</p><ul class=""><li id="ffde" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">导入所有必需的库</li></ul><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="e025" class="mh jw hi lb b fi mi mj l mk ml">import numpy as np<br/>import pandas as pd<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score,precision_score<br/>from sklearn.metrics import recall_score,confusion_matrix<br/>from sklearn.model_selection import train_test_split as tts</span></pre><ul class=""><li id="8955" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">读取数据</li></ul><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="65d0" class="mh jw hi lb b fi mi mj l mk ml">data=pd.read_csv("./titanic.csv")<br/>print(data.shape)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/5a61c90a35560b930c4fe0933a8529c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*bghJgg4grdeOEecbjMfG4g.png"/></div></figure><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="788a" class="mh jw hi lb b fi mi mj l mk ml">data.head(5)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mn"><img src="../Images/12033d6123c7b241f1157e4bd754a18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-100g8N5DfcuMpNouv5wg.png"/></div></div></figure><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="e269" class="mh jw hi lb b fi mi mj l mk ml">data.describe()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mo"><img src="../Images/770c7f0808998e8f9fbb62f7480a02c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNtldpob59dqMVWoGZxgfg.png"/></div></div></figure><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="e960" class="mh jw hi lb b fi mi mj l mk ml">data.Survived.value_counts()<br/># count the number of survivors</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/cd225af65aaf083f5a70726a5552eb39.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*tSSqZw8e0fE8TEHGJQq1AA.png"/></div></figure><ul class=""><li id="8f59" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">数据清理(在本例中，我们没有执行大范围的数据清理)</li></ul><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="1f88" class="mh jw hi lb b fi mi mj l mk ml">#considering only important fields<br/>columns=['Pclass','Survived','Sex','Fare','Age']<br/>data=data[columns]<br/>data.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mq"><img src="../Images/3ca1fc9116e97928128f286310b2cd79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*2T1WHXbAMcT_lzn_22ZNcw.png"/></div></div></figure><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="e64d" class="mh jw hi lb b fi mi mj l mk ml">#returning true at the places where null values are present<br/>print(data.isnull())</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mr"><img src="../Images/ab830d425a209f6247ea7a95731b9027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*imIhrfW0dIamD8u2D_evQA.png"/></div></figure><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="8366" class="mh jw hi lb b fi mi mj l mk ml">#removing null data points from the dataset<br/>data.dropna(axis=0,how='any',inplace=True)<br/>#we can see that the data points are reduced because null data points are removed</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ms"><img src="../Images/22d4fda9dbedf10183f59a54cfd2a7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*VRP7kHUpgL27fzU52i721Q.png"/></div></figure><ul class=""><li id="57ff" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">分割数据用于训练和进行预测</li></ul><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="4fe5" class="mh jw hi lb b fi mi mj l mk ml">#splitting the target column and the features<br/>X=data[['Pclass','Fare','Age']]<br/>Y=data['Survived']</span><span id="11e1" class="mh jw hi lb b fi mt mj l mk ml">#splitting the data for training the model<br/>x_train,x_test,y_train,y_test=tts(X,Y,test_size=0.2,random_state=42)print(x_train.shape)<br/>print(x_test.shape)<br/>print(y_train.shape)<br/>print(y_test.shape)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mu"><img src="../Images/874688c2307a073d67a5e7fe6456a3cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePBwUGspWFUM5D6bFdGibg.png"/></div></div></figure><ul class=""><li id="2540" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">训练模型并预测目标值</li></ul><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="e803" class="mh jw hi lb b fi mi mj l mk ml">model=LogisticRegression()<br/>model.fit(x_train,y_train)</span><span id="88ed" class="mh jw hi lb b fi mt mj l mk ml">predicted_y = model.predict(x_test)<br/>print(predicted_y.shape)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mv"><img src="../Images/f895f23d36a86237b59853405719a3e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*orp5kvxA1Duce1C1QvvL5Q.png"/></div></figure><ul class=""><li id="c163" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">使用混淆矩阵测量准确度、精确度、召回率和F-1分数</li></ul><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="478a" class="mh jw hi lb b fi mi mj l mk ml">c_matrix=confusion_matrix(y_test,predicted_y)<br/>print(c_matrix)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/c91d87613f2b500e50b13bf6da65400e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*6hMT2VlAxyrdMmx2JACE1g.png"/></div></figure><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="6a43" class="mh jw hi lb b fi mi mj l mk ml">print("True Negtive = ",c_matrix[0][0])<br/>print("False Positive = ",c_matrix[0][1])<br/>print("False Negtive = ",c_matrix[1][0])<br/>print("True Positive = ",c_matrix[1][1])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/8035a1fc038812211f3600f80d0320bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*KEcuDf0uDRukjCG8D5dENA.png"/></div></figure><pre class="je jf jg jh fd md lb me mf aw mg bi"><span id="3e45" class="mh jw hi lb b fi mi mj l mk ml">print(accuracy_score(y_test,predicted_y))</span><span id="d512" class="mh jw hi lb b fi mt mj l mk ml">print(precision_score(y_test,predicted_y))</span><span id="4566" class="mh jw hi lb b fi mt mj l mk ml">print(recall_score(y_test,predicted_y))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es my"><img src="../Images/0dfc69099c6ad10fd4ff08b2a46af340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*Fud0rDsWNX7ss4bNbXpyhg.png"/></div></figure><ul class=""><li id="ee87" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">您可以通过使用上述公式手动计算所有这些度量来验证这些度量。度量的接受是基于需求的，例如，我们可以为每个度量设置一个阈值，如果该度量给出了所需的值，则模型被接受。</li></ul><h1 id="a50e" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">逻辑回归用例</h1><p id="3dad" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">我们可以在目标可以分为两类的所有场景中使用逻辑回归，例如</p><ul class=""><li id="22fe" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">垃圾邮件检测</li><li id="47d1" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">贷款许可</li><li id="41e5" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">考试结果预测</li><li id="4e54" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">灾难中的幸存</li></ul><p id="b712" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在所有这些情况下，结果是二进制(0，1)，例如对于垃圾邮件检测系统，实体将是垃圾邮件或者不是垃圾邮件；对于贷款审批，银行要么批准贷款，要么不批准贷款；对于生存预测，这个人要么生存要么死亡。</p><blockquote class="lt lu lv"><p id="710e" class="if ig ju ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated">我希望你们都明白什么是逻辑回归，以及我们如何衡量模型的准确性。</p><p id="1b86" class="if ig ju ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated">并保持联系以了解更多关于机器学习的信息。</p></blockquote></div></div>    
</body>
</html>