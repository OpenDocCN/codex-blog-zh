# 多任务学习及其未来研究方向。

> 原文：<https://medium.com/codex/multi-task-learning-its-future-research-scopes-e749d22dee68?source=collection_archive---------10----------------------->

![](img/7277360f3e8e9d93695fe1b6202b8162.png)

照片来自 [istockphoto](http://istockphoto.com)

多任务学习是机器学习的一个分支，它能够同时找到不同的、多个且密切相关的任务的解决方案。在多任务学习(MTL)中，一次用来自多个任务的数据训练模型，该模型使用共享表示来学习一组相关任务之间的共同想法。与单一任务相比，MTL 确保了更高的数据处理效率和更快的学习速度。它需要同时优化多个损失函数，这允许产生数据的更一般化的表示。多任务学习还通过充当正则化器和减少过拟合来帮助提高模型的性能。这种学习主要是受人类的启发，因为当我们同时看多个相关的任务而不是一个特定的任务时，我们会学得更好。
MTL 的使用有一些具体的技巧，简单解释如下-

1.**硬参数共享** —在硬参数共享中，为所有任务设计了公共隐藏层。这减少了过度拟合，因为公共隐藏层从各种任务中学习表示。

2.**软参数共享** —在软参数共享中，模型有自己的隐藏层以及自己的权重和偏差。不同模型中这些参数之间的距离被规则化，使得参数变得相似并且可以代表所有任务。

当任务彼此不相似或不密切相关时，使用 MTL 有一个缺点。在这种情况下，MTL 不善于学习表征，从而降低了整体表现。MTL 可以用于广泛的应用，例如——物体检测、自动驾驶汽车、股票预测、语言建模等。MTL 的一些激励性使用案例是——taskonomy，它一次解决了计算机视觉中的 20 多个任务。另一个伟大的作品是 decaNLP，它解决了几个自然语言处理任务，比如情感分析、语义解析、翻译、问题回答等等。它对于实现强化学习以再现类似人类的行为也是至关重要的，这是通过 Deep Mind 的分散注意力来实现的。
此外，MTL 有助于特征选择，因为当一个特征对于多个任务很重要时，这意味着该特征可能比其他特征更重要，并且可以代表数据。因此，通过共享来自彼此密切相关的多个任务的表示，使用 MTL 可以提高机器学习算法在生产中的性能。它有很大的研究范围，有能力使机器更加智能，能够使机器更接近人类的智能。

参考文献—

[https://arxiv.org/pdf/2009.09796.pdf](https://arxiv.org/pdf/2009.09796.pdf)