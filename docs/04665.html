<html>
<head>
<title>An introduction to transfer learning and how to use it in python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迁移学习介绍以及如何在python中使用它</h1>
<blockquote>原文：<a href="https://medium.com/codex/an-introduction-to-transfer-learning-and-how-to-use-it-in-python-e38fbc339868?source=collection_archive---------6-----------------------#2021-12-24">https://medium.com/codex/an-introduction-to-transfer-learning-and-how-to-use-it-in-python-e38fbc339868?source=collection_archive---------6-----------------------#2021-12-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/1774d40b88f1afcfd255c146c01a2861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5HqpdfcHx66xD6mo"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">艾莉娜·格鲁布尼亚克在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="7dd5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">收集数据是数据科学中耗时的步骤之一。当您使用神经网络时尤其如此，因为您需要收集数百万个观察值来从头构建一个好的神经网络。不幸的是，这并不总是可能的，并且需要大量的时间和资源。那么我们能做什么呢？我们可以应用需要相对较小数据集的迁移学习，而不是从头开始。在这篇文章中，我们将讨论什么是迁移学习，为什么它有效，以及如何在python中使用Keras包。</p><h1 id="fe7f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">什么是迁移学习？</h1><p id="c0cd" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">概括地说，神经网络中有输入层、隐藏层和输出层，在隐藏层内部，有包含唯一权重的神经元。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/7f146e8591a120f1a80aac9ee5ae2fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eh08G8wU4Mm_KlxpuOzkaA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">神经网络的一般结构</figcaption></figure><p id="e68f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">迁移学习是重用另一个神经网络的隐藏层的结构和权重，该神经网络是为了解决与您的问题类似的问题而构建的。例如，如果一个网络A是为分类不同的物体而建立的，我们可以重用它的一些层来分类不同类型的衣服。这种方法在机器学习中很有用，因为它大大减少了我们训练神经网络所需的时间和资源。但是为什么会起作用呢？</p><h1 id="6453" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">迁移学习为什么有效？</h1><p id="4282" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">当你学习数学时，你从理解基本的计算开始，如+、×、。在你了解它们是如何工作的之后，你可以用它们来解决稍微复杂一点的问题。在神经网络中，它的过程与你学习数学的方式不同，但一般概念是一样的。</p><blockquote class="lb lc ld"><p id="0949" class="iv iw le ix b iy iz ja jb jc jd je jf lf jh ji jj lg jl jm jn lh jp jq jr js hb bi translated">1.从输入数据中学习基本特征<br/> 2。用这些知识去解决一个复杂的问题</p></blockquote><p id="ef9c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">神经网络将什么视为基本和复杂特征取决于它被设计来解决哪个任务。例如，如果神经网络处理图像，识别边缘和形状被认为是一项基本任务，通常在它的较低层完成。然后，使用它从较低层学习的内容，它在较高层对图像进行分类，这被认为是一项复杂的任务。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es li"><img src="../Images/789479dd91378925cd064c3cda3965a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*Guo6B6VCXECL0S29ntkkdg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">边缘检测是什么样子/原图作者:<a class="ae iu" href="https://unsplash.com/photos/lylCw4zcA7I" rel="noopener ugc nofollow" target="_blank">安德鲁·庞斯</a></figcaption></figure><p id="7692" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回到数学类比，你不仅仅在一个特定的问题中使用简单的计算。因为它是一个基本的，你可以在许多不同的情况下应用它，比如计算你在餐馆需要支付多少钱，或者你需要为某个项目做多少预算。同样的概念也适用于神经网络。</p><p id="f77f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下层所学的非常普遍，可以应用于类似的问题。因此，就像上面的类比一样，我们可以对其他类似的问题重用更低的层。</p><h1 id="f1c1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">利用迁移学习在python中进行服装分类</h1><p id="6495" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在知道了<a class="ae iu" href="https://www.bemyeyes.com/" rel="noopener ugc nofollow" target="_blank"> BeMyEyes </a>应用程序可以做什么之后，我想做一些可以让视障人士的生活更轻松的东西。由于我们每天都要穿衣服，但视障人士无法轻松区分衣服并找到它们的风格，我认为一个卷积神经网络(CNN)可以告诉我们哪些衣服搭配得很好，这可能会有很大的帮助。建立这种CNN的第一步是对不同的衣服进行分类。然而，由于很难收集数百万的数据，我决定使用迁移学习。但是在我们进入应用程序之前…</p><h2 id="1bf0" class="lj ju hi bd jv lk ll lm jz ln lo lp kd jg lq lr kh jk ls lt kl jo lu lv kp lw bi translated">在CNN中使用迁移学习时要记住的事情</h2><p id="79ce" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><strong class="ix hj">输入大小</strong> <br/>你可能会想，既然CNN是根据特定的输入形状训练的，那么当你使用与原来不同的输入形状时，迁移学习可能就不起作用了。好吧，最好调整你的训练数据的大小，以匹配你将要使用的CNN的输入形状，但这并不总是必要的。</p><p id="b396" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">比方说，您有450×450的图像，但您要使用的CNN的输入大小为299×299。根据图像的不同，将450×450的图像调整为299×299可能会模糊一些细节。或者当您有100×100的图像时，放大这些图像可能会损害图像的质量。因此，在这种情况下，建议设置不同于原始CNN的输入大小。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/ece120f9dc02666d0b01aad0748cb9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*1dquzt3B_xQ3oyVwpRJW7Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">演示了调整大小如何扭曲图像</figcaption></figure><p id="e5ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是，极端的变化会导致弹出警告消息。因此，如果是这种情况，你可能需要从你所拥有的东西中找到其他具有相似输入形状的CNN。</p><p id="22fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">解冻多少</strong> <br/>如上所述，迁移学习的总体思路是重用隐含层，用相对较小的训练集训练上层和输出层。但是我需要训练几个上层呢？这取决于你的训练集大小。</p><p id="1c38" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设你有一个小的训练集。由于网络无法从小数据集学到很多东西，因此使用大部分隐藏层并只重新训练输出层将是一个好主意。根据不同的情况，你甚至可能要删除一些隐藏层，使网络更简单。</p><p id="e1dc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">相比之下，如果你有一个大的训练集，这意味着你可以给神经网络更多的信息。因此，与小训练集相比，您可以解冻更多的层。或者你甚至可以根据情况添加更多的隐藏层。</p><h2 id="3ad0" class="lj ju hi bd jv lk ll lm jz ln lo lp kd jg lq lr kh jk ls lt kl jo lu lv kp lw bi translated">我要用CNN</h2><p id="646f" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在众多CNN模型中，我决定使用Xception架构，这是Inception架构的扩展版本，具有299×299的输入形状。你可以从Keras包中下载一个经过数百万ImageNet图像训练的，它能够对1，000个对象进行分类。更详细的解释可以在<a class="ae iu" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="733e" class="lj ju hi bd jv lk ll lm jz ln lo lp kd jg lq lr kh jk ls lt kl jo lu lv kp lw bi translated">收集数据</h2><p id="c2b6" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">为了应用迁移学习，我首先需要收集衣服图像。感谢Grigorev和他的<a class="ae iu" href="https://www.kaggle.com/agrigorev/clothing-dataset-full" rel="noopener ugc nofollow" target="_blank">项目</a>的合作者，我可以轻松下载10个不同类的训练、验证和测试集。不幸的是，Google Colab的RAM无法处理所有10个类别，所以我不得不删除5个类别，并将每个类别的大小减少到大约200张图像。</p><div class="kx ky kz la fd ab cb"><figure class="ly ij lz ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/ec1d419cd5d780fb7756922a51071934.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*M4V4wD58bk9S-cXKr1zMqA.png"/></div></figure><figure class="ly ij me ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/25b53fbd5bf627673a9f63c589064979.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*7PHfztMFoU3fU9v4ijqN3g.png"/></div></figure><figure class="ly ij mf ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/603ef110b226711accf09262968ebd83.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*EdeI62uliRWyJ3UQFa7pqg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx mg di mh mi translated">服装数据集是如何存储的</figcaption></figure></div><h2 id="b6d4" class="lj ju hi bd jv lk ll lm jz ln lo lp kd jg lq lr kh jk ls lt kl jo lu lv kp lw bi translated">预处理数据</h2><p id="7c7b" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">然后，由于CNN不能处理原始的jpg文件，我不得不将它们转换成数组。但在此之前，我必须决定是否要改变输入形状。由于数据集中的每个图像都有不同的尺寸，因此将所有图像调整为299×299比均匀地裁剪图像更容易。</p><p id="5cbc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我需要预处理的另一件事是规范化值。因为图像文件中的颜色由范围从0到255的数字表示，保持原样会导致训练时的计算负荷。因此，最好将每个像素除以255，以将值归一化到0到1的范围内，并减少计算量。</p><p id="facb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，因为我缺少训练数据集，我应用图像增强来创建更多的训练实例。在我的例子中，它只是水平翻转图像，因为内存问题，我无法添加其他增强。</p><div class="kx ky kz la fd ab cb"><figure class="ly ij mj ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/23c22ab244b90f899ef1eafd82eb9b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*Qz25fv6wYtDTX-pm8G9Rcg.png"/></div></figure><figure class="ly ij mk ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/1f1849f063f5b1ea21d6daf9f6e89d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*hbI0RyNAM6BRS35Xvgc0SA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx ml di mm mi translated">预处理图像的代码以及预处理后训练集的样子</figcaption></figure></div><p id="64ce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在预处理输入数据之后，我处理输出数据。原始输出数据如下图所示。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/459736ce1ec5d7b9af18f23822159d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*bvL9LvrIM5C71ooA5kMKtA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">原始输出数据的样子</figcaption></figure><p id="b890" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在对输出数据进行预处理时，我应该考虑一个模型的损失函数。由于检查多类分类是否在迭代过程中有所改进的最佳方式是通过比较预测标签和真实标签的概率分布，因此通常使用分类交叉熵作为损失函数。因此，我需要对训练和验证数据集应用一次性编码。幸运的是，np_utilis提供了一个名为to _ categorical()的函数来轻松应用一键编码。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/f511cb80398580e7b2e65faaea85328f.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*-OxE0F9JDmqhVdLV1cT3dQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">一键编码后输出数据的样子</figcaption></figure><h2 id="8604" class="lj ju hi bd jv lk ll lm jz ln lo lp kd jg lq lr kh jk ls lt kl jo lu lv kp lw bi translated">基于Xception架构构建模型</h2><p id="d4b2" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">下一步是构建一个模型，这可以通过下面的代码轻松完成。</p><p id="d570" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从下面的代码来看，weights="imagenet "意味着它正在下载在imagenet数据上训练的Xception的权重，input_shape=(299，299，3)意味着该模型采用299 × 299的输入大小和3个颜色通道。include_top=False意味着模型不会下载Xception的输出层，之所以包含它是因为我要重新构造输出层。然后，由于迁移学习是关于重用隐藏层的，所以base _ model.trainable被设置为False。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/e56e9b065fd237a0a96be675bb9da860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*BMQijjuK8auNu9lP_qFtVA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于从Xception架构下载权重的代码</figcaption></figure><p id="4598" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下载完隐藏层后，我开始处理神经网络的输出层。</p><p id="8f9f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如下所示，在隐藏层和输出层之间有一个全局平均池层，用于计算每个输入通道的单个平均值，而不是一个完全连接的层。所以你可能想知道为什么我可以完全连接这些层的时候只传递了3个值。这是因为我们可以避免过度拟合，并且通过这样做，模型可以对空间差异更加鲁棒。</p><p id="236b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在全局平均池层之后，我添加了具有5个节点和softmax激活函数的密集层，因为该问题是5类分类问题。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/ebc87871992c9cd7e64fa5098f5b0bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*VDXFD79xz5OwAXtlsimfIg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于构建模型输出层的代码</figcaption></figure><p id="7659" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我需要做的就是编译和拟合模型。对于超参数，由于内存问题，我无法运行gridsearchCV来找到最佳参数。因此，我选择了Adam optimizer，这样它可以自动更新学习率，并将批量大小设置为一个公共值32。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/5013682e25b1db84ba0210e8b9df193c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BUz6A6ogjaPRrdiOM8Igww.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于编译和拟合模型的代码</figcaption></figure><p id="88a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">27个周期后，训练停止，下图显示了训练过程中损失和准确度的变化。</p><div class="kx ky kz la fd ab cb"><figure class="ly ij ms ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/3c1938ed6f855ad2740757e2c3d37f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*oWLu54pekASTHs1c-uhyCQ.png"/></div></figure><figure class="ly ij mt ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/ed560a0610876430e0264ae073961095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*N6BYgVi0Yzn3i-QDfEdlOQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx mu di mv mi translated">培训和验证集的培训历史记录</figcaption></figure></div><p id="c9a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从图中可以看出，val_loss和val_accuracy在某个点之后趋于保持不变，而训练损失和准确度随着历元数的增加而不断提高。这是因为模型开始过度适应训练集。因此，当我们使用测试集进行预测时，我们可以看到准确率没有达到训练集所达到的98%左右。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/1b9a220bcdb7c5ec192563c768b9fb44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*-e7qNEpI6AWIggoORRq3tg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">预测结果与测试集的混淆矩阵</figcaption></figure><h2 id="8342" class="lj ju hi bd jv lk ll lm jz ln lo lp kd jg lq lr kh jk ls lt kl jo lu lv kp lw bi translated">提高学习率低的结果</h2><p id="c700" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">但是有方法可以改善这个结果，其中一个方法就是在学习率较低的情况下对整个网络进行微调。目前，有20，861，480个不可训练的参数，因为我们将base _ model.trainable设置为False。也就是说，隐藏层中的大多数参数不是专门针对衣服分类的。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/9ba97836e2e2ec93e1d66387336e60d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*Y5WQsYBSfYAsraVLnQ2VFw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">base_model.training设置为False时的模型摘要</figcaption></figure><p id="43d9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如果我们根据我的数据集稍微调整一下这些隐藏层，精确度可能会提高。这样做的第一步是通过设置model.trainable = True将大多数不可训练的参数变成可训练的参数。下一步是用低得多的学习率重新训练模型，因为我只想要隐藏层中的小变化。</p><div class="kx ky kz la fd ab cb"><figure class="ly ij my ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/5d15d03ede374ba8d1e1fb9fcf1ace51.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*dO3OaKof-7PglKY92hv5HA.png"/></div></figure><figure class="ly ij mz ma mb mc md paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/6fca988cd1e19b9ec781cf3baaf24b1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*0_S631v8fL1v_3GpclRDyw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx na di nb mi translated">用于解冻隐藏层和重新训练模型的代码</figcaption></figure></div><p id="c917" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于学习率很小，训练模型需要一段时间，但训练完成后，我们可以通过从每个实例中提取最高概率来检查模型是如何预测的。通过构建混淆矩阵，我们可以确认准确率从86.8%提高到88.5%。没有大幅度增加，但也算了。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/dee1d5e2c1fc5642ae3e90794c4f3ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*qs2yw8JTDiXe1P2T8Aw3bw.png"/></div></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/6c09230e3bcd687f8467811cd6ac7096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*FvDgVOVR5fve2UT9F2H14A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于提取最高概率的代码(上图)/微调模型的混淆矩阵(下图)</figcaption></figure><h1 id="a093" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">反射</h1><p id="6504" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">然而，你可能认为这还不够。这种准确性可能有许多原因，我认为最大的原因之一是训练集的大小。由于内存问题，我不得不删除大约2/3的训练集。所以下次我在CNN工作时，我应该在一个有更大内存和存储空间的环境中工作。</p></div><div class="ab cl ne nf gp ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="hb hc hd he hf"><h1 id="62b0" class="jt ju hi bd jv jw nl jy jz ka nm kc kd ke nn kg kh ki no kk kl km np ko kp kq bi translated">参考</h1><p id="591c" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">[1]詹森·布朗利。"深度学习的亚当优化算法的温和介绍."<em class="le">机器学习掌握</em>，2021年1月12日，<a class="ae iu" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/." rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/Adam-optimization-algorithm-for-deep-Learning/。</a></p><p id="06be" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[2]弗朗索瓦·乔莱。"例外:具有深度可分卷积的深度学习."ArXiv.org2017年4月4日<a class="ae iu" href="https://arxiv.org/abs/1610.02357." rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1610.02357.</a></p><p id="5b6a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[3]杰龙，奥雷连诺。<em class="le">用Scikit-Learn、Kears &amp; TensorFlow </em>进行动手机器学习。奥莱利，2017。</p><p id="d79e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[4]克拉斯。" Keras文档:迁移学习和微调."<em class="le">喀拉斯</em>，<a class="ae iu" href="https://keras.io/guides/transfer_learning/." rel="noopener ugc nofollow" target="_blank">https://keras.io/guides/transfer_learning/.</a></p><p id="c43a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[5]林，闵。"论文与代码-全球平均池解释."<em class="le">解释|论文代码</em>，<a class="ae iu" href="https://paperswithcode.com/method/global-average-pooling." rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/method/global-average-pooling.</a></p><p id="dc07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[6]阿德里安·罗斯布鲁克。"用Keras改变输入形状尺寸进行微调."<em class="le"> PyImageSearch </em>，2021年4月17日，<a class="ae iu" href="https://www.pyimagesearch.com/2019/06/24/change-input-shape-dimensions-for-fine-tuning-with-keras/." rel="noopener ugc nofollow" target="_blank">https://www . PyImageSearch . com/2019/06/24/change-input-shape-dimensions-for-fine-tuning-with-keras/。</a></p><p id="a45c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[7]阿德里安·罗斯布鲁克。" ImageNet: Vggnet、ResNet、Inception和Xception with Keras . "<em class="le"> PyImageSearch </em>，2021年6月17日，<a class="ae iu" href="https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/." rel="noopener ugc nofollow" target="_blank">https://www . PyImageSearch . com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/。</a></p><p id="f891" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[8]维萨尔，赫瓦贾。"对新图像尺寸的迁移学习."<em class="le">数据科学栈交流【2019年8月18日<a class="ae iu" href="https://datascience.stackexchange.com/questions/57639/transfer-learning-on-new-image-size." rel="noopener ugc nofollow" target="_blank">https://Data Science . Stack Exchange . com/questions/57639/transfer-learning-on-new-image-size。</a></em></p></div></div>    
</body>
</html>