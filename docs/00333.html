<html>
<head>
<title>How to Scale-out Apache Airflow 2.0 with Redis and Celery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Redis和Celery扩展Apache Airflow 2.0</h1>
<blockquote>原文：<a href="https://medium.com/codex/how-to-scale-out-apache-airflow-2-0-with-redis-and-celery-3e668e003b5c?source=collection_archive---------0-----------------------#2021-01-19">https://medium.com/codex/how-to-scale-out-apache-airflow-2-0-with-redis-and-celery-3e668e003b5c?source=collection_archive---------0-----------------------#2021-01-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="35cc" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph"><a class="ae ge" href="http://medium.com/codex" rel="noopener">法典</a></h2><div class=""/><p id="6ad9" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">Apache Airflow已经成为数据工程领域最流行的工具之一。它是一个为您提供编程创作、调度和监控工作流的平台。气流可以配置运行不同的执行器，如<a class="ae jm" href="https://airflow.apache.org/docs/apache-airflow/stable/executor/sequential.html" rel="noopener ugc nofollow" target="_blank">顺序</a>、<a class="ae jm" href="https://airflow.apache.org/docs/apache-airflow/stable/executor/debug.html" rel="noopener ugc nofollow" target="_blank">调试</a>、<a class="ae jm" href="https://airflow.apache.org/docs/apache-airflow/stable/executor/local.html" rel="noopener ugc nofollow" target="_blank">本地</a>、<a class="ae jm" href="https://airflow.apache.org/docs/apache-airflow/stable/executor/dask.html" rel="noopener ugc nofollow" target="_blank"> Dask </a>、<a class="ae jm" href="https://airflow.apache.org/docs/apache-airflow/stable/executor/celery.html" rel="noopener ugc nofollow" target="_blank">芹菜</a>、<a class="ae jm" href="https://airflow.apache.org/docs/apache-airflow/stable/executor/kubernetes.html" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a>和<a class="ae jm" href="https://airflow.apache.org/docs/apache-airflow/stable/executor/celery_kubernetes.html" rel="noopener ugc nofollow" target="_blank"> CeleryKubernetes </a>。作为一名数据工程师，您通常会在本地机器上配置Apache Airflow，以便在开发工作中使用顺序或本地执行器运行。但是，这种设置会阻止您在分布式环境中测试工作流。在本文中，我将带您一步一步地组装docker-compose，这将允许您在本地机器上模拟一个分布式环境。我们将使用Celery Executor设置气流，它将把负载分配给工作节点。我在这里讨论的方法可以扩展到在分布式生产环境中部署。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/d2ef751606bbd43e5912e297d0a37a7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUQlFLtTS_E9_K5I4FlLvQ.jpeg"/></div></div></figure><h1 id="f3db" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">先决条件:</h1><ul class=""><li id="a612" class="kx ky hi iq b ir kz iv la iz lb jd lc jh ld jl le lf lg lh bi translated">Linux (64位)</li><li id="a273" class="kx ky hi iq b ir li iv lj iz lk jd ll jh lm jl le lf lg lh bi translated">码头工人</li><li id="c706" class="kx ky hi iq b ir li iv lj iz lk jd ll jh lm jl le lf lg lh bi translated">docker-撰写</li></ul><h1 id="26c2" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤1:结构设置</h1><p id="b933" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">让我们首先明确我们将如何使用Docker建立一个分布式气流。在这个设置中，我们将使用Postgres作为后端来存储元数据，使用Redis作为消息代理。我们可以选择使用RabbitMQ而不是Redis然而，Redis比RabbitMQ更容易部署和维护。后者需要额外的努力来维护，并且很难调试崩溃。</p><p id="75b3" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">如下图所示，我们将使用docker-compose旋转六个不同的docker容器。Airflow服务器及其调度程序将共享同一个容器。我们将为Postgres和Redis使用公开的docker图像。这两个容器将用于工作节点，最后一个容器将专用于监视工作节点。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lq"><img src="../Images/6c8e395e8ebe4a8bd125936f801ecd82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*JQUBRf6OscKNdrXpk8XTfQ.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">多码头集装箱中的气流成分</figcaption></figure><p id="9eea" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">首先，让我们看看这个项目的文件夹结构。父文件夹airflow-docker包含两个文件夹(dags和config)和三个文件(docker-compose.yml、Dockerfile和entry point . sh)。DAGs文件夹包含一个示例DAG(sample _ DAG . py ),我们将在最后执行它来演示我们的分布式环境。config文件夹包含一个环境变量文件和几个支持气流配置设置的文件。</p><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="8640" class="ma ka hi lw b fi mb mc l md me">airflow-docker<br/>|__dags<br/>   |__sample-dag.py<br/>|__config<br/>   |__common.env<br/>   |__connections.yml<br/>   |__requirements.txt<br/>   |__setup_connections.py<br/>   |__variables.json<br/>|__docker-compose.yml<br/>|__Dockerfile<br/>|__entrypoint.sh</span></pre><h1 id="c731" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤2:定义环境变量和额外的Python包</h1><p id="14d1" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">让我们首先为Postgres、Redis和Airflow定义我们将在docker-compose.yml文件中使用的环境变量。出于简化的原因，我选择使用单个环境文件；但是，您可以将它们分割成单独的文件，或者直接在docker-compose文件中定义它们。</p><ul class=""><li id="4fce" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> common.env </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="8294" class="ma ka hi lw b fi mb mc l md me"># Postgres DB <br/>POSTGRES_USER=airflow<br/>POSTGRES_PASSWORD=airflow<br/>POSTGRES_DB=airflow<br/>POSTGRES_PORT=5432<br/>POSTGRES_HOST=postgres</span><span id="4090" class="ma ka hi lw b fi mi mc l md me"># Redis<br/>REDIS_HOST=redis<br/>REDIS_PORT=6379</span><span id="efd0" class="ma ka hi lw b fi mi mc l md me"># Airflow User <br/>AF_USER_NAME=airflow<br/>AF_USER_PASSWORD=airflow<br/>AF_USER_FIRST_NAME=Tejas<br/>AF_USER_LAST_NAME=M<br/>AF_USER_EMAIL=test@test.com<br/>AF_USER_ROLE=Admin</span></pre><p id="83c7" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">使用requirements.txt文件安装您可能需要的任何其他python包。</p><ul class=""><li id="37b5" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> requirements.txt </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="5b07" class="ma ka hi lw b fi mb mc l md me">snowflake-connector-python==2.3.8</span></pre><h1 id="973c" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤3:定义气流的连接和变量</h1><p id="365c" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">您需要的所有气流连接都可以在这个YAML文件中定义。</p><ul class=""><li id="1cd2" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> connections.yml </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="ea2e" class="ma ka hi lw b fi mb mc l md me">SAMPLE_DB:<br/>  conn_type: mysql<br/>  host: 192.168.254.210<br/>  login: myuser<br/>  password: mypassword<br/>  schema: sample_db<br/>  port: 3306<br/>SAMPLE_HTTP:<br/>  conn_type: http<br/>  login: myuser<br/>  password: mypassword</span></pre><p id="632b" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">任何需要的变量都可以添加到variable.json文件中。</p><ul class=""><li id="c944" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> variables.json </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="a613" class="ma ka hi lw b fi mb mc l md me">{<br/>    "NOTIFICATION_EMAILS": "test@test.com",<br/>    "sample_api_secret": "abc123"<br/>}</span></pre><h1 id="a8c3" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤4: Python脚本在Airflow中创建连接</h1><p id="caa4" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">我们将使用以下python脚本来创建上一步中定义的所有连接。</p><ul class=""><li id="bf74" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> setup_connections.py </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="6e59" class="ma ka hi lw b fi mb mc l md me">import yaml<br/>from airflow import settings<br/>from airflow.models import Connection<br/><br/>with open("connections.yml", "r") as f:<br/>    connection_dict = yaml.safe_load(f.read())</span><span id="7d12" class="ma ka hi lw b fi mi mc l md me">for connection_name, connection in connection_dict.items():<br/>    conn = Connection(<br/>        conn_id=connection_name,<br/>        conn_type=connection.get('conn_type'),<br/>        host=connection.get('host'),<br/>        login=connection.get('login'),<br/>        password=connection.get('password'),<br/>        port=connection.get('port'),<br/>        schema=connection.get('schema'),<br/>        extra=connection.get('extra')<br/>    ) <br/>    session = settings.Session<br/>    session.add(conn)<br/>    session.commit()</span></pre><h1 id="ad96" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤5:创建用于测试的DAG文件</h1><p id="8540" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">这是一个示例DAG文件，用于演示我们的分布式气流工作环境。我们使用两个BashOperators，每个都将在独立的worker节点上运行。</p><ul class=""><li id="4b8a" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> sample-dag.py </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="458f" class="ma ka hi lw b fi mb mc l md me">from airflow import DAG<br/>from airflow.operators.bash_operator import BashOperator<br/>from datetime import datetime, timedelta</span><span id="16b3" class="ma ka hi lw b fi mi mc l md me">default_args = {<br/>  'owner': 'airflow',<br/>  'depends_on_past': False,<br/>  'start_date': datetime(2021, 1, 15),<br/>  'email': ['<a class="ae jm" href="mailto:airflow@example.com" rel="noopener ugc nofollow" target="_blank">airflow@example.com</a>'],<br/>  'email_on_failure': False,<br/>  'email_on_retry': False,<br/>  'retries': 1,<br/>  'retry_delay': timedelta(minutes=5),<br/>}</span><span id="ecea" class="ma ka hi lw b fi mi mc l md me">dag = DAG(  'dist_example',<br/>        schedule_interval='0 0 * * *' ,<br/>                catchup=False,<br/>        default_args=default_args<br/>    )<br/>create_command = 'echo $(hostname)'</span><span id="f5a1" class="ma ka hi lw b fi mi mc l md me">t1 = BashOperator(<br/>  task_id='task_for_q1',<br/>  bash_command=create_command,<br/>  queue='queue_1',<br/>  dag=dag<br/>)</span><span id="25a5" class="ma ka hi lw b fi mi mc l md me">t2 = BashOperator(<br/>  task_id= 'task_for_q2',<br/>  bash_command=create_command,<br/>  queue='queue_2',<br/>  dag=dag<br/>)</span><span id="11ea" class="ma ka hi lw b fi mi mc l md me">t1 &gt;&gt; t2</span></pre><h1 id="841a" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤6:准备Dockerfile以创建基础映像</h1><p id="3fdd" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">我们将使用<strong class="iq hs"> python:3.8-slim-buster </strong>映像用于Airflow服务器、调度程序、工作节点和工作监视器。我们正在安装netcat，它将在entrypoint.sh脚本中用于检查Postgres和Redis的服务状态。我们还安装了vim编辑器，以防您出于调试目的需要查看docker中的任何配置文件。</p><p id="17cd" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">最后，我们使用pip来安装apache-airflow和所选的包。在步骤2中，您还可以在requirements.txt中定义任何额外的python包。</p><ul class=""><li id="8a78" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> Dockerfile </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="bc6d" class="ma ka hi lw b fi mb mc l md me">FROM python:3.8-slim-buster<br/>RUN apt-get update &amp;&amp; \<br/>    apt-get install -y netcat &amp;&amp; \<br/>    apt-get install vim -y &amp;&amp; \<br/>    pip3 install apache-airflow[celery,redis,postgres,crypto]==2.0.0<br/>WORKDIR /root/airflow<br/>COPY config/variables.json variables.json<br/>COPY config/connections.yml connections.yml<br/>COPY config/setup_connections.py setup_connections.py<br/>COPY config/requirements.txt requirements.txt<br/>COPY entrypoint.sh entrypoint.sh<br/>COPY dags/ dags<br/>ENTRYPOINT ["./entrypoint.sh"]</span></pre><h1 id="1b8b" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤7:定义气流停靠图像的入口点</h1><p id="aa97" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">我们将使用这个参数化的shell脚本通过docker-compose.yml启动web服务器、工作节点和工作监视器。</p><ul class=""><li id="5c19" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> entrypoint.sh </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="a3f0" class="ma ka hi lw b fi mb mc l md me">#!/usr/bin/env bash<br/>TRY_LOOP="20"<br/>wait_for_port() {<br/>  local name="$1" host="$2" port="$3"<br/>  local j=0<br/>  while ! nc -z "$host" "$port" &gt;/dev/null 2&gt;&amp;1 &lt; /dev/null; do<br/>    j=$((j+1))<br/>    if [ $j -ge $TRY_LOOP ]; then<br/>      echo &gt;&amp;2 "$(date) - $host:$port still not reachable, giving up"<br/>      exit 1<br/>    fi<br/>    echo "$(date) - waiting for $name $host... $j/$TRY_LOOP"<br/>    sleep 5<br/>  done<br/>}<br/>create_airflow_user() {<br/>  airflow users create \<br/>  --username "$AF_USER_NAME" \<br/>  --firstname "$AF_USER_FIRST_NAME" \<br/>  --lastname "$AF_USER_LAST_NAME" \<br/>  --role "$AF_USER_ROLE" \<br/>  --email "$AF_USER_EMAIL" \<br/>  --password "$AF_USER_PASSWORD"<br/>}<br/>setup_airflow_variables() {<br/>    if [ -e "variables.json" ]; then<br/>      echo "Start importing Airflow variables"<br/>      airflow variables import variables.json<br/>    fi<br/>}<br/>setup_airflow_connections() {<br/>    if [ -e "connections.yml" ]; then<br/>      echo "Start setting up Airflow connections"<br/>      python3 setup_connections.py<br/>    fi<br/>}<br/>install_python_packages() {<br/>    if [ -e "requirements.txt" ]; then<br/>      echo "Installing additional Python packages"<br/>      pip3 install -r requirements.txt<br/>    fi<br/>}<br/>wait_for_port "Postgres" "$POSTGRES_HOST" "$POSTGRES_PORT"<br/>wait_for_port "Redis" "$REDIS_HOST" "$REDIS_PORT"</span><span id="b642" class="ma ka hi lw b fi mi mc l md me">export AIRFLOW__CORE__SQL_ALCHEMY_CONN \<br/>AIRFLOW__CELERY__RESULT_BACKEND \<br/>AIRFLOW__CORE__FERNET_KEY \<br/>AIRFLOW__CORE__LOAD_EXAMPLES \<br/>AIRFLOW__CORE__EXECUTOR</span><span id="43a0" class="ma ka hi lw b fi mi mc l md me">AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgres+psycopg2://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB"<br/>AIRFLOW__CELERY__RESULT_BACKEND="db+postgresql://$POSTGRES_USER:$POSTGRES_PASSWORD@$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB"<br/>AIRFLOW__CORE__LOAD_EXAMPLES=False<br/>AIRFLOW__CORE__EXECUTOR=CeleryExecutor<br/>AIRFLOW__CORE__FERNET_KEY="wVKXj_gUFi0scVsP-HARZYyxxihQCpj3B2gA_ERaIBE="</span><span id="6113" class="ma ka hi lw b fi mi mc l md me">case "$1" in<br/>    webserver)<br/>        airflow db init<br/>        sleep 10<br/>        create_airflow_user<br/>        setup_airflow_variables<br/>        setup_airflow_connections<br/>        install_python_packages<br/>        exec airflow scheduler &amp;<br/>        exec airflow webserver<br/>        ;;<br/>    worker)<br/>        airflow db init<br/>        sleep 10<br/>        install_python_packages<br/>        exec airflow celery "$@" -q "$QUEUE_NAME"<br/>        ;;<br/>    flower)<br/>        airflow db init<br/>        sleep 10<br/>        exec airflow celery "$@"<br/>        ;;<br/>    *)<br/>        exec "$@"<br/>        ;;<br/>esac</span></pre><h1 id="4fda" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">第八步:把它们放在一起，组成一个文档</h1><p id="1141" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">这是在我们启动分布式气流环境之前将所有东西打包在一起的最后一步。我们有六种不同的服务，它们与我在步骤1中用图形描述的相匹配。Redis容器将在端口6379上运行，Postgres容器将在端口5432上运行。我们还通过环境文件common.env将所需的值传递给Postgres映像。Redis和Postgres都可以独立启动。</p><p id="b697" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">Airflow服务器需要Postgres作为元数据，因此我们在docker-compose文件中添加了一个“depends_on”标签，从而增加了依赖性。web服务器通过端口8080可用。这两个工作线程在单独的端口上可用，第一个工作线程在端口8081上可用，第二个工作线程在8082上可用。每个工作线程都与一个队列相关联。队列的名称是在名为QUEUE_NAME的环境变量下定义的。最后，工人监视器，芹菜花，将在端口5555上可用。</p><ul class=""><li id="707d" class="kx ky hi iq b ir is iv iw iz mf jd mg jh mh jl le lf lg lh bi translated"><strong class="iq hs"> docker-compose.yml </strong></li></ul><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="310b" class="ma ka hi lw b fi mb mc l md me">version: '3.7'<br/>services:</span><span id="07ef" class="ma ka hi lw b fi mi mc l md me">redis:<br/>    image: redis:latest<br/>    ports:<br/>      - "6379:6379"<br/>  <br/>  postgres:<br/>    image: postgres:9.6<br/>    env_file:<br/>      - config/common.env<br/>    ports:<br/>      - "5432:5432"<br/>    healthcheck:<br/>      test: [ "CMD-SHELL", "pg_isready -U airflow" ]<br/>      interval: 10s<br/>      timeout: 5s<br/>      retries: 5</span><span id="d282" class="ma ka hi lw b fi mi mc l md me">airflow-webserver:<br/>      build: .<br/>      restart: always<br/>      command: webserver<br/>      depends_on:<br/>        postgres:<br/>          condition: service_healthy<br/>      ports:<br/>        - "8080:8080"<br/>      env_file: <br/>        - config/common.env<br/>      healthcheck:<br/>        test: [ "CMD-SHELL", "[ -f /airflow/airflow-webserver.pid ]" ]<br/>        interval: 30s<br/>        timeout: 30s<br/>        retries: 3</span><span id="f541" class="ma ka hi lw b fi mi mc l md me">airflow-worker-1:<br/>      build: .<br/>      command: worker<br/>      restart: always<br/>      depends_on:<br/>        - airflow-webserver<br/>      ports:<br/>        - "8081:8080"<br/>      env_file: <br/>        - config/common.env<br/>      environment:<br/>        - QUEUE_NAME=queue_1</span><span id="a989" class="ma ka hi lw b fi mi mc l md me">airflow-worker-2:<br/>      build: .<br/>      command: worker<br/>      restart: always<br/>      depends_on:<br/>        - airflow-webserver<br/>      ports:<br/>        - "8082:8080"<br/>      env_file: <br/>        - config/common.env<br/>      environment:<br/>        - QUEUE_NAME=queue_2</span><span id="9963" class="ma ka hi lw b fi mi mc l md me">airflow-flower:<br/>      build: .<br/>      command: flower<br/>      restart: always<br/>      depends_on:<br/>        - airflow-worker-1<br/>        - airflow-worker-2<br/>      ports:<br/>        - "5555:5555"<br/>      env_file: <br/>        - config/common.env</span></pre><h1 id="b3a3" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤9:部署分布式气流</h1><p id="7c33" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">如果一切都配置正确，您应该能够从airflow-docker目录执行以下命令。如果您在启动docker容器时遇到任何问题，请检查您的配置文件。</p><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="42ba" class="ma ka hi lw b fi mb mc l md me">docker-compose up -d --build</span></pre><p id="54ed" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">让我们通过执行以下命令来确认您的所有容器都已启动并运行:</p><pre class="jo jp jq jr fd lv lw lx ly aw lz bi"><span id="bb7b" class="ma ka hi lw b fi mb mc l md me">docker ps</span></pre><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mj"><img src="../Images/148cefbca7d8f1331093037665c4ff57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SNqDBZUImZ2ZMlqwfep28Q.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">正在运行的容器列表</figcaption></figure><p id="c00c" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">如果您看到所有六个容器都在运行，那么您已经在本地机器上成功地部署了您的分布式气流环境。现在，让我们通过执行示例DAG文件来验证它是否在分布式模式下工作。导航到位于<a class="ae jm" href="http://localhost:8080" rel="noopener ugc nofollow" target="_blank"> http://localhost:8080 </a>的Airflow UI，使用您在common.env文件中定义的用户名和密码登录。登录后，您应该会看到一个名为“dist_example”的DAG。一旦你解除暂停，它将在几秒钟后执行。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mk"><img src="../Images/a8186803aacf4983057481694f9bc171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jafs42T2A4iMDBPPffLw2A.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">气流UI DAGs视图</figcaption></figure><p id="2b7c" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">如果它成功运行，您应该检查它的每个任务的日志，以确保这两个任务在不同的工作节点上运行。您应该会看到两个日志的不同主机名，如下所示。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es ml"><img src="../Images/ba6eb1a1935bc3ac1489a88aae7caf9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_ezT9WdDyvlqBKyziceXQ.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">任务1的日志(task_for_q1)</figcaption></figure><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mm"><img src="../Images/ecf0a298f4d2cb6a9c42acfdc9d89275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BEKhHlU4LtGqTcqezfXBFg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">任务2的日志(任务_第二季度)</figcaption></figure><p id="9939" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">您也可以通过在<a class="ae jm" href="http://localhost:5555/dashboard" rel="noopener ugc nofollow" target="_blank">http://localhost:5555/dashboard</a>导航到Worker Monitor(芹菜花)来确认这一点。您应该会看到类似的输出，如下图所示。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mn"><img src="../Images/6ce586d1cd09fb4650ec52237f39bca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1GrqmmpRAWBo79Ya1cVqFg.png"/></div></div></figure><p id="5d7d" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">如果您已经做到了这一步，那么您已经成功地部署了一个分布式气流环境。恭喜你！</p><h1 id="6895" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h1><p id="effd" class="pw-post-body-paragraph io ip hi iq b ir kz it iu iv la ix iy iz ln jb jc jd lo jf jg jh lp jj jk jl hb bi translated">在本文中，我解释了如何在您的本地机器上用Redis和Celery快速建立一个分布式气流环境。该设置将在工作流开发和测试阶段为您提供帮助。我希望这篇文章中的所有脚本可以帮助您作为一个起点，快速建立一个满足您需要的环境。我希望这些逐步说明对你有用。请在下面留下你对这篇文章的评论。</p></div></div>    
</body>
</html>