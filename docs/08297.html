<html>
<head>
<title>NLP -Beginners Guide to Classification in 4 Steps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP-初学者分类指南4个步骤</h1>
<blockquote>原文：<a href="https://medium.com/codex/nlp-beginners-guide-to-classification-in-4-steps-18bbf5a24b50?source=collection_archive---------7-----------------------#2022-07-29">https://medium.com/codex/nlp-beginners-guide-to-classification-in-4-steps-18bbf5a24b50?source=collection_archive---------7-----------------------#2022-07-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/23700051217af5e1a7833171fd1de742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*kvJCEEmLurwwLQIYcqrjog.png"/></div></figure><p id="54ae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">机器学习和“自然语言处理”(NLP)都可以揭示这些数据集中以前看不见的模式，但也可以自动化某些任务，解放人们去做机器不能做的更高价值、更具创造性的工作。在本文中，我将尝试用简单的语言简洁地解释NLP，并展示它如何用于帮助非营利组织。</p><h1 id="4fb2" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">数据问——“我们以前见过这样的文字。”:分类</h1><p id="b37f" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">我们来看看让机器学习如此成功的经典而普遍的任务:<em class="ko">分类</em>。分类采用一组输入特征并产生一个输出分类，通常是二进制的是/否。</p><p id="4fad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我从美国各种非盈利网站上搜集信息(我将在另一篇文章中讨论网络搜集)。我的起点是一个大的excel文件，包含了所有美国的非营利组织。问题是</p><ol class=""><li id="e54d" class="kp kq hi io b ip iq it iu ix kr jb ks jf kt jj ku kv kw kx bi translated">从所有内容中提取电子邮件和电话号码</li><li id="179b" class="kp kq hi io b ip ky it kz ix la jb lb jf lc jj ku kv kw kx bi translated">如果列“CJ_Mission？”正确识别了那些与刑事司法有关的非营利组织。简单！</li></ol><p id="ce67" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">过滤电子邮件和电话号码→考虑到时间限制，Python中的这两行代码可以做得很好。</p><p id="7bbe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> Github链接</strong>:这里是<a class="ae jk" href="https://github.com/abhinayasridharrajaram/NLP_Simplified-Classfication" rel="noopener ugc nofollow" target="_blank"> GitHub </a>链接，看看数据是什么样子的:</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="c99f" class="lm jm hi li b fi ln lo l lp lq"><br/>df[‘Phone’] = df[‘Content’].str.extract(r’((?:\(\d{3}\)|\d{3})(?:\s|\s?-\s?)?\d{3}(?:\s|\s?-\s?)?\d{4})’,expand= True)</span><span id="1a77" class="lm jm hi li b fi lr lo l lp lq">df[‘E-mail’] = df[‘Content’].str.extract(r’([a-zA-Z][a-zA-Z0–9._%+-]+@[a-zA-Z0–9.-]+\.[a-zA-Z]+)’)</span></pre><p id="1d63" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">问题陈述</strong>:很简单。弄清楚非营利组织是否在刑事司法领域运作。数据中已经有一栏是实习生输入的关于非营利组织是否在刑事司法领域运作的信息(是，否)。要求是快速交叉检查所有这些。</p><h2 id="453c" class="lm jm hi bd jn ls lt lu jr lv lw lx jv ix ly lz jz jb ma mb kd jf mc md kh me bi translated">EDA上的数据</h2><p id="0e47" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">我将使用实际数据的一个很小的子集来演示工作方式&amp;我还删除了原始报告中多余的数据列。</p><p id="1de1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">看这里:</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mf"><img src="../Images/f0250cdb8d0ddc40e742a1f589dcf89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9GGQTplteZJz_iGsZ2wHgg.png"/></div></div></figure><p id="4898" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">可以看到目标变量“CJ_Mission？”“是”和“不是”的程度几乎相等。很好。</p><p id="9306" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">刑事司法非营利组织似乎更罗嗦，因为平均来说它包含更多的单词。见下文。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="ce72" class="lm jm hi li b fi ln lo l lp lq">data['word_count'] = data['Mission Statement'].apply(lambda x: len(str(x).split()))<br/>print(' Avg words used in  Mission Statement column of Criminal Justice non-profits is ',data[data['CJ_Mission?']=='Yes']['word_count'].mean()) #Disaster tweets<br/>print('Avg words used in  Mission Statement column of NON- Criminal Justice non-profits is',data[data['CJ_Mission?']=='No']['word_count'].mean())<br/># PLOTTING WORD-COUNT<br/>fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))<br/>train_words=data[data['CJ_Mission?']== 'Yes']['word_count']<br/>ax1.hist(train_words,color='red')<br/>ax1.set_title('Criminal Justice')<br/>train_words=data[data['CJ_Mission?']== 'No']['word_count']<br/>ax2.hist(train_words,color='green')<br/>ax2.set_title('Non-Criminal-Justice')<br/>fig.suptitle('Words per Mission Statement')<br/>plt.show()</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mk"><img src="../Images/b12c58705b2cd8c6008f8e84fe2ccc5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vJjULMD81eus2OLKtYulzg.png"/></div></div></figure><h1 id="2d5f" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">总体方法:</h1><p id="07ef" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">一种结合两个机器学习分类器的算法。每个分类者都会阅读非营利组织“使命陈述”一栏的简要描述，然后决定它是否与刑事司法有关。</p><h2 id="51c1" class="lm jm hi bd jn ls lt lu jr lv lw lx jv ix ly lz jz jb ma mb kd jf mc md kh me bi translated">关于文本预处理你需要知道的事情</h2><p id="35ef" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">数据预处理是准备原始数据以使其适合机器学习模型的阶段。对于NLP，这包括文本清理、停用词移除、词干提取和词汇化。</p><blockquote class="ml mm mn"><p id="c3f9" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated">什么是停用词？-停用词是任何语言中的<strong class="io hj">词</strong>，它不会给句子增加太多意义。它们可以被安全地忽略，而不会牺牲句子的意义。对于一些搜索引擎，这些是一些最常见的短功能词，如，是，在，这，等等。</p><p id="c477" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated"><strong class="io hj">什么时候去掉停用词？- </strong>如果我们的任务是文本分类或情感分析，那么我们应该删除停用词，因为它们不会为我们的模型提供任何信息，即<strong class="io hj">将不需要的词排除在我们的语料库之外。对于何时删除停用字词，没有硬性规定。</strong></p><p id="0722" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated"><strong class="io hj">如何删除停用词</strong> -我们将使用一种称为雪球词干分析器的词干算法，也称为Porter2词干算法。</p><p id="0d61" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated"><strong class="io hj">什么是词干？- </strong>简单来说，词干化就是把一个词简化为它的基本词或词干，使同类的词位于一个共同的词干之下。例如——care、cared和care这几个词都在care这个词干下。</p><p id="c72f" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated"><strong class="io hj">文本清理</strong>步骤因数据类型和所需任务而异。通常，字符串被转换成小写，标点符号在文本被标记化之前被删除。<strong class="io hj">记号化</strong>是将一个字符串拆分成一系列字符串(或“记号”)的过程。</p></blockquote><h1 id="7fcb" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">实现这一目标的步骤:</h1><h1 id="28db" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">步骤1:创建一个停用词列表</h1><p id="2fb2" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">因此，首先，我们将对单词进行词干处理，以减少单词的词根，从而限制基于时态或它们是以复数还是所有格形式出现的差异。然后，我们将剥离出一个<strong class="io hj">自定义的停用词列表。</strong></p><h2 id="1fe1" class="lm jm hi bd jn ls lt lu jr lv lw lx jv ix ly lz jz jb ma mb kd jf mc md kh me bi translated">自定义停用词</h2><p id="57ec" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">你可能会想，<strong class="io hj">为什么要自定义停用词</strong>？虽然使用一组已发布的停用词相当容易，但在许多情况下(比如这次),使用这样的停用词对于某些应用程序来说是完全不够的。例如，在临床文本中，像“mcg”、“dr”和“patient”这样的术语几乎出现在你遇到的每个文档中。因此，这些术语可以被视为临床文本挖掘和检索的潜在停用词。让我们找到自己的。有3种方法可以做到这一点，我将选择前两种方法的组合。</p><blockquote class="ml mm mn"><p id="1360" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated">1.作为停用词的最常用术语</p><p id="0746" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated">你可以把前N项作为你的停用词。您还可以在排序之前消除常见的英语单词(使用发布停用词列表),以确保您的目标是特定于领域的停用词。</p><p id="c14f" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated">2.作为停用词的最不常用术语</p><p id="eceb" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated">极不常用的术语也可能对文本挖掘和检索没有用处。然而，尽管所有的标准化，如果术语仍然有一个术语频率计数，你可以删除它。这可能会大大减少您的整体功能空间。</p><p id="4ef2" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated">3.低IDF术语作为停用词</p><p id="1ed6" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated"><a class="ae jk" href="http://kavita-ganesan.com/text-mining-cheat-sheet/#.W1olu9hKids" rel="noopener ugc nofollow" target="_blank">逆文档频率(IDF) </a>基本上是指您的集合中包含特定术语ti的文档的逆分数。假设您有N个文档。并且术语ti出现在N个文档中的M个文档中。因此，ti的IDF计算如下:</p><p id="2891" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated">IDF(ti)=对数N/M</p><p id="b3ad" class="im in ko io b ip iq ir is it iu iv iw mo iy iz ja mp jc jd je mq jg jh ji jj hb bi translated">所以ti出现的文档越多，IDF得分越低。这意味着出现在每个文档中的术语的IDF值为0。如果您按照IDF分数降序排列集合中的每个ti，您可以将IDF分数最低的K个术语视为停用词。</p></blockquote><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="fed8" class="lm jm hi li b fi ln lo l lp lq"># Define a standard snowball stemmer<br/>STEM = SnowballStemmer('english')<br/># Make a list of stopwords, including the stemmed versions<br/># These are words that have no impact on the classification, and<br/># can even occasionally mess up the classifier.</span><span id="5087" class="lm jm hi li b fi lr lo l lp lq">STOP = ['in',<br/>    'non profit',<br/>    'from',<br/>    'the',<br/>    'and',<br/>    'their', <br/>    'after',<br/>    'for',<br/>    'in',<br/>    'that',<br/>    'our',<br/>    'we',<br/>    'that',<br/>    'mission',<br/>    'our',<br/>    'in ',<br/>    'of',<br/>    'to',<br/>    'a', 'people', 'IN', 'social', 'community' 'BY', 'OF', 'IN',<br/>    'and',<br/>    'to',<br/>    'the',<br/>    'of ',<br/>    'AND',<br/>    'the',<br/>    'THE',<br/>    'and',<br/>    'in ',<br/>    'as',<br/>    'is',<br/>    'by',<br/>    'of',<br/>    'to',<br/>    'a', 'CIVIL', 'organization'<br/>]</span><span id="85a0" class="lm jm hi li b fi lr lo l lp lq">STOP += [STEM.stem(i) for i in STOP]<br/>print(STOP)<br/>STOP = list(set(STOP))<br/>print(STOP)</span></pre><p id="3fd2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">注:我只是想出了我自己的一些手动调谐，检查和调整的基础上。对这个小的子集运行这个会让你知道什么是重要的，什么是不重要的，单词的频率等等。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="d70e" class="lm jm hi li b fi ln lo l lp lq">from collections import Counter<br/>Counter(" ".join(df3["Mission Statement"]).split()).most_common(100)</span></pre><h1 id="bdd8" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">第二步:标记化</h1><p id="2dc9" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">这是一个函数，我们将一个描述分解成单个的“特征”,我们将使用这些特征对其进行分类。我们将描述分成单个的单词，然后对它们进行词干处理，去掉无用的单词。从那里，我们列出单个单词，然后将它们组合成<a class="ae jk" href="https://en.wikipedia.org/wiki/Bigram" rel="noopener ugc nofollow" target="_blank">二元模型</a>。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="5f7f" class="lm jm hi li b fi ln lo l lp lq">def tokenize(desc):<br/>    """<br/>    Takes description text, strips out unwanted words and text,<br/>    and prepares it for the trainer.<br/>    """<br/>    # first lower case and strip leading/trailing whitespace<br/>    desc = desc.lower().strip()<br/>    # kill the 'do-'s and any stray punctuation<br/>    desc = desc.replace('do-', '').replace('.', '').replace(',', '')<br/>    # make a list of words by splitting on whitespace<br/>    words = desc.split(' ')<br/>    # Make sure each "word" is a real string / account for odd whitespace<br/>    words = [STEM.stem(i) for i in words if i]<br/>    <br/>    words = [i for i in words if i not in STOP]<br/>    # let's see if adding bigrams improves the accuracy<br/>    bigrams = ngrams(words, 2)<br/>    bigrams = ["%s|%s" % (i[0], i[1]) for i in bigrams]<br/>    # bigrams = [i for i in bigrams if STEMMED_BIGRAMS.get(i)]<br/>    # set up a dict<br/>    output = dict([(i, True) for i in words + bigrams])<br/>    # The NLTK trainer expects data in a certain format<br/>    return output</span></pre><h1 id="29e6" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">步骤3:提取特征</h1><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="d358" class="lm jm hi li b fi ln lo l lp lq"># open our sample file and use the CSV module to parse it<br/>f = open('NP_CJ_train_.csv', 'rU')<br/>data = list(csv.DictReader(f))</span><span id="0838" class="lm jm hi li b fi lr lo l lp lq"># Make an empty list for our processed data<br/>qualities  = []<br/># Loop through all the lines in the CSV<br/>for i in data:<br/>    desc = i.get('Mission Statement')<br/>    classify = i.get('CJ_Mission?')<br/>    feats = tokenize(desc)<br/>    qualities.append((feats, classify))</span><span id="5003" class="lm jm hi li b fi lr lo l lp lq">f.close()</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mr"><img src="../Images/ecb562732d64a8f227c509ee05b9f7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYGHJbZqEXU_5Y0pMW2KWA.png"/></div></div></figure><h1 id="3193" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">步骤4:训练分类器</h1><p id="5c72" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">在这个分析中，我们使用了两个机器学习分类器。第一个是来自<a class="ae jk" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn Python库</a>的线性<a class="ae jk" href="http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html" rel="noopener ugc nofollow" target="_blank">支持向量机</a>。第二个是最大熵分类器。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="9595" class="lm jm hi li b fi ln lo l lp lq"># Train our classifiers. Let's start with Linear SVC<br/># Make a data prep pipeline<br/>pipeline = Pipeline([<br/>    ('tfidf', TfidfTransformer()),<br/>    ('linearsvc', LinearSVC()),<br/>])<br/># make the classifier<br/>linear_svc = SklearnClassifier(pipeline)<br/># Train it<br/>linear_svc.train(qualities)</span></pre><p id="b37c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在les做最大熵</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="4aa3" class="lm jm hi li b fi ln lo l lp lq">maxent = MaxentClassifier.train(qualities)</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mr"><img src="../Images/ef2ddeb68c702c76d8fbef46da073e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*37jtVJsi2zREeDMTp3lgTQ.png"/></div></div></figure><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es ms"><img src="../Images/5d10a733a8cbb342bbbcfa6e35205334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDbzc7-9x39zZw4fQYzZ4w.png"/></div></div></figure><h1 id="619c" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">测试分类器</h1><p id="643c" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">现在让我们来测试一下！对于这个例子，我们只使用了几个非营利组织的训练样本。对于官方分析，我使用了更多数据点的训练样本。我们还选择使用两个分类器，因为尽管他们在绝大多数刑事司法主题上意见一致，但有时对于边缘案件，一个分类器会做得更好。让我们检查结果。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="f651" class="lm jm hi li b fi ln lo l lp lq">test_data = list(csv.DictReader(open('NP_CJ_test_.csv', 'rU')))<br/>d =[]<br/>for i in test_data:<br/>    desc = i.get('Mission Statement')<br/>    <br/>    classify = i.get('CJ_Mission?')<br/>    <br/>    tokenized = tokenize(desc)<br/>    <br/>    # now grab the results of our classifiers<br/>    maxent_class = maxent.classify(tokenized)<br/>  <br/>    <br/>    svc_class = linear_svc.classify(tokenized)<br/>    # Generate result using pandas<br/>    print('actual: %s | maxent: %s | linear_svc: %s |' % (classify, maxent_class, svc_class))<br/>    <br/>   <br/>    d.append(( classify,maxent_class, svc_class))<br/>   <br/>    #print(d[-1])</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mt"><img src="../Images/20629628405f74d0130e38c80c577fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5kPqsFmsuic6tjeWe7vOzg.png"/></div></div></figure><h1 id="55e2" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">快速检查结果</h1><p id="6537" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">对于SVC，这里只有一个错误的预测项。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="65b1" class="lm jm hi li b fi ln lo l lp lq">#print(d)<br/>df2 = pd.DataFrame(d)<br/>df2.columns = ['Correct_label', 'maxent_predict', 'svc_predict']<br/>#print (df2)<br/>contingency_matrix = pd.crosstab(df2['svc_predict'], df2['Correct_label'])</span><span id="030d" class="lm jm hi li b fi lr lo l lp lq">print(contingency_matrix)</span><span id="b330" class="lm jm hi li b fi lr lo l lp lq">from sklearn import metrics<br/>target_names = ['Yes', 'No']</span><span id="62c3" class="lm jm hi li b fi lr lo l lp lq">import matplotlib.pyplot as plt<br/>import seaborn as sn</span><span id="c4b3" class="lm jm hi li b fi lr lo l lp lq">plt.clf()</span><span id="3e11" class="lm jm hi li b fi lr lo l lp lq">ax = fig.add_subplot(111)</span><span id="534e" class="lm jm hi li b fi lr lo l lp lq">ax.set_aspect(1)</span><span id="34ea" class="lm jm hi li b fi lr lo l lp lq">res = sn.heatmap(contingency_matrix.T, annot=True, fmt='.2f', cmap="YlGnBu", cbar=False)</span><span id="8ce7" class="lm jm hi li b fi lr lo l lp lq">plt.savefig("crosstab_pandas.png", bbox_inches='tight', dpi=100)<br/>plt.title(' Confusion Matrix  for SVC')<br/>plt.show()</span><span id="6f05" class="lm jm hi li b fi lr lo l lp lq">print("SVC", classification_report(df2.Correct_label,df2.svc_predict , target_names=target_names))<br/>print("Maxent", classification_report(df2.Correct_label,df2.maxent_predict , target_names=target_names))</span></pre><div class="ld le lf lg fd ab cb"><figure class="mu ij mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><img src="../Images/559e7c420efc964eeb6ac0d7ae974826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*q2CUVye3NoCvmLPdvhVtjQ.png"/></div></figure><figure class="mu ij na mw mx my mz paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><img src="../Images/fea72cfce339db218e076fe935874fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*7J5jUR9O5Qt9DbY7BLGUdw.png"/></div></figure></div><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es nb"><img src="../Images/5c366778039f852ca156c30ea72de409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9N5gRIn4unVKaeJoBhCZpQ.png"/></div></div></figure><h1 id="048c" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak">结论</strong></h1><p id="8d84" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">本文展示了<strong class="io hj">如何使用NLP分析文本数据，并为机器学习模型</strong>提取特征。当然，这只是一个开始，还有很多事情可以做来改进这个模型，但我希望它为ML有志者提供了一个良好的起点。</p></div></div>    
</body>
</html>