# 对脸书基于语音的语言模型的思考

> 原文：<https://medium.com/codex/thoughts-on-speech-based-language-model-from-facebook-7db7f11f810b?source=collection_archive---------6----------------------->

![](img/3a0758dac967f16f834b7f1a5222412e.png)

马特·博茨福德在 [Unsplash](https://unsplash.com/s/photos/mic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

最近，脸书发布了一个新的语言模型——GSLM(生成口语模型)，它只使用“原始音频”，没有标签，文本，也没有自动识别。

在过去几年中，基于“文本”训练数据的超大型语言模型——如 BERT、RoBERTa 和 GPT-3——相继出现，并显示出令人印象深刻的结果。通过成功的训练，这些模型可以生成非常合理的单词，这些单词将跟随给定的输入句子，这些模型出现在许多自然语言处理应用程序的生产站点中，例如情感分析、翻译、文本摘要、文章生成等。

然而，这些模型的局限性在于它们需要大的文本数据集来进行训练。西方世界的代表性语言，包括英语，以及亚洲地区的代表性语言，如 CJK 语(中文、日语、朝鲜语)，在数据集的大小方面不会有任何大问题，但许多其他不太为人所知的“本土语言”——除了在人工智能的帮助下保存和继承本土语言的努力——确实存在用于适当训练的“不够”数据集的问题。

一个更基本的(尽管有争议)限制来自“文本”数据的固有问题。虽然我们以可互换的方式使用了几个概念，如“文本”、“语言”和“语言活动”，但如果你仔细观察，它们都是非常不同的概念。“文本”是“语言”的一个方面，虽然它包含了大量与人类“语言活动”相关的信息，但它并不代表其整体。因此，人工智能模型用‘文本’学习的世界的‘表征’与人类有很大不同。

脸书 AI 这次发布的 GSLM (Generative 口语模型)，只用‘原始音频’训练语言模型，没有‘标签’也没有‘文本’。它可以应用于难以获得大量文本数据的语言。甚至在具有大量数据的语言的情况下，我们可以很容易地想象许多应用程序(模型使用音频作为训练数据，使用音频作为直接输入/输出)取代现有的 NLP 应用程序。如果你想一想，很快就会发现，当我们将语音转换为文本时，许多信息都在中间丢失了——就像我们通过信使相互交谈时，有时必须解释更多才能传递文本的细微差别。这种来自话语的信息可能会被像脸书的 GSLM 一样创建的应用程序很好地保存和再现，并且允许更广泛的复杂的细微差别和表达。考虑到我们人类在婴儿时期是如何学习语言的，这种方法可能是一种真正的“端到端”训练神经网络的方法。神经网络可能会像我们从自己的“语言活动”中学习一样，学习更好的世界表达方式。

根据脸书·艾的说法，基线 GSLM 模型由三个部分组成:编码器(将语音音频转换为声音单元)、LM(语言模型:经过训练可以预测下一个声音单元的模型)和解码器(将声音转换为语音)。据说 GSLM 模型已经接受了 6000 小时的原始音频训练。欲了解更多关于这款车型的信息，请访问[此处](https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio/)。

![](img/d9c59a7060fb588412e33dda7fd7041f.png)

我们将发现这种新方法将如何传播，以及随着时间的推移它将产生什么影响，但已经有许多研究人员在 Twitter 和其他社交网络上交换意见。

正如 BIU 大学的约夫·戈德堡教授在推特上说的，“另一个很好的例子是，经过充分数据训练的大型网络可以真正学会模仿非常复杂的分布。[不确定它所说的“语言理解”是什么，但从学习表示复杂分布的角度来看非常非常令人印象深刻]”，这是一个令人惊讶的实验和发现。6000 小时(250 天)的原始音频似乎也不算多。

如果这个模型能够很好地作为一个预先训练的模型作为基础，与下游过程中的一些标记数据相结合(正如脸书 AI 透露的未来研究方向之一)，这种新方法将对自然语言处理系统的当前生态系统产生重大影响。

然而，有观点认为，这种新的语言模型与 2011 年基于 RNN 的现有语言模型没有根本区别，只是这种模型使用音频而不是文本。CMU 大学的 Graham Neubig 教授(在 Yoav 教授的推文中的一个帖子中)说:“我同意神经网络能够直接从语音中生成连贯的单词令人印象深刻。但是从语义上来说，生成的内容和 2011 年的 RNNLM 一样连贯:)我想还有一段路要走。”

就我个人而言，我不认为这种模式会在不久的将来演变成类似“AGI”的东西，也不相信这是一种接近“语言理解”的东西，但毫无疑问，这是一种将我们从“文本”中解放出来的新方法——文本同时是祝福和缰绳。

期待从这一努力中开启深度学习应用的新视野。