<html>
<head>
<title>Understanding Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解线性回归</h1>
<blockquote>原文：<a href="https://medium.com/codex/understanding-linear-regression-402df5a4667e?source=collection_archive---------6-----------------------#2021-01-10">https://medium.com/codex/understanding-linear-regression-402df5a4667e?source=collection_archive---------6-----------------------#2021-01-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/a9b47392713f0a62d397d1a25111cf10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dl-IaecaQ7Q7o8VQ"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">坦纳·博瑞克在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="336f" class="hw hx hy bd b fp hz ia ib ic id ie dx if translated" aria-label="kicker paragraph"><a class="ae ge" href="https://medium.com/codex" rel="noopener">抄本</a></h2><div class=""/><div class=""><h2 id="21f4" class="pw-subtitle-paragraph je ih hy bd b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv dx translated">在这篇博客中，我们将会了解机器学习中最基本的算法——线性回归</h2></div><h1 id="aa83" class="jw jx hy bd jy jz ka kb kc kd ke kf kg jn kh jo ki jq kj jr kk jt kl ju km kn bi translated">介绍</h1><p id="4b1c" class="pw-post-body-paragraph ko kp hy kq b kr ks ji kt ku kv jl kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">线性回归是一种对关系变量进行线性建模的算法。当我们说关系时，它是因变量和自变量之间的关系。在这个算法中，我们创建了一个“线性”回归线，使其“适合”我们的数据集。这是一种回归算法，这意味着它用于找出因变量的连续值。这不应该与我们的分类问题混淆。</p><p id="8621" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii">例如</strong></p><p id="ee64" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">如果你预测今天会不会下雨，你的答案要么是“会”，要么是“不会”。所以你的答案分为两组，即“是”或“否”。</p><p id="2e30" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">然而，对于房价数据集，我们预测的是不能分组的房价，因此，这是一种回归问题。</p><p id="e60b" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">既然我们已经暖和了一些，我们就要进入正题了。</p><h1 id="9572" class="jw jx hy bd jy jz ka kb kc kd ke kf kg jn kh jo ki jq kj jr kk jt kl ju km kn bi translated">解释基础知识:简单线性回归</h1><p id="517a" class="pw-post-body-paragraph ko kp hy kq b kr ks ji kt ku kv jl kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">为了理解线性回归，我们将首先从简单的线性回归开始，即涉及一个自变量和一个因变量。<em class="lp">我们的自变量又称为</em> <strong class="kq ii"> <em class="lp">【预测变量】</em> </strong> <em class="lp">，因变量称为</em> <strong class="kq ii"> <em class="lp">【反应变量】</em> </strong> <em class="lp">。</em>线性回归模型，一般来说，涉及到我们比较两个模型:</p><ul class=""><li id="eb3c" class="lq lr hy kq b kr lk ku ll kx ls lb lt lf lu lj lv lw lx ly bi translated">只包含因变量的模型</li><li id="25d9" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj lv lw lx ly bi translated">包含自变量和因变量的模型</li></ul><p id="0522" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">如果是第一个模型，我们进行某种程度的“预测”的唯一方法是找到因变量的平均值。这是一个基础模型，我们将用它来比较第二个模型的回归线。如果比较后我们没有看到任何改进，这意味着我们的自变量没有提供新的信息，丢弃它没有多大关系。</p><h2 id="ea32" class="me jx hy bd jy mf mg mh kc mi mj mk kg kx ml mm ki lb mn mo kk lf mp mq km ie bi translated">但是，我们首先如何比较这两个模型呢？</h2><p id="dbdc" class="pw-post-body-paragraph ko kp hy kq b kr ks ji kt ku kv jl kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated"><em class="lp">为此，我们将引入</em> <strong class="kq ii"> <em class="lp">残差</em> </strong> <em class="lp">的概念，它是原始值和由我们的回归线</em>  <em class="lp">预测的值之间的</em> <strong class="kq ii"> <em class="lp">的一个奇特的词。残差也称为误差。</em></strong></p><p id="5905" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">因此，为了进行比较，我们对两种模型都遵循以下步骤:</p><ul class=""><li id="5496" class="lq lr hy kq b kr lk ku ll kx ls lb lt lf lu lj lv lw lx ly bi translated">我们将首先找到原始值和我们的回归线预测值之间的残差。</li><li id="09b9" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj lv lw lx ly bi translated">我们将对所有这些残差求平方，并将它们相加。</li></ul><p id="64f0" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">这个总和被称为<strong class="kq ii">“误差平方和”</strong>或<strong class="kq ii"> SSE。</strong>以此作为比较上述两种模式的标准。SSE也被称为“未知错误”,如果一个模型有更多的未知错误，那么它需要更多的优化。</p><p id="1fd6" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">除了上交所，我们还有SSR和SST。</p><p id="e1e7" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"> SST </strong>为<strong class="kq ii">“平方和”</strong>说明了所有已知或未知的误差。它是从我们的回归线预测的因变量和因变量的均值之间的差的平方和。</p><p id="1fb3" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"> SSR </strong>是<strong class="kq ii">“平方和回归”</strong>，它解释了所有已知的错误原因，并描述了我们的线如何“符合”我们的数据集。它是因变量的值和因变量的平均值之间的差的平方和。</p><p id="40dd" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">这三个术语具有以下关系:</p><p id="8c6b" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"> SST = SSE + SSR </strong></p><p id="434c" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"> <em class="lp">注:</em> </strong> <em class="lp">对于我们这个只有因变量的模型，SSE = SST </em></p><figure class="ms mt mu mv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mr"><img src="../Images/d4ea59d4c5648b199d12ebba8f963313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gYarZmFvCQht3i5U.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">资料来源:Vitalflux.com</figcaption></figure><p id="8f45" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"> R平方值(决定系数):</strong>该值用于判断我们的回归线与数据集的吻合程度。这可以通过下面的表达式找到，</p><p id="435c" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"> R = 1-( SSE/SST) = SSR/SST </strong></p><p id="499b" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">如果<strong class="kq ii"> R </strong>接近1，那么这表明回归线与数据集非常吻合。否则，如果它接近0或者甚至是负数，它就不适合。</p><p id="b8e2" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii">代码片段:</strong></p><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es mw"><img src="../Images/70d4eb60289155fa8a16b8f3dac009f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*yNlxtzMQW9JVucwx8KVNLQ.png"/></div></figure><h1 id="d492" class="jw jx hy bd jy jz ka kb kc kd ke kf kg jn kh jo ki jq kj jr kk jt kl ju km kn bi translated">回归线</h1><p id="7d29" class="pw-post-body-paragraph ko kp hy kq b kr ks ji kt ku kv jl kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">由于这是“线性”回归，我们的回归线将是以下形式:</p><p id="f1c6" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">y = b₀+ b₁*x </p><p id="d2e1" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">对于多元线性回归，我们的直线将具有以下形式:</p><p id="d7f9" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">y = b₀+ b₁*x₁+ b₂*x₂ + …+ bₙ*xₙ </p><p id="06f9" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">在哪里，</p><ul class=""><li id="9325" class="lq lr hy kq b kr lk ku ll kx ls lb lt lf lu lj lv lw lx ly bi translated">y是我们的因变量(响应/结果)</li><li id="e6fd" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj lv lw lx ly bi translated">b₁,b₂,…,bₙ是参数，b₀是我们的截距</li><li id="e76c" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj lv lw lx ly bi translated">x₁、x₂,…、xₙ是独立变量</li></ul><p id="9229" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">要找到我们的回归线，我们首先需要参数和线截距。为了找到以上的东西，我们有两种方法:</p><ol class=""><li id="6fa8" class="lq lr hy kq b kr lk ku ll kx ls lb lt lf lu lj mx lw lx ly bi translated">最小二乘法</li><li id="a5f6" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">梯度下降</li></ol><h2 id="f7e7" class="me jx hy bd jy mf mg mh kc mi mj mk kg kx ml mm ki lb mn mo kk lf mp mq km ie bi translated">最小二乘法</h2><p id="cf95" class="pw-post-body-paragraph ko kp hy kq b kr ks ji kt ku kv jl kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">相同的步骤给出如下:</p><ol class=""><li id="c688" class="lq lr hy kq b kr lk ku ll kx ls lb lt lf lu lj mx lw lx ly bi translated">求因变量和自变量的平均值。让我们称之为yₘₑₐₙ和xₘₑₐₙ.</li><li id="e002" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">对于每个x值，从它们中减去xₘₑₐₙ，对于每个y值，从它们中减去yₘₑₐₙ。</li><li id="6ad5" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">找出(xᵢ-xₘₑₐₙ)和(yᵢ-yₘₑₐₙ)的乘积，其中1≤i≤n，n是变量值的个数。然后加上这些产品。</li><li id="1639" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">然后，平方(xᵢ-xₘₑₐₙ)并将它们加在一起。</li><li id="e8c3" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">将(3)的结果除以(4)的结果，得到回归线的斜率。</li><li id="3849" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">为了找到截距，我们用yₘₑₐₙ代替y，用xₘₑₐₙ.代替x来求解<strong class="kq ii"> b₀ = y - b₁*x </strong></li></ol><p id="b9b6" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">请注意，我们基本上是以下列方式寻找斜率的:</p><p id="3d11" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"> <em class="lp">斜率为回归线(</em> y = b₀+ b₁*x) </strong></p><p id="a783" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"><em class="lp">=【σ协方差(xi，易)/方差(x)】for 1≤I≤n</em></strong></p><figure class="ms mt mu mv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es my"><img src="../Images/3492d65261c6f78c51f4044c1bdedfcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KceVc-CCq6eEwXBiNNbP1A.png"/></div></div></figure><figure class="ms mt mu mv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mz"><img src="../Images/0f1aaba0573ab0a50dce5a6f2101c183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NdeOsa2w0oJIDfyDR5oqfQ.png"/></div></div></figure><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es na"><img src="../Images/47cf13625c4b9dd51c67aac92dc6afc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*SqoGccmTnmCohcnVGsFOlQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">绿线是使用简单线性回归的最小二乘法找到的。</figcaption></figure><h2 id="8c59" class="me jx hy bd jy mf mg mh kc mi mj mk kg kx ml mm ki lb mn mo kk lf mp mq km ie bi translated">梯度下降</h2><p id="6502" class="pw-post-body-paragraph ko kp hy kq b kr ks ji kt ku kv jl kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">在我们理解这个方法之前，我们需要知道什么是成本函数。</p><p id="5ff8" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii">成本函数:</strong>又称<strong class="kq ii">均方误差，</strong>是原始因变量和我们回归线预测的因变量之间的差的平方和。</p><figure class="ms mt mu mv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nb"><img src="../Images/bfaffd955929c3ab653ff71a26d38b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mf58YX2naGhj_kYbXygLEQ.png"/></div></div></figure><p id="bf00" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">这种方法需要我们找到成本函数相对于斜率m和截距b的一阶偏导数。但是，我们为什么需要找到它呢？</p><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es nc"><img src="../Images/1c71bf7a511c94e69876fb1d5315bf8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*DoDlzg9kggewSy6g.jpeg"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">(来源:morioh.com)注意步长在不断减小。这是因为当我们向下移动时，我们的斜率也会减小。</figcaption></figure><p id="0f05" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">在上图中，我们有一条成本曲线。假设我们在x轴上有斜率m。为了找到全局最小值(我们的成本函数是最小的)，我们通过一个可变因子一点一点地向下步进。请注意，全局(甚至局部)最小值是成本曲线斜率为0的点。这是<strong class="kq ii">汇合点</strong>。类似地，对于截距b，我们可以找到偏导数w . r . t . b。</p><p id="dbfd" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><em class="lp">对某个变量的偏导数w.r.t只表明成本函数w.r.t那个变量的变化率。</em></p><p id="292b" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">计算结果如下:</p><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es nd"><img src="../Images/8220f83287e0510519ead8ece1fa49a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*Tskkk4ObOJpHrNeA.png"/></div></figure><p id="9ac6" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii">梯度下降的步骤:</strong></p><ol class=""><li id="1124" class="lq lr hy kq b kr lk ku ll kx ls lb lt lf lu lj mx lw lx ly bi translated">求成本函数J(b，m) w.r.t m和b的偏导数。</li><li id="9841" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">通过<strong class="kq ii">学习率(α) </strong>和它们各自的偏导数的乘积来减少m和b。</li><li id="7752" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">我们对指定数量的<strong class="kq ii">迭代(时期)</strong>执行(1)和(2)步骤，直到我们达到收敛。</li><li id="ae9b" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj mx lw lx ly bi translated">调整α和历元值，直到达到收敛。</li></ol><p id="bcce" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"> <em class="lp">注意:α值不能太大，否则我们可能会绕过下图所示的收敛点。如果它太小，在它到达会聚点之前将花费很长时间。</em> </strong></p><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es ne"><img src="../Images/6f1136ef65cd1dae2d58e623893fdd23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*5CmoAdCjckOlCfse_4xX2A.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:jeremyjordan.me</figcaption></figure><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es nf"><img src="../Images/7dbf02e3a3bcf6a2f1a5e97dc7dcf9cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*sZSw4TnHPw5lP4SWWlPujw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:jeremyjordan.me</figcaption></figure><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es ng"><img src="../Images/9096a8864206cd6fa66976dfab55c07e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*q8NS8OPqKUahhiMpTVGKxA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:jeremyjordan.me</figcaption></figure><figure class="ms mt mu mv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nh"><img src="../Images/16dbcef85921f05809af20503d305228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wfXtTDE-yqqYPLM-.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">θ0是b，θ1是m</figcaption></figure><figure class="ms mt mu mv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ni"><img src="../Images/3d066714287bf4ea4444fab54a050e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/0*gf46c-B71Ujdn9HG.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">对于多元线性回归。</figcaption></figure><h1 id="9858" class="jw jx hy bd jy jz ka kb kc kd ke kf kg jn kh jo ki jq kj jr kk jt kl ju km kn bi translated">具有多个预测值的多重共线性:</h1><p id="ed3a" class="pw-post-body-paragraph ko kp hy kq b kr ks ji kt ku kv jl kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">每当我们进行多元线性回归时，除了自变量和因变量之间的“相关性”之外，自变量之间也可能存在相关性。这使得一个变量对我们的因变量有多大影响变得“模糊不清”。</p><p id="2c15" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">为了获得一个预测因子与其他预测因子“相关”程度的基本概念，我们可以使用皮尔逊相关系数，你可以在这个<a class="ae hv" rel="noopener" href="/@tanya-gupta18/intern-diaries-statistics-67c7d2e75192">博客</a>中读到。</p><p id="5de8" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">使用热图可以很好地展示这一点。</p><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es nj"><img src="../Images/0bbf4205b0a279be00b0509ada340fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*ZqQyKqKgF8y6-Gc6sbjjxw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">以房屋预测数据集中的一个例子为例，我们看到这三个因素相互之间有很高的相关性，最好使用其中一个因素来预测房屋的价格。尤其是与我们的反应变量“价格”更相关的那个。</figcaption></figure><p id="c69d" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii">代码片段:</strong></p><figure class="ms mt mu mv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nk"><img src="../Images/f19c071d21df261ab86c8864c52a8a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tCyjq0P-982t2mnB5NZYBQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">从头开始寻找多元线性回归的梯度下降。这种方法也可以用于简单的线性回归，只是你的x矩阵只包含一列而不是多列。</figcaption></figure><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es nl"><img src="../Images/0d3ded11ec4f6e4e40fc4d705584ea76.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*2MWO-JQcbNsmEBHEqRP8Fg.png"/></div></figure><p id="3224" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">在哪里，</p><ul class=""><li id="b519" class="lq lr hy kq b kr lk ku ll kx ls lb lt lf lu lj lv lw lx ly bi translated"><strong class="kq ii"> cost: </strong>存储每次迭代中所有开销值的列表；稍后用于绘制成本与迭代次数(或时期)的关系图。</li><li id="d5e4" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj lv lw lx ly bi translated"><strong class="kq ii"> min_cost: </strong>初始化为None，在迭代过程中检查最小值。</li><li id="f9b0" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj lv lw lx ly bi translated"><strong class="kq ii"> b_min: </strong>用于存储迭代过程中min_cost值的参数。</li><li id="bb08" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj lv lw lx ly bi translated"><strong class="kq ii"> h: </strong>假设值，用参数值执行x值查出。</li><li id="3edc" class="lq lr hy kq b kr lz ku ma kx mb lb mc lf md lj lv lw lx ly bi translated"><strong class="kq ii"> b: </strong>用1初始化的参数数组。</li></ul><p id="01cf" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii"/></p><p id="8ca3" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii">绘制图形的代码片段:</strong></p><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es nm"><img src="../Images/ac8fb94c0b61fe6241bceae766845b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*g90BrfA5-fh0WNhcRnbSAg.png"/></div></figure><p id="5cec" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq ii">输出:</strong></p><figure class="ms mt mu mv fd hk er es paragraph-image"><div class="er es nn"><img src="../Images/2f3ef207f88abb886eb90bfa5605fa9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*u6N7zg-8cVa337Nu6JFvDQ.png"/></div></figure><p id="91b7" class="pw-post-body-paragraph ko kp hy kq b kr lk ji kt ku ll jl kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">今天就到这里，感谢您抽出时间阅读！！！</p></div></div>    
</body>
</html>