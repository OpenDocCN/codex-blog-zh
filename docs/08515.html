<html>
<head>
<title>Pyspark DataFrame</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pyspark数据帧</h1>
<blockquote>原文：<a href="https://medium.com/codex/pyspark-for-begineers-part-2-pyspark-dataframe-60008da53e30?source=collection_archive---------3-----------------------#2022-08-13">https://medium.com/codex/pyspark-for-begineers-part-2-pyspark-dataframe-60008da53e30?source=collection_archive---------3-----------------------#2022-08-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="0bba" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">DataFrame现在是一个行业流行语，人们倾向于在各种情况下使用它。在本文中，我们将了解更多关于Pyspark中的数据框架、其特性、重要性、创建以及使用Pyspark数据框架的探索性数据分析。稍后，我们将使用一个用例。</p></blockquote><p id="67f0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">要了解更多关于Pyspark的知识并成为一名认证的Pyspark开发人员，请阅读第1部分:Pyspark初学者。</p><div class="jk jl ez fb jm jn"><a href="https://muttinenisairohith.medium.com/pyspark-for-beginners-part-1-introduction-638fb16c5092" rel="noopener follow" target="_blank"><div class="jo ab dw"><div class="jp ab jq cl cj jr"><h2 class="bd hj fi z dy js ea eb jt ed ef hh bi translated">Pyspark初学者|第1部分:简介</h2><div class="ju l"><h3 class="bd b fi z dy js ea eb jt ed ef dx translated">PySpark是Apache Spark的Python API。使用PySpark，我们可以在分布式集群上并行运行应用程序…</h3></div><div class="jv l"><p class="bd b fp z dy js ea eb jt ed ef dx translated">muttinenisairohith.medium.com</p></div></div><div class="jw l"><div class="jx l jy jz ka jw kb kc jn"/></div></div></a></div><p id="a040" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj"> Pyspark数据帧</strong></p><p id="cc1f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">Dataframes是一种类似于excel表或SQL表的数据结构，其中数据被组织成行和列。通常，行代表观察的数量。行可以有多种数据格式，比如<em class="ik">异构</em>，而列可以有相同数据类型的数据，比如<em class="ik">同构</em>。除了数据，数据帧通常还包含一些元数据；例如，列名和行名。</p><p id="e5d2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">py spark数据帧的特点</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/e24b9926c4486a55d73dde06a701cd75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/0*b2LJ1TQQrMDK7uky.png"/></div></figure><ul class=""><li id="08c0" class="kk kl hi il b im in iq ir jh km ji kn jj ko jg kp kq kr ks bi translated">数据帧在本质上是分布式的，这使得它们成为容错和高度可用的数据结构。</li><li id="1b55" class="kk kl hi il b im kt iq ku jh kv ji kw jj kx jg kp kq kr ks bi translated"><strong class="il hj">惰性求值</strong>是一种求值策略，它保持对表达式的求值，直到需要它的值。避免了重复评估。Spark中的惰性求值意味着直到一个动作被触发，执行才会开始。在Spark中，当Spark转换发生时，就会出现懒惰评估的情况。</li><li id="81a7" class="kk kl hi il b im kt iq ku jh kv ji kw jj kx jg kp kq kr ks bi translated">数据帧本质上是不可变的。这意味着一旦创建就不能改变。但是我们可以通过应用某种转换来转换它的值，就像在RDDs中一样。</li></ul><p id="bdfc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj"> PySpark数据帧创建</strong></p><p id="1508" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">首先，让我们构建Sparksession，在2.0版本之后，Sparksession成为了Spark的入口点。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="b9fd" class="ld le hi kz b fi lf lg l lh li">from pyspark.sql import SparkSession</span><span id="cbb8" class="ld le hi kz b fi lj lg l lh li">spark =SparkSession.builder.appName("Practice").getOrCreate()<br/>spark</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/50623ba21fc71e89e9538bef87d68ea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9eUQnGFaTzzwCUq59nKzHA.png"/></div></div></figure><p id="7962" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">我们创建了一个名为“实践”的应用程序SparkSession。创建SparkSession后，让我们从RDD创建一个Spark数据框架。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="6486" class="ld le hi kz b fi lf lg l lh li">from pyspark.sql import Row</span><span id="9355" class="ld le hi kz b fi lj lg l lh li">Employee = Row("firstName", "lastName", "email", "salary")</span><span id="26ad" class="ld le hi kz b fi lj lg l lh li">employee1 = Employee('Basher','armbrust','bash@edureka.co',100000)<br/>employee2 = Employee('Daniel','meng','daniel@stanford.edu',120000)<br/>employee3 = Employee('Muriel',None, 'muriel@waterloo.edu',140000)<br/>employee4 = Employee('Rachel','wendell','rach_3@edureka.co',160000)<br/>employee5 = Employee('Zach','galifikis','zach_g@edureka.co', 160000)</span><span id="0b19" class="ld le hi kz b fi lj lg l lh li">employees = [employee1,employee2,employee3,employee4,employee5]</span></pre><p id="f0ae" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">我们已经创建了一个employees实例，让我们使用<strong class="il hj"> createDataFrame </strong>命令将其转换为DataFrame。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="ea50" class="ld le hi kz b fi lf lg l lh li">df_pyspark = spark.createDataFrame(employees)<br/>df_pyspark.show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lp"><img src="../Images/f4d337085c9997cdac3c26106ed681fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLzl5arobXztnROcYh5dXQ.png"/></div></div></figure><p id="1084" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">从外部来源创建数据帧:</strong></p><p id="f7a6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">在这里，我们将了解如何从CSV、JSON等外部来源读取数据。，转换成Pyspark数据帧。对于这一部分，我将使用以下示例<a class="ae lq" href="https://github.com/muttinenisairohith/Encoding-Categorical-Data/blob/7ebb851bf47a608dfb41093771e1b16597603d96/data/test1.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="il hj"> CSV </strong> </a>。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="cf60" class="ld le hi kz b fi lf lg l lh li">df_pyspark = spark.read.csv("test1.csv", inferSchema=True, header=True)</span><span id="ab28" class="ld le hi kz b fi lj lg l lh li">df_pyspark.show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lr"><img src="../Images/c62dfdc24dd70a029e6e0b2d35d29ef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4FBtk8f1aYPVe6IsDEDAbA.png"/></div></div></figure><p id="3486" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">这里使用了read options header和inferSchema。header用于从CSV文件中收集列名，而inferSchema用于自动检测数据帧中每一列的数据类型。<br/> <strong class="il hj">注意:</strong>在没有inferSchema = True的情况下，不管列的性质如何，在Pyspark中每一列都会被读取为一个字符串。</p><p id="0937" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。show() </strong>用于直观地查看数据帧及其内容。类似于。头()在熊猫中，这里我们可以用。show(n)在数据帧中显示n行。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="355d" class="ld le hi kz b fi lf lg l lh li">df_pyspark.show() #shows top 20 rows in DataFrame<br/>df_pyspark.show(3) #shows top 3 rows in DataFrame<br/>df_pyspark.head(5) # shows top 5 rows in DataFrame but in Row Format</span></pre><p id="5e34" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。printSchema() </strong></p><p id="d3d7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">为了查看DataFrame的模式和结构，我们将使用printSchema方法。这将为我们提供数据帧中的不同列，以及该特定列的数据类型和可空条件。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="dc5d" class="ld le hi kz b fi lf lg l lh li">df_pyspark.printSchema()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ls"><img src="../Images/468b0f487a4145417cdfdc48dc5e4b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*RFV5Q0DAJI5KrRWgrdFeEg.png"/></div></figure><p id="2ec0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。count() </strong> —了解数据帧中的行数</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="173d" class="ld le hi kz b fi lf lg l lh li">df_pyspark.count()  <br/># Output: 6</span></pre><p id="c1c8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">。<strong class="il hj">列</strong> —要知道数据帧中列的名称。使用列。它将返回所有列的列表。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="7d42" class="ld le hi kz b fi lf lg l lh li">print(df_pyspark.columns)   # lists all columns in DataFrame<br/>print(len(df_pyspark.printSchema())  # lists no.of.columns</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lt"><img src="../Images/be5ff1f4344e81581cec3d0308d867db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*sxzB91fUYLqbiPt4aVR76g.png"/></div></figure><p id="ba62" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。dtypes</strong>——类似熊猫，。dtypes用于了解数据帧中每一列的数据类型</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="67dc" class="ld le hi kz b fi lf lg l lh li">print(df_pyspark.dtypes)</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lu"><img src="../Images/e5774de5c6f6b33db1a79c9f571ff00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*pnKSBJqJ6FyXdsAuOGNV-A.png"/></div></figure><p id="a270" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">注意—</strong>py spark数据帧中默认不支持就地操作。我们需要执行相等操作来转换数据帧。</p><p id="b82c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">汇总统计数据— </strong>获取数据帧中所有数值列的汇总统计数据(mean，stddev，max)，类似于pandas，<strong class="il hj"> <em class="ik">。</em>形容使用()方法。</strong></p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="fd42" class="ld le hi kz b fi lf lg l lh li">df_pyspark.describe().show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lv"><img src="../Images/6ed4e44f19c427f6f7f540e928b655cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3U6bhTpXUCoj-GP65A7Yg.png"/></div></div></figure><p id="a4b0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">选择数据帧的特定列— </strong>选择数据帧的特定列。使用选择命令。不像熊猫，切片在Pyspark是不可能的。强制使用选择命令。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="95ba" class="ld le hi kz b fi lf lg l lh li">df_pyspark.select(["Name","Age"]).show(5)</span><span id="074c" class="ld le hi kz b fi lj lg l lh li">+-------+---+<br/>|   Name|Age|<br/>+-------+---+<br/>| chandu| 31|<br/>| rohith| 30|<br/>| rashmi| 29|<br/>|Karthik| 24|<br/>|    Sai| 21|<br/>+-------+---+<br/>only showing top 5 rows</span></pre><p id="d4e8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。distinct() — </strong>在数据帧中查找不同的值。使用distinct()方法。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="36be" class="ld le hi kz b fi lf lg l lh li">df_pyspark.select(["Name"]).distinct().count()</span><span id="99ae" class="ld le hi kz b fi lj lg l lh li">#6</span></pre><p id="8c08" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">。with column()—</strong>data frame的转换函数，用于更改值、转换现有列的数据类型、创建新列，等等，如下所示:</p><ol class=""><li id="68f6" class="kk kl hi il b im in iq ir jh km ji kn jj ko jg lw kq kr ks bi translated">更改列的数据类型</li></ol><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="5657" class="ld le hi kz b fi lf lg l lh li">df_pyspark = df_pyspark.withColumn("Age",df_pyspark["Age"].cast("String"))</span><span id="aa5b" class="ld le hi kz b fi lj lg l lh li">df_pyspark.printSchema()</span></pre><p id="081f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">2.更新现有列的值</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="6b1a" class="ld le hi kz b fi lf lg l lh li">df_pyspark.withColumn("salary",df_pyspark["salary"]*100).show()</span></pre><p id="e7fd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">3.从现有列创建新列</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="e6b3" class="ld le hi kz b fi lf lg l lh li">df_pyspark.withColumn("Experience * salary", df_pyspark["Experience"]*df_pyspark["salary"]).show()</span></pre><p id="9c8a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">4.使用withColumn()添加新列— lit方法用于在整个数据帧中添加文字常量</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="b76e" class="ld le hi kz b fi lf lg l lh li">from pyspark.sql.functions import lit</span><span id="4ad4" class="ld le hi kz b fi lj lg l lh li">df_pyspark.withColumn("Country",lit("India")).show()</span></pre><p id="1dbe" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">5.重命名列</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="b516" class="ld le hi kz b fi lf lg l lh li">df_pyspark.withColumnRenamed("Name","Full Name") .show()</span></pre><p id="ab6d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">6.从PySpark数据框架中删除列</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="93db" class="ld le hi kz b fi lf lg l lh li">df_pyspark.withColumnRenamed("Name","Full Name") .show()</span></pre><p id="0b80" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj"> toPandas() </strong> —将Pyspark数据帧转换为Pandas数据帧。</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="002d" class="ld le hi kz b fi lf lg l lh li">df_pyspark.toPandas()</span></pre><p id="cfb4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">创建没有模式的空数据帧</strong></p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="de5d" class="ld le hi kz b fi lf lg l lh li">from pyspark.sql.types import StructType,StructField, StringType</span><span id="87c3" class="ld le hi kz b fi lj lg l lh li">df = spark.createDataFrame([], StructType([]))<br/>df.printSchema()</span></pre><p id="d1f3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">使用模式</strong>从空RDD创建空数据帧</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="00ce" class="ld le hi kz b fi lf lg l lh li">from pyspark.sql.types import StructType,StructField, StringType</span><span id="d8f9" class="ld le hi kz b fi lj lg l lh li">emptyRDD = spark.sparkContext.emptyRDD()</span><span id="4832" class="ld le hi kz b fi lj lg l lh li">schema = StructType([<br/>  StructField('firstname', StringType(), True),<br/>  StructField('middlename', StringType(), True),<br/>  StructField('lastname', StringType(), True)<br/>  ])</span><span id="1d6f" class="ld le hi kz b fi lj lg l lh li">#Create empty DataFrame from empty RDD<br/>df = spark.createDataFrame(emptyRDD,schema)<br/>df.printSchema()</span></pre><p id="4eb6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">在Pyspark数据帧中删除一列</strong></p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="e861" class="ld le hi kz b fi lf lg l lh li">df_pyspark.drop("Experience * salary").show()</span></pre><p id="5b1c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">py spark数据帧中的过滤器</strong></p><p id="e2cd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">PySpark <code class="du lx ly lz kz b">filter()</code>函数用于根据给定的条件或SQL表达式过滤数据帧中的行，其中()方法也可以代替filter()</p><ol class=""><li id="4dfe" class="kk kl hi il b im in iq ir jh km ji kn jj ko jg lw kq kr ks bi translated">使用列()条件过滤(选择年龄&gt; 25岁的数据)</li></ol><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="6426" class="ld le hi kz b fi lf lg l lh li">df_pyspark.filter(df_pyspark["age"]&gt;25).show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ma"><img src="../Images/713c823435edf44199baeab5081696c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfmMSb4LAHX-GxvR_q2Dow.png"/></div></div></figure><p id="d7be" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">2.使用过滤器()仅选择年龄大于25岁的姓名和年龄</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="62ec" class="ld le hi kz b fi lf lg l lh li">df_pyspark.filter(df_pyspark["age"]&gt;25).select(["Name","age"]).show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mb"><img src="../Images/1bc7b8d89f3abbae6cacf9b5bc2cf014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T37lryThnIxW1meZFN6A6w.png"/></div></div></figure><p id="f132" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">3.基于两列的筛选— And条件</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="3b94" class="ld le hi kz b fi lf lg l lh li">df_pyspark.filter((df_pyspark["age"]&gt;25) &amp; (df_pyspark["salary"]&gt;=20000)).show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mc"><img src="../Images/0113deaaa14d2404c47dd914bac92a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*KKRgxXl9l3lcTLRvKPfXtQ.png"/></div></figure><p id="4d5f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">4.基于两列或条件进行过滤</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="d33b" class="ld le hi kz b fi lf lg l lh li">df_pyspark.filter((df_pyspark["age"]&gt;25) | (df_pyspark["salary"]&gt;=20000)).show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es md"><img src="../Images/417a273d3c0caa948713509560c905ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9dQRdRjtHZEBLUV9VE6v7A.png"/></div></div></figure><p id="5311" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">4.基于两列过滤—非条件</p><pre class="ke kf kg kh fd ky kz la lb aw lc bi"><span id="1317" class="ld le hi kz b fi lf lg l lh li">df_pyspark.filter(~(df_pyspark["age"]&gt;25)).select(["Name","age"]).show()</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es me"><img src="../Images/327cb95a3b65297f5639fdebfbcf8150.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*FNo-0VfdBHu2gyZbN83VWQ.png"/></div></figure><p id="2af2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">展望未来，我们将在Pyspark数据框架中涵盖更多主题，我认为这将变得冗长。</p><p id="75d4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">我在下面的文章中添加了关于groupby()、连接、处理缺失值和其他概念的信息。一定要参考一下…</p><div class="jk jl ez fb jm jn"><a href="https://muttinenisairohith.medium.com/pyspark-for-begineers-part-3-pyspark-dataframe-db02f0fcd275" rel="noopener follow" target="_blank"><div class="jo ab dw"><div class="jp ab jq cl cj jr"><h2 class="bd hj fi z dy js ea eb jt ed ef hh bi translated">初学者用Pyspark |第3部分:Pyspark数据框架</h2><div class="ju l"><h3 class="bd b fi z dy js ea eb jt ed ef dx translated">DataFrame现在是一个行业流行语，人们倾向于在各种情况下使用它。在我们之前的文章中，我们…</h3></div><div class="jv l"><p class="bd b fp z dy js ea eb jt ed ef dx translated">muttinenisairohith.medium.com</p></div></div><div class="jw l"><div class="mf l jy jz ka jw kb kc jn"/></div></div></a></div><p id="3d5f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">快乐编码和学习…</p></div></div>    
</body>
</html>