<html>
<head>
<title>7 Different Convolutions for designing CNNs that will Level-up your Computer Vision project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">设计CNN的7种不同卷积，将提升你的计算机视觉项目</h1>
<blockquote>原文：<a href="https://medium.com/codex/7-different-convolutions-for-designing-cnns-that-will-level-up-your-computer-vision-project-fec588113a64?source=collection_archive---------3-----------------------#2021-10-29">https://medium.com/codex/7-different-convolutions-for-designing-cnns-that-will-level-up-your-computer-vision-project-fec588113a64?source=collection_archive---------3-----------------------#2021-10-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0bb3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">对基本、转置、扩张、可分、深度和点态卷积及其应用的深入评论。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/0ddd5cc649baf71950da5ecc69d96ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Mok_97lPXkxNFtuw"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">照片由<a class="ae jn" href="https://unsplash.com/@hannahbusing?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">汉娜·布斯</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="70d7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最近关于CNN架构的研究包括如此多不同的卷积变体，这让我在阅读这些论文时感到困惑。我认为，仔细研究一些更流行的卷积变体的精确定义、效果和用例(在计算机视觉和深度学习中)是值得的。这些变体旨在节省参数计数，增强推理，并利用目标问题的某些特定特征。</p><p id="e8f7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这些变体中的大多数都简单易懂，因此我将重点放在理解每种方法的好处和用例上。这些知识有望帮助你理解最近CNN架构背后的直觉，并帮助你设计自己的网络。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="9327" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">回旋</h1><p id="ac44" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">让我们对卷积的基本形式做一个简短的概述。根据PapersWithCode中的描述，</p><blockquote class="lo lp lq"><p id="69df" class="jo jp lr jq b jr js ij jt ju jv im jw ls jy jz ka lt kc kd ke lu kg kh ki kj hb bi translated"><em class="hi">卷积</em><strong class="jq hj"><em class="hi"/></strong><em class="hi">是一种矩阵运算，由一个内核和一个小的权重矩阵组成，它滑过输入数据，与它所在的输入部分执行逐元素乘法，然后将结果相加成为输出。</em></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/8d0c8d79ca7ffa7b613f2c4923420c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*1VJDP6qDY9-ExTuQVEOlVg.gif"/></div></figure><p id="f146" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这种操作对于处理图像是有利的，因为:</p><ol class=""><li id="adab" class="lw lx hi jq b jr js ju jv jx ly kb lz kf ma kj mb mc md me bi translated">它们的参数效率极高，因为图像的不同位置共享相同的权重，因此参数的数量与图像大小不成比例。</li><li id="d686" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated">卷积基本上是平移不变的。也就是说，输出不受图像中常见的小平移和大平移的影响，这与MLP不同，MLP对于1像素平移通常会给出非常不同的结果。</li></ol><p id="f9bd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">卷积的输出形状和复杂度可以使用以下参数进行配置:</p><ul class=""><li id="15aa" class="lw lx hi jq b jr js ju jv jx ly kb lz kf ma kj mk mc md me bi translated">内核大小:内核的维度，通常使用(3×3)的内核大小。</li><li id="6dc0" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mk mc md me bi translated">填充:如何填充图像的边缘，以保持卷积后的图像大小。例如，上面的演示使用了1个像素的填充。描述像素数和填充这些像素的规则。</li><li id="affe" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mk mc md me bi translated">步长:扫描图像时内核的步长。通常设置为1以保持数据形状，或者设置为2以对其进行缩减采样。上面的演示使用的步幅为2。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/a52dca70a16748afeec794298e095009.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*AnDL5w3LOB5TGTg8KjtnLw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">伊莱·本德斯基</a></figcaption></figure><p id="f854" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通过组合通过不同核卷积的每个通道的结果来预测每个输出通道。因此，需要形状为K×K的C_in核来计算一个输出通道。其中K表示内核大小，C_in、C_out分别表示输入和输出通道的数量。</p><p id="b42c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> #参数</strong> : <strong class="jq hj"> K×K×C_in×C_out </strong></p><p id="fae2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">计算</strong> : <strong class="jq hj"> H×W×C_in×C_out×K×K </strong>(步幅=1的情况下)</p><p id="1f6d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">用例</strong>:这种卷积层几乎用于计算机视觉的每个子任务。这些包括监督任务，如图像和视频分类、对象检测、分割和合成任务，如图像生成、图像超分辨率、图像到图像传输。还存在视觉之外的应用，例如用于序列建模和3D相关应用的1D卷积。</p><h2 id="e4d7" class="mm ks hi bd kt mn mo mp kx mq mr ms lb jx mt mu ld kb mv mw lf kf mx my lh mz bi translated">逐点卷积(1x1卷积)</h2><p id="0074" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">逐点卷积是具有1×1个核的卷积层的另一种说法。它们也被称为通道上的<em class="lr">卷积</em>或<em class="lr">投影层</em>。为什么会有人用这个？有两种主要的使用情形:</p><ol class=""><li id="1ed9" class="lw lx hi jq b jr js ju jv jx ly kb lz kf ma kj mb mc md me bi translated">用于改变输入的维度(也称为多个通道)。</li></ol><ul class=""><li id="8253" class="lw lx hi jq b jr js ju jv jx ly kb lz kf ma kj mk mc md me bi translated">一些网络如Inception连接从不同核计算的特征，这导致过多的通道，因此逐点卷积被应用来管理通道的数量。</li><li id="b1b4" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mk mc md me bi translated">当使用逐点卷积来压缩特征时，计算量大的模块，如自我注意模块(如挤压和激励)更可行。</li><li id="03bb" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mk mc md me bi translated">当用元素方式的和或积组合两个内积时，我们有时需要匹配通道的数量。</li></ul><p id="6461" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">该操作可以被视为沿着输入特征图的深度计算多个加权和。它可以有效地总结它们。</p><p id="9808" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">2.它以可忽略不计的成本创建了通道依赖性。这尤其是通过与缺乏这种依赖性的深度方向卷积相结合来利用的。</p><p id="f8c8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> #参数</strong> : <strong class="jq hj"> C_in×C_out </strong></p><p id="7d23" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">计算</strong> : <strong class="jq hj"> H×W×C_in×C_out </strong></p><h2 id="0f75" class="mm ks hi bd kt mn mo mp kx mq mr ms lb jx mt mu ld kb mv mw lf kf mx my lh mz bi translated">转置卷积(反卷积/反卷积)</h2><p id="1c73" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">反卷积明确计算卷积层的数学逆。虽然它广泛用于类视觉或信号处理，但它在深度学习中并不重要，因为操作的参数可以通过梯度下降来学习。</p><div class="iy iz ja jb fd ab cb"><figure class="na jc nb nc nd ne nf paragraph-image"><img src="../Images/ce75ce627f6aca70d59304e8687e223b.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*k5O8gTeGKKMxs9BDkIfV1A.gif"/></figure><figure class="na jc nb nc nd ne nf paragraph-image"><img src="../Images/ad1203cc0af37d66adb8160267b31a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*58ePlna3r4TXR3MXHmSoGw.gif"/><figcaption class="jj jk et er es jl jm bd b be z dx ng di nh ni translated">左:步幅=1，右:步幅=2</figcaption></figure></div><p id="d2f2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">转置卷积是使用卷积对图像大小进行上采样的一种更简单的方法。当步幅为1(左)时，操作与经典卷积没有区别。对于n&gt;1的跨距，输出形状扩展了n倍。这是通过在像素之间填充0来创建所需大小的扩展图像，并对扩展图像执行卷积来实现的。</p><p id="9537" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">虽然转置卷积不会隐式计算卷积的逆，但这对于深度学习来说并不重要，因为所需的滤波器(可能是逆滤波器)总是可以通过梯度下降来学习。它充分实现了增加数据空间大小的功能。</p><p id="dfc0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">重要提示:虽然转置卷积经常被混淆，但它不是反卷积/反卷积。</p><p id="bbf1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> #参数:K×K×C_in×C_out) </strong></p><p id="2344" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">用例</strong>:转置卷积用于需要上采样的网络架构。一些例子是用于语义分割的编码器-解码器型网络、自动编码器或图像合成和生成网络中的用途。转置卷积的一个问题是棋盘格伪影，它可能对图像生成/合成造成问题。这个话题超出了这篇文章的范围，应该有一个自己的话题。更多信息，请参考谷歌大脑的这篇文章。</p><p id="1613" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lr">来源:</em></p><ul class=""><li id="a3e8" class="lw lx hi jq b jr js ju jv jx ly kb lz kf ma kj mk mc md me bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/Deconvolution" rel="noopener ugc nofollow" target="_blank"><em class="lr">https://en.wikipedia.org/wiki/Deconvolution</em></a></li><li id="e199" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mk mc md me bi translated"><a class="ae jn" rel="noopener" href="/@marsxiang/convolutions-transposed-and-deconvolution-6430c358a5b6"><em class="lr">https://medium . com/@ mars Xiang/convolutions-转置与反卷积-6430c358a5b6 </em> </a></li><li id="dab6" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mk mc md me bi translated"><a class="ae jn" href="https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0" rel="noopener"><em class="lr">https://naokishibuya . medium . com/up-sampling-with-transposed-convolution-9 AE 4 F2 df 52d 0</em></a></li></ul><h2 id="40c3" class="mm ks hi bd kt mn mo mp kx mq mr ms lb jx mt mu ld kb mv mw lf kf mx my lh mz bi translated">扩张卷积(阿特鲁卷积)</h2><p id="978b" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">感受域是模型可以参考的原始图像的范围，用于对该步骤的一个像素进行推断。例如，具有一个3×3卷积的模型的输出可以考虑来自相对于每个像素的空间位置的3个像素的感受野的信息，而具有两个3×3卷积的模型具有相对于该位置的5个像素的感受野。</p><p id="7f3c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">增加核大小是增加感受野的一种方式，但是计算量也增加得非常快。对图像进行下采样还具有增加感受野的效果，因为例如8×8特征图中的3×3卷积覆盖了图像的更多部分。三个3×3卷积足以在8×8特征空间中考虑用于推断的整个图像。</p><p id="e473" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在较低的空间维度上计算特征对于图像分类来说基本上是没问题的，但是对于具有高分辨率输出的任务，尤其是语义分割，这会导致显著的信息损失。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/4018c56491b3570946f2d70e4c487da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*SVkgHoFoiMZkjy54zM_SUw.gif"/></div></figure><p id="e80a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">膨胀卷积是一种卷积，其中内核的像素是间隔的(用0填充)。间距也被认为是一个超参数，它通常具有不同的值，从上面演示中的2到DeepLab模型中的24这样的大间距。它在不增加计算量的情况下增加了内核的大小。这种设计能够从更大的感受野进行非常有效的计算，而没有信息损失或增加层数的需要。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nj"><img src="../Images/d51c4baef1ea23e7682fb23f7c6336c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16O1GcgCO69QEIuT-EQVDA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">扩张卷积的使用案例<a class="ae jn" href="https://paperswithcode.com/method/dilated-convolution" rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/method/dilated-convolution</a></figcaption></figure><p id="9e98" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">用例</strong>:在语义分割中显示最重要的用途，但在其他任务的轻量级/移动CNN架构中也被考虑。</p><p id="1958" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">提出于:<a class="ae jn" href="https://paperswithcode.com/paper/multi-scale-context-aggregation-by-dilated" rel="noopener ugc nofollow" target="_blank">通过扩张卷积的多尺度上下文聚合</a></p><h2 id="e61e" class="mm ks hi bd kt mn mo mp kx mq mr ms lb jx mt mu ld kb mv mw lf kf mx my lh mz bi translated">空间可分离卷积</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/ea328d0859b1d83f85bb2115806fe1c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgO7PF6rvz3Wrqh_4hIedA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728" rel="noopener" target="_blank">中国汪锋</a></figcaption></figure><p id="97b8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一些3×3矩阵可以表示为两个向量的矩阵乘法。因为3×3内核也是一个公共矩阵，所以它<em class="lr">可以</em>分成一个3×1和一个1×3内核，但是执行相同的操作。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nl"><img src="../Images/65d3f117e6af9f4cdf1028b47a84acfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UQ_xzadENbuXL8-X4WjMEw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">资料来源:<a class="ae jn" href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728" rel="noopener" target="_blank">中国汪锋</a></figcaption></figure><p id="50ab" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">具体来说，如上图所述，空间可分离卷积将原始卷积替换为两个阶段。这样，每个内核的参数数量和操作数量从9(3×3)减少到6。然而，众所周知，不是所有的3×3核都可以分离，因此空间可分离卷积会限制模型的能力。</p><p id="10d3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> #参数</strong> : <strong class="jq hj"> (K+K)×C_in×C_out </strong></p><p id="849d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">计算</strong>:<strong class="jq hj">h×w×C _ in×C _ out×(K+K)</strong></p><p id="e3f4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">用例</strong>:由于参数数量少得多，空间可分卷积有时用于模型压缩和轻量级架构。</p><p id="dd78" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lr">来源:</em></p><ul class=""><li id="a304" class="lw lx hi jq b jr js ju jv jx ly kb lz kf ma kj mk mc md me bi translated"><a class="ae jn" href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728" rel="noopener" target="_blank"><em class="lr">https://towards data science . com/a-basic-introduction-to-separable-convolutions-b99ec 3102728</em></a></li><li id="33dc" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mk mc md me bi translated"><a class="ae jn" href="https://eehoeskrap.tistory.com/431" rel="noopener ugc nofollow" target="_blank"><em class="lr">https://eehoeskrap.tistory.com/431</em></a></li></ul><h2 id="7453" class="mm ks hi bd kt mn mo mp kx mq mr ms lb jx mt mu ld kb mv mw lf kf mx my lh mz bi translated">深度方向卷积</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/23f1cbd7696023bf6d55b815330027fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*Thm0bwYCjH2LSANcvapvfw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">伊莱·本德斯基</a></figcaption></figure><p id="7937" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">深度方向卷积不是对每个通道的结果进行卷积和组合，而是在每个通道上独立地执行，并且结果被堆叠。我们可以直观地看到，只有当输入和输出通道的数量一致时，这才会起作用。</p><p id="5bbf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">深度方向卷积具有很高的参数和计算效率，因为参数的数量和计算复杂度都除以输出通道的数量，输出通道的数量通常高达1024。然而，速度优势与运算数量的减少不成比例，因为深度方向卷积在现代硬件上不如传统卷积优化。</p><p id="95f1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> #参数</strong>:T4 k×k×C _ in</p><p id="fe18" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">计算</strong> : <strong class="jq hj"> H×W×C_in×K×K </strong></p><p id="9601" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">用例</strong>:深度方向卷积是构建参数和计算效率更高的更复杂变体和卷积块的关键组件。</p><h2 id="d2b0" class="mm ks hi bd kt mn mo mp kx mq mr ms lb jx mt mu ld kb mv mw lf kf mx my lh mz bi translated">深度方向可分卷积</h2><p id="dde7" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">深度方向卷积，然后是点方向卷积。由于深度方向卷积在通道之间没有连接，我们用点方向卷积连接它们。Xception的作者发现在深度方向卷积后加上非线性是有用的。完整的过程如下图所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nn"><img src="../Images/d5d2b8f7398c8fec9be256b6b8231b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HGg2N6iFvCE1vMsewgX6Og.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">伊莱·本德斯基</a></figcaption></figure><p id="4666" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">空间可分离卷积在经典卷积中分离x和y轴。在这种情况下，深度方向可分离卷积可以被视为分离通道维度。</p><p id="bc2a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">与普通深度方向卷积相比，计算复杂度略有增加，尽管仍然比传统卷积小得多。然而，与简单的深度方向卷积不同，它在许多经验实验中有效地模拟了规则卷积，并在现代CNN架构中广泛使用。</p><p id="e2c1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> #参数</strong> : ( <strong class="jq hj"> K×K+C_out)×C_in </strong></p><p id="4b4d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">计算</strong>:<strong class="jq hj">h×w×C _ in×(K×K+C _ out)</strong></p><p id="83fe" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">使用案例</strong>:exception、V1/V2移动网络、V1高效网络(MnasNet)/V2等等</p><p id="3546" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">你可以在<a class="ae jn" href="https://arxiv.org/pdf/1610.02357.pdf" rel="noopener ugc nofollow" target="_blank">例外:深度可分卷积的深度学习</a>的第2节中找到深度可分卷积的复杂历史</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><p id="eeea" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这篇文章中，我们回顾了一系列卷积变体，这些变体被提议在某些情况下取代传统的卷积层。这些模块各有优缺点，用于解决不同的问题。在后续文章中，我们将回顾卷积设计，它将进一步增强我们创建CNN架构的工具箱。</p><p id="a162" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请在评论里说说建议或者问题。我会尽量在最多两天内回复你们所有人。</p><p id="e5a3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lr">令人惊叹的图像(动画)由</em><a class="ae jn" href="https://github.com/vdumoulin" rel="noopener ugc nofollow" target="_blank"><em class="lr">vdumoulin</em></a><em class="lr">在</em> <a class="ae jn" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank"> <em class="lr">麻省理工学院许可</em> </a> <em class="lr">(在许可说明下免费提供！).</em></p></div></div>    
</body>
</html>