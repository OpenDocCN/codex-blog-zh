# 分类问题中的不平衡数据

> 原文：<https://medium.com/codex/imbalanced-data-in-classification-problem-2ac08e146fa7?source=collection_archive---------3----------------------->

![](img/dab6fee84101d4f4e75db9ff7a2c56f5.png)

数据不平衡会导致问题。

在现实世界中，我们经常会遇到具有不平衡性的数据的分类问题。数据只偏向特定的标签或类别。这意味着与其他标签相比，某个特定标签在数据中的数量要少得多。存在许多具有不平衡目标变量(类别)分布的真实世界分类问题，例如欺诈检测(大量真实交易)、垃圾邮件过滤(大量良好电子邮件)和许多医疗诊断案例，如癌症检测(大量患者没有癌症)。不平衡数据影响分类问题。

# 是什么原因导致了数据的类不平衡？

数据中的类别不平衡可能是由数据采样方法或数据的域特定属性造成的。

与数据采样方法相关的不平衡是由有偏差的采样或测量误差造成的。在有偏抽样中，数据可能是从特定的地理区域或人口统计中收集的，也可能是从一个狭窄的时间段中收集的，在该时间段中，类别的分布非常不同，甚至是以不同的方式收集的。在测量误差的情况下，误差可能是在收集数据时产生的。一个错误的类别标签可能被应用于许多例子，或者数据收集的过程可能有错误，从而导致不平衡。在不平衡是由采样偏差或测量误差引起的情况下，可以通过改进采样方法或通过校正测量误差来校正不平衡。在与数据采样方法相关的不平衡的情况下，数据集不是正在解决的问题陈述的正确表示。

在由于数据的领域特定属性而导致不平衡的情况下，一个类别的自然出现或存在可能会支配其他类别。例如，在信用卡欺诈检测中，收集的大多数样本都是真实交易的。在癌症检测的医学诊断问题的情况下，由于癌症发生的罕见性，与癌症相关的例子将少得多。许多现实生活中的例子，如垃圾邮件检测和流失预测，也有不平衡的数据集。

# 为什么了解数据是否不平衡很重要？

当发现少数类的值远高于发现多数类的值时，知道数据是否不平衡是非常重要的。

这是什么意思？

以信用卡欺诈检测为例。正确识别欺诈案件是非常重要的。如果该模型将一些真实交易识别为欺诈性交易(假阳性)，那么这是可以的，但是欺诈性交易不应被识别为真实交易(假阴性)。因此，必须定义算法/模型，对假阴性给予高权重，对假阳性给予低权重。

# 如何确定数据是否不平衡？

有一个包含 10，000 条记录的数据集，目标变量标签为蓝色和粉色。此外，假设蓝色有 9990 条记录，而粉色只有 10 条记录，那么我们可以说数据是不平衡的。如果数据集中有 9900 或 9000 条蓝色记录和 100 或 1000 条粉色记录，那么数据集也是不平衡的。但是一个有 7000 条蓝色和 3000 条粉色记录的数据集不平衡吗？所以出现的第一个问题是如何确定数据集是否不平衡。

答案是用香农熵来衡量平衡。

在具有 *n* 个实例的数据集上，如果有 *k* 个大小为*的类，则 cᵢ* 的熵计算如下

![](img/d7c62922715c74cfe97612112234b6c4.png)

这相当于—

*   有单个类别时

![](img/e52ea38ec4e1ee4505baad454f1872cb.png)

当数据集高度不平衡时，熵趋向于 **0**

*   *记录 k* 当所有等级平衡时。

![](img/de63cea6a5ee71f7ca0eb37232c58b07.png)

当数据集完全平衡时，熵趋向于 **log k**

所以我们可以从上面推导出一个衡量平衡的标准**B**

![](img/2239768dc11a164c11ba8d471928805c.png)

数据集平衡的度量

对于单个类数据集 ***B*** = 0，对于完全平衡的数据集 ***B*** = 1，这意味着 ***B*** 的值越接近 1，类的分布就越平衡。换句话说， ***B*** = 0.93 的数据集比 ***B*** = 0.34 的数据集*更平衡。*

以上在 Python 中的实现如下

用于计算平衡度量的 Python 代码

# 使用哪些指标来确定模型是否合适？

确定模型是好的还是坏的 fir 的通常方法是找到模型的准确性。仅使用准确性来确定分类模型对不平衡数据的拟合优度会导致错误的解释。让我们举一个非常简单的例子，比如说，我们有这样的数据，10，000 个观察值中有 1，000 个是粉红色的，10，000 个观察值中有 9，000 个是蓝色的。如果我们的分类器总是预测蓝标，那么准确率将是 90%，因为*准确率=正确预测/总预测*。准确度将给出正确预测的百分比。因此，准确性不会总是给出关于训练分类模型的正确见解。当 TP & TN 更重要时，准确性更合适，即重点是识别正确的预测。当类分布相似时，精确度更合适。

![](img/359a3db2d8dbb056b67cc9c61bf7c42c.png)

正确预测的百分比

两类(蓝色和粉色)分类模型的混淆矩阵如下所示

![](img/2716a296676210a49169c869c10e05d6.png)

混淆矩阵

由于准确性不是模型拟合的正确指标，因此，我们应该考虑各种其他指标。

**精度** —它是对模型的*精确度*的度量。它根据预测的蓝色来衡量，实际上有多少是蓝色的。高精度表示好的分类器模型。当 FP = 0 时，精度将变为 1(高),这意味着*所有的蓝色都被正确地预测为蓝色，并且没有蓝色的例子被分类为粉红色。*

![](img/a13e16d46fa5d1d2f1445df7e9453763.png)

模型的准确性

**回忆** —这是对模型的*完整性*的度量。它测量总蓝色中正确检测到的蓝色的数量。召回也叫*灵敏度*或*真阳性率*。与精度一样，高召回值表示好的分类器模型。当 FN = 0 时，回忆将变为 1(高),这意味着*所有蓝色都被正确预测为蓝色，并且没有粉色的示例被分类为蓝色。*

![](img/b2bac20df30fbb5f1e0c228349048fbb.png)

模型的完整性

**F1 得分** — F1 得分结合精度&召回。它是精确性和回忆性的和谐统一。在不平衡的类别分布的情况下，F1 分数是比准确度更好的度量。当精确度和召回率都高时，F1 分数变高。F1 分数的最高可能值是 1.0，表示完美的精确度和召回率。F1 分数更适合 FN & FP 至关重要的时候。

![](img/b3bff588a516bec90839124c95030aef.png)

将精确度和召回率结合起来

**假阳性率** —也称为虚警率，定义为错误拒绝零假设的概率。它是当真值为负时将给出正结果的度量。因此，这是一个衡量粉红色的例子被归类为蓝色或通知一个健康的病人，他患有癌症。

![](img/a5f234290f57afd27320d9629ce32d35.png)

(别慌，是)虚惊一场

**特异性** —它也被称为真阴性率，是对被正确识别的阴性比例的测量。

![](img/7953366efc63cb59753d677b94ef22d7.png)

真正的负面

# 纠正数据不平衡的可能解决方案是什么？

纠正数据不平衡的方法有很多。这些解决方案可以大致分类如下

*   **数据复制** —复制可用数据，直到样本数量相当。复制数据不会给模型添加任何新信息。这种少数类的数据扩充被称为**合成少数过采样技术**，或 **SMOTE** 。
*   **合成数据生成** —使用各种技术创建新数据。Scikit-Learn 具有许多用于合成数据生成的功能。对于涉及图像的分类问题，可以通过对现有图像进行旋转、膨胀、裁剪和添加噪声来创建新图像。
*   **修改损失函数** —修改算法的损失，以反映对较小样本集进行错误分类时的较大误差。
*   **模型变化** —增加模型/算法的复杂性，以便两个类别可以令人满意地分开。在模型/算法中进行更改时应小心谨慎，以避免过度拟合。