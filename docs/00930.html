<html>
<head>
<title>A Dataflow Journey: from PubSub to BigQuery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据流之旅:从PubSub到BigQuery</h1>
<blockquote>原文：<a href="https://medium.com/codex/a-dataflow-journey-from-pubsub-to-bigquery-68eb3270c93?source=collection_archive---------2-----------------------#2021-03-26">https://medium.com/codex/a-dataflow-journey-from-pubsub-to-bigquery-68eb3270c93?source=collection_archive---------2-----------------------#2021-03-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="a547" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">药典</h2><div class=""/><div class=""><h2 id="3214" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">利用Google云服务和Apache Beam，用Python构建一个定制的流数据管道</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/d9838e753a86e95249621a376f2d0457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UHKHkNAmOTk84UFvJpG8w.png"/></div></div></figure><p id="dc19" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">如果您需要处理大量的流数据，您通常会发现托管解决方案是最简单的选择。这些流程很少是一致的，构建您自己的解决方案的价格是无法承受的，并且您必须管理架构和代码。有许多托管服务:在AWS上你可以使用<a class="ae ko" href="https://aws.amazon.com/kinesis/" rel="noopener ugc nofollow" target="_blank"> Kinesis </a>，Azure有<a class="ae ko" href="https://azure.microsoft.com/en-in/services/stream-analytics/" rel="noopener ugc nofollow" target="_blank">流分析</a>，IBM有<a class="ae ko" href="https://www.ibm.com/in-en/cloud/streaming-analytics" rel="noopener ugc nofollow" target="_blank">流分析</a>，你几乎总能找到一个针对<a class="ae ko" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Kafka </a>服务器的托管解决方案。<br/>我会试着解释如何开始使用谷歌云解决方案:<strong class="ju hs">数据流</strong></p><h1 id="6d24" class="kp kq hi bd kr ks kt ku kv kw kx ky kz ix la iy lb ja lc jb ld jd le je lf lg bi translated">介绍</h1><p id="ce4f" class="pw-post-body-paragraph js jt hi ju b jv lh is jx jy li iv ka kb lj kd ke kf lk kh ki kj ll kl km kn hb bi translated">简单介绍一下我们将要使用的谷歌云服务</p><p id="93a7" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><strong class="ju hs"> Google PubSub <br/> </strong>顾名思义，PubSub是一个<em class="lm">发布者-订阅者</em>托管服务。在PubSub中，您可以定义一些<em class="lm">主题</em>(或频道)，其中一些服务(<em class="lm">发布者</em>)可以插入消息，而其他服务(<em class="lm">订阅者</em>)可以消费它们。</p><p id="2d08" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">你应该不需要太多的话来介绍谷歌云最著名的服务:BigQuery是一个完全可扩展的、快速的、云上的关系数据库。<br/>配合标准SQL语言使用，主要用于BI分析。</p><p id="113f" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><strong class="ju hs">谷歌数据流</strong> <br/>基于<a class="ae ko" href="https://beam.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hs"> Apache Beam </strong> </a>，这款谷歌云服务使用相同的代码以批处理或流模式进行数据处理，提供水平可扩展性，以校准所需的资源。</p><p id="c0f1" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">该服务提供了许多现成的<a class="ae ko" href="https://cloud.google.com/dataflow/docs/guides/templates/provided-templates" rel="noopener ugc nofollow" target="_blank">模板</a>。在它们之间，还有一个遵循我们稍后将详述的同一管道:从PubSub读取消息并将它们写入BigQuery。<br/>每个模板都是用Java编写的，可以在<a class="ae ko" href="https://github.com/GoogleCloudPlatform/DataflowTemplates/tree/master/src/main/java/com/google/cloud/teleport/templates" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到，然而，我们将看到用Python创建的带有自定义操作的管道，以便更好地理解如何自己创建一个管道。</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h1 id="66c5" class="kp kq hi bd kr ks lu ku kv kw lv ky kz ix lw iy lb ja lx jb ld jd ly je lf lg bi translated">把手放在某物或者某人身上</h1><h2 id="614b" class="lz kq hi bd kr ma mb mc kv md me mf kz kb mg mh lb kf mi mj ld kj mk ml lf ho bi translated">步骤1 —项目准备</h2><p id="e2dd" class="pw-post-body-paragraph js jt hi ju b jv lh is jx jy li iv ka kb lj kd ke kf lk kh ki kj ll kl km kn hb bi translated"><strong class="ju hs"> 1。创建一个谷歌云项目<br/> </strong>这是开始在谷歌云平台上工作的第一个基本步骤，我就不深究细节了，你可以在这里找到官方指南<a class="ae ko" href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#console" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="1116" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><strong class="ju hs"> 2。创建发布订阅主题和订阅</strong></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mm"><img src="../Images/b67f26cc7f733940aa325bf008cc35f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1zSroyjwxHqXB56pJIY0wQ.png"/></div></div></figure><p id="1f5d" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">正如我们前面介绍的，PubSub需要一个<em class="lm">主题</em>来存储我们的消息，直到它们被确认。<br/>您可以简单地从界面创建一个主题。<br/>之后，我们将需要一个将由数据流使用的“拉”订阅。拉意味着我们的用户将是请求数据的一方。</p><p id="37a7" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><strong class="ju hs"> 3。创建一个BigQuery数据集和表<br/> </strong>一个<em class="lm">数据集</em>是BigQuery的顶级容器单元，它包含任意数量的表。我们可以使用任何一种<a class="ae ko" href="https://cloud.google.com/bigquery/docs/datasets" rel="noopener ugc nofollow" target="_blank">可能的方式</a>来创建数据集。<br/>之后就是普通的SQL表，在我们的例子中，我们需要一个具有以下模式的表:</p><pre class="jh ji jj jk fd mn mo mp mq aw mr bi"><span id="8496" class="lz kq hi mo b fi ms mt l mu mv">timestamp:TIMESTAMP,attr1:FLOAT,msg:STRING</span></pre><p id="b2be" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><em class="lm">补充说明:</em> BigQuery是一种“按需付费”的服务，它同时考虑了存储和成本分析。只要有可能，就创建一个时间分区表，这样查询会更有效，这对于流管道特别有用，在这里可以根据<em class="lm">摄取时间</em>进行分区，可以使用DATE(_PARTITIONTIME)进行查询。</p><p id="1932" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><strong class="ju hs"> 4。创建一个云存储桶<br/> </strong>云存储基本上只是一个为你的文件提供存储服务的地方，主要的存储空间叫做<em class="lm">桶</em>，在这里你可以上传文件或者创建不同的子目录。Apache beam Dataflow runner使用我们需要的存储桶来存储您的代码、需求和任何临时文件。</p><p id="c7e1" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">我们可以从界面创建一个bucket，它不必是多区域的，但我建议您在您将运行作业的同一区域中创建它。标准存储类就可以了。</p><p id="4568" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><strong class="ju hs"> 5。创建一个服务帐户<br/> </strong>我们将需要一个服务帐户，它将作为接收器，写入器，并负责部署生产管道。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mw"><img src="../Images/58277a878c0b42746ab1ef2ddaf4901e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*AHCxFIcWi4Nptv9_1jsD0g.png"/></div></figure><p id="5711" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">您可以通过访问<a class="ae ko" href="https://console.cloud.google.com/apis/credentials" rel="noopener ugc nofollow" target="_blank"> <em class="lm"> Api &amp;服务/凭证</em> </a> <em class="lm"> </em>页面来创建它，在这里您还可以直接授予所需的角色:</p><ul class=""><li id="d6d5" class="mx my hi ju b jv jw jy jz kb mz kf na kj nb kn nc nd ne nf bi translated"><em class="lm">“发布/订阅用户”</em></li><li id="d502" class="mx my hi ju b jv ng jy nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><em class="lm">“big query数据编辑器”</em></li><li id="86b9" class="mx my hi ju b jv ng jy nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><em class="lm">“存储管理员”</em></li><li id="80a8" class="mx my hi ju b jv ng jy nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><em class="lm">“服务账户用户”</em></li><li id="3ddb" class="mx my hi ju b jv ng jy nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><em class="lm">“数据流管理”</em></li><li id="bbc3" class="mx my hi ju b jv ng jy nh kb ni kf nj kj nk kn nc nd ne nf bi translated">此外，如果您想使用发布者模拟器发送一些测试消息，请添加<em class="lm">“发布/订阅发布者”</em></li></ul><p id="726b" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">创建之后，我们需要通过访问它的详细页面来创建和下载一个新的JSON键。</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h2 id="82f4" class="lz kq hi bd kr ma mb mc kv md me mf kz kb mg mh lb kf mi mj ld kj mk ml lf ho bi translated">步骤2 —编写管道代码</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nl"><img src="../Images/82a4840c10c8877989f07683f11133c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*eS25DN79hRI3jIaEABLhzw.png"/></div></figure><p id="2ba7" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">我们将使用apache-beam sdk用Python编写一个管道。<strong class="ju hs"><br/></strong><a class="ae ko" href="https://beam.apache.org/documentation/programming-guide/" rel="noopener ugc nofollow" target="_blank">官方编程指南</a>实际上非常清晰完整，我推荐你阅读，但是对于我们的例子，我们只需要知道几个概念:</p><p id="0be6" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><strong class="ju hs">管道</strong>对象由一系列<strong class="ju hs">步骤</strong>组成，这包括读取、写入和修改你的数据。数据以<strong class="ju hs">p集合</strong>的形式通过各个步骤传递，如果您操作的是批处理管道，则可以是<em class="lm">有界的</em>，如果是流式管道，则可以是<em class="lm">无界的</em>。一个<strong class="ju hs"> PTransform </strong>是你的阶跃函数，它取一些PCollections，再输出一些。</p><p id="458d" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">在流式管道中，<strong class="ju hs">窗口</strong>的概念也非常重要。聚合无限的数据可能会很棘手，这就是为什么您可以定义一些规则来决定如何拆分和分析数据，例如，<em class="lm">固定时间窗口</em>意味着您的数据将以固定的方式按时间戳拆分；<em class="lm">滑动时间窗口</em>类似，但窗口会重叠，这样您就不会意外丢失时间窗口边界上两个数据点之间的连接；<em class="lm">会话窗口</em>适用于特定的用例，在这些用例中，您不知道会有多少数据到达，也不知道会持续多长时间，但您可以识别开始和结束。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nm"><img src="../Images/e1e44956b508c6edec99ec47a86b5ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RC2wzaeCyLwNiidpQjxvNw.jpeg"/></div></div></figure><p id="1112" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">下面是Python管道的简单编码示例:</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="nn no l"/></div></figure><p id="bd13" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">您会注意到我们是如何简单地将管道对象定义为<em class="lm"> p </em>，然后配置3个步骤(<em class="lm">前面加上管道字符“|”</em>):</p><ul class=""><li id="f27f" class="mx my hi ju b jv jw jy jz kb mz kf na kj nb kn nc nd ne nf bi translated">ReadFromPubSub ，一个来自beam-gcp库的PTransform，可以用主题或订阅名初始化，我选择了后者，使用之前创建的订阅</li><li id="7883" class="mx my hi ju b jv ng jy nh kb ni kf nj kj nk kn nc nd ne nf bi translated"><em class="lm"> CustomParse </em>，一个自定义的PTransform，它显示了在管道中集成您的代码是多么容易，它调用上面的<em class="lm">custom parse</em>类并触发它的<em class="lm"> process </em>方法，该方法只是将摄取时间戳添加到被处理的对象中</li></ul><pre class="jh ji jj jk fd mn mo mp mq aw mr bi"><span id="2bac" class="lz kq hi mo b fi ms mt l mu mv">class CustomParsing(beam.DoFn):<br/>    def process(self, element, timestamp=beam.DoFn.TimestampParam):<br/>        parsed = json.loads(element.decode("utf-8"))<br/>        parsed["timestamp"] = timestamp.to_rfc3339()<br/>        yield parsed</span></pre><ul class=""><li id="d975" class="mx my hi ju b jv jw jy jz kb mz kf na kj nb kn nc nd ne nf bi translated"><em class="lm"> WriteToBigQuery </em>，最后定义的步骤使用了beam-gcp库中的另一个PTrasform，该PTrasform直接将上一步返回的数据写入指定的BigQuery表</li></ul><p id="acbd" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">在Apache Beam中编写管道就是这么简单，只需一个文件，我们精确地定义了我们的数据处理策略，可以应用任何定制，许多服务已经集成并提供了它们的库。</p><h2 id="4d86" class="lz kq hi bd kr ma mb mc kv md me mf kz kb mg mh lb kf mi mj ld kj mk ml lf ho bi translated"><strong class="ak">步骤3——运行管道</strong></h2><p id="0b1b" class="pw-post-body-paragraph js jt hi ju b jv lh is jx jy li iv ka kb lj kd ke kf lk kh ki kj ll kl km kn hb bi translated"><strong class="ju hs">环境设置:</strong>本地运行管道唯一需要的是python环境，您可以根据自己的喜好使用Virtualenv或Docker。<br/>你可以用官方的<a class="ae ko" href="https://cloud.google.com/sdk/gcloud/reference/beta/emulators/" rel="noopener ugc nofollow" target="_blank">模拟器</a>在本地复制PubSub，但是BigQuery没有直接的替代品，我们的目标是专注于管道，所以对于这两个，我们将直接使用云服务。</p><p id="b8d5" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">设置好环境并准备好python文件后，要运行管道，只需使用以下命令运行文件(注意将作为管道选项传递的- streaming参数):</p><pre class="jh ji jj jk fd mn mo mp mq aw mr bi"><span id="fe54" class="lz kq hi mo b fi ms mt l mu mv">python apache_beam_pipeline_with_custom_transform.py <br/>--streaming<br/>--input_subscription projects/PROJECT_ID/subscriptions/SUB_NAME<br/>--output_table PROJECT_ID:DATASET_NAME.TABLE_NAME<br/>--output_schema "timestamp:TIMESTAMP,attr1:FLOAT,msg:STRING"</span></pre><h2 id="abaa" class="lz kq hi bd kr ma mb mc kv md me mf kz kb mg mh lb kf mi mj ld kj mk ml lf ho bi translated">第四步——测试它</h2><p id="4d15" class="pw-post-body-paragraph js jt hi ju b jv lh is jx jy li iv ka kb lj kd ke kf lk kh ki kj ll kl km kn hb bi translated">由于我们已经在管道中为PubSub和BigQuery使用了云服务，为了测试和调试这些步骤，我们只需要一个触发器:在PubSub上发布一条消息。</p><p id="5865" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">在下面的要点中，我创建了一个简单的publisher，然后用它向PubSub发送100万条顺序消息，这些消息将立即被我们仍在运行的管道检索和处理。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="nn no l"/></div></figure><h1 id="122c" class="kp kq hi bd kr ks kt ku kv kw kx ky kz ix la iy lb ja lc jb ld jd le je lf lg bi translated">部署</h1><p id="7a3a" class="pw-post-body-paragraph js jt hi ju b jv lh is jx jy li iv ka kb lj kd ke kf lk kh ki kj ll kl km kn hb bi translated">现在我们的管道已经准备好了，我们只需要上传它，它将开始处理实时数据，您可以使用GCloud SDK在本地或在GCP供应的机器云外壳中运行以下命令</p><pre class="jh ji jj jk fd mn mo mp mq aw mr bi"><span id="5da6" class="lz kq hi mo b fi ms mt l mu mv">python apache_beam_pipeline_with_custom_transform.py <br/>--streaming<br/>--runner DataflowRunner <br/>--project PROJECT_ID <br/>--region europe-west1 <br/>--temp_location gs://dataflow-test-306521/ <br/>--job_name dataflow-custom-pipeline-v1 <br/>--max_num_workers 2</span></pre><p id="5685" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">这个命令将像我们之前做的那样运行管道，但是使用不同的<a class="ae ko" href="https://beam.apache.org/documentation/runners/dataflow/" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hs">运行器</strong> </a>，它将在云存储上上传您的文件并部署每个必要的基础设施。在Google Cloud中，流作业必须使用n1-standard-2或更高的计算引擎机器类型，您可以更改这一点，并按照此<a class="ae ko" href="https://cloud.google.com/dataflow/docs/guides/specifying-exec-params" rel="noopener ugc nofollow" target="_blank">引用</a>应用其他执行参数。</p><p id="0f26" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">如果上述命令失败并出现403代码，这意味着您可能没有为服务帐户提供正确的IAM角色。如果一切按预期运行，将需要几分钟进行设置，您可以按照执行日志进行操作。<em class="lm">警告:</em>即使您取消shell命令，作业仍将在数据流上运行，您需要手动取消或清空它。请注意，您可以停止数据流作业，但不能删除它们，并且每个作业都必须有唯一的名称。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es np"><img src="../Images/c31c9274b1367d7caba44f180e5775f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ARSk6KMTn2qYcZdQvUgnng.png"/></div></div><figcaption class="nq nr et er es ns nt bd b be z dx translated">每一步下面显示的时间是墙时间:这一步花费的时间的近似值，这有助于您识别慢的步骤</figcaption></figure><p id="7825" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">您的管道现已启动并运行！<br/>您现在可以再次使用PubSub模拟器，或者使用真实的发布者测试生产负载，开始发布消息，它们将被处理。</p><p id="8a29" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated">DataFlow的界面将向您显示每一步的吞吐量和时间，使您能够更好地分析您的瓶颈以及管道的哪一部分可以改进。您还可以使用云监控设置一些<a class="ae ko" href="https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring" rel="noopener ugc nofollow" target="_blank">警报策略</a>。</p><h1 id="1ffa" class="kp kq hi bd kr ks kt ku kv kw kx ky kz ix la iy lb ja lc jb ld jd le je lf lg bi translated">结论</h1><p id="31f9" class="pw-post-body-paragraph js jt hi ju b jv lh is jx jy li iv ka kb lj kd ke kf lk kh ki kj ll kl km kn hb bi translated">虽然这可能不是一个真实的用例，但我想分享一个简单的例子，它通过一点点定制将不同的GCP服务交织在一起。<br/>不要忘记，这里的主角是Apache Beam和它的Python sdk，虽然许多供应商提供了他们的官方连接器(参见<a class="ae ko" href="https://beam.apache.org/releases/pydoc/2.19.0/apache_beam.io.aws.html" rel="noopener ugc nofollow" target="_blank"> AWS S3包</a>)并准备使用模板，但编写定制管道是极其容易的，sdk让您在每个步骤上都完全自由。</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h2 id="3bca" class="lz kq hi bd kr ma mb mc kv md me mf kz kb mg mh lb kf mi mj ld kj mk ml lf ho bi translated">费用</h2><p id="234b" class="pw-post-body-paragraph js jt hi ju b jv lh is jx jy li iv ka kb lj kd ke kf lk kh ki kj ll kl km kn hb bi translated">关于维护这样一条管道的成本的一点分析。<br/>与许多托管服务一样，数据流也有<a class="ae ko" href="https://cloud.google.com/dataflow/pricing" rel="noopener ugc nofollow" target="_blank">按使用付费的定价</a>，这取决于您是运行批处理还是流管道(流成本更高)。以下是一些测试的结果，但实际成本会因处理数据的大小和数量而有很大差异。<br/>考虑你可能正在使用的任何其他云服务的成本，如BigQuery的流插入成本和每条消息的PubSub成本，后者仅在2M消息之后开始计费。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nu"><img src="../Images/70803848d97ceb2991b5721ab4e0d184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v5pUTvxiEEsaFKhEX1urXg.png"/></div></div><figcaption class="nq nr et er es ns nt bd b be z dx translated">运行一个数据流作业，用一个n1-standard1工人在2小时内接收100万个事件，成本为0.20美元，但这不足以评估一个真实的使用案例</figcaption></figure><p id="cb97" class="pw-post-body-paragraph js jt hi ju b jv jw is jx jy jz iv ka kb kc kd ke kf kg kh ki kj kk kl km kn hb bi translated"><strong class="ju hs">边注</strong>，2021年5月26日谷歌宣布<a class="ae ko" href="https://cloud.google.com/blog/products/data-analytics/simplify-and-automate-data-processing-with-dataflow-prime" rel="noopener ugc nofollow" target="_blank"><em class="lm">data flow Prime</em></a>应该优化资源使用并增加一些功能。它应该会在今年晚些时候发布</p></div></div>    
</body>
</html>