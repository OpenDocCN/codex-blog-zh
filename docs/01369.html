<html>
<head>
<title>Single Layer Perceptron and Activation Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单层感知器和激活函数</h1>
<blockquote>原文：<a href="https://medium.com/codex/single-layer-perceptron-and-activation-function-b6b74b4aae66?source=collection_archive---------5-----------------------#2021-04-22">https://medium.com/codex/single-layer-perceptron-and-activation-function-b6b74b4aae66?source=collection_archive---------5-----------------------#2021-04-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="2b6b" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">让学习开始吧…</h2><div class=""/><div class=""><h2 id="3843" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">对感知器和不同类型激活函数的冗长而简短的介绍</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/e0c8ddd7700a0ca3f44df1c610b85987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dIBvzc8-yqUDpo_F"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">照片由<a class="ae jw" href="https://unsplash.com/@donramxn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拉蒙·萨利内罗</a>在<a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="7e18" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">单层感知器(SLP)是基于阈值传递函数的前馈网络。SLP是最简单类型的人工神经网络，并且只能对具有二元目标的线性可分情况进行分类。激活函数是决定神经网络输出的数学方程。该函数附属于网络中的每个神经元，并根据每个神经元的输入是否与模型的预测相关来确定是否应该激活该函数。</p><h2 id="4265" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">它是如何工作的</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ln"><img src="../Images/6ce4d7ae048ac0d9e23f6a99f25abb2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/0*vutowYUZtL3aTwO_"/></div></figure><p id="bc88" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">感知器是线性分类器；也就是说，它是一种通过用直线分隔两个类别来对输入进行分类的算法。输入通常是特征向量<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">x</em></strong></code>乘以权重<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">w</em></strong></code>并加到偏差<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">b</em></strong></code>:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lt"><img src="../Images/3e47b85c09bf0bdcef868bf030a2d01a.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*xHhplmJrPrfhua8wFDaCaA.png"/></div></figure><p id="79dc" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">单层感知器不包括隐藏层，隐藏层允许神经网络对特征层次进行建模。因此，它是一个浅层神经网络，这阻止了它执行非线性分类。</p><h2 id="9afc" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">使用单位阶跃函数进行训练</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lu"><img src="../Images/5aad08c7bc22114b3f36f381b01073c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/0*G5hIyEF2AckWb_ht"/></div></figure><ul class=""><li id="d732" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ma mb mc md bi translated">将我们的二元分类设置中的正负类标记为<code class="du lo lp lq lr b"><strong class="jz hs">1</strong></code>和<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">-1</em></strong></code></li><li id="a85f" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">输入值<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">x</em></strong></code>和权重<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">w</em></strong></code>的线性组合为<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">(z=w₁x₁+⋯+wₘxₘ)</em></strong></code></li><li id="a9ed" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">定义激活函数<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">g(z)</em></strong></code>，其中如果<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">g(z)</em></strong></code>大于定义的阈值<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">θ</em></strong></code>，则预测<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">1</em></strong></code>，否则预测<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">-1</em></strong></code>；在这种情况下，该激活功能<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">g</em></strong></code>是简单<strong class="jz hs">单位阶跃功能</strong>的替代形式，有时也被称为<strong class="jz hs">大阶跃功能</strong></li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mj"><img src="../Images/15640023b6836219fe6faa8e1663c366.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*O3KdD2B4CZIltbeKiQhgxw.png"/></div></div></figure><p id="e514" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">总而言之，我们最终得出:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mk"><img src="../Images/dfc469dfe99f5d30fbaccd93b9e36b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*aOHwfHpdOobMa1gGOP6gew.png"/></div></div></figure><p id="240a" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">在哪里，</p><ul class=""><li id="2217" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ma mb mc md bi translated"><code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">w</em></strong></code>是特征(权重)向量，</li><li id="fb9d" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated"><code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">x</em></strong></code>是来自训练数据集的<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">m</em></strong></code>维样本:</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ml"><img src="../Images/f31299e796e5b11e9322ac18919b1a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*kmHJg-xkzBs3EoJwyuV0yA.png"/></div></figure><p id="c747" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">为了简化符号，我们将<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">θ</em></strong></code>带到等式的左侧，并定义<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">w₀=−θ</em></strong></code>和<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">x₀=1</em></strong></code> <em class="ls"> </em>(也称为<strong class="jz hs">偏差</strong>)</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mj"><img src="../Images/0cd4058e3d3828ceb768c12f77b09642.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*Eqh3H466Bn1gVr_MmyNNiA.png"/></div></div></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mm"><img src="../Images/c3722542fafb4b54bb773e311a2ff135.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/0*Q3NIs2MuBgoUMuV0"/></div></figure><p id="4eca" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">这样一来，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mk"><img src="../Images/ada0b3ad18def1b4b11d9916e899c22d.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*LTjRKLK7ZgDImrRb8qGKSg.png"/></div></div></figure><h2 id="23b1" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">学习规则</h2><p id="39d0" class="pw-post-body-paragraph jx jy hi jz b ka mn is kc kd mo iv kf kg mp ki kj kk mq km kn ko mr kq kr ks hb bi translated">初始感知器规则相当简单，可以通过以下步骤进行总结:</p><ol class=""><li id="730b" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ms mb mc md bi translated">将权重初始化为<code class="du lo lp lq lr b"><strong class="jz hs">0</strong></code>或小随机数。</li><li id="2c91" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ms mb mc md bi translated">对于每个训练样本<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">xⁱ</em></strong></code>:计算输出值并更新权重。</li><li id="e6a2" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ms mb mc md bi translated">输出值是我们之前定义的单位阶跃函数预测的类别标签，权重更新可以更正式地写成<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">wⱼ = wⱼ + Δwⱼ.</em></strong></code></li><li id="c6a1" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ms mb mc md bi translated">用于在每个增量更新权重的值由学习规则计算:<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">Δwⱼ = η(targetⁱ − outputⁱ) xⱼⁱ</em></strong></code> <br/>其中<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">η</em></strong></code>是学习率(在<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0.0</em></strong></code>和<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">1.0</em></strong></code>之间的常数)，<strong class="jz hs">目标</strong>是真实类别标签，<strong class="jz hs">输出</strong>是预测类别标签。</li><li id="ddda" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ms mb mc md bi translated">权重向量中的所有权重被同时更新</li></ol><p id="6c5e" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">只有当这两个类是线性可分的，感知器的收敛性才能得到保证。如果这两个类不能被线性决策边界分开，我们可以设置训练数据集<strong class="jz hs">时期</strong>上的最大通过次数和/或可容忍的错误分类数量的阈值<strong class="jz hs"/>。</p></div><div class="ab cl mt mu gp mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="hb hc hd he hf"><h1 id="48f8" class="na ku hi bd kv nb nc nd kz ne nf ng ld ix nh iy lg ja ni jb lj jd nj je lm nk bi translated">单位阶跃函数与激活函数</h1><p id="cb44" class="pw-post-body-paragraph jx jy hi jz b ka mn is kc kd mo iv kf kg mp ki kj kk mq km kn ko mr kq kr ks hb bi translated"><strong class="jz hs">激活函数</strong>是神经网络的决策单元。他们计算神经节点的净输出。这里，<strong class="jz hs"> Heaviside阶跃函数</strong>是神经网络中最常见的激活函数之一。该函数产生二进制输出。这就是它也被称为二元阶跃函数的原因。当输入超过阈值限制时，该函数产生1(或true ),而当输入未超过阈值时，该函数产生0(或false)。这就是为什么，它们对于二元分类研究非常有用。</p><p id="0ba8" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> heaviside阶跃函数</strong>通常只在<strong class="jz hs">单层感知器</strong>中有用，这是一种早期的神经网络，可用于输入数据可<strong class="jz hs">线性分离</strong>的情况下的分类。然而，多层神经网络或多层感知器更令人感兴趣，因为它们是通用函数逼近器，并且它们能够区分不可线性分离的数据。使用<strong class="jz hs">反向传播</strong>训练多层感知器。反向传播的一个要求是一个可微分的激活函数。这是因为反向传播在这个函数上使用<strong class="jz hs">梯度下降</strong>来更新网络权重。</p><blockquote class="nl nm nn"><p id="457e" class="jx jy ls jz b ka kb is kc kd ke iv kf no kh ki kj np kl km kn nq kp kq kr ks hb bi translated">亥维赛阶梯函数在<code class="du lo lp lq lr b"><strong class="jz hs">x = 0</strong></code>是不可微的，而在其他地方<code class="du lo lp lq lr b"><strong class="jz hs"><em class="hi">f(x) = x; −∞ → ∞</em></strong></code>它的导数是<code class="du lo lp lq lr b"><strong class="jz hs"><em class="hi">0</em></strong></code>。这意味着梯度下降将无法在更新权重方面取得进展，反向传播将失败。</p></blockquote><p id="d747" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">神经网络被称为通用函数逼近器。神经网络的主要潜在目标是学习复杂的非线性函数。如果我们在多层神经网络中不应用任何非线性，我们只是试图使用线性超平面来分离类别。</p><p id="bb75" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">非线性函数有:</p><ul class=""><li id="70e1" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ma mb mc md bi translated"><strong class="jz hs">微分或微分</strong>:y轴的变化相对于x轴的变化。它也被称为斜坡。</li><li id="57d8" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated"><strong class="jz hs">单调函数</strong>:要么完全不增，要么完全不减的函数。</li><li id="6ee6" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">主要根据它们的<strong class="jz hs">范围或曲线</strong>来划分</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nr"><img src="../Images/8252b63858f24c6343e0480c8e5207c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TIE2ERtsqiNOpp7m"/></div></div></figure><h2 id="b3c6" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">乙状结肠或逻辑激活功能</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ns"><img src="../Images/7e787db872823bfcd6ed4d76ce0c79ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/0*ZTu1DBTDpBff5wgL"/></div></figure><p id="aed0" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们使用sigmoid函数的主要原因是因为它存在于<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code> <strong class="jz hs"> <em class="ls"> </em> </strong>和<strong class="jz hs"> <em class="ls"> </em> </strong> <code class="du lo lp lq lr b"><strong class="jz hs">1</strong></code>之间。因此，它特别适用于我们必须预测概率作为输出的模型。由于任何事情的概率只存在于<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>和<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">1</em></strong></code>之间，所以sigmoid是正确的选择。它也经常被称为挤压功能。其目的是在输入空间中引入非线性。非线性是我们摇摆的地方，网络学习捕捉复杂的关系。当通过sigmoid函数的大负数变成<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>，大正数变成<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">1</em></strong></code>。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nt"><img src="../Images/553a0c1cd4f909ecbaead108470f8892.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*PwtC8bJHDo4YGX1_HBK0rg.png"/></div></figure><ul class=""><li id="28b0" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ma mb mc md bi translated"><strong class="jz hs">杀伤梯度</strong>:乙状结肠神经元在边界上饱和，因此这些区域的局部梯度几乎为零。大的正值被压缩到接近1，大的负值被压缩到接近0。因此，有效地使局部梯度接近0。结果，在反向传播期间，该梯度被乘以最终目标函数的该神经元输出的梯度，因此它将有效地“杀死”梯度，并且没有信号将通过神经元流向其权重。此外，我们必须注意初始化sigmoid神经元的权重以避免饱和，因为如果初始权重太大，那么大多数神经元将会饱和，因此网络将很难学习。</li><li id="b5d0" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated"><strong class="jz hs">非零中心输出</strong>:输出总是在0和1之间，这意味着应用sigmoid后的输出总是正的，因此，在梯度下降期间，反向传播期间权重的梯度将总是正的或负的，这取决于神经元的输出。结果，梯度更新在不同的方向上走得太远，这使得优化更加困难。</li><li id="aa3d" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">函数是<strong class="jz hs">可微的</strong>，我们可以找到任意两点处的sigmoid曲线的斜率。</li><li id="9a93" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">函数是<strong class="jz hs">单调的</strong>，但函数的导数不是。</li></ul><h2 id="9d30" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">双曲正切激活函数</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nu"><img src="../Images/0cf7c6ffd5b64c973a1a22f2f324063d.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*UW25VTgkffKJLA5N"/></div></figure><p id="901d" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">它基本上是一个移位的乙状结肠神经元。它基本上接受一个实数值，并在<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">-1</em></strong></code>和<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">+1</em></strong></code>之间压缩它。类似于乙状结肠神经元，它在大的正值和负值时饱和。然而，它的输出总是以零为中心，这是有帮助的，因为网络后面层中的神经元将接收以零为中心的输入。因此，在实践中，双曲正切激活函数在隐藏层中优于sigmoid。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nv"><img src="../Images/b57a0994158f87e25a814be0eaf80477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cdT-K-_WIby2_Ht17oxqUg.png"/></div></div></figure><ul class=""><li id="de15" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ma mb mc md bi translated">函数是<strong class="jz hs">可微的</strong>。</li><li id="af77" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">函数<strong class="jz hs">单调</strong>而其导数不单调。</li><li id="a7f6" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">双曲正切函数主要用于两类之间的分类。</li><li id="aa5a" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated"><strong class="jz hs">双曲正切和逻辑sigmoid激活函数均用于前馈网络。</strong></li></ul><h2 id="324d" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">整流线性单位</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nw"><img src="../Images/e91bc54717f2a0079f23b700ae12be52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AhdMtN-1FAmorGtd"/></div></div></figure><p id="6e7b" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">最近，它变得流行起来，因为发现与Sigmoid或Tanh激活函数相比，它大大加速了随机梯度下降的收敛。它基本上将输入阈值设置为零，即ReLU神经元输入中的所有负值都设置为零。这降低了模型根据数据适当拟合或训练的能力。给予ReLU激活函数的任何负输入都会使图形中的值立即变为零，这反过来会通过不恰当地映射负值来影响结果图形。根据输入的符号，梯度为<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>或<code class="du lo lp lq lr b"><strong class="jz hs">1</strong></code>。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nx"><img src="../Images/117b965b608eeb000ecb20b71d6a0112.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*WTJTm3BrzpzznsextV1mSA.png"/></div></figure><ul class=""><li id="a022" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ma mb mc md bi translated"><strong class="jz hs">激活稀疏度</strong> : ReLU和Tanh激活函数几乎总是在神经网络中被激活，导致几乎所有的激活都在计算网络的最终输出时被处理。理想情况下，我们希望只有一部分神经元激活，并对网络的最终输出做出贡献，因此，我们希望网络中的一部分神经元是被动的。ReLU给了我们这个好处。因此，由于ReLU的特性，有可能<em class="ls"> 50% </em>的神经元给予<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>激活，从而导致更少的神经元触发，结果网络变得更轻，我们可以更快地计算输出。</li><li id="3431" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated"><strong class="jz hs">死神经元</strong>:由于ReLu中的水平线(对于负<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">x</em></strong></code>)，梯度可以走向<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>。对于ReLu区域中的激活，梯度将为<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>，因此在下降过程中不会调整权重。这意味着，那些进入这种状态的神经元将停止对错误/输入的变化做出反应(仅仅因为梯度是<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>，没有任何变化)。这就是所谓的将死雷鲁问题。这个问题会导致几个神经元死亡，没有反应，使网络的大部分处于被动状态。因此,“死亡”的神经元将停止响应输出误差的变化，因此，在反向传播期间，参数将永远不会更新。</li><li id="7627" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated"><strong class="jz hs">范围</strong> : <strong class="jz hs"> <em class="ls"> </em> </strong> <code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0 → ∞</em></strong></code></li><li id="4477" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">函数及其<strong class="jz hs">导数</strong>都是<strong class="jz hs">单调</strong>。</li></ul><h2 id="dd2a" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">泄漏ReLU</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ny"><img src="../Images/e5036e055327d36a998d586b5482476b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cy0gXICObfRu_o8Z"/></div></div></figure><p id="ab61" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">只是传统ReLU函数的扩展。正如我们看到的，对于小于<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>的值，梯度是<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>，这导致这些区域中的“死亡神经元”。为了解决这个问题，Leaky ReLU就派上了用场。也就是说，我们不是将小于<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>的值定义为<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0</em></strong></code>，而是将负值定义为输入的一个小的线性组合。常用的小值是<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">0.01</em></strong></code>。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nz"><img src="../Images/048712163d4f8ae77c41d1566080cf25.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*u_FhmSCXJGOrWfUs1w47sg.png"/></div></figure><ul class=""><li id="5a5e" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ma mb mc md bi translated">通过做一点小小的改变，漏ReLU的思想可以被进一步扩展。我们可以学习乘数，而不是将<code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">z</em></strong></code>乘以一个常数，并在我们的过程中将其视为一个附加的超参数。这就是所谓的参数ReLU。</li><li id="1b94" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated"><strong class="jz hs">范围</strong> : <code class="du lo lp lq lr b"><strong class="jz hs"><em class="ls">−∞ → ∞</em></strong></code></li><li id="0ddd" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ma mb mc md bi translated">函数及其<strong class="jz hs">导数</strong>本质上是<strong class="jz hs">单调</strong>。</li></ul></div><div class="ab cl mt mu gp mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="hb hc hd he hf"><p id="cd5c" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">参考:</p><ol class=""><li id="75da" class="lv lw hi jz b ka kb kd ke kg lx kk ly ko lz ks ms mb mc md bi translated"><a class="ae jw" href="https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2015 _ single layer _ neurons . html</a></li><li id="76de" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ms mb mc md bi translated"><a class="ae jw" href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener" target="_blank">https://towards data science . com/activation-functions-neural-networks-1 CBD 9 F8 d 91d 6</a></li><li id="8f2a" class="lv lw hi jz b ka me kd mf kg mg kk mh ko mi ks ms mb mc md bi translated"><a class="ae jw" href="https://towardsdatascience.com/activation-functions-b63185778794" rel="noopener" target="_blank">https://towards data science . com/activation-functions-b 63185778794</a></li></ol><p id="22cb" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">欢迎任何反馈或建设性的批评。</p></div></div>    
</body>
</html>