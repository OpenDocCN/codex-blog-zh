<html>
<head>
<title>Pandas Data Processing Tasks Translated to PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熊猫数据处理任务翻译成PySpark</h1>
<blockquote>原文：<a href="https://medium.com/codex/pandas-data-processing-tasks-translated-to-pyspark-af231efc8c13?source=collection_archive---------5-----------------------#2021-06-13">https://medium.com/codex/pandas-data-processing-tasks-translated-to-pyspark-af231efc8c13?source=collection_archive---------5-----------------------#2021-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/04a7e9fe108924c83f149c36535c3f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Es62uDD0tuLRckOE"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">由<a class="ae iu" href="https://unsplash.com/@sortino?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">约书亚·索蒂诺</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="8031" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们生活在大数据时代。随着互联网的发展，数据以巨大的数量和高度的可变性快速增长。大数据处理可能会让你头疼，因为它自然需要大量的运行时间。Apache Spark(或Spark)是处理大数据的流行工具之一。</p><p id="15a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Spark是用于大规模数据处理的统一分析引擎。借助Spark，我们可以快速执行数据处理，并将处理任务分配到多台计算机上。人们使用Spark是因为它可以在Python、Scala、Java、R和SQL等流行的编程语言中部署。它还有一堆支持流数据、机器学习和图形处理的库。</p><p id="00dd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中一个Spark接口是PySpark，它允许您使用Python APIs编写Spark应用程序。PySpark支持Spark的功能，如Spark SQL、DataFrame、Streaming、MLlib(机器学习)和Spark Core。作为一个经常和熊猫打交道的人，我发现PySpark可以做我在那里做的事情。然而，实现却大不相同。我会列出一些我平时在熊猫里执行的数据处理任务，翻译成PySpark。</p><h1 id="4df4" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">py spark里的熊猫</strong></h1><h2 id="4ba7" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated">创建Spark会话</h2><p id="f0b9" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">在我们开始使用Spark之前，我们需要创建Spark会话。这是用数据集和数据帧API编程Spark的入口点。从下面的代码可以看出，我们可以设置会话的应用程序名称。然后通过使用<code class="du lk ll lm ln b">getOrCreate</code>方法，创建一个Spark会话。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h2 id="101b" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated"><strong class="ak">创建数据帧并读取文件</strong></h2><p id="d801" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">熊猫提供了很多读取文件的方法。下面是读取逗号分隔值(CSV)文件的示例。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="88d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要在PySpark中执行类似的任务，我们可以做的是创建一个空的数据帧。之后，空数据帧与CSV数据联合。PySpark数据帧在弹性分布式数据集(RDDs)之上实现，可并行操作。这样的实现使得PySpark转换数据的速度比熊猫还要快。</p><p id="61e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以使用PySpark创建一个数据模式。Spark DataFrames模式被定义为类型化列的集合。我们使用<code class="du lk ll lm ln b">StructType</code>方法来存储整个模式，使用<code class="du lk ll lm ln b">StructField</code>来定义各个列。我们还使用<code class="du lk ll lm ln b">StringType</code>和<code class="du lk ll lm ln b">IntegerType</code>来定义每一列的数据类型。然后，我们可以使用前面定义的模式，通过使用<code class="du lk ll lm ln b">CreateDataFrame</code>方法创建一个空的数据帧。之后，我们读取CSV，然后将其与空数据帧合并。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h2 id="7eff" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jg ky kz kh jk la lb kl jo lc ld kp le bi translated"><strong class="ak">数据转换</strong></h2><p id="886e" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">PySpark在一个名为<code class="du lk ll lm ln b">pyspark.sql.functions</code>的模块中提供了数据转换工具。这个模块存储了许多转换数据的函数方法，如转换为日期时间、连接列等。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="f4fd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们将看到我在过去的项目中使用PySpark进行的一些数据转换，并与Pandas进行比较。</p><ul class=""><li id="01d1" class="lu lv hi ix b iy iz jc jd jg lw jk lx jo ly js lz ma mb mc bi translated"><strong class="ix hj">将字符串转换为日期时间</strong></li></ul><p id="6c7b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将字符串转换为日期时间是数据清理过程中的常见做法。目的是确保我们的数据有一个合适的日期格式。在熊猫身上，我们可以用<code class="du lk ll lm ln b">to_datetime</code>的方法。在PySpark中，我们可以使用来自<code class="du lk ll lm ln b">pyspark.sql.functions</code>模块的<code class="du lk ll lm ln b">to_timestamp</code>。从代码中，我们还可以看到，为了导航该列，我们需要使用PySpark中的<code class="du lk ll lm ln b">withColumn</code>方法。熊猫和PySpark写日期格式也有区别。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><ul class=""><li id="d6f6" class="lu lv hi ix b iy iz jc jd jg lw jk lx jo ly js lz ma mb mc bi translated"><strong class="ix hj">获取时差</strong></li></ul><p id="2d2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面的示例代码展示了Pandas和PySpark在获取到达时间和离开时间之间的差异。除了日期格式书写的不同，我们还可以发现Pandas和PySpark如何减去到达和离开时间的不同。在PySpark中，我们需要使用<code class="du lk ll lm ln b">unix_timestamp</code>方法来标准化日期格式。然后，由于<code class="du lk ll lm ln b">triptime</code>列以分钟的形式返回时差，在PySpark中，我们需要将减法结果除以60，然后使用<code class="du lk ll lm ln b">IntegerType()</code>对象将其转换为整数，与之前创建模式的对象相同。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><ul class=""><li id="cd63" class="lu lv hi ix b iy iz jc jd jg lw jk lx jo ly js lz ma mb mc bi translated"><strong class="ix hj">连接列中的字符串</strong></li></ul><p id="bf71" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在PySpark中，函数中有一个名为<code class="du lk ll lm ln b">lit</code>的方法来声明一个随机字符串。在本例中，我们使用它将分隔符<code class="du lk ll lm ln b">-</code>放在<code class="du lk ll lm ln b">source_airport</code>和<code class="du lk ll lm ln b">destination_airport</code>之间。然后用一种叫做<code class="du lk ll lm ln b">concat</code>的方法，把它们包装起来，得到航班的路线。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><ul class=""><li id="87ef" class="lu lv hi ix b iy iz jc jd jg lw jk lx jo ly js lz ma mb mc bi translated"><strong class="ix hj">条件列转换</strong></li></ul><p id="ddf8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了在Pandas中应用条件列转换，我们可以使用名为<code class="du lk ll lm ln b">where</code>的NumPy方法。在PySpark中，相当于调用<code class="du lk ll lm ln b">when</code>的<code class="du lk ll lm ln b">functions</code>方法来声明真值，调用<code class="du lk ll lm ln b">otherwise</code>来声明假值。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><ul class=""><li id="1e7a" class="lu lv hi ix b iy iz jc jd jg lw jk lx jo ly js lz ma mb mc bi translated"><strong class="ix hj">删除列</strong></li></ul><p id="c57f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于删除列功能，我们看不出Pandas和PySpark之间有太大的区别。在PySpark中，我们只需要将<code class="du lk ll lm ln b">*</code>放在被删除列的列表之前。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h1 id="6019" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="935d" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">从上面的例子中，我们可以看到PySpark在数据转换任务中可以做熊猫做的事情，从读取文件到删除列。当然，还有更多相当于熊猫的PySpark数据转换功能。我建议你参考<a class="ae iu" href="https://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank"> PySpark文档</a>来探索更多的功能。</p><p id="71e7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望这篇文章对刚开始使用PySpark的你有所帮助，尤其是已经熟悉熊猫，玩大数据的你。要练习使用PySpark，您可以在您的设备上设置Spark。但如果你想消除在你的设备上设置Spark的复杂性，你可以随时使用Google Colab，因为它支持Spark。</p></div></div>    
</body>
</html>