<html>
<head>
<title>Statistical Language Model: N-gram to calculate the Probability of word sequence using Python.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">统计语言模型:使用Python计算单词序列概率的N-gram。</h1>
<blockquote>原文：<a href="https://medium.com/codex/statistical-language-model-n-gram-to-calculate-the-probability-of-word-sequence-using-python-2e54a1084250?source=collection_archive---------1-----------------------#2022-04-04">https://medium.com/codex/statistical-language-model-n-gram-to-calculate-the-probability-of-word-sequence-using-python-2e54a1084250?source=collection_archive---------1-----------------------#2022-04-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6586" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">N-gram分步实施综合指南。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/f5131047334d0cb40b5779b7db9cc629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wCBcO4Z7WWOa3ZVBZlL50A.png"/></div></div></figure><p id="f123" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi kf translated"><span class="l kg kh ki bm kj kk kl km kn di"> T </span>这篇文章介绍了n-gram的逐步python实现，以预测给定数据集的给定句子的概率。本文主要介绍了语言模型的解释<strong class="jl hj"> N元语法</strong>，以及它在python中的实现。实现分为11个步骤，每个步骤都有描述和代码，后面是每个代码的输出。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="07fd" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">目录</h1><ul class=""><li id="cb15" class="ln lo hi jl b jm lp jp lq js lr jw ls ka lt ke lu lv lw lx bi translated"><a class="ae ly" href="#ab36" rel="noopener ugc nofollow"> <em class="lz">什么是语言模型？</em> </a></li><li id="ce15" class="ln lo hi jl b jm ma jp mb js mc jw md ka me ke lu lv lw lx bi translated"><a class="ae ly" href="#ae2e" rel="noopener ugc nofollow"> <em class="lz">什么是统计语言建模？</em>T11】</a></li><li id="9805" class="ln lo hi jl b jm ma jp mb js mc jw md ka me ke lu lv lw lx bi translated"><a class="ae ly" href="#a14c" rel="noopener ugc nofollow"> <em class="lz">什么是N-gram？</em>T15】</a></li><li id="a7e6" class="ln lo hi jl b jm ma jp mb js mc jw md ka me ke lu lv lw lx bi translated"><a class="ae ly" href="#d1c9" rel="noopener ugc nofollow"><em class="lz">N-gram在python中的实现</em> </a></li></ul></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="ab36" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">什么是语言模型？</h1><p id="9510" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">在自然语言处理中，语言模型是字母序列的概率分布。语言模型分析文本数据来计算单词概率。它使用一种算法来解释数据，这种算法为自然语言中的上下文建立规则。</p><blockquote class="mi"><p id="8b24" class="mj mk hi bd ml mm mn mo mp mq mr ke dx translated">语言建模是确定单词序列概率的艺术。这在许多领域都很有用，包括语音识别、光学字符识别、手写识别、机器翻译和拼写纠正</p><p id="f9bd" class="mj mk hi bd ml mm mn mo mp mq mr ke dx translated">——语言建模的一点进展，2001年</p></blockquote></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="ae2e" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">什么是统计语言建模(SLM)？</h1><p id="bd5f" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">统计语言建模旨在创建一个能够准确估计自然语言分布的统计语言模型。统计语言模型(SLM)是字符串S上的概率分布P(s ),它试图反映字符串S作为短语出现的频率。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="a14c" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">什么是N-gram？</h1><p id="eef7" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">N-gram是一种统计语言模型，它为句子和单词序列分配概率。词序可以是2个词、3个词、4个词等。比如n字。n元语法也被称为n个单词的序列。N-gram语言模型是基于一系列单词的计数来确定概率的语言模型。根据字数统计，N-gram可以是:</p><ul class=""><li id="ef54" class="ln lo hi jl b jm jn jp jq js ms jw mt ka mu ke lu lv lw lx bi translated">T21:只有一个单词的序列</li><li id="a2ae" class="ln lo hi jl b jm ma jp mb js mc jw md ka me ke lu lv lw lx bi translated"><strong class="jl hj">二元模型</strong>:两个单词的序列</li><li id="d071" class="ln lo hi jl b jm ma jp mb js mc jw md ka me ke lu lv lw lx bi translated"><strong class="jl hj">三元组</strong>:三个字的序列</li></ul><p id="15d9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们用一个例子来理解N-gram。考虑下面的句子:</p><p id="f6dc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">“无论你走到哪里，都要保持乐观”</p><p id="bc35" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">1-gram(或unigram)是一个单词序列。对于上面的句子，单字可以简单地是:“保持”、“传播”、“积极”、“无论何处”、“你”、“去”。</p><p id="2a77" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一个2-gram(或bigram)是两个单词的单词序列，像“继续传播”，“传播积极”，“无论何处都是积极的”，“无论你在哪里”，或“你去哪里”。三字格(或三元格)是由三个单词组成的单词序列，如“继续传播积极”、“在任何地方传播积极”、“无论你在哪里都积极”或“无论你去哪里”。</p><p id="a67f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">当我们使用二元模型来预测下一个单词的条件概率时，我们因此做出如下近似:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/35a8354e7b22a04914e664dfc60a94d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*Sr0vMFTzivFJVCi4q83seA.png"/></div></figure><p id="b248" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一个词的概率只取决于前一个词的假设叫做<strong class="jl hj">马尔可夫假设。</strong></p><p id="e734" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一种估计概率的直观方法叫做最大似然估计或MLE。我们通过从语料库中获取计数，并对计数进行归一化，使它们位于0和1之间，从而获得n元模型参数的<strong class="jl hj">最大似然估计</strong>或<strong class="jl hj"> MLE </strong>估计。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/c0f79905732a0b57ac5c682c598bce83.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*StdQfKUjSogpasjMfgWJWA.png"/></div></figure></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="d1c9" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">N元语法在Python中的实现</h1><blockquote class="mx my mz"><p id="e34f" class="jj jk lz jl b jm jn ij jo jp jq im jr na jt ju jv nb jx jy jz nc kb kc kd ke hb bi translated">注意:我已经提供了Python代码及其输出。在文章的最后，您可以找到完整的全长代码</p></blockquote><h2 id="39c0" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">第一步:数据语料库</h2><p id="3354" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">在这个实现中，我们从用户那里获取输入数据。可以输入python中nltk模块提供的数据集。</p><pre class="iy iz ja jb fd nr ns nt nu aw nv bi"><span id="5f39" class="nd kw hi ns b fi nw nx l ny nz">d=input("Enter corpus = ")</span></pre><p id="b0f9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oa"><img src="../Images/f9253087429335e6b5b8d14de0777cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*R5gIEiamyAw8tsGMpTKifg.png"/></div></figure><h2 id="5cdc" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">第二步:预处理</h2><p id="f42e" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">在这一步中，数据被转换成小写，标点符号(这里是句点符号)被删除，以去除数据中无用的部分或干扰。这里我们使用eos标签来标记句子的开始和结束。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="e27e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es od"><img src="../Images/8a1f9484b59a7de56d416ac51d9d3b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eWQ9cNQdhA-mT6A8d0Sj1w.png"/></div></div></figure><h2 id="7345" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">步骤3:标记化</h2><p id="fc78" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">预处理过的数据现在被转换成一系列标记。这些标记有助于理解上下文或开发NLP的模型。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="c728" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oe"><img src="../Images/30f88389a7e81d7c87cfa534be9fecf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9zyFRp9IIt_cFGtDou52wA.png"/></div></div></figure><h2 id="96a7" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">步骤4:计算令牌的频率</h2><p id="d9ad" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">在此步骤中，定义一个空字典来保存标记化数据集中每个标记的频率。输出屏幕截图中显示了给定数据集中每个标记的出现频率。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="4f66" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es of"><img src="../Images/2e16044fa54a6c35c353b0295b1c891f.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*yB5OoANnYAfNEvNMgu-qDQ.png"/></div></figure><h2 id="ae27" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">第五步:生成n元语法</h2><p id="9e8c" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">步骤3中生成的标记用于生成n元语法。这里，代码中的k表示n-grams中的n。在这个实现中，我们将使用二元模型(k=n=2)来计算一个句子的概率。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="ffbe" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es og"><img src="../Images/9777db8cb3a515f2aa18c385dbaa5799.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*QWJFuu2ehj2t-9wLAkVmyQ.png"/></div></figure><h2 id="e799" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">第六步:计算n-gram的频率</h2><p id="ba04" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">dct1是包含步骤5中生成的n元语法作为关键字的字典。计算数据集中每个n元语法的频率，并将其作为值添加到字典dct1中相应的n元语法键。在接下来的步骤中，将需要这些频率来计算概率。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="107a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oh"><img src="../Images/18cfbd433a35d891b33cee6f667fb3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*b6d8TIhDysZWhBOC2ZkZhg.png"/></div></figure><h2 id="4000" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">步骤7:概率表/二元表</h2><p id="6f20" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">在这一步中计算每个n元文法的概率，并存储在矩阵中(这里是l)。计算n-gram概率的公式如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oi"><img src="../Images/4ae37aa9a98b5cda204d3ed96941cdba.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*ZVGuhA7URQs_rJm--8Be2w.png"/></div></figure><p id="3b14" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">举个例子，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oj"><img src="../Images/dbe63f729d211f5c10135fc4facffaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*BnAFhlELX-HDY03n1phXTA.png"/></div></figure><p id="961f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">类似地，每个n-gram的概率被计算并存储在概率表refer输出图像中。这个概率表用于计算给定单词序列的概率。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="995b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ok"><img src="../Images/753a9498844baceaefe6d0f0002c0409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5oewE2SLebbfsbVdB_eE8A.png"/></div></div></figure><h2 id="644f" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">步骤8:输入文本检查概率</h2><p id="da9a" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">拿一句话来计算它的概率。</p><pre class="iy iz ja jb fd nr ns nt nu aw nv bi"><span id="e905" class="nd kw hi ns b fi nw nx l ny nz">text = input("Enter Text = \n")</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ol"><img src="../Images/bd4b95f8bf3005c1f1d3457b9f31bee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*k_J_q4uapbL40NV34od3QQ.png"/></div></figure><h2 id="3cf1" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">步骤9:对给定的文本重复步骤2- 6</h2><p id="6c25" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">使用前面步骤中创建的函数对输入文本进行预处理、标记化并生成n元语法。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="45e7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es om"><img src="../Images/c5180f95b7791a15b2528c9b5b605d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kchATrCf3FyATeyFs-HEyA.png"/></div></div></figure><h2 id="4d81" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">步骤10:计算二元模型概率</h2><p id="6fc2" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">在这一步中，计算每个n元语法的概率，这将在进一步的步骤中使用。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="13a6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es on"><img src="../Images/76779db9eec9098436ffebedf4bc3037.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*rXTKezheyfQBHS6Aj8YYSw.png"/></div></figure><h2 id="76cf" class="nd kw hi bd kx ne nf ng lb nh ni nj lf js nk nl lh jw nm nn lj ka no np ll nq bi translated">步骤11:计算单词序列的概率</h2><p id="942f" class="pw-post-body-paragraph jj jk hi jl b jm lp ij jo jp lq im jr js mf ju jv jw mg jy jz ka mh kc kd ke hb bi translated">使用<strong class="jl hj">概率链规则计算完整单词序列的概率。</strong></p><p id="ae56" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">将概率链规则应用于单词，我们得到:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oo"><img src="../Images/d58cb4bb5f7acd191758978ce1b7248f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*epXYtUpatOFz7iP8EMw8TQ.png"/></div></figure><p id="298d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这里，计算句子的概率:</p><p id="e970" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> P("我不喜欢绿色的鸡蛋和火腿")</strong></p><p id="1625" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">= P(I | EOS)* P(do | I)* P(not | do)* P(like | not)* P(green | like)* P(eggs | green)* P(and | eggs)* P(ham | and)* P(EOS | ham)</p><p id="e923" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi">= 2 * 2 * 1 * 1 * 1 * 1 * 1 * 1 * 1 * 1 * 1 * 1 * 1 * 1</p><p id="4333" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">= <strong class="jl hj"> 0.1665 </strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob oc l"/></div></figure><p id="b35a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es op"><img src="../Images/eeca08bfa6ee1d83d37ad75d8c0f0a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDhYsS7eU5m4FIl_YUq2UQ.png"/></div></div></figure></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><p id="bf22" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我的GitHub上提供了完整的完整实现:<a class="ae ly" href="https://github.com/Minakshee25/Natural-Language-Processing" rel="noopener ugc nofollow" target="_blank">minak shee 25/自然语言处理(github.com)</a>。</p><p id="82e8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">或者访问以下Repl存储库:</p><div class="oq or ez fb os ot"><a href="https://replit.com/join/xnuenbxpml-minaksheenaraya" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab dw"><div class="ov ab ow cl cj ox"><h2 class="bd hj fi z dy oy ea eb oz ed ef hh bi translated">邀请在Replit上合作</h2><div class="pa l"><h3 class="bd b fi z dy oy ea eb oz ed ef dx translated">使用Replit多人游戏一起编码</h3></div><div class="pb l"><p class="bd b fp z dy oy ea eb oz ed ef dx translated">replit.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph jh ot"/></div></div></a></div></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><p id="dcb7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">使用这些n元语法和某些单词在某些序列中出现的概率可以改进自动补全系统的预测。同样，我们使用can NLP和n-grams来训练基于语音的个人助理机器人。例如，使用3-gram或trigram训练模型，机器人将能够理解句子之间的差异，如“温度是多少？”和“设定温度”</p><p id="2235" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">希望你觉得这篇中型文章有用！做个好人，如果博客对你有帮助，就为它鼓掌吧:-)</p><p id="42f6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">关注我的类似文章！</p><blockquote class="mx my mz"><p id="c717" class="jj jk lz jl b jm jn ij jo jp jq im jr na jt ju jv nb jx jy jz nc kb kc kd ke hb bi translated">您可以通过以下方式与我联系:</p><p id="51d3" class="jj jk lz jl b jm jn ij jo jp jq im jr na jt ju jv nb jx jy jz nc kb kc kd ke hb bi translated">领英<strong class="jl hj">🧑‍💼:<a class="ae ly" href="https://www.linkedin.com/in/minakshee-n-408b1a199/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/minakshee-n-408b1a199/</a></strong></p></blockquote></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><p id="c2af" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">如果你喜欢这篇文章，这里有一些你可能喜欢的文章:</strong></p><div class="oq or ez fb os ot"><a rel="noopener follow" target="_blank" href="/@minakshee.narayankar2000/error-free-installation-of-owncloud-server-on-windows-10-within-30-minutes-using-wsl-6450c38b1ae0"><div class="ou ab dw"><div class="ov ab ow cl cj ox"><h2 class="bd hj fi z dy oy ea eb oz ed ef hh bi translated">使用WSL在30分钟内将Owncloud服务器无误地安装到Windows 10上！</h2><div class="pa l"><h3 class="bd b fi z dy oy ea eb oz ed ef dx translated">Owncloud服务器的安装很繁琐，不是因为步骤冗长，而是因为大多数命令都不够用…</h3></div><div class="pb l"><p class="bd b fp z dy oy ea eb oz ed ef dx translated">medium.com</p></div></div><div class="pc l"><div class="pi l pe pf pg pc ph jh ot"/></div></div></a></div></div></div>    
</body>
</html>