<html>
<head>
<title>Hyperparameter Optimization: Grid vs Random</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数优化:网格与随机</h1>
<blockquote>原文：<a href="https://medium.com/codex/hyperparameter-optimization-grid-vs-random-3d87c7724f4f?source=collection_archive---------16-----------------------#2021-07-13">https://medium.com/codex/hyperparameter-optimization-grid-vs-random-3d87c7724f4f?source=collection_archive---------16-----------------------#2021-07-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="23d3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">泰坦尼克号数据集作为案例研究</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/30c5d67ee0be7a20a3ee1c73628e6e7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*bVqmDHNOo0wX4tdMbTXVWw.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">图片来源:nationalgeographic.org</figcaption></figure><p id="3dca" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi kf translated"><span class="l kg kh ki bm kj kk kl km kn di">T</span><strong class="jl hj">机器学习</strong> ( <strong class="jl hj"> ML </strong>)的美妙之处在于，计算机算法能够通过<em class="ko">从一组样本数据(称为训练数据)中不断学习</em>，直到<a class="ae kp" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">损失函数</a>收敛到可能的最小值，而无需人工干预。</p><p id="16d6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该模型在其与训练数据的交互过程中学习一些<a class="ae kp" href="https://en.wikipedia.org/wiki/Parameter" rel="noopener ugc nofollow" target="_blank">参数</a>的值。在大多数情况下，这些参数可以简化为数学关系，并根据特定模型对训练数据的操作自动设置。示例包括<a class="ae kp" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">线性回归</a>中的截距和斜率以及<a class="ae kp" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>中的权重和偏差。</p><blockquote class="kq"><p id="d0d0" class="kr ks hi bd kt ku kv kw kx ky kz ke dx translated">超参数是一个参数，其值用于控制学习过程，与训练期间学习的(其他)参数形成对比。</p></blockquote><p id="6209" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">但是，还有另一类参数，它们的值是在训练模型之前设置的，因为它们不能通过将模型拟合到数据来学习。它们被称为<a class="ae kp" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数</a>。超参数是一个参数，其值用于控制学习过程，与训练期间学习的(其他)参数形成对比。示例包括学习速率和梯度下降中的迭代次数，以及神经网络中的层数和神经元数。</p><p id="f6e6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">超参数非常重要，因为它们直接控制训练算法的行为，并对模型的速度和整体性能产生重大影响。</p><blockquote class="kq"><p id="2d7e" class="kr ks hi bd kt ku kv kw kx ky kz ke dx translated">H <strong class="ak">超参数优化</strong>或<strong class="ak">调整</strong>是为特定学习算法选择一组最佳超参数的过程。</p></blockquote><p id="af45" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">超参数的精确调节可以提高性能。这被称为<a class="ae kp" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank">超参数优化</a>或调整:为特定学习算法选择一组最佳超参数的过程。这涉及为学习算法提供最佳性能的值的混合。</p><p id="cde8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在本文中，我将比较两种最广泛使用的超参数调优方法:</p><ul class=""><li id="0221" class="lf lg hi jl b jm jn jp jq js lh jw li ka lj ke lk ll lm ln bi translated"><strong class="jl hj"> GridSearchCV </strong>，以及</li><li id="0e28" class="lf lg hi jl b jm lo jp lp js lq jw lr ka ls ke lk ll lm ln bi translated"><strong class="jl hj">randomsearccv</strong>。</li></ul><p id="b039" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> GridSearchCV </strong>此后，选择具有最佳损失函数的值的组合，并为模型手动设置。这是简单和详尽的，但可能是耗时和昂贵的计算。</p><p id="c12b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> RandomizedSearchCV </strong> <br/>与探索网格中每个可能组合的GridSearchCV不同，RandomizedSearchCV评估一组随机的超参数值以找到最佳组合。这种方法速度更快，因为它减少了计算，但它可能不会返回最佳结果。</p><h1 id="c1ff" class="lt lu hi bd lv lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated"><strong class="ak">泰坦尼克号数据集</strong></h1><blockquote class="kq"><p id="b027" class="kr ks hi bd kt ku kv kw kx ky kz ke dx translated">在过去的100年里，这艘船的遗产和传说以各种形式被记录下来，成为虚构和非虚构作品和电影的灵感来源…</p></blockquote><p id="89ee" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">我们将使用机器学习最喜欢的新娘，Kaggle 上的<a class="ae kp" href="https://www.kaggle.com/c/titanic/overview" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集。</a></p><p id="183a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">永不沉没的泰坦尼克号于1912年4月10日从英国南安普敦出发，开始了它的首航，驶向纽约。4月14日，在旅程的第四天，在纽芬兰以南约375英里(600公里)的地方，她在晚上11点40分左右与一座冰山相撞，导致船上的恐慌和与时间的赛跑。这艘客轮最终在4月15日凌晨沉没，导致2224名乘客和船员中的1500多人遇难。</p><p id="d2f1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在过去的100年里，该船的遗产和传说以各种形式被记录下来，成为虚构和非虚构作品和电影的灵感来源，该事件导致了海事法规和安全措施的重大变化。</p><p id="70cc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，机器学习——人类最新的不可能领域——向雄心勃勃的前辈“永不沉没的泰坦尼克号”致敬也就不足为奇了。因此，它已经成为有抱负的数据科学家和机器学习爱好者的首选。</p><h2 id="f743" class="ml lu hi bd lv mm mn mo lz mp mq mr md js ms mt mf jw mu mv mh ka mw mx mj my bi translated">卡格尔比赛</h2><p id="6486" class="pw-post-body-paragraph jj jk hi jl b jm mz ij jo jp na im jr js nb ju jv jw nc jy jz ka nd kc kd ke hb bi translated">泰坦尼克号ML比赛是开始你进入机器学习比赛的旅程并熟悉Kaggle平台如何工作的最佳挑战。</p><p id="834f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">比赛很简单:使用机器学习创建一个模型，预测哪些乘客在泰坦尼克号沉船中幸存。</p><p id="f24c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在比赛中，你可以访问两个类似的数据集，即“train.csv”和“test.csv”，其中包括乘客的信息，如姓名、年龄、性别、社会经济阶层、上车地点等。训练数据包含一个针对幸存者的额外列，这是您为测试数据寻找的内容。</p><p id="06ad" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然而，本文的目的只是使用数据集作为案例研究，分析两个最著名的超参数优化策略返回最佳结果所需的输出和时间。</p><h1 id="a745" class="lt lu hi bd lv lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">结果</h1><p id="49cb" class="pw-post-body-paragraph jj jk hi jl b jm mz ij jo jp na im jr js nb ju jv jw nc jy jz ka nd kc kd ke hb bi translated">您可以查看完整的笔记本:<a class="ae kp" href="https://www.kaggle.com/mabalogun/hyperparameter-optimization-grid-vs-random" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/mabalogun/hyperparameter-optimization-grid-vs-random</a></p><h2 id="9009" class="ml lu hi bd lv mm mn mo lz mp mq mr md js ms mt mf jw mu mv mh ka mw mx mj my bi translated">GridSearchCV</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ne"><img src="../Images/780ac3831417266e4da7842e848ed9e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*Df36HIkCYSI2yHa5XbuwTw.png"/></div></figure><p id="5244" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">GridSearchCV花了3.66秒探索超参数的24个可能值。最高等级是平均验证分数0.812，标准偏差0.008。</p><h2 id="9c12" class="ml lu hi bd lv mm mn mo lz mp mq mr md js ms mt mf jw mu mv mh ka mw mx mj my bi translated">RandomSearchCV</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/4d0b41eb2eed115ad6d8ad862a0f1af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*yJk9V1hhRErWqcIK13Koeg.png"/></div></figure><p id="d6e0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">RandomSearchCV花费0.98秒获得0.812的平均验证分数，标准偏差为0.018，这与GridSearchCV的结果相同。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/be3eee13adfcbd7c0098bd39189ece8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*u9FnJTt1bNUQd20JftyS8Q.png"/></div></figure><h1 id="e67e" class="lt lu hi bd lv lw lx ly lz ma mb mc md io me ip mf ir mg is mh iu mi iv mj mk bi translated">结论</h1><p id="36eb" class="pw-post-body-paragraph jj jk hi jl b jm mz ij jo jp na im jr js nb ju jv jw nc jy jz ka nd kc kd ke hb bi translated">得到的超参数是相同的，因此输出也是相同的，而RandomSearchCV方法比GridSearchCV快373%。</p><p id="9b74" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">虽然从上面可以清楚地推断出GridSearchCV与RandomSearchCV相比可能没有定性优势，但要得出确定的结论，需要的不仅仅是一个数据集和一个模型。</p><p id="9426" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然而，可以明确地说，在某些情况下，RandomSearchCV可能是比通常使用的GridSearchCV更好的方法，特别是如果您对要优化的超参数和要使用的值有一个大致的概念。</p></div></div>    
</body>
</html>