<html>
<head>
<title>EfficientNetV2: Smaller Models and Faster Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EfficientNetV2:更小的模型和更快的培训</h1>
<blockquote>原文：<a href="https://medium.com/codex/efficientnetv2-smaller-models-and-faster-training-fd174bed360c?source=collection_archive---------2-----------------------#2021-09-16">https://medium.com/codex/efficientnetv2-smaller-models-and-faster-training-fd174bed360c?source=collection_archive---------2-----------------------#2021-09-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2f2c598bc1c6759745dec2dce6543bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hdNuTskr89I9HB0W"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@i_am_g?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Guillaume Jaillet </a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="1b28" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在寻找最有效的CNN架构方面已经有许多实验。一个<em class="jt">高效的</em>架构必须在许多瓶颈之间找到一个完美的平衡，比如准确性、参数、FLOPs或推理时间。许多论文根据需要关注不同的瓶颈。例如，DenseNet和EfficientNet试图用较少的参数来提高精度。RegNet、ResNeSt和MNasNet优化了推理速度。NFNets专注于提高训练速度。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ju"><img src="../Images/8cbe3f6becce8692705c298c4462b3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*SpKw9gjqsVkRk4g8nh25wA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">训练高效+参数高效</figcaption></figure><p id="45c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇全新的论文(2021年6月)提供了关于如何有效地训练CNN和提高训练速度的深刻见解。重点讨论和改进CNN培训的以下突破:</p><ul class=""><li id="053e" class="jz ka hi ix b iy iz jc jd jg kb jk kc jo kd js ke kf kg kh bi translated">神经结构搜索(NAS):使用随机搜索/强化学习来做出最佳模型设计选择并找到超参数。</li><li id="a065" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">缩放策略:关于如何有效地将小网络升级为大网络的指南，例如EfficientNet的复合缩放规则。</li><li id="151a" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">培训战略:例如，新的正规化方法、培训效率准则。</li><li id="4137" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">渐进学习:通过逐渐增加图像大小来加速训练。</li><li id="29ba" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">各种类型的卷积和积木:例如，深度方向conv，深度方向可分离conv，压缩和激发(se)，MB Conv，融合MB Conv。</li></ul><p id="aebb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">EfficientNetV2讨论了EfficientNet管道的低效之处，并用一种新策略改进了每个组件。在本帖中，我们将讨论EfficientNet的问题，并提供本文中提出的解决方案。</p></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><h2 id="2e75" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated">EfficientNet(又名EfficientNetV1)有什么问题？</h2><p id="f444" class="pw-post-body-paragraph iv iw hi ix b iy lp ja jb jc lq je jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">EfficientNet管道搜索使用NAS优化FLOPs (~speed)和准确性的网络EfficientNet-B0。应用复合缩放规则，通过同时增加深度(层数)、宽度(通道数)和图像大小来放大该网络，并找到高效网络B1-B7。本文讨论了EfficientNetV1的三个瓶颈。</p><p id="0a95" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">使用非常大的图像尺寸进行训练很慢:</strong> EfficientNet以指数方式缩放输入图像分辨率(例如，B7输入600×600的图像)。这导致了严重的内存瓶颈，并迫使批处理变小，从而损害了性能。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/38d1a944fb377c14148dce6d11bccb64.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*VF4nWy3JUsNgCbeIFGPdBA.png"/></div></figure><p id="a5a3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">深度方向卷积在早期层中较慢，但在后期阶段有效:</strong>具有深度方向卷积的MBConv模块和没有深度方向卷积的融合MBConv模块被认为是EfficientNetV2中的构建块。深度方向卷积的参数明显较少，但速度较慢，因为它没有利用现代加速器。</p><p id="7faa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于#通道相对较小，早期层的参数优势不是很大，因此常规MBConv模块和融合MBConv模块的适当混合可能是最佳选择(表3)。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/e3bf3dcac79474b71643f29579820828.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*KO4rLOQ-Y7Tc_tsufVH-KA.png"/></div></figure><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/8d7a068be70340b4473cc4818eea6291.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*fvFga3A8QuorAd5qomO_Ew.png"/></div></figure><p id="575e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">同等地扩大每个阶段是次优的:</strong>通过复合比例法则天真地扩大网络的所有阶段不是最好的主意。直观地说，每个阶段对训练速度和参数效率的贡献是不同的。</p><h1 id="8d75" class="lx kv hi bd kw ly lz ma la mb mc md le me mf mg lh mh mi mj lk mk ml mm ln mn bi translated">改进的NAS和扩展</h1><p id="a427" class="pw-post-body-paragraph iv iw hi ix b iy lp ja jb jc lq je jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">作为前面讨论的EfficientNetV1问题的解决方案，本文提出了一种改进的NAS算法和扩展策略。</p><h2 id="cc4f" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated">美国国家科学院（National Academy of Sciences）</h2><p id="7d7e" class="pw-post-body-paragraph iv iw hi ix b iy lp ja jb jc lq je jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">新定义的搜索空间是以下定义的简化版本:</p><ul class=""><li id="62ee" class="jz ka hi ix b iy iz jc jd jg kb jk kc jo kd js ke kf kg kh bi translated">卷积运算类型:{MBConv，Fused-MBConv}</li><li id="2281" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">层数</li><li id="b210" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">内核大小:{3×3，5×5}</li><li id="fca0" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">膨胀率(在MBConv内部):{1，4，6}</li></ul><p id="e92f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们所看到的，搜索空间非常窄，因为EfficientNetV1的大多数不必要的搜索选项都被删除了。此外，最佳通道尺寸采用EfficientNetV1中已搜索的参数。</p><p id="6fbb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">新的搜索奖励使用简单的加权乘积A×(S^w)×(P^v将模型准确度a、#训练步骤s和#参数p结合起来。具体来说，w=-0.07和v=-0.05是根据经验得到的。</p><h2 id="e424" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated">结果</h2><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/b6880f6d71dbd35f35d2c0c889a20f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*23aC3f4dL0PiXmu5ogF2iQ.png"/></div></figure><p id="4104" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上表描述了使用改进的NAS算法搜索的EffNetV2-S的基准体系结构。我们观察到</p><ul class=""><li id="d81f" class="jz ka hi ix b iy iz jc jd jg kb jk kc jo kd js ke kf kg kh bi translated">早期层发现Fused-MBConv更有效，而后者更喜欢原始的MBConv模块。</li><li id="f5eb" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">早期层更喜欢较小的膨胀比。</li><li id="7453" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">每一层都倾向于3×3内核，比5×5内核有更多的层，这在EfficientNetV1中略有利用。</li><li id="431e" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">最终阶段(阶段7)可能由于其大的参数大小而被完全移除。</li></ul><h2 id="f1ab" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated">扩展策略</h2><p id="c290" class="pw-post-body-paragraph iv iw hi ix b iy lp ja jb jc lq je jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">我们对EfficientNetV1的扩展策略进行了两处直接修改。</p><ol class=""><li id="e337" class="jz ka hi ix b iy iz jc jd jg kb jk kc jo kd js mp kf kg kh bi translated">将最大推断图像大小限制为480。</li><li id="656e" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js mp kf kg kh bi translated">在后续阶段添加更多层以增加网络容量，而不会增加太多运行时开销。</li></ol><p id="9f52" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这两个微小的修改甚至给EfficientNetV1的性能权衡带来了显著的变化。仅仅通过修改缩放策略，黑色的帕累托曲线被改进为灰色曲线。使用这种修改的缩放策略来定义较大的对应EffNetV2-M和EffNetV2-L。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/c0109898a4294de54a15eb33e2ca8aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*PiaVjgWvvc29liRG96Dp_w.png"/></div></figure><h2 id="d65b" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated">渐进式学习</h2><p id="b7b2" class="pw-post-body-paragraph iv iw hi ix b iy lp ja jb jc lq je jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">渐进式学习是一种通过逐步增加模型容量(也称为图像大小)来促进训练的策略。准确地说，图像大小在训练期间逐渐增加。然而，它们通常会导致最终精度下降。</p><p id="1df2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">EfficientNetV2通过增加正则化程度和图像大小来改进渐进式学习。直观上，使用小图像大小的训练=小网络容量因此需要弱正则化，而使用大图像大小的训练需要更强的正则化来对抗由于网络容量增加而导致的过拟合。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/50c31b00087007aea0463b70e26b0afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*FtTw02L7AYqtyB07vtj4Rg.png"/></div></figure><p id="757c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据该算法，我们根据I的值同时增加图像大小si和正则化幅度φI。正则化幅度φI控制丢失率、随机增加幅度和混合率。</p><h2 id="af36" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated">摘要</h2><p id="86b6" class="pw-post-body-paragraph iv iw hi ix b iy lp ja jb jc lq je jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">该白皮书指出了原始EfficientNet的体系结构和扩展策略中的低效之处。改进的NAS算法利用了EfficientNetV1的先验知识，并且能够自适应地搜索有效块和重要的超参数。搜索到的架构传达了一致且重要的模式，这些模式提供了关于高效CNN架构的见解，在设计网络架构时应加以利用。</p><p id="83fa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">复合缩放策略在参数和内存效率方面稍有修改。</p><p id="935a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与基线V1网络相比，改进的渐进式学习算法与改进的网络架构相结合，在相同的计算能力下达到类似的精度时，将EfficientNetV2的训练速度提高了11倍。</p></div></div>    
</body>
</html>