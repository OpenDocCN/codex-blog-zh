<html>
<head>
<title>African Language Speech Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">非洲语言语音识别</h1>
<blockquote>原文：<a href="https://medium.com/codex/african-language-speech-recognition-f6b7b16e5457?source=collection_archive---------14-----------------------#2022-06-10">https://medium.com/codex/african-language-speech-recognition-f6b7b16e5457?source=collection_archive---------14-----------------------#2022-06-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0d2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想到与你周围的人交流的最简单的方法，自然而然的就是<strong class="ih hj">说话</strong>！！随着通信方式的发展，世界每天都在发展，因为在这个过程中，<strong class="ih hj">机器</strong>是最好的工具..</p><p id="5896" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di">A</span>automatic<strong class="ih hj">S</strong>peech<strong class="ih hj">R</strong>ecognition "<strong class="ih hj">ASR</strong>是一种允许人类使用他们的声音与计算机接口说话的技术，其方式类似于正常的人类对话，目前开发的ASR技术的最先进版本被称为<strong class="ih hj">N</strong>natural<strong class="ih hj">L</strong>language<strong class="ih hj">P</strong>processing "<strong class="ih hj">NLP</strong>。尽管有许多成熟的语音识别系统可用，如谷歌助手，亚马逊Alexa。然而，所有这些语音助手都只支持有限的语言！</p><p id="9e12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，世界粮食计划署希望部署一种智能表格，收集非洲两个不同国家市场上买卖的食物的营养信息，这两个国家是<strong class="ih hj">埃塞尔比亚</strong>和<strong class="ih hj">肯亚</strong>，这需要被选中的人在他们的手机上安装一个应用程序，每当他们购买食物时，他们用他们的声音激活应用程序，用他们自己的语言登记他们刚刚购买的物品清单。应用程序中的智能系统有望实时转录语音到文本。</p><blockquote class="jm jn jo"><p id="51aa" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">在这个博客中，我们将学习如何构建一个深度学习模型，能够将阿姆哈拉语和斯瓦希里语的语音转换成文本。</p></blockquote><p id="789e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个项目，我们将使用音频和文本文件，但在我们开始查看数据之前，让我们首先刷新一些关于声音的基本概念，并看看我们如何对这些数据实施深度学习模型。</p><p id="ce9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di">S</span>d<strong class="ih hj">ound</strong>是通过空气的振动产生的。我们听到的所有声音都是我们经常在波形中表示的高低压组合，在这个图像中我们可以看到如何通过采样将波形转换为数组，因此我们可以实现深度学习！</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/1445664c51895fb2416e679b0ece9012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vO5UzCfhZaC3hZfocXe0Q.png"/></div></div></figure><h1 id="2b25" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">数据</h1><ul class=""><li id="49dd" class="ld le hi ih b ii lf im lg iq lh iu li iy lj jc lk ll lm ln bi translated">阿姆哈拉语:<a class="ae lo" href="http://Speech Recognition ASR format (~20hr training and ~2hrs test)" rel="noopener ugc nofollow" target="_blank">语音识别ASR格式(~20小时训练和~ 2小时测试)</a></li><li id="5f71" class="ld le hi ih b ii lp im lq iq lr iu ls iy lt jc lk ll lm ln bi translated">斯瓦希里语:<a class="ae lo" href="http://Speech Recognition ASR format (~10hr training and ~1.8hrs test)" rel="noopener ugc nofollow" target="_blank">语音识别ASR格式(~ 10小时训练和~ 1.8小时测试)</a></li></ul><h1 id="df97" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">使用Python进行数据预处理的步骤</h1><p id="1bd5" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated"><strong class="ih hj">加载音频</strong>为了在Python中对音频数据进行预处理，我们使用了名为<a class="ae lo" href="https://librosa.org/doc/latest/index.html" rel="noopener ugc nofollow" target="_blank"> librosa </a>的包，它允许我们将音频作为numpy数组加载到我们的笔记本中进行分析和操作，然后我们能够<strong class="ih hj">加载从斯瓦希里语和阿姆哈拉语数据集给出的转录</strong>，对于阿姆哈拉语，大多数转录都是单个字符，实际上是单词的一部分。对于斯瓦希里语文本，我们注意到停用词构成了数据的主要部分，这意味着我们需要进行更多的清理，例如，这是阿姆哈拉语文本清理后的样子:</p><pre class="ju jv jw jx fd lx ly lz ma aw mb bi"><span id="b005" class="mc kg hi ly b fi md me l mf mg">['የተለያዩ የትግራይ አውራጃ ተወላጆች ገንዘባቸውን አዋጥተው የልማት ተቋማትን እንዲመሰርቱ ትልማ አይፈቅድም',<br/> 'የጠመንጃ ተኩስ ተከፈተና አራት የኤርትራ ወታደሮች ተገደሉ',<br/> 'ላነሷቸው ጥያቄዎች የሰጡትን መልስ አቅርበነዋል',<br/> 'እብዱ አስፋልቱ ላይ የኰለኰለው ድንጋይ መኪና አላሳልፍ አለ',<br/> 'ጠጁን ኰመኰመ ኰመኰመና ሚስቱን ሲያሰቃያት አደረ',<br/> 'ድንቹ በደንብ ስለተኰተኰተ በጥሩ ሁኔታ ኰረተ',<br/> 'በድህነቱ ላይ ይህ ክፉ በሽታ ስለያዘው ሰውነቱ በጣም ኰሰሰ',<br/> 'በሩን እንዲህ በሀይል አታንኳኲ ብዬ አልነበረም እንዴ',<br/> 'በለጠች የበየነ የበኩር ልጅ ነች',<br/> 'የቆላ ቁስልና ቁርጥማት በጣም አሰቃቂ በሽታዎች ናቸው']</span></pre><p id="e3d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">采样语音数据在一个由一系列数字组成的数组中表示，每个数字表示特定时刻声音的强度或振幅。相似测量的数量由<strong class="ih hj">采样率</strong>决定，采样率定义了每秒从连续信号获取的样本数量，以形成离散信号。我们选择44，100Hz的标准采样速率。</p><p id="6d30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们必须<strong class="ih hj">转换频道</strong>，一些声音文件是单声道的，而大多数是立体声的。由于神经网络模型期望所有项目具有相同的维度，因此我们使用名为convert_channels的函数将单声道文件转换为立体声，其中我们将音频数据在数组中向左移动，形成原始数据的副本，并保存它。</p><p id="581a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据扩充</strong>有助于从现有数据中生成更多数据，我们使用numpy的滚动函数来生成时间偏移。这有助于我们模型推广到更广泛的输入。</p></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><h1 id="ebb1" class="kf kg hi bd kh ki mo kk kl km mp ko kp kq mq ks kt ku mr kw kx ky ms la lb lc bi translated"><strong class="ak">特征提取:</strong></h1><p id="58f0" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">直接输入神经网络结构的常用特征是频谱图和梅尔频率倒谱系数(MFCCs)。</p><p id="85fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">频谱图</strong>是音频信号随时间变化的频谱的直观图示。因此，它包括信号的时间和频率两个方面，并且通过对信号应用短时傅立叶变换(STFT)来获得。它通常被描绘成热图。在此图像中，垂直轴显示频率，水平轴显示剪辑的时间。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mt"><img src="../Images/95a8ae09984f7064bfeb5e204da1688d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFQd9JgzsgEa5NMni9lBYA.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">光谱图</figcaption></figure><p id="4081" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">信号的梅尔倒谱系数MFCC </strong>是一小组特征(通常约10-20)，它们简明地描述了频谱包络的整体形状。它模拟了人类声音的特征。</p></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><p id="d48e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">声学建模</strong>在ASR中用于表示<a class="ae lo" href="https://en.wikipedia.org/wiki/Audio_signal" rel="noopener ugc nofollow" target="_blank">音频信号</a>和<a class="ae lo" href="https://en.wikipedia.org/wiki/Phonemes" rel="noopener ugc nofollow" target="_blank">音素</a>或其他组成语音的语言单位之间的关系。该模型试图将音频信号映射到语音的基本单元，例如音素。在我们的例子中，我们使用存储音频特征功能来执行我们的声学建模，并且我们开发的并且将转化为我们的学习模型的主要基本结构是chroma_stft、spec_cent、spec_bw、rolloff、zcr，当然还有mfcc特征。例如这个斯瓦希里语</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es my"><img src="../Images/f447632a38886b9c0ba1e75d43abbb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QHxqn20hk4XglGdgVqDL2A.png"/></div></div></figure><h1 id="f37f" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">模型架构</h1><p id="1f84" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">现在音频输入数据和相应的标签都是数组格式，应用NLP技术就更容易了。我们可以使用标签编码将音频文件标签转换成整数，以便机器学习。标记的数据集将帮助我们在神经网络模型输出层预测结果。这些有助于将数据集训练和验证到nD阵列中。</p><p id="2a3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这一阶段，我们应用其他预处理技术，如删除列、规范化等。总结我们最后的训练数据来建立模型。下一步是将数据集分为训练、测试和验证，这也是我们对其他模型所做的。我们可以利用有线电视新闻网、RNN、LSTM、反恐中心等。深度神经算法来构建和训练语音应用(如语音识别)的模型。用转换成具有各自标签的n维阵列的标准大小几秒音频块训练的模型将导致预测测试音频输入的输出标签。由于输出标签会在二进制之外变化，我们正在讨论建立一个多类标签分类方法。</p><blockquote class="jm jn jo"><p id="227b" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><strong class="ih hj">深度学习</strong>可以定义为一种机器学习技术，通过提供输入和期望的输出，并让计算机找到解决方案，使用神经网络来解决问题。我们希望建立一个深度学习模型，将语音转换为文本。</p></blockquote><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mz"><img src="../Images/4fd03dee40c6ef674ea378848f438b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*-4hROFl4dJkTgstyhK5b0g.png"/></div></figure><p id="148c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习架构-CNN -RNN-LSTM &amp; CTC</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es na"><img src="../Images/808408eeb89ec3236a63f2649d952425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KO8Nj8Rb0tPQH-OyCxMjA.png"/></div></div></figure><p id="5ed8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">评估* </strong></p><p id="3267" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型空间探索* </strong></p><p id="884f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">记录&amp;预测* </strong></p><p id="df93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">指标—单词错误率WER* </strong></p><p id="c66c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论</strong></p><p id="6569" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个项目，我们比较了不同的神经网络模型。我们得到的最佳预测来自CNN</p><p id="c702" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> * </strong>未来的工作将是再次查看评估和损失函数，比较模型并找到最佳预测。</p><p id="fd00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><p id="5929" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lo" href="https://github.com/rafaesam/nlp_swahili_amharic.git" rel="noopener ugc nofollow" target="_blank">https://github.com/rafaesam/nlp_swahili_amharic.git</a></p><p id="dbf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lo" href="https://heartbeat.comet.ml/the-3-deep-learning-frameworks-for-end-to-end-speech-recognition-that-power-your-devices-37b891ddc380" rel="noopener ugc nofollow" target="_blank">https://heart beat . comet . ml/the-3-deep-learning-framework-for-end-to-end-speech-recognition-the-power-your-devices-37b 891 DDC 380</a></p><p id="e3ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lo" href="http://programming pytorch for deep learning reference “Programming PyTorch for Deep Learning by Ian Pointer (O’Reilly). Copyright 2019 Ian Pointer, 978-1-492-04535-9.”" rel="noopener ugc nofollow" target="_blank">《深度学习PyTorch编程》，作者伊恩·指针(O'Reilly)。版权所有2019伊恩指针，978–1–492–04535–9。”</a></p><div class="nb nc ez fb nd ne"><a href="https://usabilitygeek.com/automatic-speech-recognition-asr-software-an-introduction/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">自动语音识别(ASR)软件-简介-可用性极客</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">就技术发展而言，我们可能还需要至少几十年的时间才能……</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">usabilitygeek.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns kd ne"/></div></div></a></div><p id="232a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://www.youtube.com/watch?v=RX_ptt44mq4 T4】</p></div></div>    
</body>
</html>